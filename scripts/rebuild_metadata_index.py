#!/usr/bin/env python
"""
ä»ç°æœ‰ChromaDBæ•°æ®é‡å»ºJSONå…ƒæ•°æ®ç´¢å¼•
"""

import asyncio
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.chat.memory_system.memory_metadata_index import MemoryMetadataIndexEntry
from src.chat.memory_system.memory_system import MemorySystem
from src.common.logger import get_logger

logger = get_logger(__name__)


async def rebuild_metadata_index():
    """ä»ChromaDBé‡å»ºå…ƒæ•°æ®ç´¢å¼•"""
    print("=" * 80)
    print("é‡å»ºJSONå…ƒæ•°æ®ç´¢å¼•")
    print("=" * 80)

    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    print("\nğŸ”§ åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ...")
    ms = MemorySystem()
    await ms.initialize()
    print("âœ… è®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–")

    if not hasattr(ms.unified_storage, "metadata_index"):
        print("âŒ å…ƒæ•°æ®ç´¢å¼•ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return

    # è·å–æ‰€æœ‰è®°å¿†
    print("\nğŸ“¥ ä»ChromaDBè·å–æ‰€æœ‰è®°å¿†...")
    from src.common.vector_db import vector_db_service

    try:
        # è·å–é›†åˆä¸­çš„æ‰€æœ‰è®°å¿†ID
        collection_name = ms.unified_storage.config.memory_collection
        result = vector_db_service.get(
            collection_name=collection_name, include=["documents", "metadatas", "embeddings"]
        )

        if not result or not result.get("ids"):
            print("âŒ ChromaDBä¸­æ²¡æœ‰æ‰¾åˆ°è®°å¿†æ•°æ®")
            return

        ids = result["ids"]
        metadatas = result.get("metadatas", [])

        print(f"âœ… æ‰¾åˆ° {len(ids)} æ¡è®°å¿†")

        # é‡å»ºå…ƒæ•°æ®ç´¢å¼•
        print("\nğŸ”¨ å¼€å§‹é‡å»ºå…ƒæ•°æ®ç´¢å¼•...")
        entries = []
        success_count = 0

        for i, (memory_id, metadata) in enumerate(zip(ids, metadatas, strict=False), 1):
            try:
                # ä»ChromaDBå…ƒæ•°æ®é‡å»ºç´¢å¼•æ¡ç›®
                import orjson

                entry = MemoryMetadataIndexEntry(
                    memory_id=memory_id,
                    user_id=metadata.get("user_id", "unknown"),
                    memory_type=metadata.get("memory_type", "general"),
                    subjects=orjson.loads(metadata.get("subjects", "[]")),
                    objects=[metadata.get("object")] if metadata.get("object") else [],
                    keywords=orjson.loads(metadata.get("keywords", "[]")),
                    tags=orjson.loads(metadata.get("tags", "[]")),
                    importance=2,  # é»˜è®¤NORMAL
                    confidence=2,  # é»˜è®¤MEDIUM
                    created_at=metadata.get("created_at", 0.0),
                    access_count=metadata.get("access_count", 0),
                    chat_id=metadata.get("chat_id"),
                    content_preview=None,
                )

                # å°è¯•è§£æimportanceå’Œconfidenceçš„æšä¸¾åç§°
                if "importance" in metadata:
                    imp_str = metadata["importance"]
                    if imp_str == "LOW":
                        entry.importance = 1
                    elif imp_str == "NORMAL":
                        entry.importance = 2
                    elif imp_str == "HIGH":
                        entry.importance = 3
                    elif imp_str == "CRITICAL":
                        entry.importance = 4

                if "confidence" in metadata:
                    conf_str = metadata["confidence"]
                    if conf_str == "LOW":
                        entry.confidence = 1
                    elif conf_str == "MEDIUM":
                        entry.confidence = 2
                    elif conf_str == "HIGH":
                        entry.confidence = 3
                    elif conf_str == "VERIFIED":
                        entry.confidence = 4

                entries.append(entry)
                success_count += 1

                if i % 100 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {i}/{len(ids)} ({success_count} æˆåŠŸ)")

            except Exception as e:
                logger.warning(f"å¤„ç†è®°å¿† {memory_id} å¤±è´¥: {e}")
                continue

        print(f"\nâœ… æˆåŠŸè§£æ {success_count}/{len(ids)} æ¡è®°å¿†å…ƒæ•°æ®")

        # æ‰¹é‡æ›´æ–°ç´¢å¼•
        print("\nğŸ’¾ ä¿å­˜å…ƒæ•°æ®ç´¢å¼•...")
        ms.unified_storage.metadata_index.batch_add_or_update(entries)
        ms.unified_storage.metadata_index.save()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = ms.unified_storage.metadata_index.get_stats()
        print("\nğŸ“Š é‡å»ºåçš„ç´¢å¼•ç»Ÿè®¡:")
        print(f"  - æ€»è®°å¿†æ•°: {stats['total_memories']}")
        print(f"  - ä¸»è¯­æ•°é‡: {stats['subjects_count']}")
        print(f"  - å…³é”®è¯æ•°é‡: {stats['keywords_count']}")
        print(f"  - æ ‡ç­¾æ•°é‡: {stats['tags_count']}")
        print("  - ç±»å‹åˆ†å¸ƒ:")
        for mtype, count in stats["types"].items():
            print(f"    - {mtype}: {count}")

        print("\nâœ… å…ƒæ•°æ®ç´¢å¼•é‡å»ºå®Œæˆï¼")

    except Exception as e:
        logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
        print(f"âŒ é‡å»ºç´¢å¼•å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(rebuild_metadata_index())
