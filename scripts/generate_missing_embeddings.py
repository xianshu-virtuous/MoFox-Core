"""
ä¸ºç°æœ‰èŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡

æ‰¹é‡ä¸ºå›¾å­˜å‚¨ä¸­ç¼ºå°‘åµŒå…¥å‘é‡çš„èŠ‚ç‚¹ç”Ÿæˆå¹¶ç´¢å¼•åµŒå…¥å‘é‡

ä½¿ç”¨åœºæ™¯:
1. å†å²è®°å¿†èŠ‚ç‚¹æ²¡æœ‰åµŒå…¥å‘é‡
2. åµŒå…¥ç”Ÿæˆå™¨ä¹‹å‰æœªé…ç½®ï¼Œç°åœ¨éœ€è¦è¡¥å……ç”Ÿæˆ
3. å‘é‡ç´¢å¼•æŸåéœ€è¦é‡å»º

ä½¿ç”¨æ–¹æ³•:
    python scripts/generate_missing_embeddings.py [--node-types TOPIC,OBJECT] [--batch-size 50]

å‚æ•°è¯´æ˜:
    --node-types: éœ€è¦ç”ŸæˆåµŒå…¥çš„èŠ‚ç‚¹ç±»å‹ï¼Œé»˜è®¤ä¸º TOPIC,OBJECT
    --batch-size: æ‰¹é‡å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º 50
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


async def generate_missing_embeddings(
    target_node_types: list[str] | None = None,
    batch_size: int = 50,
):
    """
    ä¸ºç¼ºå¤±åµŒå…¥å‘é‡çš„èŠ‚ç‚¹ç”ŸæˆåµŒå…¥

    Args:
        target_node_types: éœ€è¦å¤„ç†çš„èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆå¦‚ ["ä¸»é¢˜", "å®¢ä½“"]ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    from src.common.logger import get_logger
    from src.memory_graph.manager_singleton import get_memory_manager, initialize_memory_manager
    from src.memory_graph.models import NodeType

    logger = get_logger("generate_missing_embeddings")

    if target_node_types is None:
        target_node_types = [NodeType.TOPIC.value, NodeType.OBJECT.value]

    print(f"\n{'='*80}")
    print("ğŸ”§ ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡")
    print(f"{'='*80}\n")
    print(f"ç›®æ ‡èŠ‚ç‚¹ç±»å‹: {', '.join(target_node_types)}")
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}\n")

    # 1. åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨...")
    await initialize_memory_manager()
    manager = get_memory_manager()

    if manager is None:
        print("âŒ è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
        return

    print("âœ… è®°å¿†ç®¡ç†å™¨å·²åˆå§‹åŒ–\n")

    # 2. è·å–å·²ç´¢å¼•çš„èŠ‚ç‚¹ID
    print("ğŸ” æ£€æŸ¥ç°æœ‰å‘é‡ç´¢å¼•...")
    existing_node_ids = set()
    try:
        vector_count = manager.vector_store.collection.count()
        if vector_count > 0:
            # åˆ†æ‰¹è·å–æ‰€æœ‰å·²ç´¢å¼•çš„ID
            batch_size_check = 1000
            for offset in range(0, vector_count, batch_size_check):
                limit = min(batch_size_check, vector_count - offset)
                result = manager.vector_store.collection.get(
                    limit=limit,
                    offset=offset,
                )
                if result and "ids" in result:
                    existing_node_ids.update(result["ids"])

        print(f"âœ… å‘ç° {len(existing_node_ids)} ä¸ªå·²ç´¢å¼•èŠ‚ç‚¹\n")
    except Exception as e:
        logger.warning(f"è·å–å·²ç´¢å¼•èŠ‚ç‚¹IDå¤±è´¥: {e}")
        print("âš ï¸  æ— æ³•è·å–å·²ç´¢å¼•èŠ‚ç‚¹ï¼Œå°†å°è¯•è·³è¿‡é‡å¤é¡¹\n")

    # 3. æ”¶é›†éœ€è¦ç”ŸæˆåµŒå…¥çš„èŠ‚ç‚¹
    print("ğŸ” æ‰«æéœ€è¦ç”ŸæˆåµŒå…¥çš„èŠ‚ç‚¹...")
    all_memories = manager.graph_store.get_all_memories()

    nodes_to_process = []
    total_target_nodes = 0
    type_stats = {nt: {"total": 0, "need_emb": 0, "already_indexed": 0} for nt in target_node_types}

    for memory in all_memories:
        for node in memory.nodes:
            if node.node_type.value in target_node_types:
                total_target_nodes += 1
                type_stats[node.node_type.value]["total"] += 1

                # æ£€æŸ¥æ˜¯å¦å·²åœ¨å‘é‡ç´¢å¼•ä¸­
                if node.id in existing_node_ids:
                    type_stats[node.node_type.value]["already_indexed"] += 1
                    continue

                if not node.has_embedding():
                    nodes_to_process.append({
                        "node": node,
                        "memory_id": memory.id,
                    })
                    type_stats[node.node_type.value]["need_emb"] += 1

    print("\nğŸ“Š æ‰«æç»“æœ:")
    for node_type in target_node_types:
        stats = type_stats[node_type]
        already_ok = stats["already_indexed"]
        coverage = (stats["total"] - stats["need_emb"]) / stats["total"] * 100 if stats["total"] > 0 else 0
        print(f"  - {node_type}: {stats['total']} ä¸ªèŠ‚ç‚¹, {stats['need_emb']} ä¸ªç¼ºå¤±åµŒå…¥, "
              f"{already_ok} ä¸ªå·²ç´¢å¼• (è¦†ç›–ç‡: {coverage:.1f}%)")

    print(f"\n  æ€»è®¡: {total_target_nodes} ä¸ªç›®æ ‡èŠ‚ç‚¹, {len(nodes_to_process)} ä¸ªéœ€è¦ç”ŸæˆåµŒå…¥\n")

    if len(nodes_to_process) == 0:
        print("âœ… æ‰€æœ‰èŠ‚ç‚¹å·²æœ‰åµŒå…¥å‘é‡ï¼Œæ— éœ€ç”Ÿæˆ")
        return

    # 3. æ‰¹é‡ç”ŸæˆåµŒå…¥
    print("ğŸš€ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...\n")

    total_batches = (len(nodes_to_process) + batch_size - 1) // batch_size
    success_count = 0
    failed_count = 0
    indexed_count = 0

    for i in range(0, len(nodes_to_process), batch_size):
        batch = nodes_to_process[i : i + batch_size]
        batch_num = i // batch_size + 1

        print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªèŠ‚ç‚¹)...")

        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = [item["node"].content for item in batch]

            # æ‰¹é‡ç”ŸæˆåµŒå…¥
            embeddings = await manager.embedding_generator.generate_batch(texts)

            # ä¸ºèŠ‚ç‚¹è®¾ç½®åµŒå…¥å¹¶ç´¢å¼•
            batch_nodes_for_index = []

            for j, (item, embedding) in enumerate(zip(batch, embeddings)):
                node = item["node"]

                if embedding is not None:
                    # è®¾ç½®åµŒå…¥å‘é‡
                    node.embedding = embedding
                    batch_nodes_for_index.append(node)
                    success_count += 1
                else:
                    failed_count += 1
                    logger.warning(f"  âš ï¸  èŠ‚ç‚¹ {node.id[:8]}... '{node.content[:30]}' åµŒå…¥ç”Ÿæˆå¤±è´¥")

            # æ‰¹é‡ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
            if batch_nodes_for_index:
                try:
                    await manager.vector_store.add_nodes_batch(batch_nodes_for_index)
                    indexed_count += len(batch_nodes_for_index)
                    print(f"  âœ… æˆåŠŸ: {len(batch_nodes_for_index)}/{len(batch)} ä¸ªèŠ‚ç‚¹å·²ç”Ÿæˆå¹¶ç´¢å¼•")
                except Exception as e:
                    # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ·»åŠ ï¼ˆè·³è¿‡é‡å¤ï¼‰
                    logger.warning(f"  æ‰¹é‡ç´¢å¼•å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ·»åŠ : {e}")
                    individual_success = 0
                    for node in batch_nodes_for_index:
                        try:
                            await manager.vector_store.add_node(node)
                            individual_success += 1
                            indexed_count += 1
                        except Exception as e2:
                            if "Expected IDs to be unique" in str(e2):
                                logger.debug(f"    è·³è¿‡å·²å­˜åœ¨èŠ‚ç‚¹: {node.id}")
                            else:
                                logger.error(f"    èŠ‚ç‚¹ {node.id} ç´¢å¼•å¤±è´¥: {e2}")
                    print(f"  âš ï¸  é€ä¸ªç´¢å¼•: {individual_success}/{len(batch_nodes_for_index)} ä¸ªæˆåŠŸ")

        except Exception as e:
            failed_count += len(batch)
            logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥")
            print(f"  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")

        # æ˜¾ç¤ºè¿›åº¦
        total_processed = min(i + batch_size, len(nodes_to_process))
        progress = total_processed / len(nodes_to_process) * 100
        print(f"  ğŸ“Š æ€»è¿›åº¦: {total_processed}/{len(nodes_to_process)} ({progress:.1f}%)\n")

    # 4. ä¿å­˜å›¾æ•°æ®ï¼ˆæ›´æ–°èŠ‚ç‚¹çš„ embedding å­—æ®µï¼‰
    print("ğŸ’¾ ä¿å­˜å›¾æ•°æ®...")
    try:
        await manager.persistence.save_graph_store(manager.graph_store)
        print("âœ… å›¾æ•°æ®å·²ä¿å­˜\n")
    except Exception as e:
        logger.error("ä¿å­˜å›¾æ•°æ®å¤±è´¥")
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}\n")

    # 5. éªŒè¯ç»“æœ
    print("ğŸ” éªŒè¯å‘é‡ç´¢å¼•...")
    final_vector_count = manager.vector_store.collection.count()
    stats = manager.graph_store.get_statistics()
    total_nodes = stats["total_nodes"]

    print(f"\n{'='*80}")
    print("ğŸ“Š ç”Ÿæˆå®Œæˆ")
    print(f"{'='*80}")
    print(f"å¤„ç†èŠ‚ç‚¹æ•°: {len(nodes_to_process)}")
    print(f"æˆåŠŸç”Ÿæˆ: {success_count}")
    print(f"å¤±è´¥æ•°é‡: {failed_count}")
    print(f"æˆåŠŸç´¢å¼•: {indexed_count}")
    print(f"å‘é‡ç´¢å¼•èŠ‚ç‚¹æ•°: {final_vector_count}")
    print(f"å›¾å­˜å‚¨èŠ‚ç‚¹æ•°: {total_nodes}")
    print(f"ç´¢å¼•è¦†ç›–ç‡: {final_vector_count / total_nodes * 100:.1f}%\n")

    # 6. æµ‹è¯•æœç´¢
    print("ğŸ§ª æµ‹è¯•æœç´¢åŠŸèƒ½...")
    test_queries = ["å°çº¢å¸½è•¾å…‹", "æ‹¾é£", "æ°ç‘å–µ"]

    for query in test_queries:
        results = await manager.search_memories(query=query, top_k=3)
        if results:
            print(f"\nâœ… æŸ¥è¯¢ '{query}' æ‰¾åˆ° {len(results)} æ¡è®°å¿†:")
            for i, memory in enumerate(results[:2], 1):
                subject_node = memory.get_subject_node()
                # è·å–ä¸»é¢˜èŠ‚ç‚¹ï¼ˆéå†æ‰€æœ‰èŠ‚ç‚¹æ‰¾TOPICç±»å‹ï¼‰
                from src.memory_graph.models import NodeType
                topic_nodes = [n for n in memory.nodes if n.node_type == NodeType.TOPIC]
                subject = subject_node.content if subject_node else "?"
                topic = topic_nodes[0].content if topic_nodes else "?"
                print(f"  {i}. {subject} - {topic} (é‡è¦æ€§: {memory.importance:.2f})")
        else:
            print(f"\nâš ï¸  æŸ¥è¯¢ '{query}' è¿”å› 0 æ¡ç»“æœ")


async def main():
    import argparse

    parser = argparse.ArgumentParser(description="ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥å‘é‡")
    parser.add_argument(
        "--node-types",
        type=str,
        default="ä¸»é¢˜,å®¢ä½“",
        help="éœ€è¦ç”ŸæˆåµŒå…¥çš„èŠ‚ç‚¹ç±»å‹ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤ï¼šä¸»é¢˜,å®¢ä½“ï¼‰",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ï¼š50ï¼‰",
    )

    args = parser.parse_args()

    target_types = [t.strip() for t in args.node_types.split(",")]
    await generate_missing_embeddings(
        target_node_types=target_types,
        batch_size=args.batch_size,
    )


if __name__ == "__main__":
    asyncio.run(main())
