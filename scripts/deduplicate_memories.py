"""
è®°å¿†å»é‡å·¥å…·

åŠŸèƒ½ï¼š
1. æ‰«ææ‰€æœ‰æ ‡è®°ä¸º"ç›¸ä¼¼"å…³ç³»çš„è®°å¿†è¾¹
2. å¯¹ç›¸ä¼¼è®°å¿†è¿›è¡Œå»é‡ï¼ˆä¿ç•™é‡è¦æ€§é«˜çš„ï¼Œåˆ é™¤å¦ä¸€ä¸ªï¼‰
3. æ”¯æŒå¹²è¿è¡Œæ¨¡å¼ï¼ˆé¢„è§ˆä¸æ‰§è¡Œï¼‰
4. æä¾›è¯¦ç»†çš„å»é‡æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…åˆ é™¤ï¼‰
    python scripts/deduplicate_memories.py --dry-run

    # æ‰§è¡Œå»é‡
    python scripts/deduplicate_memories.py

    # æŒ‡å®šç›¸ä¼¼åº¦é˜ˆå€¼
    python scripts/deduplicate_memories.py --threshold 0.9

    # æŒ‡å®šæ•°æ®ç›®å½•
    python scripts/deduplicate_memories.py --data-dir data/memory_graph
"""
import argparse
import asyncio
import sys
from datetime import datetime
from pathlib import Path

import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.common.logger import get_logger
from src.memory_graph.manager_singleton import initialize_memory_manager, shutdown_memory_manager

logger = get_logger(__name__)


class MemoryDeduplicator:
    """è®°å¿†å»é‡å™¨"""

    def __init__(self, data_dir: str = "data/memory_graph", dry_run: bool = False, threshold: float = 0.85):
        self.data_dir = data_dir
        self.dry_run = dry_run
        self.threshold = threshold
        self.manager = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_memories": 0,
            "similar_pairs": 0,
            "duplicates_found": 0,
            "duplicates_removed": 0,
            "errors": 0,
        }

    async def initialize(self):
        """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨"""
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨ (data_dir={self.data_dir})...")
        self.manager = await initialize_memory_manager(data_dir=self.data_dir)
        if not self.manager:
            raise RuntimeError("è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")

        self.stats["total_memories"] = len(self.manager.graph_store.get_all_memories())
        logger.info(f"âœ… è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œå…± {self.stats['total_memories']} æ¡è®°å¿†")

    async def find_similar_pairs(self) -> list[tuple[str, str, float]]:
        """
        æŸ¥æ‰¾æ‰€æœ‰ç›¸ä¼¼çš„è®°å¿†å¯¹ï¼ˆé€šè¿‡å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼‰

        Returns:
            [(memory_id_1, memory_id_2, similarity), ...]
        """
        logger.info("æ­£åœ¨æ‰«æç›¸ä¼¼è®°å¿†å¯¹...")
        similar_pairs = []
        seen_pairs = set()  # é¿å…é‡å¤

        # è·å–æ‰€æœ‰è®°å¿†
        all_memories = self.manager.graph_store.get_all_memories()
        total_memories = len(all_memories)

        logger.info(f"å¼€å§‹è®¡ç®— {total_memories} æ¡è®°å¿†çš„ç›¸ä¼¼åº¦...")

        # ä¸¤ä¸¤æ¯”è¾ƒè®°å¿†çš„ç›¸ä¼¼åº¦
        for i, memory_i in enumerate(all_memories):
            # æ¯å¤„ç†10æ¡è®°å¿†è®©å‡ºæ§åˆ¶æƒ
            if i % 10 == 0:
                await asyncio.sleep(0)
                if i > 0:
                    logger.info(f"è¿›åº¦: {i}/{total_memories} ({i*100//total_memories}%)")

            # è·å–è®°å¿†içš„å‘é‡ï¼ˆä»ä¸»é¢˜èŠ‚ç‚¹ï¼‰
            vector_i = None
            for node in memory_i.nodes:
                if node.embedding is not None:
                    vector_i = node.embedding
                    break

            if vector_i is None:
                continue

            # ä¸åç»­è®°å¿†æ¯”è¾ƒ
            for j in range(i + 1, total_memories):
                memory_j = all_memories[j]

                # è·å–è®°å¿†jçš„å‘é‡
                vector_j = None
                for node in memory_j.nodes:
                    if node.embedding is not None:
                        vector_j = node.embedding
                        break

                if vector_j is None:
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(vector_i, vector_j)

                # åªä¿å­˜æ»¡è¶³é˜ˆå€¼çš„ç›¸ä¼¼å¯¹
                if similarity >= self.threshold:
                    pair_key = tuple(sorted([memory_i.id, memory_j.id]))
                    if pair_key not in seen_pairs:
                        seen_pairs.add(pair_key)
                        similar_pairs.append((memory_i.id, memory_j.id, similarity))

        self.stats["similar_pairs"] = len(similar_pairs)
        logger.info(f"æ‰¾åˆ° {len(similar_pairs)} å¯¹ç›¸ä¼¼è®°å¿†ï¼ˆé˜ˆå€¼>={self.threshold}ï¼‰")

        return similar_pairs

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            vec1_norm = np.linalg.norm(vec1)
            vec2_norm = np.linalg.norm(vec2)

            if vec1_norm == 0 or vec2_norm == 0:
                return 0.0

            similarity = np.dot(vec1, vec2) / (vec1_norm * vec2_norm)
            return float(similarity)
        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    def decide_which_to_keep(self, mem_id_1: str, mem_id_2: str) -> tuple[str | None, str | None]:
        """
        å†³å®šä¿ç•™å“ªä¸ªè®°å¿†ï¼Œåˆ é™¤å“ªä¸ª

        ä¼˜å…ˆçº§ï¼š
        1. é‡è¦æ€§æ›´é«˜çš„
        2. æ¿€æ´»åº¦æ›´é«˜çš„
        3. åˆ›å»ºæ—¶é—´æ›´æ—©çš„

        Returns:
            (keep_id, remove_id)
        """
        mem1 = self.manager.graph_store.get_memory_by_id(mem_id_1)
        mem2 = self.manager.graph_store.get_memory_by_id(mem_id_2)

        if not mem1 or not mem2:
            logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {mem_id_1} or {mem_id_2}")
            return None, None

        # æ¯”è¾ƒé‡è¦æ€§
        if mem1.importance > mem2.importance:
            return mem_id_1, mem_id_2
        elif mem1.importance < mem2.importance:
            return mem_id_2, mem_id_1

        # é‡è¦æ€§ç›¸åŒï¼Œæ¯”è¾ƒæ¿€æ´»åº¦
        if mem1.activation > mem2.activation:
            return mem_id_1, mem_id_2
        elif mem1.activation < mem2.activation:
            return mem_id_2, mem_id_1

        # æ¿€æ´»åº¦ä¹Ÿç›¸åŒï¼Œä¿ç•™æ›´æ—©åˆ›å»ºçš„
        if mem1.created_at < mem2.created_at:
            return mem_id_1, mem_id_2
        else:
            return mem_id_2, mem_id_1

    async def deduplicate_pair(self, mem_id_1: str, mem_id_2: str, similarity: float) -> bool:
        """
        å»é‡ä¸€å¯¹ç›¸ä¼¼è®°å¿†

        Returns:
            æ˜¯å¦æˆåŠŸå»é‡
        """
        keep_id, remove_id = self.decide_which_to_keep(mem_id_1, mem_id_2)

        if not keep_id or not remove_id:
            self.stats["errors"] += 1
            return False

        keep_mem = self.manager.graph_store.get_memory_by_id(keep_id)
        remove_mem = self.manager.graph_store.get_memory_by_id(remove_id)

        logger.info("")
        logger.info(f"{'[é¢„è§ˆ]' if self.dry_run else '[æ‰§è¡Œ]'} å»é‡ç›¸ä¼¼è®°å¿†å¯¹ (ç›¸ä¼¼åº¦={similarity:.3f}):")
        logger.info(f"  ä¿ç•™: {keep_id}")
        logger.info(f"    - ä¸»é¢˜: {keep_mem.metadata.get('topic', 'N/A')}")
        logger.info(f"    - é‡è¦æ€§: {keep_mem.importance:.2f}")
        logger.info(f"    - æ¿€æ´»åº¦: {keep_mem.activation:.2f}")
        logger.info(f"    - åˆ›å»ºæ—¶é—´: {keep_mem.created_at}")
        logger.info(f"  åˆ é™¤: {remove_id}")
        logger.info(f"    - ä¸»é¢˜: {remove_mem.metadata.get('topic', 'N/A')}")
        logger.info(f"    - é‡è¦æ€§: {remove_mem.importance:.2f}")
        logger.info(f"    - æ¿€æ´»åº¦: {remove_mem.activation:.2f}")
        logger.info(f"    - åˆ›å»ºæ—¶é—´: {remove_mem.created_at}")

        if self.dry_run:
            logger.info("  [é¢„è§ˆæ¨¡å¼] ä¸æ‰§è¡Œå®é™…åˆ é™¤")
            self.stats["duplicates_found"] += 1
            return True

        try:
            # å¢å¼ºä¿ç•™è®°å¿†çš„å±æ€§
            keep_mem.importance = min(1.0, keep_mem.importance + 0.05)
            keep_mem.activation = min(1.0, keep_mem.activation + 0.05)

            # ç´¯åŠ è®¿é—®æ¬¡æ•°
            if hasattr(keep_mem, "access_count") and hasattr(remove_mem, "access_count"):
                keep_mem.access_count += remove_mem.access_count

            # åˆ é™¤ç›¸ä¼¼è®°å¿†
            await self.manager.delete_memory(remove_id)

            self.stats["duplicates_removed"] += 1
            logger.info("  âœ… åˆ é™¤æˆåŠŸ")

            # è®©å‡ºæ§åˆ¶æƒ
            await asyncio.sleep(0)

            return True

        except Exception as e:
            logger.error(f"  âŒ åˆ é™¤å¤±è´¥: {e}")
            self.stats["errors"] += 1
            return False

    async def run(self):
        """æ‰§è¡Œå»é‡"""
        start_time = datetime.now()

        print("="*70)
        print("è®°å¿†å»é‡å·¥å…·")
        print("="*70)
        print(f"æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {self.threshold}")
        print(f"æ¨¡å¼: {'é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…åˆ é™¤ï¼‰' if self.dry_run else 'æ‰§è¡Œæ¨¡å¼ï¼ˆä¼šå®é™…åˆ é™¤ï¼‰'}")
        print("="*70)
        print()

        # åˆå§‹åŒ–
        await self.initialize()

        # æŸ¥æ‰¾ç›¸ä¼¼å¯¹
        similar_pairs = await self.find_similar_pairs()

        if not similar_pairs:
            logger.info("æœªæ‰¾åˆ°éœ€è¦å»é‡çš„ç›¸ä¼¼è®°å¿†å¯¹")
            print()
            print("="*70)
            print("æœªæ‰¾åˆ°éœ€è¦å»é‡çš„è®°å¿†")
            print("="*70)
            return

        # å»é‡å¤„ç†
        logger.info(f"å¼€å§‹{'é¢„è§ˆ' if self.dry_run else 'æ‰§è¡Œ'}å»é‡...")
        print()

        processed_pairs = set()  # é¿å…é‡å¤å¤„ç†

        for mem_id_1, mem_id_2, similarity in similar_pairs:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå¯èƒ½ä¸€ä¸ªè®°å¿†å·²è¢«åˆ é™¤ï¼‰
            pair_key = tuple(sorted([mem_id_1, mem_id_2]))
            if pair_key in processed_pairs:
                continue

            # æ£€æŸ¥è®°å¿†æ˜¯å¦ä»å­˜åœ¨
            if not self.manager.graph_store.get_memory_by_id(mem_id_1):
                logger.debug(f"è®°å¿† {mem_id_1} å·²ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            if not self.manager.graph_store.get_memory_by_id(mem_id_2):
                logger.debug(f"è®°å¿† {mem_id_2} å·²ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue

            # æ‰§è¡Œå»é‡
            success = await self.deduplicate_pair(mem_id_1, mem_id_2, similarity)

            if success:
                processed_pairs.add(pair_key)

        # ä¿å­˜æ•°æ®ï¼ˆå¦‚æœä¸æ˜¯å¹²è¿è¡Œï¼‰
        if not self.dry_run:
            logger.info("æ­£åœ¨ä¿å­˜æ•°æ®...")
            await self.manager.persistence.save_graph_store(self.manager.graph_store)
            logger.info("âœ… æ•°æ®å·²ä¿å­˜")

        # ç»Ÿè®¡æŠ¥å‘Š
        elapsed = (datetime.now() - start_time).total_seconds()

        print()
        print("="*70)
        print("å»é‡æŠ¥å‘Š")
        print("="*70)
        print(f"æ€»è®°å¿†æ•°: {self.stats['total_memories']}")
        print(f"ç›¸ä¼¼è®°å¿†å¯¹: {self.stats['similar_pairs']}")
        print(f"å‘ç°é‡å¤: {self.stats['duplicates_found'] if self.dry_run else self.stats['duplicates_removed']}")
        print(f"{'é¢„è§ˆé€šè¿‡' if self.dry_run else 'æˆåŠŸåˆ é™¤'}: {self.stats['duplicates_found'] if self.dry_run else self.stats['duplicates_removed']}")
        print(f"é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"è€—æ—¶: {elapsed:.2f}ç§’")

        if self.dry_run:
            print()
            print("âš ï¸ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…åˆ é™¤ä»»ä½•è®°å¿†")
            print("ğŸ’¡ è¦æ‰§è¡Œå®é™…åˆ é™¤ï¼Œè¯·è¿è¡Œ: python scripts/deduplicate_memories.py")
        else:
            print()
            print("âœ… å»é‡å®Œæˆï¼")
            final_count = len(self.manager.graph_store.get_all_memories())
            print(f"ğŸ“Š æœ€ç»ˆè®°å¿†æ•°: {final_count} (å‡å°‘ {self.stats['total_memories'] - final_count} æ¡)")

        print("="*70)

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.manager:
            await shutdown_memory_manager()


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="è®°å¿†å»é‡å·¥å…· - å¯¹æ ‡è®°ä¸ºç›¸ä¼¼çš„è®°å¿†è¿›è¡Œä¸€é”®å»é‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # é¢„è§ˆæ¨¡å¼ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰
  python scripts/deduplicate_memories.py --dry-run

  # æ‰§è¡Œå»é‡
  python scripts/deduplicate_memories.py

  # æŒ‡å®šç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆåªå¤„ç†ç›¸ä¼¼åº¦>=0.9çš„è®°å¿†å¯¹ï¼‰
  python scripts/deduplicate_memories.py --threshold 0.9

  # æŒ‡å®šæ•°æ®ç›®å½•
  python scripts/deduplicate_memories.py --data-dir data/memory_graph

  # ç»„åˆä½¿ç”¨
  python scripts/deduplicate_memories.py --dry-run --threshold 0.95 --data-dir data/test
        """
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…åˆ é™¤è®°å¿†ï¼ˆæ¨èå…ˆè¿è¡Œæ­¤æ¨¡å¼ï¼‰"
    )

    parser.add_argument(
        "--threshold",
        type=float,
        default=0.85,
        help="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªå¤„ç†ç›¸ä¼¼åº¦>=æ­¤å€¼çš„è®°å¿†å¯¹ï¼ˆé»˜è®¤: 0.85ï¼‰"
    )

    parser.add_argument(
        "--data-dir",
        type=str,
        default="data/memory_graph",
        help="è®°å¿†æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: data/memory_graphï¼‰"
    )

    args = parser.parse_args()

    # åˆ›å»ºå»é‡å™¨
    deduplicator = MemoryDeduplicator(
        data_dir=args.data_dir,
        dry_run=args.dry_run,
        threshold=args.threshold
    )

    try:
        # æ‰§è¡Œå»é‡
        await deduplicator.run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†èµ„æº
        await deduplicator.cleanup()

    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
