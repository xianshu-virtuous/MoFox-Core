"""
è®°å¿†æ¸…ç†è„šæœ¬

åŠŸèƒ½ï¼š
1. éå†æ‰€æœ‰é•¿æœŸè®°å¿†
2. ä½¿ç”¨ LLM è¯„ä¼°æ¯æ¡è®°å¿†çš„ä»·å€¼
3. åˆ é™¤æ— æ•ˆ/ä½ä»·å€¼è®°å¿†
4. åˆå¹¶/ç²¾ç®€ç›¸ä¼¼è®°å¿†

ä½¿ç”¨æ–¹å¼ï¼š
cd Bot
python scripts/memory_cleaner.py [--dry-run] [--batch-size 10]
"""

import argparse
import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config.config import model_config
from src.llm_models.utils_model import LLMRequest
from src.config.config import global_config


# ==================== é…ç½® ====================

# LLM è¯„ä¼°æç¤ºè¯
EVALUATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªéå¸¸ä¸¥æ ¼çš„è®°å¿†ä»·å€¼è¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¤§å¹…æ¸…ç†ä½è´¨é‡è®°å¿†ï¼Œåªä¿ç•™çœŸæ­£æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

## æ ¸å¿ƒåŸåˆ™ï¼šå®ç¼ºæ¯‹æ»¥ï¼
- é»˜è®¤æ€åº¦æ˜¯ DELETEï¼ˆåˆ é™¤ï¼‰
- åªæœ‰éå¸¸æ˜ç¡®ã€æœ‰å…·ä½“ä¿¡æ¯çš„è®°å¿†æ‰èƒ½ä¿ç•™
- æœ‰ä»»ä½•ç–‘è™‘å°±åˆ é™¤

## å¿…é¡»åˆ é™¤çš„è®°å¿†ï¼ˆç›´æ¥ deleteï¼‰ï¼š

1. **æ— æ„ä¹‰å†…å®¹**ï¼š
   - å•å­—/çŸ­è¯­å›å¤ï¼š"ï¼Ÿ"ã€"1"ã€"å¥½"ã€"å“¦"ã€"å•Š"ã€"å—¯"ã€"å“ˆå“ˆ"ã€"å‘œå‘œ"
   - è¡¨æƒ…åŒ…ã€é¢œæ–‡å­—ã€emoji åˆ·å±
   - "æŸäººå‘äº†å›¾ç‰‡/è¡¨æƒ…/è¯­éŸ³"ç­‰æ— å®è´¨å†…å®¹
   - ä¹±ç ã€æ— æ³•ç†è§£çš„å†…å®¹

2. **æ¨¡ç³Š/ç¼ºä¹ä¸Šä¸‹æ–‡çš„ä¿¡æ¯**ï¼š
   - "ç”¨æˆ·è¯´äº†ä»€ä¹ˆ" ä½†æ²¡æœ‰å…·ä½“å†…å®¹
   - "æŸäººå’ŒæŸäººèŠå¤©" ä½†ä¸çŸ¥é“èŠä»€ä¹ˆ
   - æ³›æ³›çš„æè¿°å¦‚"ç”¨æˆ·å¾ˆå¼€å¿ƒ"ä½†ä¸çŸ¥é“åŸå› 
   - æŒ‡ä»£ä¸æ˜çš„å†…å®¹ï¼ˆ"é‚£ä¸ª"ã€"è¿™ä¸ª"ã€"å®ƒ"ï¼‰

3. **æ°´ç¾¤/æ— è¥å…»èŠå¤©**ï¼š
   - ç¾¤å†…çš„æ—¥å¸¸å¯’æš„ã€é—®å¥½
   - é—²èŠã€çŒæ°´ã€æŠ–æœºçµ
   - æ— å®é™…ä¿¡æ¯çš„äº’åŠ¨
   - å¤è¯»ã€ç©æ¢—ã€æ¥é¾™
   - è®¨è®ºä¸ç”¨æˆ·ä¸ªäººæ— å…³çš„è¯é¢˜

4. **ä¸´æ—¶/è¿‡æ—¶ä¿¡æ¯**ï¼š
   - æ¸¸æˆçŠ¶æ€ã€åœ¨çº¿çŠ¶æ€
   - å·²è¿‡æœŸçš„æ´»åŠ¨ã€äº‹ä»¶
   - å¤©æ°”ã€æ—¶é—´ç­‰å³æ—¶ä¿¡æ¯
   - "åˆšæ‰"ã€"ç°åœ¨"ç­‰æ—¶æ•ˆæ€§è¡¨è¿°

5. **é‡å¤/å†—ä½™**ï¼š
   - ç›¸åŒå†…å®¹çš„å¤šæ¡è®°å½•
   - å¯ä»¥åˆå¹¶çš„ç›¸ä¼¼ä¿¡æ¯

6. **AIè‡ªèº«çš„è®°å¿†**ï¼š
   - AIè¯´äº†ä»€ä¹ˆè¯
   - AIçš„å›å¤å†…å®¹
   - AIçš„æƒ³æ³•/è®¡åˆ’

## å¯ä»¥ä¿ç•™çš„è®°å¿†ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š

1. **æœ‰æ˜ç¡®çš„ä¸»ä½“**ï¼šçŸ¥é“æ˜¯è°ï¼ˆå…·ä½“çš„ç”¨æˆ·å/æ˜µç§°/IDï¼‰
2. **æœ‰å…·ä½“çš„å†…å®¹**ï¼šçŸ¥é“å…·ä½“è¯´äº†ä»€ä¹ˆã€åšäº†ä»€ä¹ˆã€æ˜¯ä»€ä¹ˆ
3. **æœ‰é•¿æœŸä»·å€¼**ï¼šè¿™ä¸ªä¿¡æ¯åœ¨ä¸€ä¸ªæœˆåä»ç„¶æœ‰å‚è€ƒæ„ä¹‰

**ä¿ç•™ç¤ºä¾‹**ï¼š
- "ç”¨æˆ·å¼ ä¸‰è¯´ä»–æ˜¯ç¨‹åºå‘˜ï¼Œåœ¨æ­å·å·¥ä½œ" âœ…
- "æå››è¯´ä»–å–œæ¬¢æ‰“ç¯®çƒï¼Œæ¯å‘¨ä¸‰éƒ½ä¼šå»" âœ…  
- "å°æ˜è¯´ä»–å¥³æœ‹å‹å«å°çº¢ï¼Œåœ¨ä¸€èµ·2å¹´äº†" âœ…
- "ç”¨æˆ·Açš„ç”Ÿæ—¥æ˜¯3æœˆ15æ—¥" âœ…

**åˆ é™¤ç¤ºä¾‹**ï¼š
- "ç”¨æˆ·å‘äº†ä¸ªè¡¨æƒ…" âŒ
- "ç¾¤é‡Œåœ¨èŠå¤©" âŒ
- "æŸäººè¯´äº†ä»€ä¹ˆ" âŒ
- "ä»Šå¤©å¤©æ°”å¾ˆå¥½" âŒ
- "å“ˆå“ˆå“ˆå¤ªå¥½ç¬‘äº†" âŒ

## å¾…è¯„ä¼°è®°å¿†

{memories}

## è¾“å‡ºè¦æ±‚

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š

```json
{{
    "evaluations": [
        {{
            "memory_id": "è®°å¿†çš„IDï¼ˆä»ä¸Šé¢å¤åˆ¶ï¼‰",
            "action": "delete",
            "reason": "åˆ é™¤åŸå› "
        }},
        {{
            "memory_id": "å¦ä¸€ä¸ªID",
            "action": "keep", 
            "reason": "ä¿ç•™åŸå› "
        }}
    ]
}}
```

action åªèƒ½æ˜¯ï¼š
- "delete": åˆ é™¤ï¼ˆåº”è¯¥æ˜¯å¤§å¤šæ•°ï¼‰
- "keep": ä¿ç•™ï¼ˆåªæœ‰é«˜ä»·å€¼è®°å¿†ï¼‰
- "summarize": ç²¾ç®€ï¼ˆå¾ˆå°‘ç”¨ï¼Œåªæœ‰å†…å®¹è¿‡é•¿ä½†æœ‰ä»·å€¼æ—¶ï¼‰

å¦‚æœ action æ˜¯ summarizeï¼Œéœ€è¦åŠ  "new_content": "ç²¾ç®€åçš„å†…å®¹"

ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""


class MemoryCleaner:
    """è®°å¿†æ¸…ç†å™¨"""

    def __init__(self, dry_run: bool = True, batch_size: int = 10, concurrency: int = 5):
        """
        åˆå§‹åŒ–æ¸…ç†å™¨
        
        Args:
            dry_run: æ˜¯å¦ä¸ºæ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…ä¿®æ”¹æ•°æ®ï¼‰
            batch_size: æ¯æ‰¹å¤„ç†çš„è®°å¿†æ•°é‡
            concurrency: å¹¶å‘è¯·æ±‚æ•°
        """
        self.dry_run = dry_run
        self.batch_size = batch_size
        self.concurrency = concurrency
        self.data_dir = project_root / "data" / "memory_graph"
        self.memory_file = self.data_dir / "memory_graph.json"
        self.backup_dir = self.data_dir / "backups"
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore: asyncio.Semaphore | None = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total": 0,
            "kept": 0,
            "deleted": 0,
            "summarized": 0,
            "errors": 0,
            "deleted_nodes": 0,
            "deleted_edges": 0,
        }
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = self.data_dir / f"cleanup_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.cleanup_log = []

    def load_memories(self) -> dict:
        """åŠ è½½è®°å¿†æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½è®°å¿†æ–‡ä»¶: {self.memory_file}")
        
        if not self.memory_file.exists():
            raise FileNotFoundError(f"è®°å¿†æ–‡ä»¶ä¸å­˜åœ¨: {self.memory_file}")
        
        with open(self.memory_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        return data

    def extract_memory_text(self, memory_dict: dict) -> str:
        """ä»è®°å¿†å­—å…¸ä¸­æå–å¯è¯»æ–‡æœ¬"""
        parts = []
        
        # æå–åŸºæœ¬ä¿¡æ¯
        memory_id = memory_dict.get("id", "unknown")
        parts.append(f"ID: {memory_id}")
        
        # æå–èŠ‚ç‚¹å†…å®¹
        nodes = memory_dict.get("nodes", [])
        for node in nodes:
            node_type = node.get("node_type", "")
            content = node.get("content", "")
            if content:
                parts.append(f"[{node_type}] {content}")
        
        # æå–è¾¹å…³ç³»
        edges = memory_dict.get("edges", [])
        for edge in edges:
            relation = edge.get("relation", "")
            if relation:
                parts.append(f"å…³ç³»: {relation}")
        
        # æå–å…ƒæ•°æ®
        metadata = memory_dict.get("metadata", {})
        if metadata:
            if "context" in metadata:
                parts.append(f"ä¸Šä¸‹æ–‡: {metadata['context']}")
            if "emotion" in metadata:
                parts.append(f"æƒ…æ„Ÿ: {metadata['emotion']}")
        
        # æå–é‡è¦æ€§å’ŒçŠ¶æ€
        importance = memory_dict.get("importance", 0)
        status = memory_dict.get("status", "unknown")
        created_at = memory_dict.get("created_at", "unknown")
        
        parts.append(f"é‡è¦æ€§: {importance}, çŠ¶æ€: {status}, åˆ›å»ºæ—¶é—´: {created_at}")
        
        return "\n".join(parts)

    async def evaluate_batch(self, memories: list[dict], batch_id: int = 0) -> tuple[int, list[dict]]:
        """
        ä½¿ç”¨ LLM è¯„ä¼°ä¸€æ‰¹è®°å¿†ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰
        
        Args:
            memories: è®°å¿†å­—å…¸åˆ—è¡¨
            batch_id: æ‰¹æ¬¡ç¼–å·
            
        Returns:
            (æ‰¹æ¬¡ID, è¯„ä¼°ç»“æœåˆ—è¡¨)
        """
        async with self.semaphore:
            # æ„å»ºè®°å¿†æ–‡æœ¬
            memory_texts = []
            for i, mem in enumerate(memories):
                text = self.extract_memory_text(mem)
                memory_texts.append(f"=== è®°å¿† {i+1} ===\n{text}")
            
            combined_text = "\n\n".join(memory_texts)
            prompt = EVALUATION_PROMPT.format(memories=combined_text)
            
            try:
                # ä½¿ç”¨ LLMRequest è°ƒç”¨æ¨¡å‹
                if model_config is None:
                    raise RuntimeError("model_config æœªåˆå§‹åŒ–ï¼Œè¯·ç¡®ä¿å·²åŠ è½½é…ç½®")
                task_config = model_config.model_task_config.utils
                llm = LLMRequest(task_config, request_type="memory_cleanup")
                response_text, (reasoning, model_name, _) = await llm.generate_response_async(
                    prompt=prompt,
                    temperature=0.2,
                    max_tokens=4000,
                )
                
                print(f"   âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ (æ¨¡å‹: {model_name})")
                
                # è§£æ JSON å“åº”
                response_text = response_text.strip()
                
                # å°è¯•æå– JSON
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                elif "```" in response_text:
                    json_start = response_text.find("```") + 3
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                
                result = json.loads(response_text)
                evaluations = result.get("evaluations", [])
                
                # ä¸ºè¯„ä¼°ç»“æœæ·»åŠ å®é™…çš„ memory_id
                for j, eval_result in enumerate(evaluations):
                    if j < len(memories):
                        eval_result["memory_id"] = memories[j].get("id", f"unknown_{batch_id}_{j}")
                
                return (batch_id, evaluations)
                
            except json.JSONDecodeError as e:
                print(f"   âŒ æ‰¹æ¬¡ {batch_id} JSON è§£æå¤±è´¥: {e}")
                return (batch_id, [])
            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡ {batch_id} LLM è°ƒç”¨å¤±è´¥: {e}")
                return (batch_id, [])

    async def initialize(self):
        """åˆå§‹åŒ–ï¼ˆåˆ›å»ºä¿¡å·é‡ï¼‰"""
        self.semaphore = asyncio.Semaphore(self.concurrency)
        print(f"ğŸ”§ åˆå§‹åŒ–å®Œæˆ (å¹¶å‘æ•°: {self.concurrency})")

    def create_backup(self, data: dict):
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        backup_file = self.backup_dir / f"memory_graph_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        print(f"ğŸ’¾ åˆ›å»ºå¤‡ä»½: {backup_file}")
        with open(backup_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return backup_file

    def apply_changes(self, data: dict, evaluations: list[dict]) -> dict:
        """
        åº”ç”¨è¯„ä¼°ç»“æœåˆ°æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®
            evaluations: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            ä¿®æ”¹åçš„æ•°æ®
        """
        # åˆ›å»ºè¯„ä¼°ç»“æœç´¢å¼•
        eval_map = {e["memory_id"]: e for e in evaluations if "memory_id" in e}
        
        # éœ€è¦åˆ é™¤çš„è®°å¿† ID
        to_delete = set()
        # éœ€è¦æ›´æ–°çš„è®°å¿†
        to_update = {}
        
        for eval_result in evaluations:
            memory_id = eval_result.get("memory_id")
            action = eval_result.get("action")
            
            if action == "delete":
                to_delete.add(memory_id)
                self.stats["deleted"] += 1
                self.cleanup_log.append({
                    "memory_id": memory_id,
                    "action": "delete",
                    "reason": eval_result.get("reason", ""),
                    "timestamp": datetime.now().isoformat()
                })
            elif action == "summarize":
                to_update[memory_id] = eval_result.get("new_content")
                self.stats["summarized"] += 1
                self.cleanup_log.append({
                    "memory_id": memory_id,
                    "action": "summarize",
                    "reason": eval_result.get("reason", ""),
                    "new_content": eval_result.get("new_content"),
                    "timestamp": datetime.now().isoformat()
                })
            else:
                self.stats["kept"] += 1
        
        if self.dry_run:
            print("ğŸ” [DRY RUN] ä¸å®é™…ä¿®æ”¹æ•°æ®")
            return data
        
        # å®é™…ä¿®æ”¹æ•°æ®
        # 1. åˆ é™¤è®°å¿†
        memories = data.get("memories", {})
        for mem_id in to_delete:
            if mem_id in memories:
                del memories[mem_id]
        
        # 2. æ›´æ–°è®°å¿†å†…å®¹
        for mem_id, new_content in to_update.items():
            if mem_id in memories:
                # æ›´æ–°ä¸»é¢˜èŠ‚ç‚¹çš„å†…å®¹
                memory = memories[mem_id]
                for node in memory.get("nodes", []):
                    if node.get("node_type") in ["ä¸»é¢˜", "topic", "TOPIC"]:
                        node["content"] = new_content
                        break
        
        # 3. æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
        data = self.cleanup_orphaned_nodes_and_edges(data)
        
        return data
    
    def cleanup_orphaned_nodes_and_edges(self, data: dict) -> dict:
        """
        æ¸…ç†å­¤ç«‹çš„èŠ‚ç‚¹å’Œè¾¹
        
        å­¤ç«‹èŠ‚ç‚¹ï¼šå…¶ metadata.memory_ids ä¸­çš„æ‰€æœ‰è®°å¿†éƒ½å·²è¢«åˆ é™¤
        å­¤ç«‹è¾¹ï¼šå…¶ source æˆ– target èŠ‚ç‚¹å·²è¢«åˆ é™¤
        """
        print("\nğŸ”— æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹...")
        
        # è·å–å½“å‰æ‰€æœ‰æœ‰æ•ˆçš„è®°å¿† ID
        valid_memory_ids = set(data.get("memories", {}).keys())
        print(f"   æœ‰æ•ˆè®°å¿†æ•°: {len(valid_memory_ids)}")
        
        # æ¸…ç†èŠ‚ç‚¹
        nodes = data.get("nodes", [])
        original_node_count = len(nodes)
        
        valid_nodes = []
        valid_node_ids = set()
        
        for node in nodes:
            node_id = node.get("id")
            metadata = node.get("metadata", {})
            memory_ids = metadata.get("memory_ids", [])
            
            # æ£€æŸ¥èŠ‚ç‚¹å…³è”çš„è®°å¿†æ˜¯å¦è¿˜å­˜åœ¨
            if memory_ids:
                # è¿‡æ»¤æ‰å·²åˆ é™¤çš„è®°å¿† ID
                remaining_memory_ids = [mid for mid in memory_ids if mid in valid_memory_ids]
                
                if remaining_memory_ids:
                    # æ›´æ–° metadata ä¸­çš„ memory_ids
                    metadata["memory_ids"] = remaining_memory_ids
                    valid_nodes.append(node)
                    valid_node_ids.add(node_id)
                # å¦‚æœæ²¡æœ‰å‰©ä½™çš„æœ‰æ•ˆè®°å¿† IDï¼ŒèŠ‚ç‚¹è¢«ä¸¢å¼ƒ
            else:
                # æ²¡æœ‰ memory_ids çš„èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯å…¶ä»–æ–¹å¼åˆ›å»ºçš„ï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦è¢«æŸä¸ªè®°å¿†å¼•ç”¨
                # ä¿å®ˆå¤„ç†ï¼šä¿ç•™è¿™äº›èŠ‚ç‚¹
                valid_nodes.append(node)
                valid_node_ids.add(node_id)
        
        deleted_nodes = original_node_count - len(valid_nodes)
        data["nodes"] = valid_nodes
        print(f"   âœ… èŠ‚ç‚¹: {original_node_count} â†’ {len(valid_nodes)} (åˆ é™¤ {deleted_nodes})")
        
        # æ¸…ç†è¾¹
        edges = data.get("edges", [])
        original_edge_count = len(edges)
        
        valid_edges = []
        for edge in edges:
            source = edge.get("source")
            target = edge.get("target")
            
            # åªä¿ç•™ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹
            if source in valid_node_ids and target in valid_node_ids:
                valid_edges.append(edge)
        
        deleted_edges = original_edge_count - len(valid_edges)
        data["edges"] = valid_edges
        print(f"   âœ… è¾¹: {original_edge_count} â†’ {len(valid_edges)} (åˆ é™¤ {deleted_edges})")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["deleted_nodes"] = deleted_nodes
        self.stats["deleted_edges"] = deleted_edges
        
        return data

    def save_data(self, data: dict):
        """ä¿å­˜ä¿®æ”¹åçš„æ•°æ®"""
        if self.dry_run:
            print("ğŸ” [DRY RUN] è·³è¿‡ä¿å­˜")
            return
        
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ°: {self.memory_file}")
        with open(self.memory_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def save_log(self):
        """ä¿å­˜æ¸…ç†æ—¥å¿—"""
        print(f"ğŸ“ ä¿å­˜æ¸…ç†æ—¥å¿—åˆ°: {self.log_file}")
        with open(self.log_file, "w", encoding="utf-8") as f:
            json.dump({
                "stats": self.stats,
                "dry_run": self.dry_run,
                "timestamp": datetime.now().isoformat(),
                "log": self.cleanup_log
            }, f, ensure_ascii=False, indent=2)

    async def run(self):
        """è¿è¡Œæ¸…ç†æµç¨‹"""
        print("=" * 60)
        print("ğŸ§¹ è®°å¿†æ¸…ç†è„šæœ¬ (é«˜å¹¶å‘ç‰ˆ)")
        print("=" * 60)
        print(f"æ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œ (DRY RUN)' if self.dry_run else 'å®é™…æ‰§è¡Œ'}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"å¹¶å‘æ•°: {self.concurrency}")
        print("=" * 60)
        
        # åˆå§‹åŒ–
        await self.initialize()
        
        # åŠ è½½æ•°æ®
        data = self.load_memories()
        
        # è·å–æ‰€æœ‰è®°å¿†
        memories = data.get("memories", {})
        memory_list = list(memories.values())
        self.stats["total"] = len(memory_list)
        
        print(f"ğŸ“Š æ€»è®°å¿†æ•°: {self.stats['total']}")
        
        if not memory_list:
            print("âš ï¸ æ²¡æœ‰è®°å¿†éœ€è¦å¤„ç†")
            return
        
        # åˆ›å»ºå¤‡ä»½
        if not self.dry_run:
            self.create_backup(data)
        
        # åˆ†æ‰¹
        batches = []
        for i in range(0, len(memory_list), self.batch_size):
            batch = memory_list[i:i + self.batch_size]
            batches.append(batch)
        
        total_batches = len(batches)
        print(f"ğŸ“¦ å…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œå¼€å§‹å¹¶å‘å¤„ç†...\n")
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        start_time = datetime.now()
        tasks = [
            self.evaluate_batch(batch, batch_id=idx)
            for idx, batch in enumerate(batches)
        ]
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()
        
        # æ”¶é›†æ‰€æœ‰è¯„ä¼°ç»“æœ
        all_evaluations = []
        success_count = 0
        error_count = 0
        
        for result in results:
            if isinstance(result, Exception):
                print(f"   âŒ æ‰¹æ¬¡å¼‚å¸¸: {result}")
                error_count += 1
            elif isinstance(result, tuple):
                batch_id, evaluations = result
                if evaluations:
                    all_evaluations.extend(evaluations)
                    success_count += 1
                else:
                    error_count += 1
        
        print(f"\nâ±ï¸ å¹¶å‘å¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        print(f"   æˆåŠŸæ‰¹æ¬¡: {success_count}/{total_batches}, å¤±è´¥: {error_count}")
        
        # ç»Ÿè®¡è¯„ä¼°ç»“æœ
        delete_count = sum(1 for e in all_evaluations if e.get("action") == "delete")
        keep_count = sum(1 for e in all_evaluations if e.get("action") == "keep")
        summarize_count = sum(1 for e in all_evaluations if e.get("action") == "summarize")
        
        print(f"   ğŸ“Š è¯„ä¼°ç»“æœ: ä¿ç•™ {keep_count}, åˆ é™¤ {delete_count}, ç²¾ç®€ {summarize_count}")
        
        # åº”ç”¨æ›´æ”¹
        print("\n" + "=" * 60)
        print("ğŸ“Š åº”ç”¨æ›´æ”¹...")
        data = self.apply_changes(data, all_evaluations)
        
        # ä¿å­˜æ•°æ®
        self.save_data(data)
        
        # ä¿å­˜æ—¥å¿—
        self.save_log()
        
        # æ‰“å°ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸…ç†ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è®°å¿†æ•°: {self.stats['total']}")
        print(f"ä¿ç•™: {self.stats['kept']}")
        print(f"åˆ é™¤: {self.stats['deleted']}")
        print(f"ç²¾ç®€: {self.stats['summarized']}")
        print(f"åˆ é™¤èŠ‚ç‚¹: {self.stats['deleted_nodes']}")
        print(f"åˆ é™¤è¾¹: {self.stats['deleted_edges']}")
        print(f"é”™è¯¯: {self.stats['errors']}")
        print(f"å¤„ç†é€Ÿåº¦: {self.stats['total'] / elapsed:.1f} æ¡/ç§’")
        print("=" * 60)
        
        if self.dry_run:
            print("\nâš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œå®é™…æ•°æ®æœªè¢«ä¿®æ”¹")
            print("å¦‚è¦å®é™…æ‰§è¡Œï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")

    async def run_cleanup_only(self):
        """åªæ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹ï¼Œä¸é‡æ–°è¯„ä¼°è®°å¿†"""
        print("=" * 60)
        print("ğŸ”— å­¤ç«‹èŠ‚ç‚¹/è¾¹æ¸…ç†æ¨¡å¼")
        print("=" * 60)
        print(f"æ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œ (DRY RUN)' if self.dry_run else 'å®é™…æ‰§è¡Œ'}")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        data = self.load_memories()
        
        # ç»Ÿè®¡åŸå§‹æ•°æ®
        memories = data.get("memories", {})
        nodes = data.get("nodes", [])
        edges = data.get("edges", [])
        
        print(f"ğŸ“Š å½“å‰çŠ¶æ€: {len(memories)} æ¡è®°å¿†, {len(nodes)} ä¸ªèŠ‚ç‚¹, {len(edges)} æ¡è¾¹")
        
        if not self.dry_run:
            self.create_backup(data)
        
        # æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
        if self.dry_run:
            # æ¨¡æ‹Ÿè¿è¡Œï¼šç»Ÿè®¡ä½†ä¸ä¿®æ”¹
            valid_memory_ids = set(memories.keys())
            
            # ç»Ÿè®¡è¦åˆ é™¤çš„èŠ‚ç‚¹
            nodes_to_keep = 0
            for node in nodes:
                metadata = node.get("metadata", {})
                memory_ids = metadata.get("memory_ids", [])
                if memory_ids:
                    remaining = [mid for mid in memory_ids if mid in valid_memory_ids]
                    if remaining:
                        nodes_to_keep += 1
                else:
                    nodes_to_keep += 1
            
            nodes_to_delete = len(nodes) - nodes_to_keep
            
            # ç»Ÿè®¡è¦åˆ é™¤çš„è¾¹ï¼ˆéœ€è¦å…ˆç¡®å®šå“ªäº›èŠ‚ç‚¹ä¼šè¢«ä¿ç•™ï¼‰
            valid_node_ids = set()
            for node in nodes:
                metadata = node.get("metadata", {})
                memory_ids = metadata.get("memory_ids", [])
                if memory_ids:
                    remaining = [mid for mid in memory_ids if mid in valid_memory_ids]
                    if remaining:
                        valid_node_ids.add(node.get("id"))
                else:
                    valid_node_ids.add(node.get("id"))
            
            edges_to_keep = sum(1 for e in edges if e.get("source") in valid_node_ids and e.get("target") in valid_node_ids)
            edges_to_delete = len(edges) - edges_to_keep
            
            print(f"\nğŸ” [DRY RUN] é¢„è®¡æ¸…ç†:")
            print(f"   èŠ‚ç‚¹: {len(nodes)} â†’ {nodes_to_keep} (åˆ é™¤ {nodes_to_delete})")
            print(f"   è¾¹: {len(edges)} â†’ {edges_to_keep} (åˆ é™¤ {edges_to_delete})")
            print("\nâš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œå®é™…æ•°æ®æœªè¢«ä¿®æ”¹")
            print("å¦‚è¦å®é™…æ‰§è¡Œï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
        else:
            data = self.cleanup_orphaned_nodes_and_edges(data)
            self.save_data(data)
            
            print(f"\nâœ… æ¸…ç†å®Œæˆ!")
            print(f"   åˆ é™¤èŠ‚ç‚¹: {self.stats['deleted_nodes']}")
            print(f"   åˆ é™¤è¾¹: {self.stats['deleted_edges']}")


async def main():
    parser = argparse.ArgumentParser(description="è®°å¿†æ¸…ç†è„šæœ¬ (é«˜å¹¶å‘ç‰ˆ)")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="æ¯æ‰¹å¤„ç†çš„è®°å¿†æ•°é‡ (é»˜è®¤: 10)"
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=10,
        help="å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 10)"
    )
    parser.add_argument(
        "--cleanup-only",
        action="store_true",
        help="åªæ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹ï¼Œä¸é‡æ–°è¯„ä¼°è®°å¿†"
    )
    
    args = parser.parse_args()
    
    cleaner = MemoryCleaner(
        dry_run=args.dry_run,
        batch_size=args.batch_size,
        concurrency=args.concurrency,
    )
    
    if args.cleanup_only:
        await cleaner.run_cleanup_only()
    else:
        await cleaner.run()


if __name__ == "__main__":
    asyncio.run(main())
