#!/usr/bin/env python3
"""æ•°æ®åº“è¿ç§»è„šæœ¬

æ”¯æŒåœ¨ä¸åŒæ•°æ®åº“ä¹‹é—´è¿ç§»æ•°æ®ï¼š
- SQLite <-> MySQL
- SQLite <-> PostgreSQL
- MySQL <-> PostgreSQL

ä½¿ç”¨æ–¹æ³•:
    python scripts/migrate_database.py --help
    python scripts/migrate_database.py --source sqlite --target postgresql
    python scripts/migrate_database.py --source mysql --target postgresql --batch-size 5000
    
    # äº¤äº’å¼å‘å¯¼æ¨¡å¼ï¼ˆæ¨èï¼‰
    python scripts/migrate_database.py

æ³¨æ„äº‹é¡¹:
1. è¿ç§»å‰è¯·å¤‡ä»½æºæ•°æ®åº“
2. ç›®æ ‡æ•°æ®åº“åº”è¯¥æ˜¯ç©ºçš„æˆ–ä¸å­˜åœ¨çš„ï¼ˆè„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»ºè¡¨ï¼‰
3. è¿ç§»è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
4. è¿ç§»åˆ° PostgreSQL æ—¶ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
   - ä¿®å¤å¸ƒå°”åˆ—ç±»å‹ï¼ˆSQLite INTEGER -> PostgreSQL BOOLEANï¼‰
   - é‡ç½®åºåˆ—å€¼ï¼ˆé¿å…ä¸»é”®å†²çªï¼‰

å®ç°ç»†èŠ‚:
- ä½¿ç”¨ SQLAlchemy è¿›è¡Œæ•°æ®åº“è¿æ¥å’Œå…ƒæ•°æ®ç®¡ç†
- é‡‡ç”¨æµå¼è¿ç§»ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®
- æ”¯æŒ SQLiteã€MySQLã€PostgreSQL ä¹‹é—´çš„äº’ç›¸è¿ç§»
- æ‰¹é‡æ’å…¥å¤±è´¥æ—¶è‡ªåŠ¨é™çº§ä¸ºé€è¡Œæ’å…¥ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™æ•°æ®
"""

from __future__ import annotations

import argparse
import logging
import os
import sys
from getpass import getpass

# =============================================================================
# è®¾ç½®æ—¥å¿—
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="[%(levelname)s] %(message)s",
)

logger = logging.getLogger(__name__)

# =============================================================================
# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥ä¾¿å‹å¥½æŠ¥é”™ï¼‰
# =============================================================================

try:
    import tomllib
except ImportError:
    tomllib = None

from typing import Any, Iterable, Callable

from datetime import datetime as dt

from sqlalchemy import (
    create_engine,
    MetaData,
    Table,
    inspect,
    text,
    types as sqltypes,
)
from sqlalchemy.engine import Engine, Connection
from sqlalchemy.exc import SQLAlchemyError

# ====== ä¸ºäº†åœ¨ Windows ä¸Šæ›´å‹å¥½çš„è¾“å‡ºä¸­æ–‡ï¼Œæå‰è®¾ç½®ç¯å¢ƒ ======
# æœ‰äº› Windows ç»ˆç«¯é»˜è®¤ç¼–ç ä¸æ˜¯ UTF-8ï¼Œè¿™é‡Œåšä¸ªå…¼å®¹
if os.name == "nt":
    try:
        import ctypes

        ctypes.windll.kernel32.SetConsoleOutputCP(65001)
    except Exception:
        pass


# =============================================================================
# é…ç½®ç›¸å…³å·¥å…·
# =============================================================================


def get_project_root() -> str:
    """è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰è„šæœ¬çš„ä¸Šçº§ç›®å½•ï¼‰"""
    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


PROJECT_ROOT = get_project_root()


def load_bot_config() -> dict:
    """åŠ è½½ config/bot_config.toml é…ç½®æ–‡ä»¶

    è¿”å›:
        dict: é…ç½®å­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å›ç©ºå­—å…¸
    """
    config_path = os.path.join(PROJECT_ROOT, "config", "bot_config.toml")
    if not os.path.exists(config_path):
        logger.warning("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: %s", config_path)
        return {}

    if tomllib is None:
        logger.warning("å½“å‰ Python ç‰ˆæœ¬ä¸æ”¯æŒ tomllibï¼Œè¯·ä½¿ç”¨ Python 3.11+ æˆ–æ‰‹åŠ¨å®‰è£… tomli")
        return {}

    try:
        with open(config_path, "rb") as f:
            config = tomllib.load(f)
        return config
    except Exception as e:
        logger.error("è§£æé…ç½®æ–‡ä»¶å¤±è´¥: %s", e)
        return {}


def get_database_config_from_toml(db_type: str) -> dict | None:
    """ä» bot_config.toml ä¸­è¯»å–æ•°æ®åº“é…ç½®

    Args:
        db_type: æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒ "sqlite"ã€"mysql"ã€"postgresql"

    Returns:
        dict: æ•°æ®åº“é…ç½®å­—å…¸ï¼Œå¦‚æœå¯¹åº”é…ç½®ä¸å­˜åœ¨åˆ™è¿”å› None
    """
    config_data = load_bot_config()
    if not config_data:
        return None

    # å…¼å®¹æ—§ç»“æ„å’Œæ–°ç»“æ„
    # æ—§ç»“æ„: é¡¶å±‚ç›´æ¥æœ‰ db_type ç›¸å…³å­—æ®µ
    # æ–°ç»“æ„: åœ¨ [database] ä¸‹æœ‰ db_type ç›¸å…³å­—æ®µ
    db_config = config_data.get("database", {})

    if db_type == "sqlite":
        sqlite_path = (
            db_config.get("sqlite_path")
            or config_data.get("sqlite_path")
            or "data/MaiBot.db"
        )
        if not os.path.isabs(sqlite_path):
            sqlite_path = os.path.join(PROJECT_ROOT, sqlite_path)
        return {"path": sqlite_path}

    elif db_type == "mysql":
        return {
            "host": db_config.get("mysql_host")
            or config_data.get("mysql_host")
            or "localhost",
            "port": db_config.get("mysql_port")
            or config_data.get("mysql_port")
            or 3306,
            "database": db_config.get("mysql_database")
            or config_data.get("mysql_database")
            or "maibot",
            "user": db_config.get("mysql_user")
            or config_data.get("mysql_user")
            or "root",
            "password": db_config.get("mysql_password")
            or config_data.get("mysql_password")
            or "",
            "charset": db_config.get("mysql_charset")
            or config_data.get("mysql_charset")
            or "utf8mb4",
        }

    elif db_type == "postgresql":
        return {
            "host": db_config.get("postgresql_host")
            or config_data.get("postgresql_host")
            or "localhost",
            "port": db_config.get("postgresql_port")
            or config_data.get("postgresql_port")
            or 5432,
            "database": db_config.get("postgresql_database")
            or config_data.get("postgresql_database")
            or "maibot",
            "user": db_config.get("postgresql_user")
            or config_data.get("postgresql_user")
            or "postgres",
            "password": db_config.get("postgresql_password")
            or config_data.get("postgresql_password")
            or "",
            "schema": db_config.get("postgresql_schema")
            or config_data.get("postgresql_schema")
            or "public",
        }

    return None


# =============================================================================
# æ•°æ®åº“è¿æ¥ç›¸å…³
# =============================================================================


def create_sqlite_engine(sqlite_path: str) -> Engine:
    """ï¿½ï¿½ï¿½ï¿½ SQLite ï¿½ï¿½ï¿½ï¿½"""
    if not os.path.isabs(sqlite_path):
        sqlite_path = os.path.join(PROJECT_ROOT, sqlite_path)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(sqlite_path), exist_ok=True)

    url = f"sqlite:///{sqlite_path}"
    logger.info("ä½¿ç”¨ SQLite æ•°æ®åº“: %s", sqlite_path)
    engine = create_engine(
        url,
        future=True,
        connect_args={
            "timeout": 30,  # wait a bit if the db is locked
            "check_same_thread": False,
        },
    )
    # Increase busy timeout to reduce "database is locked" errors on SQLite
    with engine.connect() as conn:
        conn.execute(text("PRAGMA busy_timeout=30000"))
    return engine


def create_postgresql_engine(
    host: str,
    port: int,
    database: str,
    user: str,
    password: str,
    schema: str = "public",
) -> Engine:
    """åˆ›å»º PostgreSQL å¼•æ“"""
    # åœ¨å¯¼å…¥ psycopg2 ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè§£å†³ Windows ç¼–ç é—®é¢˜
    # psycopg2 åœ¨ Windows ä¸Šè¿æ¥æ—¶ï¼Œå¦‚æœå®¢æˆ·ç«¯ç¼–ç ä¸æœåŠ¡å™¨ä¸ä¸€è‡´å¯èƒ½ä¼šæœ‰é—®é¢˜
    os.environ.setdefault("PGCLIENTENCODING", "utf-8")

    # å»¶è¿Ÿå¯¼å…¥ psycopg2ï¼Œä»¥ä¾¿å‹å¥½æç¤º
    try:
        import psycopg2  # noqa: F401
    except ImportError:
        logger.error("éœ€è¦å®‰è£… psycopg2-binary æ‰èƒ½è¿æ¥ PostgreSQL: pip install psycopg2-binary")
        raise

    url = f"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}"
    logger.info("ä½¿ç”¨ PostgreSQL æ•°æ®åº“: %s@%s:%s/%s (schema=%s)", user, host, port, database, schema)
    engine = create_engine(url, future=True)
    # ä¸ºäº†æ–¹ä¾¿ï¼Œè®¾ç½® search_path
    with engine.connect() as conn:
        conn.execute(text(f"SET search_path TO {schema}"))
    return engine


def create_engine_by_type(db_type: str, config: dict) -> Engine:
    """æ ¹æ®æ•°æ®åº“ç±»å‹åˆ›å»ºå¯¹åº”çš„ SQLAlchemy Engine

    Args:
        db_type: æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒ sqlite/mysql/postgresql
        config: é…ç½®å­—å…¸

    Returns:
        Engine: SQLAlchemy å¼•æ“å®ä¾‹
    """
    db_type = db_type.lower()
    if db_type == "sqlite":
        return create_sqlite_engine(config["path"])
    elif db_type == "mysql":
        return create_mysql_engine(
            host=config["host"],
            port=config["port"],
            database=config["database"],
            user=config["user"],
            password=config["password"],
            charset=config.get("charset", "utf8mb4"),
        )
    elif db_type == "postgresql":
        return create_postgresql_engine(
            host=config["host"],
            port=config["port"],
            database=config["database"],
            user=config["user"],
            password=config["password"],
            schema=config.get("schema", "public"),
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================


def chunked_iterable(iterable: Iterable, size: int) -> Iterable[list]:
    """å°†å¯è¿­ä»£å¯¹è±¡åˆ†å—

    Args:
        iterable: å¯è¿­ä»£å¯¹è±¡
        size: æ¯å—å¤§å°

    Yields:
        list: åˆ†å—åˆ—è¡¨
    """
    chunk: list[Any] = []
    for item in iterable:
        chunk.append(item)
        if len(chunk) >= size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk


def get_table_row_count(conn: Connection, table: Table) -> int:
    """è·å–è¡¨çš„è¡Œæ•°"""
    try:
        result = conn.execute(text(f"SELECT COUNT(*) FROM {table.name}"))
        return int(result.scalar() or 0)
    except SQLAlchemyError as e:
        logger.warning("è·å–è¡¨è¡Œæ•°å¤±è´¥ %s: %s", table.name, e)
        return 0


def convert_value_for_target(
    val: Any,
    col_name: str,
    source_col_type: Any,
    target_col_type: Any,
    target_dialect: str,
    target_col_nullable: bool = True,
) -> Any:
    """è½¬æ¢å€¼ä»¥é€‚é…ç›®æ ‡æ•°æ®åº“ç±»å‹

    å¤„ç†ä»¥ä¸‹æƒ…å†µ:
    1. ç©ºå­—ç¬¦ä¸²æ—¥æœŸæ—¶é—´ -> None
    2. SQLite INTEGER (0/1) -> PostgreSQL BOOLEAN
    3. å­—ç¬¦ä¸²æ—¥æœŸæ—¶é—´ -> datetime å¯¹è±¡
    4. è·³è¿‡ä¸»é”® id (è®©ç›®æ ‡æ•°æ®åº“è‡ªå¢)
    5. å¯¹äº NOT NULL åˆ—ï¼Œæä¾›åˆé€‚çš„é»˜è®¤å€¼

    Args:
        val: åŸå§‹å€¼
        col_name: åˆ—å
        source_col_type: æºåˆ—ç±»å‹
        target_col_type: ç›®æ ‡åˆ—ç±»å‹
        target_dialect: ç›®æ ‡æ•°æ®åº“æ–¹è¨€åç§°
        target_col_nullable: ç›®æ ‡åˆ—æ˜¯å¦å…è®¸ NULL

    Returns:
        è½¬æ¢åçš„å€¼
    """
    # è·å–ç›®æ ‡ç±»å‹çš„ç±»å
    target_type_name = target_col_type.__class__.__name__.upper()
    source_type_name = source_col_type.__class__.__name__.upper()

    # å¤„ç† None å€¼
    if val is None:
        # å¦‚æœç›®æ ‡åˆ—ä¸å…è®¸ NULLï¼Œæä¾›é»˜è®¤å€¼
        if not target_col_nullable:
            # Boolean ç±»å‹çš„é»˜è®¤å€¼æ˜¯ False
            if target_type_name == "BOOLEAN" or isinstance(target_col_type, sqltypes.Boolean):
                return False
            # æ•°å€¼ç±»å‹çš„é»˜è®¤å€¼
            if target_type_name in ("INTEGER", "BIGINT", "SMALLINT") or isinstance(target_col_type, sqltypes.Integer):
                return 0
            if target_type_name in ("FLOAT", "DOUBLE", "REAL", "NUMERIC", "DECIMAL", "DOUBLE_PRECISION") or isinstance(target_col_type, sqltypes.Float):
                return 0.0
            # æ—¥æœŸæ—¶é—´ç±»å‹çš„é»˜è®¤å€¼
            if target_type_name in ("DATETIME", "TIMESTAMP") or isinstance(target_col_type, sqltypes.DateTime):
                return dt.now()
            # å­—ç¬¦ä¸²ç±»å‹çš„é»˜è®¤å€¼
            if target_type_name in ("VARCHAR", "STRING", "TEXT") or isinstance(target_col_type, (sqltypes.String, sqltypes.Text)):
                return ""
            # å…¶ä»–ç±»å‹ä¹Ÿè¿”å›ç©ºå­—ç¬¦ä¸²ä½œä¸ºå…œåº•
            return ""
        return None

    # å¤„ç† Boolean ç±»å‹è½¬æ¢
    # SQLite ä¸­ Boolean å®é™…å­˜å‚¨ä¸º INTEGER (0/1)
    if target_type_name == "BOOLEAN" or isinstance(target_col_type, sqltypes.Boolean):
        if isinstance(val, bool):
            return val
        if isinstance(val, (int, float)):
            return bool(val)
        if isinstance(val, str):
            val_lower = val.lower().strip()
            if val_lower in ("true", "1", "yes"):
                return True
            elif val_lower in ("false", "0", "no", ""):
                return False
        return bool(val) if val else False

    # å¤„ç† DateTime ç±»å‹è½¬æ¢
    if target_type_name in ("DATETIME", "TIMESTAMP") or isinstance(target_col_type, sqltypes.DateTime):
        if isinstance(val, dt):
            return val
        if isinstance(val, str):
            val = val.strip()
            # ç©ºå­—ç¬¦ä¸² -> None
            if val == "":
                return None
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            for fmt in [
                "%Y-%m-%d %H:%M:%S.%f",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S.%f",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%d",
            ]:
                try:
                    return dt.strptime(val, fmt)
                except ValueError:
                    continue
            # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯• fromisoformat
            try:
                return dt.fromisoformat(val)
            except ValueError:
                logger.warning("æ— æ³•è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸² '%s' (åˆ—: %s)ï¼Œè®¾ä¸º None", val, col_name)
                return None
        # å¦‚æœæ˜¯æ•°å€¼ï¼ˆæ—¶é—´æˆ³ï¼‰ï¼Œå°è¯•è½¬æ¢
        if isinstance(val, (int, float)) and val > 0:
            try:
                return dt.fromtimestamp(val)
            except (OSError, ValueError, OverflowError):
                return None
        return None

    # å¤„ç† Float ç±»å‹
    if target_type_name == "FLOAT" or isinstance(target_col_type, sqltypes.Float):
        if isinstance(val, (int, float)):
            return float(val)
        if isinstance(val, str):
            val = val.strip()
            if val == "":
                return None
            try:
                return float(val)
            except ValueError:
                return None
        return val

    # å¤„ç† Integer ç±»å‹
    if target_type_name == "INTEGER" or isinstance(target_col_type, sqltypes.Integer):
        if isinstance(val, int):
            return val
        if isinstance(val, float):
            return int(val)
        if isinstance(val, str):
            val = val.strip()
            if val == "":
                return None
            try:
                return int(float(val))
            except ValueError:
                return None
        return val

    return val


def copy_table_structure(source_table: Table, target_metadata: MetaData, target_engine: Engine) -> Table:
    """å¤åˆ¶è¡¨ç»“æ„åˆ°ç›®æ ‡æ•°æ®åº“ï¼Œä½¿å…¶ç»“æ„ä¿æŒä¸€è‡´"""
    target_is_sqlite = target_engine.dialect.name == "sqlite"
    target_is_pg = target_engine.dialect.name == "postgresql"

    columns = []
    for c in source_table.columns:
        new_col = c.copy()

        # SQLite ä¸æ”¯æŒ nextval ç­‰ server_default
        if target_is_sqlite:
            new_col.server_default = None

        # PostgreSQL éœ€è¦å°†éƒ¨åˆ† SQLite ç‰¹æœ‰ç±»å‹è½¬æ¢
        if target_is_pg:
            col_type = new_col.type
            # SQLite DATETIME -> é€šç”¨ DateTime
            if isinstance(col_type, sqltypes.DateTime) or col_type.__class__.__name__ in {"DATETIME", "DateTime"}:
                new_col.type = sqltypes.DateTime()
            # TEXT(50) ç­‰é•¿åº¦å—é™çš„ TEXT åœ¨ PG æ— æ•ˆï¼Œæ”¹ç”¨ String(length)
            elif isinstance(col_type, sqltypes.Text) and getattr(col_type, "length", None):
                new_col.type = sqltypes.String(length=col_type.length)

        columns.append(new_col)

    # ä¸ºé¿å…è¿­ä»£çº¦æŸé›†åˆæ—¶å‡ºç° â€œSet changed size during iterationâ€ï¼Œè¿™é‡Œä¸å¤åˆ¶è¡¨çº§çº¦æŸ
    target_table = Table(
        source_table.name,
        target_metadata,
        *columns,
    )
    target_metadata.create_all(target_engine, tables=[target_table])
    return target_table


def migrate_table_data(
    source_conn: Connection,
    target_engine: Engine,
    source_table: Table,
    target_table: Table,
    batch_size: int = 1000,
    target_dialect: str = "postgresql",
    row_limit: int | None = None,
) -> tuple[int, int]:
    """è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®

    Args:
        source_conn: æºæ•°æ®åº“è¿æ¥
        target_engine: ç›®æ ‡æ•°æ®åº“å¼•æ“ï¼ˆæ³¨æ„ï¼šæ”¹ä¸º engine è€Œä¸æ˜¯ connectionï¼‰
        source_table: æºè¡¨å¯¹è±¡
        target_table: ç›®æ ‡è¡¨å¯¹è±¡
        batch_size: æ¯æ‰¹æ¬¡å¤„ç†å¤§å°
        target_dialect: ç›®æ ‡æ•°æ®åº“æ–¹è¨€ (sqlite/mysql/postgresql)
        row_limit: æœ€å¤§è¿ç§»è¡Œæ•°é™åˆ¶ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶

    Returns:
        tuple[int, int]: (è¿ç§»è¡Œæ•°, é”™è¯¯æ•°é‡)
    """
    total_rows = get_table_row_count(source_conn, source_table)
    logger.info(
        "å¼€å§‹è¿ç§»è¡¨: %s (å…± %s è¡Œ)",
        source_table.name,
        total_rows if total_rows else "æœªçŸ¥",
    )

    migrated_rows = 0
    error_count = 0
    conversion_warnings = 0

    # æ„å»ºæºåˆ—åˆ°ç›®æ ‡åˆ—çš„æ˜ å°„
    target_cols_by_name = {c.key: c for c in target_table.columns}

    # è¯†åˆ«ä¸»é”®åˆ—ï¼ˆé€šå¸¸æ˜¯ idï¼‰ï¼Œè¿ç§»æ—¶ä¿ç•™åŸå§‹ ID ä»¥é¿å…é‡å¤æ•°æ®
    primary_key_cols = {c.key for c in source_table.primary_key.columns}

    # ä½¿ç”¨æµå¼æŸ¥è¯¢ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šæ•°æ®
    # ä½¿ç”¨ text() åŸå§‹ SQL æŸ¥è¯¢ï¼Œé¿å… SQLAlchemy è‡ªåŠ¨ç±»å‹è½¬æ¢ï¼ˆå¦‚ DateTimeï¼‰å¯¼è‡´çš„é”™è¯¯
    try:
        # æ„å»ºåŸå§‹ SQL æŸ¥è¯¢è¯­å¥
        col_names = [c.key for c in source_table.columns]
        if row_limit:
            # æŒ‰æ—¶é—´æˆ– ID å€’åºå–æœ€æ–°çš„ row_limit æ¡
            raw_sql = text(f"SELECT {', '.join(col_names)} FROM {source_table.name} ORDER BY id DESC LIMIT {row_limit}")
            logger.info("  é™åˆ¶è¿ç§»æœ€æ–° %d è¡Œ", row_limit)
        else:
            raw_sql = text(f"SELECT {', '.join(col_names)} FROM {source_table.name}")
        result = source_conn.execute(raw_sql)
    except SQLAlchemyError as e:
        logger.error("æŸ¥è¯¢è¡¨ %s å¤±è´¥: %s", source_table.name, e)
        return 0, 1

    def insert_batch(rows: list[dict]):
        """æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡ï¼Œæ‰¹æ¬¡å¤±è´¥æ—¶é™çº§ä¸ºé€è¡Œæ’å…¥"""
        nonlocal migrated_rows, error_count
        if not rows:
            return
        try:
            # æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡
            with target_engine.begin() as target_conn:
                target_conn.execute(target_table.insert(), rows)
            migrated_rows += len(rows)
            logger.info("  å·²è¿ç§» %d/%s è¡Œ", migrated_rows, total_rows or "?")
        except SQLAlchemyError as e:
            # æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œé™çº§ä¸ºé€è¡Œæ’å…¥
            logger.warning("æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œé™çº§ä¸ºé€è¡Œæ’å…¥ (å…± %d è¡Œ): %s", len(rows), str(e)[:200])
            for row in rows:
                try:
                    with target_engine.begin() as target_conn:
                        target_conn.execute(target_table.insert(), [row])
                    migrated_rows += 1
                except SQLAlchemyError as row_e:
                    # è®°å½•å¤±è´¥çš„è¡Œä¿¡æ¯
                    row_id = row.get("id", "unknown")
                    logger.error("æ’å…¥è¡Œå¤±è´¥ (id=%s): %s", row_id, str(row_e)[:200])
                    error_count += 1
            logger.info("  é€è¡Œæ’å…¥å®Œæˆï¼Œå·²è¿ç§» %d/%s è¡Œ", migrated_rows, total_rows or "?")

    batch: list[dict] = []
    null_char_replacements = 0

    # æ„å»ºåˆ—ååˆ—è¡¨ï¼ˆç”¨äºé€šè¿‡ç´¢å¼•è®¿é—®åŸå§‹ SQL ç»“æœï¼‰
    col_list = list(source_table.columns)
    col_name_to_idx = {c.key: idx for idx, c in enumerate(col_list)}

    for row in result:
        row_dict = {}
        for col in col_list:
            col_key = col.key

            # ä¿ç•™ä¸»é”®åˆ—ï¼ˆidï¼‰ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            # æ³¨æ„ï¼šå¦‚æœç›®æ ‡è¡¨ä½¿ç”¨è‡ªå¢ä¸»é”®ï¼Œå¯èƒ½éœ€è¦é‡ç½®åºåˆ—

            # é€šè¿‡ç´¢å¼•è·å–åŸå§‹å€¼ï¼ˆé¿å… SQLAlchemy è‡ªåŠ¨ç±»å‹è½¬æ¢ï¼‰
            col_idx = col_name_to_idx[col_key]
            val = row[col_idx]

            # å¤„ç† NUL å­—ç¬¦
            if isinstance(val, str) and "\x00" in val:
                val = val.replace("\x00", "")
                null_char_replacements += 1

            # è·å–ç›®æ ‡åˆ—ç±»å‹è¿›è¡Œè½¬æ¢
            target_col = target_cols_by_name.get(col_key)
            if target_col is not None:
                try:
                    val = convert_value_for_target(
                        val=val,
                        col_name=col_key,
                        source_col_type=col.type,
                        target_col_type=target_col.type,
                        target_dialect=target_dialect,
                        target_col_nullable=target_col.nullable if target_col.nullable is not None else True,
                    )
                except Exception as e:
                    conversion_warnings += 1
                    if conversion_warnings <= 5:
                        logger.warning(
                            "å€¼è½¬æ¢å¼‚å¸¸ (è¡¨=%s, åˆ—=%s, å€¼=%r): %s",
                            source_table.name, col_key, val, e
                        )

            row_dict[col_key] = val

        batch.append(row_dict)
        if len(batch) >= batch_size:
            insert_batch(batch)
            batch = []

    if batch:
        insert_batch(batch)

    logger.info(
        "å®Œæˆè¿ç§»è¡¨: %s (æˆåŠŸ: %d è¡Œ, å¤±è´¥: %d è¡Œ)",
        source_table.name,
        migrated_rows,
        error_count,
    )
    if null_char_replacements:
        logger.warning(
            "è¡¨ %s ä¸­ %d ä¸ªå­—ç¬¦ä¸²å€¼åŒ…å« NUL å·²è¢«ç§»é™¤åå†™å…¥ç›®æ ‡åº“",
            source_table.name,
            null_char_replacements,
        )
    if conversion_warnings:
        logger.warning(
            "è¡¨ %s ä¸­ %d ä¸ªå€¼å‘ç”Ÿç±»å‹è½¬æ¢è­¦å‘Š",
            source_table.name,
            conversion_warnings,
        )

    return migrated_rows, error_count


def confirm_action(prompt: str, default: bool = False) -> bool:
    """ç¡®è®¤æ“ä½œ

    Args:
        prompt: æç¤ºä¿¡æ¯
        default: é»˜è®¤å€¼

    Returns:
        bool: ç”¨æˆ·æ˜¯å¦ç¡®è®¤
    """
    while True:
        if default:
            choice = input(f"{prompt} [Y/n]: ").strip().lower()
            if choice == "":
                return True
        else:
            choice = input(f"{prompt} [y/N]: ").strip().lower()
            if choice == "":
                return False

        if choice in ("y", "yes"):
            return True
        elif choice in ("n", "no"):
            return False
        else:
            print("è¯·è¾“å…¥ y æˆ– n")


# =============================================================================
# è¿ç§»å™¨å®ç°
# =============================================================================


class DatabaseMigrator:
    """é€šç”¨æ•°æ®åº“è¿ç§»å™¨"""

    def __init__(
        self,
        source_type: str,
        target_type: str,
        batch_size: int = 1000,
        source_config: dict | None = None,
        target_config: dict | None = None,
        skip_tables: set | None = None,
        only_tables: set | None = None,
        no_create_tables: bool = False,
    ):
        """åˆå§‹åŒ–è¿ç§»å™¨

        Args:
            source_type: æºæ•°æ®åº“ç±»å‹
            target_type: ç›®æ ‡æ•°æ®åº“ç±»å‹
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            source_config: æºæ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
            target_config: ç›®æ ‡æ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼‰
            skip_tables: è¦è·³è¿‡çš„è¡¨åé›†åˆ
            only_tables: åªè¿ç§»çš„è¡¨åé›†åˆï¼ˆè®¾ç½®åå¿½ç•¥ skip_tablesï¼‰
            no_create_tables: æ˜¯å¦è·³è¿‡åˆ›å»ºè¡¨ç»“æ„ï¼ˆå‡è®¾ç›®æ ‡è¡¨å·²å­˜åœ¨ï¼‰
        """
        self.source_type = source_type.lower()
        self.target_type = target_type.lower()
        self.batch_size = batch_size
        self.source_config = source_config
        self.target_config = target_config
        self.skip_tables = skip_tables or set()
        self.only_tables = only_tables or set()
        self.no_create_tables = no_create_tables

        self._validate_database_types()

        self.source_engine: Any = None
        self.target_engine: Any = None
        self.metadata = MetaData()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "tables_migrated": 0,
            "rows_migrated": 0,
            "errors": [],
            "start_time": None,
            "end_time": None,
        }

    def _validate_database_types(self):
        """éªŒè¯æ•°æ®åº“ç±»å‹"""
        supported_types = {"sqlite", "mysql", "postgresql"}
        if self.source_type not in supported_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æºæ•°æ®åº“ç±»å‹: {self.source_type}")
        if self.target_type not in supported_types:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡æ•°æ®åº“ç±»å‹: {self.target_type}")

    def _load_source_config(self) -> dict:
        """åŠ è½½æºæ•°æ®åº“é…ç½®

        å¦‚æœåˆå§‹åŒ–æ—¶æä¾›äº† source_configï¼Œåˆ™ç›´æ¥ä½¿ç”¨ï¼›
        å¦åˆ™ä» bot_config.toml ä¸­è¯»å–ã€‚
        """
        if self.source_config:
            logger.info("ä½¿ç”¨ä¼ å…¥çš„æºæ•°æ®åº“é…ç½®")
            return self.source_config

        logger.info("æœªæä¾›æºæ•°æ®åº“é…ç½®ï¼Œå°è¯•ä» bot_config.toml è¯»å–")
        config = get_database_config_from_toml(self.source_type)
        if not config:
            raise ValueError("æ— æ³•ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æºæ•°æ®åº“é…ç½®ï¼Œè¯·æ£€æŸ¥ config/bot_config.toml")

        logger.info("æˆåŠŸä»é…ç½®æ–‡ä»¶è¯»å–æºæ•°æ®åº“é…ç½®")
        return config

    def _load_target_config(self) -> dict:
        """åŠ è½½ç›®æ ‡æ•°æ®åº“é…ç½®

        ç›®æ ‡æ•°æ®åº“é…ç½®å¿…é¡»é€šè¿‡åˆå§‹åŒ–å‚æ•°æä¾›ï¼Œæˆ–è€…é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ„å»ºã€‚
        """
        if not self.target_config:
            raise ValueError("æœªæä¾›ç›®æ ‡æ•°æ®åº“é…ç½®ï¼Œè¯·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæˆ–åœ¨äº¤äº’æ¨¡å¼ä¸­è¾“å…¥")
        logger.info("ä½¿ç”¨ä¼ å…¥çš„ç›®æ ‡æ•°æ®åº“é…ç½®")
        return self.target_config

    def _connect_databases(self):
        """è¿æ¥æºæ•°æ®åº“å’Œç›®æ ‡æ•°æ®åº“"""
        # æºæ•°æ®åº“é…ç½®
        source_config = self._load_source_config()
        # ç›®æ ‡æ•°æ®åº“é…ç½®
        target_config = self._load_target_config()

        # é˜²æ­¢æº/ç›®æ ‡ SQLite æŒ‡å‘åŒä¸€è·¯å¾„å¯¼è‡´è‡ªæˆ‘è¦†ç›–åŠé”
        if (
            self.source_type == "sqlite"
            and self.target_type == "sqlite"
            and os.path.abspath(source_config.get("path", "")) == os.path.abspath(target_config.get("path", ""))
        ):
            raise ValueError("æºæ•°æ®åº“ä¸ç›®æ ‡æ•°æ®åº“ä¸èƒ½æ˜¯åŒä¸€ä¸ª SQLite æ–‡ä»¶ï¼Œè¯·ä¸ºç›®æ ‡æŒ‡å®šä¸åŒçš„è·¯å¾„")

        # åˆ›å»ºå¼•æ“
        self.source_engine = create_engine_by_type(self.source_type, source_config)
        self.target_engine = create_engine_by_type(self.target_type, target_config)

        # åå°„æºæ•°æ®åº“å…ƒæ•°æ®
        logger.info("æ­£åœ¨åå°„æºæ•°æ®åº“å…ƒæ•°æ®...")
        self.metadata.reflect(bind=self.source_engine)
        logger.info("å‘ç° %d å¼ è¡¨: %s", len(self.metadata.tables), ", ".join(self.metadata.tables.keys()))

    def _get_tables_in_dependency_order(self) -> list[Table]:
        """è·å–æŒ‰ä¾èµ–é¡ºåºæ’åºçš„è¡¨åˆ—è¡¨

        ä¸ºäº†é¿å…å¤–é”®çº¦æŸé—®é¢˜ï¼Œåˆ›å»ºè¡¨æ—¶éœ€è¦æŒ‰ç…§ä¾èµ–é¡ºåºï¼Œ
        ä¾‹å¦‚å…ˆåˆ›å»ºè¢«å¼•ç”¨çš„è¡¨ï¼Œå†åˆ›å»ºå¼•ç”¨å®ƒä»¬çš„è¡¨ã€‚
        """
        inspector = inspect(self.source_engine)

        # æ„å»ºä¾èµ–å›¾ï¼štable -> set(dependent_tables)
        dependencies: dict[str, set[str]] = {}
        for table_name in self.metadata.tables:
            dependencies[table_name] = set()

        for table_name, table in self.metadata.tables.items():
            fks = inspector.get_foreign_keys(table_name)
            for fk in fks:
                # è¢«å¼•ç”¨çš„è¡¨
                referred_table = fk["referred_table"]
                if referred_table in dependencies:
                    dependencies[table_name].add(referred_table)

        # æ‹“æ‰‘æ’åº
        sorted_tables: list[Table] = []
        visited: set[str] = set()
        temp_mark: set[str] = set()

        def visit(table_name: str):
            if table_name in visited:
                return
            if table_name in temp_mark:
                logger.warning("æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œè¡¨: %s", table_name)
                return
            temp_mark.add(table_name)
            for dep in dependencies[table_name]:
                visit(dep)
            temp_mark.remove(table_name)
            visited.add(table_name)
            sorted_tables.append(self.metadata.tables[table_name])

        for table_name in dependencies:
            if table_name not in visited:
                visit(table_name)

        return sorted_tables

    def _drop_target_tables(self):
        """åˆ é™¤ç›®æ ‡æ•°æ®åº“ä¸­å·²æœ‰çš„è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰

        ä½¿ç”¨ Engine.begin() è¿›è¡Œè¿æ¥ä»¥æ”¯æŒ autobegin å’Œ begin å…¼å®¹ SQLAlchemy 2.0 çš„å†™æ³•
        """
        if self.target_engine is None:
            logger.warning("ç›®æ ‡æ•°æ®åº“å¼•æ“å°šæœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤è¡¨")
            return

        with self.target_engine.begin() as conn:
            inspector = inspect(conn)
            existing_tables = inspector.get_table_names()

            if not existing_tables:
                logger.info("ç›®æ ‡æ•°æ®åº“ä¸­æ²¡æœ‰å·²å­˜åœ¨çš„è¡¨ï¼Œæ— éœ€åˆ é™¤")
                return

            logger.info("ç›®æ ‡æ•°æ®åº“ä¸­çš„å½“å‰è¡¨: %s", ", ".join(existing_tables))
            if confirm_action("æ˜¯å¦åˆ é™¤ç›®æ ‡æ•°æ®åº“ä¸­ç°æœ‰çš„è¡¨åˆ—è¡¨ï¼Ÿæ­¤æ“ä½œä¸å¯æ’¤é”€", default=False):
                for table_name in existing_tables:
                    try:
                        logger.info("åˆ é™¤ç›®æ ‡æ•°æ®åº“è¡¨: %s", table_name)
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
                    except SQLAlchemyError as e:
                        logger.error("åˆ é™¤ %s å¤±è´¥: %s", table_name, e)
                        self.stats["errors"].append(
                            f"åˆ é™¤ {table_name} å¤±è´¥: {e}"
                        )
            else:
                logger.info("è·³è¿‡åˆ é™¤ç›®æ ‡æ•°æ®åº“ä¸­çš„è¡¨ï¼Œç»§ç»­è¿ç§»è¿‡ç¨‹")

    def migrate(self):
        """æ‰§è¡Œè¿ç§»æ“ä½œ"""
        import time

        self.stats["start_time"] = time.time()

        # è¿æ¥æ•°æ®åº“
        self._connect_databases()

        # è·å–è¡¨çš„ä¾èµ–é¡ºåº
        tables = self._get_tables_in_dependency_order()
        logger.info("æŒ‰ä¾èµ–é¡ºåºè¿ç§»è¡¨: %s", ", ".join(t.name for t in tables))

        # å¦‚æœæŒ‡å®šäº† only_tablesï¼Œåˆ™è¿‡æ»¤è¡¨åˆ—è¡¨
        if self.only_tables:
            tables = [t for t in tables if t.name in self.only_tables]
            logger.info("åªè¿ç§»æŒ‡å®šçš„è¡¨: %s", ", ".join(t.name for t in tables))
            if not tables:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é… --only-tables çš„è¡¨")
                return

        # åˆ é™¤ç›®æ ‡åº“ä¸­å·²æœ‰è¡¨ï¼ˆå¯é€‰ï¼‰- å¦‚æœæ˜¯å¢é‡è¿ç§»åˆ™è·³è¿‡
        if not self.no_create_tables:
            self._drop_target_tables()

        # è·å–ç›®æ ‡æ•°æ®åº“æ–¹è¨€
        target_dialect = self.target_engine.dialect.name

        # å¼€å§‹è¿ç§»
        with self.source_engine.connect() as source_conn:
            for source_table in tables:
                # è·³è¿‡æŒ‡å®šçš„è¡¨ï¼ˆä»…åœ¨æœªæŒ‡å®š only_tables æ—¶ç”Ÿæ•ˆï¼‰
                if not self.only_tables and source_table.name in self.skip_tables:
                    logger.info("è·³è¿‡è¡¨: %s (åœ¨ skip_tables åˆ—è¡¨ä¸­)", source_table.name)
                    continue

                try:
                    # åœ¨ç›®æ ‡åº“ä¸­åˆ›å»ºè¡¨ç»“æ„ï¼ˆé™¤éæŒ‡å®šäº† no_create_tablesï¼‰
                    if self.no_create_tables:
                        # åå°„ç›®æ ‡æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è¡¨ç»“æ„
                        target_metadata = MetaData()
                        target_metadata.reflect(bind=self.target_engine, only=[source_table.name])
                        target_table = target_metadata.tables.get(source_table.name)
                        if target_table is None:
                            logger.error("ç›®æ ‡æ•°æ®åº“ä¸­ä¸å­˜åœ¨è¡¨: %sï¼Œè¯·å…ˆåˆ›å»ºè¡¨ç»“æ„æˆ–ç§»é™¤ --no-create-tables å‚æ•°", source_table.name)
                            self.stats["errors"].append(f"ç›®æ ‡æ•°æ®åº“ä¸­ä¸å­˜åœ¨è¡¨: {source_table.name}")
                            continue
                        logger.info("ä½¿ç”¨ç›®æ ‡æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è¡¨ç»“æ„: %s", source_table.name)
                    else:
                        target_table = copy_table_structure(source_table, MetaData(), self.target_engine)

                    # å¯¹ messages è¡¨é™åˆ¶è¿ç§»è¡Œæ•°ï¼ˆåªè¿ç§»æœ€æ–° 1 ä¸‡æ¡ï¼‰
                    row_limit = None
                    if source_table.name == "messages":
                        row_limit = 10000
                        logger.info("messages è¡¨å°†åªè¿ç§»æœ€æ–° %d æ¡è®°å½•", row_limit)

                    # æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹äº‹åŠ¡ï¼Œä¼ å…¥ engine è€Œä¸æ˜¯ connection
                    migrated_rows, error_count = migrate_table_data(
                        source_conn,
                        self.target_engine,
                        source_table,
                        target_table,
                        batch_size=self.batch_size,
                        target_dialect=target_dialect,
                        row_limit=row_limit,
                    )

                    self.stats["tables_migrated"] += 1
                    self.stats["rows_migrated"] += migrated_rows
                    if error_count > 0:
                        self.stats["errors"].append(
                            f"è¡¨ {source_table.name} è¿ç§»å¤±è´¥ {error_count} è¡Œ"
                        )

                except Exception as e:
                    logger.error("è¿ç§»è¡¨ %s æ—¶å‘ç”Ÿé”™è¯¯: %s", source_table.name, e)
                    self.stats["errors"].append(f"è¡¨ {source_table.name} è¿ç§»å¤±è´¥: {e}")

        self.stats["end_time"] = time.time()
        
        # è¿ç§»å®Œæˆåï¼Œè‡ªåŠ¨ä¿®å¤ PostgreSQL ç‰¹æœ‰é—®é¢˜
        if self.target_type == "postgresql" and self.target_engine:
            fix_postgresql_boolean_columns(self.target_engine)
            fix_postgresql_sequences(self.target_engine)

    def print_summary(self):
        """æ‰“å°è¿ç§»æ€»ç»“"""
        import time

        duration = None
        if self.stats["start_time"] is not None and self.stats["end_time"] is not None:
            duration = self.stats["end_time"] - self.stats["start_time"]

        print("\n" + "=" * 60)
        print("è¿ç§»å®Œæˆï¼")
        print(f"  è¿ç§»è¡¨æ•°é‡: {self.stats['tables_migrated']}")
        print(f"  è¿ç§»è¡Œæ•°é‡: {self.stats['rows_migrated']}")
        if duration is not None:
            print(f"  æ€»è€—æ—¶: {duration:.2f} ç§’")
        if self.stats["errors"]:
            print("  âš ï¸ å‘ç”Ÿé”™è¯¯:")
            for err in self.stats["errors"]:
                print(f"    - {err}")
        else:
            print("  æ²¡æœ‰å‘ç”Ÿé”™è¯¯ ğŸ‰")
        print("=" * 60 + "\n")

    def run(self):
        """è¿è¡Œè¿ç§»å¹¶æ‰“å°æ€»ç»“"""
        self.migrate()
        self.print_summary()
        return self.stats


# =============================================================================
# å‘½ä»¤è¡Œå‚æ•°è§£æ
# =============================================================================


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ•°æ®åº“è¿ç§»å·¥å…· - åœ¨ SQLiteã€MySQLã€PostgreSQL ä¹‹é—´è¿ç§»æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ç¤ºä¾‹:
  # ä» SQLite è¿ç§»åˆ° PostgreSQL
  python scripts/migrate_database.py \
    --source sqlite \
    --target postgresql \
    --target-host localhost \
    --target-port 5432 \
    --target-database maibot \
    --target-user postgres \
    --target-password your_password

  # ä» SQLite è¿ç§»åˆ° MySQL
  python scripts/migrate_database.py \
    --source sqlite \
    --target mysql \
    --target-host localhost \
    --target-port 3306 \
    --target-database maibot \
    --target-user root \
    --target-password your_password

  # ä½¿ç”¨äº¤äº’å¼å‘å¯¼æ¨¡å¼ï¼ˆæ¨èï¼‰
  python scripts/migrate_database.py
  python scripts/migrate_database.py --interactive
        """,
    )

    # åŸºæœ¬å‚æ•°
    parser.add_argument(
        "--source",
        type=str,
        choices=["sqlite", "mysql", "postgresql"],
        help="æºæ•°æ®åº“ç±»å‹ï¼ˆä¸æŒ‡å®šæ—¶ï¼Œåœ¨äº¤äº’æ¨¡å¼ä¸­é€‰æ‹©ï¼‰",
    )
    parser.add_argument(
        "--target",
        type=str,
        choices=["sqlite", "mysql", "postgresql"],
        help="ç›®æ ‡æ•°æ®åº“ç±»å‹ï¼ˆä¸æŒ‡å®šæ—¶ï¼Œåœ¨äº¤äº’æ¨¡å¼ä¸­é€‰æ‹©ï¼‰",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰",
    )

    parser.add_argument(
        "--interactive",
        action="store_true",
        help="å¯ç”¨äº¤äº’å¼å‘å¯¼æ¨¡å¼ï¼ˆæ¨èï¼šç›´æ¥è¿è¡Œè„šæœ¬æˆ–åŠ ä¸Šæ­¤å‚æ•°ï¼‰",
    )

    # æºæ•°æ®åº“å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä» bot_config.toml è¯»å–ï¼‰
    source_group = parser.add_argument_group("æºæ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä» bot_config.toml è¯»å–ï¼‰")
    source_group.add_argument("--source-path", type=str, help="SQLite æ•°æ®åº“è·¯å¾„")
    source_group.add_argument("--source-host", type=str, help="MySQL/PostgreSQL ä¸»æœº")
    source_group.add_argument("--source-port", type=int, help="MySQL/PostgreSQL ç«¯å£")
    source_group.add_argument("--source-database", type=str, help="æ•°æ®åº“å")
    source_group.add_argument("--source-user", type=str, help="ç”¨æˆ·å")
    source_group.add_argument("--source-password", type=str, help="å¯†ç ")

    # ç›®æ ‡æ•°æ®åº“å‚æ•°
    target_group = parser.add_argument_group("ç›®æ ‡æ•°æ®åº“é…ç½®")
    target_group.add_argument("--target-path", type=str, help="SQLite æ•°æ®åº“è·¯å¾„")
    target_group.add_argument("--target-host", type=str, help="MySQL/PostgreSQL ä¸»æœº")
    target_group.add_argument("--target-port", type=int, help="MySQL/PostgreSQL ç«¯å£")
    target_group.add_argument("--target-database", type=str, help="æ•°æ®åº“å")
    target_group.add_argument("--target-user", type=str, help="ç”¨æˆ·å")
    target_group.add_argument("--target-password", type=str, help="å¯†ç ")
    target_group.add_argument("--target-schema", type=str, default="public", help="PostgreSQL schema")
    target_group.add_argument("--target-charset", type=str, default="utf8mb4", help="MySQL å­—ç¬¦é›†")

    # è·³è¿‡è¡¨å‚æ•°
    parser.add_argument(
        "--skip-tables",
        type=str,
        default="",
        help="è·³è¿‡è¿ç§»çš„è¡¨åï¼Œå¤šä¸ªè¡¨åç”¨é€—å·åˆ†éš”ï¼ˆå¦‚: messages,logsï¼‰",
    )

    # åªè¿ç§»æŒ‡å®šè¡¨å‚æ•°
    parser.add_argument(
        "--only-tables",
        type=str,
        default="",
        help="åªè¿ç§»æŒ‡å®šçš„è¡¨åï¼Œå¤šä¸ªè¡¨åç”¨é€—å·åˆ†éš”ï¼ˆå¦‚: user_relationships,maizone_schedule_statusï¼‰ã€‚è®¾ç½®åå°†å¿½ç•¥ --skip-tables",
    )

    # ä¸åˆ›å»ºè¡¨ç»“æ„ï¼Œå‡è®¾ç›®æ ‡è¡¨å·²å­˜åœ¨
    parser.add_argument(
        "--no-create-tables",
        action="store_true",
        help="ä¸åˆ›å»ºè¡¨ç»“æ„ï¼Œå‡è®¾ç›®æ ‡æ•°æ®åº“ä¸­çš„è¡¨å·²å­˜åœ¨ã€‚ç”¨äºå¢é‡è¿ç§»æŒ‡å®šè¡¨çš„æ•°æ®",
    )

    return parser.parse_args()


def build_config_from_args(args, prefix: str, db_type: str) -> dict | None:
    """ä»å‘½ä»¤è¡Œå‚æ•°æ„å»ºé…ç½®

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        prefix: å‚æ•°å‰ç¼€ ("source" æˆ– "target")
        db_type: æ•°æ®åº“ç±»å‹

    Returns:
        é…ç½®å­—å…¸æˆ– None
    """
    if db_type == "sqlite":
        path = getattr(args, f"{prefix}_path", None)
        if path:
            return {"path": path}
        return None

    elif db_type in ("mysql", "postgresql"):
        host = getattr(args, f"{prefix}_host", None)
        if not host:
            return None

        config = {
            "host": host,
            "port": getattr(args, f"{prefix}_port") or (3306 if db_type == "mysql" else 5432),
            "database": getattr(args, f"{prefix}_database") or "maibot",
            "user": getattr(args, f"{prefix}_user") or ("root" if db_type == "mysql" else "postgres"),
            "password": getattr(args, f"{prefix}_password") or "",
        }

        if db_type == "mysql":
            config["charset"] = getattr(args, f"{prefix}_charset", "utf8mb4")
        elif db_type == "postgresql":
            config["schema"] = getattr(args, f"{prefix}_schema", "public")

        return config

    return None


def _ask_choice(prompt: str, options: list[str], default_index: int | None = None) -> str:
    """åœ¨æ§åˆ¶å°ä¸­è®©ç”¨æˆ·ä»å¤šä¸ªé€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ª"""
    while True:
        print()
        print(prompt)
        for i, opt in enumerate(options, start=1):
            default_mark = ""
            if default_index is not None and i - 1 == default_index:
                default_mark = "  (é»˜è®¤)"
            print(f"  {i}) {opt}{default_mark}")
        ans = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·: ").strip()
        if not ans and default_index is not None:
            return options[default_index]
        if ans.isdigit():
            idx = int(ans)
            if 1 <= idx <= len(options):
                return options[idx - 1]
        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")


def _ask_int(prompt: str, default: int | None = None) -> int:
    """åœ¨æ§åˆ¶å°ä¸­è¾“å…¥æ­£æ•´æ•°"""
    while True:
        suffix = f" (é»˜è®¤ {default})" if default is not None else ""
        raw = input(f"{prompt}{suffix}: ").strip()
        if not raw and default is not None:
            return default
        try:
            value = int(raw)
            if value <= 0:
                raise ValueError()
            return value
        except ValueError:
            print("âŒ è¯·è¾“å…¥ä¸€ä¸ªå¤§äº 0 çš„æ•´æ•°ã€‚")


def _ask_str(
    prompt: str,
    default: str | None = None,
    allow_empty: bool = False,
    is_password: bool = False,
) -> str:
    """åœ¨æ§åˆ¶å°ä¸­è¾“å…¥å­—ç¬¦ä¸²ï¼Œå¯é€‰é»˜è®¤å€¼/å¯†ç è¾“å…¥"""
    while True:
        suffix = f" (é»˜è®¤: {default})" if default is not None else ""
        full_prompt = f"{prompt}{suffix}: "
        raw = getpass(full_prompt) if is_password else input(full_prompt)
        raw = raw.strip()
        if not raw:
            if default is not None:
                return default
            if allow_empty:
                return ""
            print("âŒ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
            continue
        return raw


def interactive_setup() -> dict:
    """äº¤äº’å¼å‘å¯¼ï¼Œè¿”å›ç”¨äºåˆå§‹åŒ– DatabaseMigrator çš„å‚æ•°å­—å…¸"""
    print("=" * 60)
    print("ğŸŒŸ æ•°æ®åº“è¿ç§»å‘å¯¼")
    print("åªéœ€å›ç­”å‡ ä¸ªé—®é¢˜ï¼Œæˆ‘ä¼šå¸®ä½ æ„é€ è¿ç§»é…ç½®ã€‚")
    print("=" * 60)

    db_types = ["sqlite", "mysql", "postgresql"]

    # é€‰æ‹©æºæ•°æ®åº“
    source_type = _ask_choice("è¯·é€‰æ‹©ã€æºæ•°æ®åº“ç±»å‹ã€‘:", db_types, default_index=0)

    # é€‰æ‹©ç›®æ ‡æ•°æ®åº“ï¼ˆä¸èƒ½ä¸æºç›¸åŒï¼‰
    while True:
        default_idx = 2 if len(db_types) >= 3 else 0
        target_type = _ask_choice("è¯·é€‰æ‹©ã€ç›®æ ‡æ•°æ®åº“ç±»å‹ã€‘:", db_types, default_index=default_idx)
        if target_type != source_type:
            break
        print("âŒ ç›®æ ‡æ•°æ®åº“ä¸èƒ½å’Œæºæ•°æ®åº“ç›¸åŒï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")

    # æ‰¹é‡å¤§å°
    batch_size = _ask_int("è¯·è¾“å…¥æ‰¹é‡å¤§å° batch-size", default=1000)

    # æºæ•°æ®åº“é…ç½®ï¼šé»˜è®¤ä½¿ç”¨ bot_config.toml
    print()
    print("æºæ•°æ®åº“é…ç½®ï¼š")
    print("  é»˜è®¤ä¼šä» config/bot_config.toml ä¸­è¯»å–å¯¹åº”é…ç½®ã€‚")
    use_default_source = input("æ˜¯å¦ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ã€æºæ•°æ®åº“ã€‘é…ç½®? [Y/n]: ").strip().lower()
    if use_default_source in ("", "y", "yes"):
        source_config = None  # è®© DatabaseMigrator è‡ªå·±å»è¯»é…ç½®
    else:
        # ç®€å•äº¤äº’å¼é…ç½®æºæ•°æ®åº“
        print("è¯·æ‰‹åŠ¨è¾“å…¥æºæ•°æ®åº“è¿æ¥ä¿¡æ¯ï¼š")
        if source_type == "sqlite":
            source_path = _ask_str("æº SQLite æ–‡ä»¶è·¯å¾„", default="data/MaiBot.db")
            source_config = {"path": source_path}
        else:
            port_default = 3306 if source_type == "mysql" else 5432
            user_default = "root" if source_type == "mysql" else "postgres"
            host = _ask_str("æºæ•°æ®åº“ host", default="localhost")
            port = _ask_int("æºæ•°æ®åº“ port", default=port_default)
            database = _ask_str("æºæ•°æ®åº“å", default="maibot")
            user = _ask_str("æºæ•°æ®åº“ç”¨æˆ·å", default=user_default)
            password = _ask_str("æºæ•°æ®åº“å¯†ç ï¼ˆè¾“å…¥æ—¶ä¸å›æ˜¾ï¼‰", default="", is_password=True)
            source_config = {
                "host": host,
                "port": port,
                "database": database,
                "user": user,
                "password": password,
            }
            if source_type == "mysql":
                source_config["charset"] = _ask_str("æºæ•°æ®åº“å­—ç¬¦é›†", default="utf8mb4")
            elif source_type == "postgresql":
                source_config["schema"] = _ask_str("æºæ•°æ®åº“ schema", default="public")

    # ç›®æ ‡æ•°æ®åº“é…ç½®ï¼ˆå¿…é¡»æ˜¾å¼ç¡®è®¤ï¼‰
    print()
    print("ç›®æ ‡æ•°æ®åº“é…ç½®ï¼š")
    if target_type == "sqlite":
        target_path = _ask_str(
            "ç›®æ ‡ SQLite æ–‡ä»¶è·¯å¾„ï¼ˆè‹¥ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰",
            default="data/MaiBot.db",
        )
        target_config = {"path": target_path}
    else:
        port_default = 3306 if target_type == "mysql" else 5432
        user_default = "root" if target_type == "mysql" else "postgres"
        host = _ask_str("ç›®æ ‡æ•°æ®åº“ host", default="localhost")
        port = _ask_int("ç›®æ ‡æ•°æ®åº“ port", default=port_default)
        database = _ask_str("ç›®æ ‡æ•°æ®åº“å", default="maibot")
        user = _ask_str("ç›®æ ‡æ•°æ®åº“ç”¨æˆ·å", default=user_default)
        password = _ask_str("ç›®æ ‡æ•°æ®åº“å¯†ç ï¼ˆè¾“å…¥æ—¶ä¸å›æ˜¾ï¼‰", default="", is_password=True)

        target_config = {
            "host": host,
            "port": port,
            "database": database,
            "user": user,
            "password": password,
        }
        if target_type == "mysql":
            target_config["charset"] = _ask_str("ç›®æ ‡æ•°æ®åº“å­—ç¬¦é›†", default="utf8mb4")
        elif target_type == "postgresql":
            target_config["schema"] = _ask_str("ç›®æ ‡æ•°æ®åº“ schema", default="public")

    print()
    print("=" * 60)
    print("è¿ç§»é…ç½®ç¡®è®¤ï¼š")
    print(f"  æºæ•°æ®åº“ç±»å‹: {source_type}")
    print(f"  ç›®æ ‡æ•°æ®åº“ç±»å‹: {target_type}")
    print(f"  æ‰¹é‡å¤§å°: {batch_size}")
    print("âš ï¸ è¯·ç¡®è®¤ç›®æ ‡æ•°æ®åº“ä¸ºç©ºæˆ–å¯ä»¥è¢«è¦†ç›–ï¼Œå¹¶ä¸”å·²å¤‡ä»½æºæ•°æ®åº“ã€‚")
    confirm = input("æ˜¯å¦å¼€å§‹è¿ç§»ï¼Ÿ[Y/n]: ").strip().lower()
    if confirm not in ("", "y", "yes"):
        print("å·²å–æ¶ˆè¿ç§»ã€‚")
        sys.exit(0)

    return {
        "source_type": source_type,
        "target_type": target_type,
        "batch_size": batch_size,
        "source_config": source_config,
        "target_config": target_config,
    }


def fix_postgresql_sequences(engine: Engine):
    """ä¿®å¤ PostgreSQL åºåˆ—å€¼
    
    è¿ç§»æ•°æ®åï¼ŒPostgreSQL çš„åºåˆ—ï¼ˆç”¨äºè‡ªå¢ä¸»é”®ï¼‰å¯èƒ½æ²¡æœ‰æ›´æ–°åˆ°æ­£ç¡®çš„å€¼ï¼Œ
    å¯¼è‡´æ’å…¥æ–°è®°å½•æ—¶å‡ºç°ä¸»é”®å†²çªã€‚æ­¤å‡½æ•°ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶é‡ç½®æ‰€æœ‰åºåˆ—ã€‚
    
    Args:
        engine: PostgreSQL æ•°æ®åº“å¼•æ“
    """
    if engine.dialect.name != "postgresql":
        logger.info("é PostgreSQL æ•°æ®åº“ï¼Œè·³è¿‡åºåˆ—ä¿®å¤")
        return
    
    logger.info("æ­£åœ¨ä¿®å¤ PostgreSQL åºåˆ—...")
    
    with engine.connect() as conn:
        # è·å–æ‰€æœ‰å¸¦æœ‰åºåˆ—çš„è¡¨
        result = conn.execute(text('''
            SELECT 
                t.table_name,
                c.column_name,
                pg_get_serial_sequence(t.table_name, c.column_name) as sequence_name
            FROM information_schema.tables t
            JOIN information_schema.columns c 
                ON t.table_name = c.table_name AND t.table_schema = c.table_schema
            WHERE t.table_schema = 'public' 
            AND t.table_type = 'BASE TABLE'
            AND c.column_default LIKE 'nextval%'
            ORDER BY t.table_name
        '''))
        
        sequences = result.fetchall()
        logger.info("å‘ç° %d ä¸ªå¸¦åºåˆ—çš„è¡¨", len(sequences))
        
        fixed_count = 0
        for table_name, column_name, seq_name in sequences:
            if seq_name:
                try:
                    # è·å–å½“å‰è¡¨ä¸­è¯¥åˆ—çš„æœ€å¤§å€¼
                    max_result = conn.execute(text(f'SELECT COALESCE(MAX({column_name}), 0) FROM {table_name}'))
                    max_val = max_result.scalar()
                    
                    # è®¾ç½®åºåˆ—çš„ä¸‹ä¸€ä¸ªå€¼
                    next_val = max_val + 1
                    conn.execute(text(f"SELECT setval('{seq_name}', {next_val}, false)"))
                    conn.commit()
                    
                    logger.info("  âœ… %s.%s: æœ€å¤§å€¼=%d, åºåˆ—è®¾ä¸º=%d", table_name, column_name, max_val, next_val)
                    fixed_count += 1
                except Exception as e:
                    logger.warning("  âŒ %s.%s: ä¿®å¤å¤±è´¥ - %s", table_name, column_name, e)
        
        logger.info("åºåˆ—ä¿®å¤å®Œæˆï¼å…±ä¿®å¤ %d ä¸ªåºåˆ—", fixed_count)


def fix_postgresql_boolean_columns(engine: Engine):
    """ä¿®å¤ PostgreSQL å¸ƒå°”åˆ—ç±»å‹
    
    ä» SQLite è¿ç§»åï¼Œå¸ƒå°”åˆ—å¯èƒ½æ˜¯ INTEGER ç±»å‹ã€‚æ­¤å‡½æ•°å°†å…¶è½¬æ¢ä¸º BOOLEANã€‚
    
    Args:
        engine: PostgreSQL æ•°æ®åº“å¼•æ“
    """
    if engine.dialect.name != "postgresql":
        logger.info("é PostgreSQL æ•°æ®åº“ï¼Œè·³è¿‡å¸ƒå°”åˆ—ä¿®å¤")
        return
    
    # å·²çŸ¥éœ€è¦è½¬æ¢ä¸º BOOLEAN çš„åˆ—
    BOOLEAN_COLUMNS = {
        'messages': ['is_mentioned', 'is_emoji', 'is_picid', 'is_command', 
                     'is_notify', 'is_public_notice', 'should_reply', 'should_act'],
        'action_records': ['action_done', 'action_build_into_prompt'],
    }
    
    logger.info("æ­£åœ¨æ£€æŸ¥å¹¶ä¿®å¤ PostgreSQL å¸ƒå°”åˆ—...")
    
    with engine.connect() as conn:
        fixed_count = 0
        for table_name, columns in BOOLEAN_COLUMNS.items():
            for col_name in columns:
                try:
                    # æ£€æŸ¥å½“å‰ç±»å‹
                    result = conn.execute(text(f'''
                        SELECT data_type FROM information_schema.columns 
                        WHERE table_name = '{table_name}' AND column_name = '{col_name}'
                    '''))
                    row = result.fetchone()
                    if row and row[0] != 'boolean':
                        # éœ€è¦ä¿®å¤
                        conn.execute(text(f'''
                            ALTER TABLE {table_name} 
                            ALTER COLUMN {col_name} TYPE BOOLEAN 
                            USING CASE WHEN {col_name} = 0 THEN FALSE ELSE TRUE END
                        '''))
                        conn.commit()
                        logger.info("  âœ… %s.%s: %s -> BOOLEAN", table_name, col_name, row[0])
                        fixed_count += 1
                except Exception as e:
                    logger.warning("  âš ï¸ %s.%s: æ£€æŸ¥/ä¿®å¤å¤±è´¥ - %s", table_name, col_name, e)
        
        if fixed_count > 0:
            logger.info("å¸ƒå°”åˆ—ä¿®å¤å®Œæˆï¼å…±ä¿®å¤ %d åˆ—", fixed_count)
        else:
            logger.info("æ‰€æœ‰å¸ƒå°”åˆ—ç±»å‹æ­£ç¡®ï¼Œæ— éœ€ä¿®å¤")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # å¦‚æœæ²¡æœ‰ä»»ä½•å‚æ•°ï¼Œæˆ–è€…æ˜¾å¼æŒ‡å®š --interactiveï¼Œåˆ™è¿›å…¥äº¤äº’æ¨¡å¼
    if args.interactive or len(sys.argv) == 1:
        params = interactive_setup()
        try:
            migrator = DatabaseMigrator(**params)
            stats = migrator.run()
            if stats["errors"]:
                sys.exit(1)
            return
        except KeyboardInterrupt:
            print("\nè¿ç§»è¢«ç”¨æˆ·ä¸­æ–­")
            sys.exit(130)
        except Exception as e:
            print(f"è¿ç§»å¤±è´¥: {e}")
            sys.exit(1)

    # éäº¤äº’æ¨¡å¼ï¼šä¿æŒåŸæœ‰è¡Œä¸ºï¼Œä½†å¦‚æœæ²¡ç»™ source/targetï¼Œå°±æç¤ºé”™è¯¯
    if not args.source or not args.target:
        print("é”™è¯¯: éäº¤äº’æ¨¡å¼ä¸‹å¿…é¡»æŒ‡å®š --source å’Œ --targetã€‚")
        print("ä½ ä¹Ÿå¯ä»¥ç›´æ¥è¿è¡Œè„šæœ¬æˆ–æ·»åŠ  --interactive ä½¿ç”¨äº¤äº’å¼å‘å¯¼ã€‚")
        sys.exit(2)

    # æ„å»ºé…ç½®
    source_config = build_config_from_args(args, "source", args.source)
    target_config = build_config_from_args(args, "target", args.target)

    # éªŒè¯ç›®æ ‡é…ç½®
    if target_config is None:
        if args.target == "sqlite":
            if not args.target_path:
                print("é”™è¯¯: ç›®æ ‡æ•°æ®åº“ä¸º SQLite æ—¶ï¼Œå¿…é¡»æŒ‡å®š --target-pathï¼ˆæˆ–ä½¿ç”¨äº¤äº’æ¨¡å¼ï¼‰")
                sys.exit(1)
            target_config = {"path": args.target_path}
        else:
            if not args.target_host:
                print(f"é”™è¯¯: ç›®æ ‡æ•°æ®åº“ä¸º {args.target} æ—¶ï¼Œå¿…é¡»æŒ‡å®š --target-hostï¼ˆæˆ–ä½¿ç”¨äº¤äº’æ¨¡å¼ï¼‰")
                sys.exit(1)

    try:
        # è§£æè·³è¿‡çš„è¡¨
        skip_tables = set()
        if args.skip_tables:
            skip_tables = {t.strip() for t in args.skip_tables.split(",") if t.strip()}
            logger.info("å°†è·³è¿‡ä»¥ä¸‹è¡¨: %s", ", ".join(skip_tables))

        # è§£æåªè¿ç§»çš„è¡¨
        only_tables = set()
        if args.only_tables:
            only_tables = {t.strip() for t in args.only_tables.split(",") if t.strip()}
            logger.info("å°†åªè¿ç§»ä»¥ä¸‹è¡¨: %s", ", ".join(only_tables))

        migrator = DatabaseMigrator(
            source_type=args.source,
            target_type=args.target,
            batch_size=args.batch_size,
            source_config=source_config,
            target_config=target_config,
            skip_tables=skip_tables,
            only_tables=only_tables,
            no_create_tables=args.no_create_tables,
        )

        stats = migrator.run()

        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¿”å›éé›¶é€€å‡ºç 
        if stats["errors"]:
            sys.exit(1)

    except KeyboardInterrupt:
        print("\nè¿ç§»è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"è¿ç§»å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
