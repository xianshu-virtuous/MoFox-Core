import asyncio
import datetime
import os
import shutil
import sys
from pathlib import Path

import aiofiles
import orjson
from json_repair import repair_json

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)

from src.chat.knowledge.embedding_store import EmbeddingManager
from src.chat.knowledge.kg_manager import KGManager
from src.chat.knowledge.open_ie import OpenIE
from src.chat.knowledge.utils.hash import get_sha256
from src.common.logger import get_logger
from src.config.config import model_config
from src.llm_models.utils_model import LLMRequest

logger = get_logger("LPMM_LearningTool")
ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
RAW_DATA_PATH = os.path.join(ROOT_PATH, "data", "lpmm_raw_data")
OPENIE_OUTPUT_DIR = os.path.join(ROOT_PATH, "data", "openie")
TEMP_DIR = os.path.join(ROOT_PATH, "temp", "lpmm_cache")

# ========== æ€§èƒ½é…ç½®å‚æ•° ==========
#
# çŸ¥è¯†æå–ï¼ˆæ­¥éª¤2ï¼štxtè½¬jsonï¼‰å¹¶å‘æ§åˆ¶
# - æ§åˆ¶åŒæ—¶è¿›è¡Œçš„LLMæå–è¯·æ±‚æ•°é‡
# - æ¨èå€¼: 3-10ï¼Œå–å†³äºAPIé€Ÿç‡é™åˆ¶
# - è¿‡é«˜å¯èƒ½è§¦å‘429é”™è¯¯ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
MAX_EXTRACTION_CONCURRENCY = 5

# æ•°æ®å¯¼å…¥ï¼ˆæ­¥éª¤3ï¼šç”Ÿæˆembeddingï¼‰æ€§èƒ½é…ç½®
# - max_workers: å¹¶å‘æ‰¹æ¬¡æ•°ï¼ˆæ¯æ‰¹æ¬¡å¹¶è¡Œå¤„ç†ï¼‰
# - chunk_size: æ¯æ‰¹æ¬¡åŒ…å«çš„å­—ç¬¦ä¸²æ•°
# - ç†è®ºå¹¶å‘ = max_workers Ã— chunk_size
# - æ¨èé…ç½®:
#   * é«˜æ€§èƒ½APIï¼ˆOpenAIï¼‰: max_workers=20-30, chunk_size=30-50
#   * ä¸­ç­‰API: max_workers=10-15, chunk_size=20-30
#   * æœ¬åœ°/æ…¢é€ŸAPI: max_workers=5-10, chunk_size=10-20
EMBEDDING_MAX_WORKERS = 20  # å¹¶å‘æ‰¹æ¬¡æ•°
EMBEDDING_CHUNK_SIZE = 30   # æ¯æ‰¹æ¬¡å­—ç¬¦ä¸²æ•°
# ===================================

# --- ç¼“å­˜æ¸…ç† ---


def clear_cache():
    """æ¸…ç† lpmm_learning_tool.py ç”Ÿæˆçš„ç¼“å­˜æ–‡ä»¶"""
    logger.info("--- å¼€å§‹æ¸…ç†ç¼“å­˜ ---")
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
            logger.info(f"æˆåŠŸåˆ é™¤ç¼“å­˜ç›®å½•: {TEMP_DIR}")
        except OSError as e:
            logger.error(f"åˆ é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}")
    else:
        logger.info("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
    logger.info("--- ç¼“å­˜æ¸…ç†å®Œæˆ ---")


# --- æ¨¡å—ä¸€ï¼šæ•°æ®é¢„å¤„ç† ---


def process_text_file(file_path):
    with open(file_path, encoding="utf-8") as f:
        raw = f.read()
    return [p.strip() for p in raw.split("\n\n") if p.strip()]


def preprocess_raw_data():
    logger.info("--- æ­¥éª¤ 1: å¼€å§‹æ•°æ®é¢„å¤„ç† ---")
    os.makedirs(RAW_DATA_PATH, exist_ok=True)
    raw_files = list(Path(RAW_DATA_PATH).glob("*.txt"))
    if not raw_files:
        logger.warning(f"è­¦å‘Š: åœ¨ '{RAW_DATA_PATH}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶")
        return []

    all_paragraphs = []
    for file in raw_files:
        logger.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file.name}")
        all_paragraphs.extend(process_text_file(file))

    unique_paragraphs = {get_sha256(p): p for p in all_paragraphs}
    logger.info(f"å…±æ‰¾åˆ° {len(all_paragraphs)} ä¸ªæ®µè½ï¼Œå»é‡åå‰©ä½™ {len(unique_paragraphs)} ä¸ªã€‚")
    logger.info("--- æ•°æ®é¢„å¤„ç†å®Œæˆ ---")
    return unique_paragraphs


# --- æ¨¡å—äºŒï¼šä¿¡æ¯æå– ---


def _parse_and_repair_json(json_string: str) -> dict | None:
    """
    å°è¯•è§£æJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ä¿®å¤å¹¶é‡æ–°è§£æã€‚

    è¯¥å‡½æ•°é¦–å…ˆä¼šæ¸…ç†å­—ç¬¦ä¸²ï¼Œå»é™¤å¸¸è§çš„Markdownä»£ç å—æ ‡è®°ï¼Œ
    ç„¶åå°è¯•ç›´æ¥è§£æã€‚å¦‚æœè§£æå¤±è´¥ï¼Œå®ƒä¼šè°ƒç”¨ `repair_json`
    è¿›è¡Œä¿®å¤ï¼Œå¹¶å†æ¬¡å°è¯•è§£æã€‚

    Args:
        json_string: ä»LLMè·å–çš„ã€å¯èƒ½æ ¼å¼ä¸æ­£ç¡®çš„JSONå­—ç¬¦ä¸²ã€‚

    Returns:
        è§£æåçš„å­—å…¸ã€‚å¦‚æœæœ€ç»ˆæ— æ³•è§£æï¼Œåˆ™è¿”å› Noneï¼Œå¹¶è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—ã€‚
    """
    if not isinstance(json_string, str):
        logger.error(f"è¾“å…¥å†…å®¹éå­—ç¬¦ä¸²ï¼Œæ— æ³•è§£æ: {type(json_string)}")
        return None

    # 1. é¢„å¤„ç†ï¼šå»é™¤å¸¸è§çš„å¤šä½™å­—ç¬¦ï¼Œå¦‚Markdownä»£ç å—æ ‡è®°
    cleaned_string = json_string.strip()
    if cleaned_string.startswith("```json"):
        cleaned_string = cleaned_string[7:].strip()
    elif cleaned_string.startswith("```"):
        cleaned_string = cleaned_string[3:].strip()

    if cleaned_string.endswith("```"):
        cleaned_string = cleaned_string[:-3].strip()

    # 2. æ€§èƒ½ä¼˜åŒ–ï¼šä¹è§‚åœ°å°è¯•ç›´æ¥è§£æ
    try:
        return orjson.loads(cleaned_string)
    except orjson.JSONDecodeError:
        logger.warning("ç›´æ¥è§£æJSONå¤±è´¥ï¼Œå°†å°è¯•ä¿®å¤...")

        # 3. ä¿®å¤ä¸æœ€ç»ˆè§£æ
        repaired_json_str = ""
        try:
            repaired_json_str = repair_json(cleaned_string)
            return orjson.loads(repaired_json_str)
        except Exception as e:
            # 4. å¢å¼ºé”™è¯¯å¤„ç†ï¼šè®°å½•è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯
            logger.error(f"ä¿®å¤å¹¶è§£æJSONåä¾ç„¶å¤±è´¥: {e}")
            logger.error(f"åŸå§‹å­—ç¬¦ä¸² (æ¸…ç†å): {cleaned_string}")
            logger.error(f"ä¿®å¤åå°è¯•è§£æçš„å­—ç¬¦ä¸²: {repaired_json_str}")
            return None


def get_extraction_prompt(paragraph: str) -> str:
    return f"""
è¯·ä»ä»¥ä¸‹æ®µè½ä¸­æå–å…³é”®ä¿¡æ¯ã€‚ä½ éœ€è¦æå–ä¸¤ç§ç±»å‹çš„ä¿¡æ¯ï¼š
1.  **å®ä½“ (Entities)**: è¯†åˆ«å¹¶åˆ—å‡ºæ®µè½ä¸­æ‰€æœ‰é‡è¦çš„åè¯æˆ–åè¯çŸ­è¯­ã€‚
2.  **ä¸‰å…ƒç»„ (Triples)**: ä»¥ [ä¸»è¯­, è°“è¯­, å®¾è¯­] çš„æ ¼å¼ï¼Œæå–æ®µè½ä¸­æè¿°å…³ç³»æˆ–äº‹å®çš„æ ¸å¿ƒä¿¡æ¯ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ³¨é‡Šï¼š
{{
    "entities": ["å®ä½“1", "å®ä½“2"],
    "triples": [["ä¸»è¯­1", "è°“è¯­1", "å®¾è¯­1"]]
}}

è¿™æ˜¯ä½ éœ€è¦å¤„ç†çš„æ®µè½ï¼š
---
{paragraph}
---
"""


async def extract_info_async(pg_hash, paragraph, llm_api):
    """
    å¼‚æ­¥æå–å•ä¸ªæ®µè½çš„ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜æ”¯æŒï¼‰

    Args:
        pg_hash: æ®µè½å“ˆå¸Œå€¼
        paragraph: æ®µè½æ–‡æœ¬
        llm_api: LLMè¯·æ±‚å®ä¾‹

    Returns:
        tuple: (doc_itemæˆ–None, failed_hashæˆ–None)
    """
    temp_file_path = os.path.join(TEMP_DIR, f"{pg_hash}.json")

    # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥æ–‡ä»¶æ£€æŸ¥ï¼Œé¿å…é˜»å¡
    if os.path.exists(temp_file_path):
        try:
            async with aiofiles.open(temp_file_path, "rb") as f:
                content = await f.read()
                return orjson.loads(content), None
        except orjson.JSONDecodeError:
            # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤å¹¶é‡æ–°ç”Ÿæˆ
            try:
                os.remove(temp_file_path)
            except OSError:
                pass

    prompt = get_extraction_prompt(paragraph)
    content = None
    try:
        content, (_, _, _) = await llm_api.generate_response_async(prompt)

        # è°ƒç”¨å°è£…å¥½çš„å‡½æ•°å¤„ç†JSONè§£æå’Œä¿®å¤
        extracted_data = _parse_and_repair_json(content)

        if extracted_data is None:
            raise ValueError("æ— æ³•ä»LLMè¾“å‡ºä¸­è§£ææœ‰æ•ˆçš„JSONæ•°æ®")

        doc_item = {
            "idx": pg_hash,
            "passage": paragraph,
            "extracted_entities": extracted_data.get("entities", []),
            "extracted_triples": extracted_data.get("triples", []),
        }

        # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¼‚æ­¥å†™å…¥ï¼‰
        async with aiofiles.open(temp_file_path, "wb") as f:
            await f.write(orjson.dumps(doc_item))

        return doc_item, None
    except Exception as e:
        logger.error(f"æå–ä¿¡æ¯å¤±è´¥ï¼š{pg_hash}, é”™è¯¯ï¼š{e}")
        if content:
            logger.error(f"å¯¼è‡´è§£æå¤±è´¥çš„åŸå§‹è¾“å‡º: {content}")
        return None, pg_hash


async def extract_information(paragraphs_dict, model_set):
    """
    ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨çœŸæ­£çš„å¼‚æ­¥å¹¶å‘ä»£æ›¿å¤šçº¿ç¨‹

    è¿™æ ·å¯ä»¥ï¼š
    1. é¿å… event loop closed é”™è¯¯
    2. æ›´é«˜æ•ˆåœ°åˆ©ç”¨ I/O èµ„æº
    3. ä¸æˆ‘ä»¬ä¼˜åŒ–çš„ LLM è¯·æ±‚å±‚æ— ç¼é›†æˆ

    å¹¶å‘æ§åˆ¶ï¼š
    - ä½¿ç”¨ä¿¡å·é‡é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º 5ï¼Œé˜²æ­¢è§¦å‘ API é€Ÿç‡é™åˆ¶

    Args:
        paragraphs_dict: {hash: paragraph} å­—å…¸
        model_set: æ¨¡å‹é…ç½®
    """
    logger.info("--- æ­¥éª¤ 2: å¼€å§‹ä¿¡æ¯æå– ---")
    os.makedirs(OPENIE_OUTPUT_DIR, exist_ok=True)
    os.makedirs(TEMP_DIR, exist_ok=True)

    failed_hashes, open_ie_docs = [], []

    # ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆ›å»ºå•ä¸ª LLM è¯·æ±‚å®ä¾‹ï¼Œå¤ç”¨è¿æ¥
    llm_api = LLMRequest(model_set=model_set, request_type="lpmm_extraction")

    # ğŸ”§ å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶æœ€å¤§å¹¶å‘æ•°ï¼Œé˜²æ­¢é€Ÿç‡é™åˆ¶
    semaphore = asyncio.Semaphore(MAX_EXTRACTION_CONCURRENCY)

    async def extract_with_semaphore(pg_hash, paragraph):
        """å¸¦ä¿¡å·é‡æ§åˆ¶çš„æå–å‡½æ•°"""
        async with semaphore:
            return await extract_info_async(pg_hash, paragraph, llm_api)

    # åˆ›å»ºæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰
    tasks = [
        extract_with_semaphore(p_hash, paragraph)
        for p_hash, paragraph in paragraphs_dict.items()
    ]

    total = len(tasks)
    completed = 0

    logger.info(f"å¼€å§‹æå– {total} ä¸ªæ®µè½çš„ä¿¡æ¯ï¼ˆæœ€å¤§å¹¶å‘: {MAX_EXTRACTION_CONCURRENCY}ï¼‰")

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        MofNCompleteColumn(),
        "â€¢",
        TimeElapsedColumn(),
        "<",
        TimeRemainingColumn(),
    ) as progress:
        task = progress.add_task("[cyan]æ­£åœ¨æå–ä¿¡æ¯...", total=total)

        # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        # return_exceptions=True ç¡®ä¿å•ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
        for coro in asyncio.as_completed(tasks):
            doc_item, failed_hash = await coro
            if failed_hash:
                failed_hashes.append(failed_hash)
            elif doc_item:
                open_ie_docs.append(doc_item)

            completed += 1
            progress.update(task, advance=1)

    if open_ie_docs:
        all_entities = [e for doc in open_ie_docs for e in doc["extracted_entities"]]
        num_entities = len(all_entities)
        avg_ent_chars = round(sum(len(e) for e in all_entities) / num_entities, 4) if num_entities else 0
        avg_ent_words = round(sum(len(e.split()) for e in all_entities) / num_entities, 4) if num_entities else 0
        openie_obj = OpenIE(docs=open_ie_docs, avg_ent_chars=avg_ent_chars, avg_ent_words=avg_ent_words)

        now = datetime.datetime.now()
        filename = now.strftime("%Y-%m-%d-%H-%M-%S-openie.json")
        output_path = os.path.join(OPENIE_OUTPUT_DIR, filename)
        async with aiofiles.open(output_path, "wb") as f:
            await f.write(orjson.dumps(openie_obj._to_dict()))
        logger.info(f"ä¿¡æ¯æå–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        logger.info(f"æˆåŠŸæå– {len(open_ie_docs)} ä¸ªæ®µè½çš„ä¿¡æ¯")

    if failed_hashes:
        logger.error(f"ä»¥ä¸‹ {len(failed_hashes)} ä¸ªæ®µè½æå–å¤±è´¥: {failed_hashes}")
    logger.info("--- ä¿¡æ¯æå–å®Œæˆ ---")


# --- æ¨¡å—ä¸‰ï¼šæ•°æ®å¯¼å…¥ ---


async def import_data(openie_obj: OpenIE | None = None):
    """
    å°†OpenIEæ•°æ®å¯¼å…¥çŸ¥è¯†åº“ï¼ˆEmbedding Store å’Œ KGï¼‰

    Args:
        openie_obj (Optional[OpenIE], optional): å¦‚æœæä¾›ï¼Œåˆ™ç›´æ¥ä½¿ç”¨è¿™ä¸ªOpenIEå¯¹è±¡ï¼›
                                                 å¦åˆ™ï¼Œå°†è‡ªåŠ¨ä»é»˜è®¤æ–‡ä»¶å¤¹åŠ è½½æœ€æ–°çš„OpenIEæ–‡ä»¶ã€‚
                                                 é»˜è®¤ä¸º None.
    """
    logger.info("--- æ­¥éª¤ 3: å¼€å§‹æ•°æ®å¯¼å…¥ ---")
    # ä½¿ç”¨é…ç½®çš„å¹¶å‘å‚æ•°ä»¥åŠ é€Ÿ embedding ç”Ÿæˆ
    # max_workers: å¹¶å‘æ‰¹æ¬¡æ•°ï¼Œchunk_size: æ¯æ‰¹æ¬¡å¤„ç†çš„å­—ç¬¦ä¸²æ•°
    embed_manager = EmbeddingManager(max_workers=EMBEDDING_MAX_WORKERS, chunk_size=EMBEDDING_CHUNK_SIZE)
    kg_manager = KGManager()

    logger.info("æ­£åœ¨åŠ è½½ç°æœ‰çš„ Embedding åº“...")
    try:
        embed_manager.load_from_file()
    except Exception as e:
        logger.warning(f"åŠ è½½ Embedding åº“å¤±è´¥: {e}ã€‚")

    logger.info("æ­£åœ¨åŠ è½½ç°æœ‰çš„ KG...")
    try:
        kg_manager.load_from_file()
    except Exception as e:
        logger.warning(f"åŠ è½½ KG å¤±è´¥: {e}ã€‚")

    try:
        if openie_obj:
            openie_data = openie_obj
            logger.info("å·²ä½¿ç”¨æŒ‡å®šçš„ OpenIE å¯¹è±¡ã€‚")
        else:
            openie_data = OpenIE.load()
    except Exception as e:
        logger.error(f"åŠ è½½OpenIEæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return

    raw_paragraphs = openie_data.extract_raw_paragraph_dict()
    triple_list_data = openie_data.extract_triple_dict()

    new_raw_paragraphs, new_triple_list_data = {}, {}
    stored_embeds = embed_manager.stored_pg_hashes
    stored_kgs = kg_manager.stored_paragraph_hashes

    for p_hash, raw_p in raw_paragraphs.items():
        if p_hash not in stored_embeds and p_hash not in stored_kgs:
            new_raw_paragraphs[p_hash] = raw_p
            new_triple_list_data[p_hash] = triple_list_data.get(p_hash, [])

    if not new_raw_paragraphs:
        logger.info("æ²¡æœ‰æ–°çš„æ®µè½éœ€è¦å¤„ç†ã€‚")
    else:
        logger.info(f"å»é‡å®Œæˆï¼Œå‘ç° {len(new_raw_paragraphs)} ä¸ªæ–°æ®µè½ã€‚")
        logger.info("å¼€å§‹ç”Ÿæˆ Embedding...")
        await embed_manager.store_new_data_set(new_raw_paragraphs, new_triple_list_data)
        embed_manager.rebuild_faiss_index()
        embed_manager.save_to_file()
        logger.info("Embedding å¤„ç†å®Œæˆï¼")

        logger.info("å¼€å§‹æ„å»º KG...")
        kg_manager.build_kg(new_triple_list_data, embed_manager)
        kg_manager.save_to_file()
        logger.info("KG æ„å»ºå®Œæˆï¼")

    logger.info("--- æ•°æ®å¯¼å…¥å®Œæˆ ---")


def import_from_specific_file():
    """ä»ç”¨æˆ·æŒ‡å®šçš„ openie.json æ–‡ä»¶å¯¼å…¥æ•°æ®"""
    file_path = input("è¯·è¾“å…¥ openie.json æ–‡ä»¶çš„å®Œæ•´è·¯å¾„: ").strip()

    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {file_path}")
        return

    if not file_path.endswith(".json"):
        logger.error("è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„ .json æ–‡ä»¶è·¯å¾„ã€‚")
        return

    try:
        logger.info(f"æ­£åœ¨ä» {file_path} åŠ è½½ OpenIE æ•°æ®...")
        openie_obj = OpenIE.load()
        asyncio.run(import_data(openie_obj=openie_obj))
    except Exception as e:
        logger.error(f"ä»æŒ‡å®šæ–‡ä»¶å¯¼å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# --- ä¸»å‡½æ•° ---


def rebuild_faiss_only():
    """ä»…é‡å»º FAISS ç´¢å¼•ï¼Œä¸é‡æ–°å¯¼å…¥æ•°æ®"""
    logger.info("--- é‡å»º FAISS ç´¢å¼• ---")
    # é‡å»ºç´¢å¼•ä¸éœ€è¦å¹¶å‘å‚æ•°ï¼ˆä¸æ¶‰åŠ embedding ç”Ÿæˆï¼‰
    embed_manager = EmbeddingManager()

    logger.info("æ­£åœ¨åŠ è½½ç°æœ‰çš„ Embedding åº“...")
    try:
        embed_manager.load_from_file()
        logger.info("å¼€å§‹é‡å»º FAISS ç´¢å¼•...")
        embed_manager.rebuild_faiss_index()
        embed_manager.save_to_file()
        logger.info("âœ… FAISS ç´¢å¼•é‡å»ºå®Œæˆï¼")
    except Exception as e:
        logger.error(f"é‡å»º FAISS ç´¢å¼•æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def main():
    # ä½¿ç”¨ os.path.relpath åˆ›å»ºç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„å‹å¥½è·¯å¾„
    raw_data_relpath = os.path.relpath(RAW_DATA_PATH, os.path.join(ROOT_PATH, ".."))
    openie_output_relpath = os.path.relpath(OPENIE_OUTPUT_DIR, os.path.join(ROOT_PATH, ".."))

    print("=== LPMM çŸ¥è¯†åº“å­¦ä¹ å·¥å…· ===")
    print(f"1. [æ•°æ®é¢„å¤„ç†] -> è¯»å– .txt æ–‡ä»¶ (æ¥æº: ./{raw_data_relpath}/)")
    print(f"2. [ä¿¡æ¯æå–] -> æå–ä¿¡æ¯å¹¶å­˜ä¸º .json (è¾“å‡ºè‡³: ./{openie_output_relpath}/)")
    print("3. [æ•°æ®å¯¼å…¥] -> ä» openie æ–‡ä»¶å¤¹è‡ªåŠ¨å¯¼å…¥æœ€æ–°çŸ¥è¯†")
    print("4. [å…¨æµç¨‹] -> æŒ‰é¡ºåºæ‰§è¡Œ 1 -> 2 -> 3")
    print("5. [æŒ‡å®šå¯¼å…¥] -> ä»ç‰¹å®šçš„ openie.json æ–‡ä»¶å¯¼å…¥çŸ¥è¯†")
    print("6. [æ¸…ç†ç¼“å­˜] -> åˆ é™¤æ‰€æœ‰å·²æå–ä¿¡æ¯çš„ç¼“å­˜")
    print("7. [é‡å»ºç´¢å¼•] -> ä»…é‡å»º FAISS ç´¢å¼•ï¼ˆæ•°æ®å·²å¯¼å…¥æ—¶ä½¿ç”¨ï¼‰")
    print("0. [é€€å‡º]")
    print("-" * 30)
    choice = input("è¯·è¾“å…¥ä½ çš„é€‰æ‹© (0-7): ").strip()

    if choice == "1":
        preprocess_raw_data()
    elif choice == "2":
        paragraphs = preprocess_raw_data()
        if paragraphs:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ asyncio.run è°ƒç”¨å¼‚æ­¥å‡½æ•°
            asyncio.run(extract_information(paragraphs, model_config.model_task_config.lpmm_qa))
    elif choice == "3":
        asyncio.run(import_data())
    elif choice == "4":
        paragraphs = preprocess_raw_data()
        if paragraphs:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ asyncio.run è°ƒç”¨å¼‚æ­¥å‡½æ•°
            asyncio.run(extract_information(paragraphs, model_config.model_task_config.lpmm_qa))
            asyncio.run(import_data())
    elif choice == "5":
        import_from_specific_file()
    elif choice == "6":
        clear_cache()
    elif choice == "7":
        rebuild_faiss_only()
    elif choice == "0":
        sys.exit(0)
    else:
        print("æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬ã€‚")


if __name__ == "__main__":
    main()
