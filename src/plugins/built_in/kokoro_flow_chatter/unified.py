"""
Kokoro Flow Chatter - ç»Ÿä¸€æ¨¡å¼

ç»Ÿä¸€æ¨¡å¼ï¼ˆUnified Modeï¼‰ï¼š
- ä½¿ç”¨æ¨¡å—åŒ–çš„æç¤ºè¯ç»„ä»¶æ„å»ºæç¤ºè¯
- System Prompt + User Prompt çš„æ ‡å‡†ç»“æ„
- ä¸€æ¬¡ LLM è°ƒç”¨å®Œæˆæ€è€ƒ + å›å¤ç”Ÿæˆ
- è¾“å‡º JSON æ ¼å¼ï¼šthought + actions + max_wait_seconds

ä¸åˆ†ç¦»æ¨¡å¼ï¼ˆSplit Modeï¼‰çš„åŒºåˆ«ï¼š
- ç»Ÿä¸€æ¨¡å¼ï¼šä¸€æ¬¡è°ƒç”¨å®Œæˆæ‰€æœ‰å·¥ä½œï¼Œactions ä¸­ç›´æ¥åŒ…å«å›å¤å†…å®¹
- åˆ†ç¦»æ¨¡å¼ï¼šPlanner + Replyer ä¸¤æ¬¡è°ƒç”¨ï¼Œå…ˆè§„åˆ’å†ç”Ÿæˆå›å¤
"""

import time
from datetime import datetime
from typing import TYPE_CHECKING, Optional

from src.common.logger import get_logger
from src.config.config import global_config
from src.plugin_system.apis import llm_api
from src.utils.json_parser import extract_and_parse_json

from .models import LLMResponse, EventType
from .session import KokoroSession

# ç»Ÿä¸€æ¨¡å¼ä¸“ç”¨çš„æç¤ºè¯æ¨¡å—
from . import prompt_modules_unified as prompt_modules

if TYPE_CHECKING:
    from src.chat.message_receive.chat_stream import ChatStream
    from src.common.data_models.message_manager_data_model import StreamContext
    from src.plugin_system.base.component_types import ActionInfo

logger = get_logger("kfc_unified")


class UnifiedPromptGenerator:
    """
    ç»Ÿä¸€æ¨¡å¼æç¤ºè¯ç”Ÿæˆå™¨
    
    ä¸ºç»Ÿä¸€æ¨¡å¼æ„å»ºæç¤ºè¯ï¼š
    - generate_system_prompt: æ„å»ºç³»ç»Ÿæç¤ºè¯
    - generate_responding_prompt: å›åº”æ¶ˆæ¯åœºæ™¯
    - generate_timeout_prompt: è¶…æ—¶å†³ç­–åœºæ™¯
    - generate_proactive_prompt: ä¸»åŠ¨æ€è€ƒåœºæ™¯
    """
    
    def __init__(self):
        pass
    
    async def generate_system_prompt(
        self,
        session: KokoroSession,
        available_actions: Optional[dict] = None,
        context_data: Optional[dict[str, str]] = None,
        chat_stream: Optional["ChatStream"] = None,
    ) -> str:
        """
        ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        
        ä½¿ç”¨ prompt_modules.build_system_prompt() æ„å»ºæ¨¡å—åŒ–çš„æç¤ºè¯
        """
        return prompt_modules.build_system_prompt(
            session=session,
            available_actions=available_actions,
            context_data=context_data,
            chat_stream=chat_stream,
        )
    
    async def generate_responding_prompt(
        self,
        session: KokoroSession,
        message_content: str,
        sender_name: str,
        sender_id: str,
        message_time: Optional[float] = None,
        available_actions: Optional[dict] = None,
        context: Optional["StreamContext"] = None,
        context_data: Optional[dict[str, str]] = None,
        chat_stream: Optional["ChatStream"] = None,
        all_unread_messages: Optional[list] = None,
    ) -> tuple[str, str]:
        """
        ç”Ÿæˆå›åº”æ¶ˆæ¯åœºæ™¯çš„æç¤ºè¯
        
        Returns:
            tuple[str, str]: (ç³»ç»Ÿæç¤ºè¯, ç”¨æˆ·æç¤ºè¯)
        """
        # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        system_prompt = await self.generate_system_prompt(
            session,
            available_actions,
            context_data=context_data,
            chat_stream=chat_stream,
        )
        
        # æ„å»ºå™äº‹å†å²
        if context:
            narrative_history = prompt_modules.format_history_from_context(
                context, session.mental_log
            )
        else:
            narrative_history = prompt_modules.format_narrative_history(session.mental_log)
        
        # æ ¼å¼åŒ–æ”¶åˆ°çš„æ¶ˆæ¯
        incoming_messages = prompt_modules.format_incoming_messages(
            message_content=message_content,
            sender_name=sender_name,
            sender_id=sender_id,
            message_time=message_time,
            all_unread_messages=all_unread_messages,
        )
        
        # ä½¿ç”¨ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
        user_prompt = prompt_modules.RESPONDING_USER_PROMPT_TEMPLATE.format(
            narrative_history=narrative_history,
            incoming_messages=incoming_messages,
        )
        
        return system_prompt, user_prompt
    
    async def generate_timeout_prompt(
        self,
        session: KokoroSession,
        available_actions: Optional[dict] = None,
        context_data: Optional[dict[str, str]] = None,
        chat_stream: Optional["ChatStream"] = None,
    ) -> tuple[str, str]:
        """
        ç”Ÿæˆè¶…æ—¶å†³ç­–åœºæ™¯çš„æç¤ºè¯
        
        Returns:
            tuple[str, str]: (ç³»ç»Ÿæç¤ºè¯, ç”¨æˆ·æç¤ºè¯)
        """
        # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        system_prompt = await self.generate_system_prompt(
            session,
            available_actions,
            context_data=context_data,
            chat_stream=chat_stream,
        )
        
        # æ„å»ºå™äº‹å†å²
        narrative_history = prompt_modules.format_narrative_history(session.mental_log)
        
        # è®¡ç®—ç­‰å¾…æ—¶é—´
        wait_duration = session.waiting_config.get_elapsed_seconds()
        
        # ç”Ÿæˆè¿ç»­è¿½é—®è­¦å‘Šï¼ˆä½¿ç”¨ waiting_config.thinking_count ä½œä¸ºè¿½é—®è®¡æ•°ï¼‰
        followup_count = session.waiting_config.thinking_count
        max_followups = 3  # æœ€å¤šè¿½é—®3æ¬¡
        
        if followup_count >= max_followups:
            followup_warning = f"""âš ï¸ **é‡è¦æé†’**ï¼š
ä½ å·²ç»è¿ç»­è¿½é—®äº† {followup_count} æ¬¡ï¼Œå¯¹æ–¹éƒ½æ²¡æœ‰å›å¤ã€‚
**å¼ºçƒˆå»ºè®®ä¸è¦å†å‘æ¶ˆæ¯äº†**â€”â€”ç»§ç»­è¿½é—®ä¼šæ˜¾å¾—å¾ˆç¼ äººã€å¾ˆä¸å°Šé‡å¯¹æ–¹çš„ç©ºé—´ã€‚
å¯¹æ–¹å¯èƒ½çœŸçš„åœ¨å¿™ï¼Œæˆ–è€…æš‚æ—¶ä¸æƒ³å›å¤ï¼Œè¿™éƒ½æ˜¯æ­£å¸¸çš„ã€‚
è¯·é€‰æ‹© `do_nothing` ç»§ç»­ç­‰å¾…ï¼Œæˆ–è€…ç›´æ¥ç»“æŸå¯¹è¯ï¼ˆè®¾ç½® `max_wait_seconds: 0`ï¼‰ã€‚"""
        elif followup_count > 0:
            followup_warning = f"""ğŸ“ æç¤ºï¼šè¿™å·²ç»æ˜¯ä½ ç¬¬ {followup_count + 1} æ¬¡ç­‰å¾…å¯¹æ–¹å›å¤äº†ã€‚
å¦‚æœå¯¹æ–¹æŒç»­æ²¡æœ‰å›åº”ï¼Œå¯èƒ½çœŸçš„åœ¨å¿™æˆ–ä¸æ–¹ä¾¿ï¼Œä¸éœ€è¦æ€¥ç€è¿½é—®ã€‚"""
        else:
            followup_warning = ""
        
        # è·å–æœ€åä¸€æ¡ Bot æ¶ˆæ¯
        last_bot_message = "ï¼ˆæ²¡æœ‰è®°å½•ï¼‰"
        for entry in reversed(session.mental_log):
            if entry.event_type == EventType.BOT_PLANNING:
                for action in entry.actions:
                    if action.get("type") in ("reply", "kfc_reply"):
                        content = action.get("content", "")
                        if content:
                            last_bot_message = content
                            break
                if last_bot_message != "ï¼ˆæ²¡æœ‰è®°å½•ï¼‰":
                    break
        
        # ä½¿ç”¨ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
        user_prompt = prompt_modules.TIMEOUT_DECISION_USER_PROMPT_TEMPLATE.format(
            narrative_history=narrative_history,
            wait_duration_seconds=wait_duration,
            wait_duration_minutes=wait_duration / 60,
            expected_user_reaction=session.waiting_config.expected_reaction or "ä¸ç¡®å®š",
            followup_warning=followup_warning,
            last_bot_message=last_bot_message,
        )
        
        return system_prompt, user_prompt
    
    async def generate_proactive_prompt(
        self,
        session: KokoroSession,
        trigger_context: str,
        available_actions: Optional[dict] = None,
        context_data: Optional[dict[str, str]] = None,
        chat_stream: Optional["ChatStream"] = None,
    ) -> tuple[str, str]:
        """
        ç”Ÿæˆä¸»åŠ¨æ€è€ƒåœºæ™¯çš„æç¤ºè¯
        
        Returns:
            tuple[str, str]: (ç³»ç»Ÿæç¤ºè¯, ç”¨æˆ·æç¤ºè¯)
        """
        # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
        system_prompt = await self.generate_system_prompt(
            session,
            available_actions,
            context_data=context_data,
            chat_stream=chat_stream,
        )
        
        # æ„å»ºå™äº‹å†å²
        narrative_history = prompt_modules.format_narrative_history(
            session.mental_log, max_entries=10
        )
        
        # è®¡ç®—æ²‰é»˜æ—¶é•¿
        silence_seconds = time.time() - session.last_activity_at
        if silence_seconds < 3600:
            silence_duration = f"{silence_seconds / 60:.0f}åˆ†é’Ÿ"
        else:
            silence_duration = f"{silence_seconds / 3600:.1f}å°æ—¶"
        
        # å½“å‰æ—¶é—´
        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
        
        # ä» context_data è·å–å…³ç³»ä¿¡æ¯
        relation_block = ""
        if context_data:
            relation_info = context_data.get("relation_info", "")
            if relation_info:
                relation_block = f"### ä½ ä¸å¯¹æ–¹çš„å…³ç³»\n{relation_info}"
        
        if not relation_block:
            # å›é€€ï¼šä½¿ç”¨é»˜è®¤å…³ç³»æè¿°
            relation_block = """### ä½ ä¸å¯¹æ–¹çš„å…³ç³»
- ä½ ä»¬è¿˜ä¸å¤ªç†Ÿæ‚‰
- æ­£åœ¨æ…¢æ…¢äº†è§£ä¸­"""
        
        # ä½¿ç”¨ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
        user_prompt = prompt_modules.PROACTIVE_THINKING_USER_PROMPT_TEMPLATE.format(
            narrative_history=narrative_history,
            current_time=current_time,
            silence_duration=silence_duration,
            relation_block=relation_block,
            trigger_context=trigger_context,
        )
        
        return system_prompt, user_prompt
    
    def build_messages_for_llm(
        self,
        system_prompt: str,
        user_prompt: str,
        stream_id: str = "",
    ) -> str:
        """
        æ„å»º LLM è¯·æ±‚çš„å®Œæ•´æç¤ºè¯
        
        å°† system + user åˆå¹¶ä¸ºå•ä¸ªæç¤ºè¯å­—ç¬¦ä¸²
        """
        # åˆå¹¶æç¤ºè¯
        full_prompt = f"{system_prompt}\n\n---\n\n{user_prompt}"
        
        # DEBUGæ—¥å¿—ï¼šæ‰“å°å®Œæ•´çš„KFCæç¤ºè¯ï¼ˆåªåœ¨ DEBUG çº§åˆ«è¾“å‡ºï¼‰
        logger.debug(
            f"Final KFC prompt constructed for stream {stream_id}:\n"
            f"--- PROMPT START ---\n"
            f"{full_prompt}\n"
            f"--- PROMPT END ---"
        )
        
        return full_prompt


# å…¨å±€æç¤ºè¯ç”Ÿæˆå™¨å®ä¾‹
_prompt_generator: Optional[UnifiedPromptGenerator] = None


def get_unified_prompt_generator() -> UnifiedPromptGenerator:
    """è·å–å…¨å±€æç¤ºè¯ç”Ÿæˆå™¨å®ä¾‹"""
    global _prompt_generator
    if _prompt_generator is None:
        _prompt_generator = UnifiedPromptGenerator()
    return _prompt_generator


async def generate_unified_response(
    session: KokoroSession,
    user_name: str,
    situation_type: str = "new_message",
    chat_stream: Optional["ChatStream"] = None,
    available_actions: Optional[dict] = None,
    extra_context: Optional[dict] = None,
) -> LLMResponse:
    """
    ç»Ÿä¸€æ¨¡å¼ï¼šå•æ¬¡ LLM è°ƒç”¨ç”Ÿæˆå®Œæ•´å“åº”
    
    è°ƒç”¨æ–¹å¼ï¼š
    - ä½¿ç”¨ UnifiedPromptGenerator ç”Ÿæˆ System + User æç¤ºè¯
    - ä½¿ç”¨ replyer æ¨¡å‹è°ƒç”¨ LLM
    - è§£æ JSON å“åº”ï¼ˆthought + actions + max_wait_secondsï¼‰
    
    Args:
        session: ä¼šè¯å¯¹è±¡
        user_name: ç”¨æˆ·åç§°
        situation_type: æƒ…å†µç±»å‹ (new_message/timeout/proactive)
        chat_stream: èŠå¤©æµå¯¹è±¡
        available_actions: å¯ç”¨åŠ¨ä½œå­—å…¸
        extra_context: é¢å¤–ä¸Šä¸‹æ–‡
        
    Returns:
        LLMResponse å¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„æ€è€ƒå’ŒåŠ¨ä½œ
    """
    try:
        prompt_generator = get_unified_prompt_generator()
        extra_context = extra_context or {}
        
        # è·å–ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå…³ç³»ã€è®°å¿†ç­‰ï¼‰
        context_data = await _build_context_data(user_name, chat_stream, session.user_id)
        
        # æ ¹æ®æƒ…å†µç±»å‹é€‰æ‹©æç¤ºè¯ç”Ÿæˆæ–¹æ³•
        if situation_type == "timeout":
            system_prompt, user_prompt = await prompt_generator.generate_timeout_prompt(
                session=session,
                available_actions=available_actions,
                context_data=context_data,
                chat_stream=chat_stream,
            )
        elif situation_type == "proactive":
            trigger_context = extra_context.get("trigger_reason", "")
            system_prompt, user_prompt = await prompt_generator.generate_proactive_prompt(
                session=session,
                trigger_context=trigger_context,
                available_actions=available_actions,
                context_data=context_data,
                chat_stream=chat_stream,
            )
        else:
            # é»˜è®¤ä¸ºå›åº”æ¶ˆæ¯åœºæ™¯ (new_message, reply_in_time, reply_late)
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            message_content, sender_name, sender_id, message_time, all_unread = _get_last_user_message(
                session, user_name, chat_stream
            )
            
            system_prompt, user_prompt = await prompt_generator.generate_responding_prompt(
                session=session,
                message_content=message_content,
                sender_name=sender_name,
                sender_id=sender_id,
                message_time=message_time,
                available_actions=available_actions,
                context=chat_stream.context if chat_stream else None,
                context_data=context_data,
                chat_stream=chat_stream,
                all_unread_messages=all_unread,
            )
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = prompt_generator.build_messages_for_llm(
            system_prompt,
            user_prompt,
            stream_id=chat_stream.stream_id if chat_stream else "",
        )
        
        # æ˜¾ç¤ºæç¤ºè¯ï¼ˆè°ƒè¯•æ¨¡å¼ - åªæœ‰åœ¨é…ç½®ä¸­å¼€å¯æ—¶æ‰è¾“å‡ºï¼‰
        if global_config and global_config.debug.show_prompt:
            logger.info(
                f"[KFC] å®Œæ•´æç¤ºè¯ (stream={chat_stream.stream_id if chat_stream else 'unknown'}):\n"
                f"--- PROMPT START ---\n"
                f"{prompt}\n"
                f"--- PROMPT END ---"
            )
        
        # è·å– replyer æ¨¡å‹é…ç½®å¹¶è°ƒç”¨ LLM
        models = llm_api.get_available_models()
        replyer_config = models.get("replyer")
        
        if not replyer_config:
            logger.error("[KFC Unified] æœªæ‰¾åˆ° replyer æ¨¡å‹é…ç½®")
            return LLMResponse.create_error_response("æœªæ‰¾åˆ° replyer æ¨¡å‹é…ç½®")
        
        # è°ƒç”¨ LLMï¼ˆä½¿ç”¨åˆå¹¶åçš„æç¤ºè¯ï¼‰
        success, raw_response, reasoning, model_name = await llm_api.generate_with_model(
            prompt=prompt,
            model_config=replyer_config,
            request_type="kokoro_flow_chatter.unified",
        )
        
        if not success:
            logger.error(f"[KFC Unified] LLM è°ƒç”¨å¤±è´¥: {raw_response}")
            return LLMResponse.create_error_response(raw_response)
        
        # è¾“å‡ºåŸå§‹ JSON å“åº”ï¼ˆDEBUG çº§åˆ«ï¼Œç”¨äºè°ƒè¯•ï¼‰
        logger.debug(
            f"Raw JSON response from LLM for stream {chat_stream.stream_id if chat_stream else 'unknown'}:\n"
            f"--- JSON START ---\n"
            f"{raw_response}\n"
            f"--- JSON END ---"
        )
        
        # è§£æå“åº”
        return _parse_unified_response(raw_response, chat_stream.stream_id if chat_stream else None)
        
    except Exception as e:
        logger.error(f"[KFC Unified] ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return LLMResponse.create_error_response(str(e))


async def _build_context_data(
    user_name: str,
    chat_stream: Optional["ChatStream"],
    user_id: Optional[str] = None,
) -> dict[str, str]:
    """
    æ„å»ºä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå…³ç³»ã€è®°å¿†ã€è¡¨è¾¾ä¹ æƒ¯ç­‰ï¼‰
    """
    if not chat_stream:
        return {
            "relation_info": f"ä½ ä¸ {user_name} è¿˜ä¸å¤ªç†Ÿæ‚‰ï¼Œè¿™æ˜¯æ—©æœŸçš„äº¤æµé˜¶æ®µã€‚",
            "memory_block": "",
            "expression_habits": "",
            "schedule": "",
        }
    
    try:
        from .context_builder import KFCContextBuilder
        
        builder = KFCContextBuilder(chat_stream)
        
        # è·å–æœ€è¿‘çš„æ¶ˆæ¯ä½œä¸º target_messageï¼ˆç”¨äºè®°å¿†æ£€ç´¢ï¼‰
        target_message = ""
        if chat_stream.context:
            unread = chat_stream.context.get_unread_messages()
            if unread:
                target_message = unread[-1].processed_plain_text or unread[-1].display_message or ""
        
        context_data = await builder.build_all_context(
            sender_name=user_name,
            target_message=target_message,
            context=chat_stream.context,
            user_id=user_id,
        )
        
        return context_data
        
    except Exception as e:
        logger.warning(f"æ„å»ºä¸Šä¸‹æ–‡æ•°æ®å¤±è´¥: {e}")
        return {
            "relation_info": f"ä½ ä¸ {user_name} è¿˜ä¸å¤ªç†Ÿæ‚‰ï¼Œè¿™æ˜¯æ—©æœŸçš„äº¤æµé˜¶æ®µã€‚",
            "memory_block": "",
            "expression_habits": "",
            "schedule": "",
        }


def _get_last_user_message(
    session: KokoroSession,
    user_name: str,
    chat_stream: Optional["ChatStream"],
) -> tuple[str, str, str, float, Optional[list]]:
    """
    è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    
    Returns:
        tuple: (æ¶ˆæ¯å†…å®¹, å‘é€è€…åç§°, å‘é€è€…ID, æ¶ˆæ¯æ—¶é—´, æ‰€æœ‰æœªè¯»æ¶ˆæ¯åˆ—è¡¨)
    """
    message_content = ""
    sender_name = user_name
    sender_id = session.user_id or ""
    message_time = time.time()
    all_unread = None
    
    # ä» chat_stream è·å–æœªè¯»æ¶ˆæ¯
    if chat_stream and chat_stream.context:
        unread = chat_stream.context.get_unread_messages()
        if unread:
            all_unread = unread if len(unread) > 1 else None
            last_msg = unread[-1]
            message_content = last_msg.processed_plain_text or last_msg.display_message or ""
            if last_msg.user_info:
                sender_name = last_msg.user_info.user_nickname or user_name
                sender_id = str(last_msg.user_info.user_id)
            message_time = last_msg.time or time.time()
    
    # å¦‚æœæ²¡æœ‰ä» chat_stream è·å–åˆ°ï¼Œä» mental_log è·å–
    if not message_content:
        for entry in reversed(session.mental_log):
            if entry.event_type == EventType.USER_MESSAGE:
                message_content = entry.content or ""
                sender_name = entry.user_name or user_name
                message_time = entry.timestamp
                break
    
    return message_content, sender_name, sender_id, message_time, all_unread


def _parse_unified_response(raw_response: str, stream_id: str | None = None) -> LLMResponse:
    """
    è§£æç»Ÿä¸€æ¨¡å¼çš„ LLM å“åº”
    
    å“åº”æ ¼å¼ï¼š
    {
        "thought": "...",
        "expected_user_reaction": "...",
        "max_wait_seconds": 300,
        "actions": [{"type": "reply", "content": "..."}]
    }
    """
    data = extract_and_parse_json(raw_response, strict=False)
    
    if not data or not isinstance(data, dict):
        logger.warning(f"[KFC Unified] æ— æ³•è§£æ JSON: {raw_response[:200]}...")
        return LLMResponse.create_error_response("æ— æ³•è§£æå“åº”æ ¼å¼")
    
    # å…¼å®¹æ—§ç‰ˆçš„å­—æ®µå
    # expected_user_reaction -> expected_reaction
    if "expected_user_reaction" in data and "expected_reaction" not in data:
        data["expected_reaction"] = data["expected_user_reaction"]
    
    # å…¼å®¹æ—§ç‰ˆçš„ reply -> kfc_reply
    actions = data.get("actions", [])
    for action in actions:
        if isinstance(action, dict):
            if action.get("type") == "reply":
                action["type"] = "kfc_reply"
    
    response = LLMResponse.from_dict(data)
    
    # ç¾åŒ–æ—¥å¿—è¾“å‡ºï¼šå†…å¿ƒæ€è€ƒ + å›å¤å†…å®¹
    _log_pretty_response(response, stream_id)
    
    return response


def _log_pretty_response(response: LLMResponse, stream_id: str | None = None) -> None:
    """ç®€æ´è¾“å‡º LLM å“åº”æ—¥å¿—"""
    if not response.thought and not response.actions:
        logger.warning("[KFC] å“åº”ä¸ºç©º")
        return
    
    stream_tag = f"({stream_id[:8]}) " if stream_id else ""
    
    # æ”¶é›†å›å¤å†…å®¹å’Œå…¶ä»–åŠ¨ä½œ
    replies = []
    actions = []
    for action in response.actions:
        if action.type == "kfc_reply":
            content = action.params.get("content", "")
            if content:
                replies.append(content)
        elif action.type not in ("do_nothing", "no_action"):
            actions.append(action.type)
    
    # é€è¡Œè¾“å‡ºï¼Œç®€æ´æ˜äº†
    if response.thought:
        logger.info(f"[KFC] {stream_tag}ğŸ’­ {response.thought}")
    
    for i, reply in enumerate(replies):
        if len(replies) > 1:
            logger.info(f"[KFC] ğŸ’¬ [{i+1}] {reply}")
        else:
            logger.info(f"[KFC] ğŸ’¬ {reply}")
    
    if actions:
        logger.info(f"[KFC] ğŸ¯ {', '.join(actions)}")
    
    if response.max_wait_seconds > 0 or response.expected_reaction:
        meta = f"â± {response.max_wait_seconds}s" if response.max_wait_seconds > 0 else ""
        if response.expected_reaction:
            meta += f" é¢„æœŸ: {response.expected_reaction}"
        logger.info(f"[KFC] {meta.strip()}")