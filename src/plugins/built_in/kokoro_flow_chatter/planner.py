"""
Kokoro Flow Chatter - Planner

è§„åˆ’å™¨ï¼šè´Ÿè´£åˆ†ææƒ…å¢ƒå¹¶ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’
- è¾“å…¥ï¼šä¼šè¯çŠ¶æ€ã€ç”¨æˆ·æ¶ˆæ¯ã€æƒ…å¢ƒç±»å‹
- è¾“å‡ºï¼šLLMResponseï¼ˆåŒ…å« thoughtã€actionsã€expected_reactionã€max_wait_secondsï¼‰
- ä¸è´Ÿè´£ç”Ÿæˆå…·ä½“å›å¤æ–‡æœ¬ï¼Œåªå†³å®š"è¦åšä»€ä¹ˆ"
"""

from typing import TYPE_CHECKING, Optional

from src.common.logger import get_logger
from src.plugin_system.apis import llm_api
from src.utils.json_parser import extract_and_parse_json

from .models import LLMResponse
from .prompt.builder import get_prompt_builder
from .session import KokoroSession

if TYPE_CHECKING:
    from src.chat.message_receive.chat_stream import ChatStream

logger = get_logger("kfc_planner")


async def generate_plan(
    session: KokoroSession,
    user_name: str,
    situation_type: str = "new_message",
    chat_stream: Optional["ChatStream"] = None,
    available_actions: Optional[dict] = None,
    extra_context: Optional[dict] = None,
) -> LLMResponse:
    """
    ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’
    
    Args:
        session: ä¼šè¯å¯¹è±¡
        user_name: ç”¨æˆ·åç§°
        situation_type: æƒ…å†µç±»å‹
        chat_stream: èŠå¤©æµå¯¹è±¡
        available_actions: å¯ç”¨åŠ¨ä½œå­—å…¸
        extra_context: é¢å¤–ä¸Šä¸‹æ–‡
        
    Returns:
        LLMResponse å¯¹è±¡ï¼ŒåŒ…å«è®¡åˆ’ä¿¡æ¯
    """
    try:
        # 1. æ„å»ºè§„åˆ’å™¨æç¤ºè¯
        prompt_builder = get_prompt_builder()
        prompt = await prompt_builder.build_planner_prompt(
            session=session,
            user_name=user_name,
            situation_type=situation_type,
            chat_stream=chat_stream,
            available_actions=available_actions,
            extra_context=extra_context,
        )
        
        from src.config.config import global_config
        if global_config and global_config.debug.show_prompt:
            logger.info(f"[KFC Planner] ç”Ÿæˆçš„è§„åˆ’æç¤ºè¯:\n{prompt}")
        
        # 2. è·å– planner æ¨¡å‹é…ç½®å¹¶è°ƒç”¨ LLM
        models = llm_api.get_available_models()
        planner_config = models.get("planner")
        
        if not planner_config:
            logger.error("[KFC Planner] æœªæ‰¾åˆ° planner æ¨¡å‹é…ç½®")
            return LLMResponse.create_error_response("æœªæ‰¾åˆ° planner æ¨¡å‹é…ç½®")
        
        success, raw_response, reasoning, model_name = await llm_api.generate_with_model(
            prompt=prompt,
            model_config=planner_config,
            request_type="kokoro_flow_chatter.plan",
        )
        
        if not success:
            logger.error(f"[KFC Planner] LLM è°ƒç”¨å¤±è´¥: {raw_response}")
            return LLMResponse.create_error_response(raw_response)
        
        logger.debug(f"[KFC Planner] LLM å“åº” (model={model_name}):\n{raw_response}")
        
        # 3. è§£æå“åº”
        return _parse_response(raw_response)
        
    except Exception as e:
        logger.error(f"[KFC Planner] ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return LLMResponse.create_error_response(str(e))


def _parse_response(raw_response: str) -> LLMResponse:
    """è§£æ LLM å“åº”"""
    data = extract_and_parse_json(raw_response, strict=False)
    
    if not data or not isinstance(data, dict):
        logger.warning(f"[KFC Planner] æ— æ³•è§£æ JSON: {raw_response[:200]}...")
        return LLMResponse.create_error_response("æ— æ³•è§£æå“åº”æ ¼å¼")
    
    response = LLMResponse.from_dict(data)
    
    if response.thought:
        # ä½¿ç”¨ logger è¾“å‡ºç¾åŒ–æ—¥å¿—ï¼ˆé¢œè‰²é€šè¿‡ logger ç³»ç»Ÿé…ç½®ï¼‰
        logger.info(f"ğŸ’­ {response.thought}")
        
        actions_str = ", ".join(a.type for a in response.actions)
        logger.debug(f"actions={actions_str}")
    else:
        logger.warning("å“åº”ç¼ºå°‘ thought")
    
    return response
