"""å¤šçº§ç¼“å­˜ç®¡ç†å™¨

å®ç°é«˜æ€§èƒ½çš„å¤šçº§ç¼“å­˜ç³»ç»Ÿï¼š
- L1ç¼“å­˜ï¼šå†…å­˜ç¼“å­˜ï¼Œ1000é¡¹ï¼Œ60ç§’TTLï¼Œç”¨äºçƒ­ç‚¹æ•°æ®
- L2ç¼“å­˜ï¼šæ‰©å±•ç¼“å­˜ï¼Œ10000é¡¹ï¼Œ300ç§’TTLï¼Œç”¨äºæ¸©æ•°æ®
- LRUæ·˜æ±°ç­–ç•¥ï¼šè‡ªåŠ¨æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„æ•°æ®
- æ™ºèƒ½é¢„çƒ­ï¼šå¯åŠ¨æ—¶é¢„åŠ è½½é«˜é¢‘æ•°æ®
- ç»Ÿè®¡ä¿¡æ¯ï¼šå‘½ä¸­ç‡ã€æ·˜æ±°ç‡ç­‰ç›‘æ§æ•°æ®
"""

import asyncio
import builtins
import time
from collections import OrderedDict
from collections.abc import Callable
from dataclasses import dataclass
from typing import Any, Generic, TypeVar

from src.common.logger import get_logger
from src.common.memory_utils import estimate_cache_item_size

logger = get_logger("cache_manager")

T = TypeVar("T")


@dataclass
class CacheEntry(Generic[T]):
    """ç¼“å­˜æ¡ç›®

    Attributes:
        value: ç¼“å­˜çš„å€¼
        created_at: åˆ›å»ºæ—¶é—´æˆ³
        last_accessed: æœ€åè®¿é—®æ—¶é—´æˆ³
        access_count: è®¿é—®æ¬¡æ•°
        size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    """
    value: T
    created_at: float
    last_accessed: float
    access_count: int = 0
    size: int = 0


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Attributes:
        hits: å‘½ä¸­æ¬¡æ•°
        misses: æœªå‘½ä¸­æ¬¡æ•°
        evictions: æ·˜æ±°æ¬¡æ•°
        total_size: æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        item_count: æ¡ç›®æ•°é‡
    """
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    total_size: int = 0
    item_count: int = 0

    @property
    def hit_rate(self) -> float:
        """å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    @property
    def eviction_rate(self) -> float:
        """æ·˜æ±°ç‡"""
        return self.evictions / self.item_count if self.item_count > 0 else 0.0


class LRUCache(Generic[T]):
    """LRUç¼“å­˜å®ç°

    ä½¿ç”¨OrderedDictå®ç°O(1)çš„get/setæ“ä½œ
    """

    def __init__(
        self,
        max_size: int,
        ttl: float,
        name: str = "cache",
    ):
        """åˆå§‹åŒ–LRUç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            name: ç¼“å­˜åç§°ï¼Œç”¨äºæ—¥å¿—
        """
        self.max_size = max_size
        self.ttl = ttl
        self.name = name
        self._cache: OrderedDict[str, CacheEntry[T]] = OrderedDict()
        self._lock = asyncio.Lock()
        self._stats = CacheStats()

    async def get(self, key: str) -> T | None:
        """è·å–ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®

        Returns:
            ç¼“å­˜å€¼ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²è¿‡æœŸè¿”å›None
        """
        async with self._lock:
            entry = self._cache.get(key)

            if entry is None:
                self._stats.misses += 1
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            now = time.time()
            if now - entry.created_at > self.ttl:
                # è¿‡æœŸï¼Œåˆ é™¤æ¡ç›®
                del self._cache[key]
                self._stats.misses += 1
                self._stats.evictions += 1
                self._stats.item_count -= 1
                self._stats.total_size -= entry.size
                return None

            # å‘½ä¸­ï¼Œæ›´æ–°è®¿é—®ä¿¡æ¯
            entry.last_accessed = now
            entry.access_count += 1
            self._stats.hits += 1

            # ç§»åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self._cache.move_to_end(key)

            return entry.value

    async def set(
        self,
        key: str,
        value: T,
        size: int | None = None,
        ttl: float | None = None,
    ) -> None:
        """è®¾ç½®ç¼“å­˜å€¼

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™å°è¯•ä¼°ç®—
            ttl: è‡ªå®šä¹‰è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤TTL
        """
        async with self._lock:
            now = time.time()

            # å¦‚æœé”®å·²å­˜åœ¨ï¼Œæ›´æ–°å€¼
            if key in self._cache:
                old_entry = self._cache[key]
                self._stats.total_size -= old_entry.size

            # ä¼°ç®—å¤§å°
            if size is None:
                size = self._estimate_size(value)

            # åˆ›å»ºæ–°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†ttlï¼Œåˆ™ä¿®æ”¹created_atæ¥å®ç°è‡ªå®šä¹‰TTLï¼‰
            # é€šè¿‡è°ƒæ•´created_atï¼Œä½¿å¾—: now - created_at + custom_ttl = self.ttl
            # å³: created_at = now - (self.ttl - custom_ttl)
            if ttl is not None and ttl != self.ttl:
                # è°ƒæ•´åˆ›å»ºæ—¶é—´ä»¥å®ç°è‡ªå®šä¹‰TTL
                adjusted_created_at = now - (self.ttl - ttl)
                logger.debug(
                    f"[{self.name}] ä½¿ç”¨è‡ªå®šä¹‰TTL {ttl}s (é»˜è®¤{self.ttl}s) for key: {key}"
                )
            else:
                adjusted_created_at = now

            entry = CacheEntry(
                value=value,
                created_at=adjusted_created_at,
                last_accessed=now,
                access_count=0,
                size=size,
            )

            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
            while len(self._cache) >= self.max_size:
                oldest_key, oldest_entry = self._cache.popitem(last=False)
                self._stats.evictions += 1
                self._stats.item_count -= 1
                self._stats.total_size -= oldest_entry.size
                logger.debug(
                    f"[{self.name}] æ·˜æ±°ç¼“å­˜æ¡ç›®: {oldest_key} "
                    f"(è®¿é—®{oldest_entry.access_count}æ¬¡)"
                )

            # æ·»åŠ æ–°æ¡ç›®
            self._cache[key] = entry
            self._stats.item_count += 1
            self._stats.total_size += size

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ¡ç›®

        Args:
            key: ç¼“å­˜é”®

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        async with self._lock:
            entry = self._cache.pop(key, None)
            if entry:
                self._stats.item_count -= 1
                self._stats.total_size -= entry.size
                return True
            return False

    async def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        async with self._lock:
            self._cache.clear()
            self._stats = CacheStats()

    async def get_stats(self) -> CacheStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        async with self._lock:
            return CacheStats(
                hits=self._stats.hits,
                misses=self._stats.misses,
                evictions=self._stats.evictions,
                total_size=self._stats.total_size,
                item_count=self._stats.item_count,
            )

    def _estimate_size(self, value: Any) -> int:
        """ä¼°ç®—æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰- ä½¿ç”¨å‡†ç¡®çš„ä¼°ç®—æ–¹æ³•

        ä½¿ç”¨æ·±åº¦é€’å½’ä¼°ç®—ï¼Œæ¯” sys.getsizeof() æ›´å‡†ç¡®
        """
        try:
            return estimate_cache_item_size(value)
        except (TypeError, AttributeError):
            # æ— æ³•è·å–å¤§å°ï¼Œè¿”å›é»˜è®¤å€¼
            return 1024


class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨

    å®ç°ä¸¤çº§ç¼“å­˜æ¶æ„ï¼š
    - L1: é«˜é€Ÿç¼“å­˜ï¼Œå°å®¹é‡ï¼ŒçŸ­TTL
    - L2: æ‰©å±•ç¼“å­˜ï¼Œå¤§å®¹é‡ï¼Œé•¿TTL

    æŸ¥è¯¢æ—¶å…ˆæŸ¥L1ï¼Œæœªå‘½ä¸­å†æŸ¥L2ï¼Œæœªå‘½ä¸­å†ä»æ•°æ®æºåŠ è½½
    """

    def __init__(
        self,
        l1_max_size: int = 1000,
        l1_ttl: float = 60,
        l2_max_size: int = 10000,
        l2_ttl: float = 300,
        max_memory_mb: int = 100,
        max_item_size_mb: int = 1,
    ):
        """åˆå§‹åŒ–å¤šçº§ç¼“å­˜

        Args:
            l1_max_size: L1ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            l1_ttl: L1ç¼“å­˜TTLï¼ˆç§’ï¼‰
            l2_max_size: L2ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            l2_ttl: L2ç¼“å­˜TTLï¼ˆç§’ï¼‰
            max_memory_mb: æœ€å¤§å†…å­˜å ç”¨ï¼ˆMBï¼‰
            max_item_size_mb: å•ä¸ªç¼“å­˜æ¡ç›®æœ€å¤§å¤§å°ï¼ˆMBï¼‰
        """
        self.l1_cache: LRUCache[Any] = LRUCache(l1_max_size, l1_ttl, "L1")
        self.l2_cache: LRUCache[Any] = LRUCache(l2_max_size, l2_ttl, "L2")
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_item_size_bytes = max_item_size_mb * 1024 * 1024
        self._cleanup_task: asyncio.Task | None = None
        self._is_closing = False  # ğŸ”§ æ·»åŠ å…³é—­æ ‡å¿—

        logger.info(
            f"å¤šçº§ç¼“å­˜åˆå§‹åŒ–: L1({l1_max_size}é¡¹/{l1_ttl}s) "
            f"L2({l2_max_size}é¡¹/{l2_ttl}s) å†…å­˜ä¸Šé™({max_memory_mb}MB) "
            f"å•é¡¹ä¸Šé™({max_item_size_mb}MB)"
        )

    async def get(
        self,
        key: str,
        loader: Callable[[], Any] | None = None,
    ) -> Any | None:
        """ä»ç¼“å­˜è·å–æ•°æ®

        æŸ¥è¯¢é¡ºåºï¼šL1 -> L2 -> loader

        Args:
            key: ç¼“å­˜é”®
            loader: æ•°æ®åŠ è½½å‡½æ•°ï¼Œå½“ç¼“å­˜æœªå‘½ä¸­æ—¶è°ƒç”¨

        Returns:
            ç¼“å­˜å€¼æˆ–åŠ è½½çš„å€¼ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨è¿”å›None
        """
        # 1. å°è¯•ä»L1è·å–
        value = await self.l1_cache.get(key)
        if value is not None:
            logger.debug(f"L1ç¼“å­˜å‘½ä¸­: {key}")
            return value

        # 2. å°è¯•ä»L2è·å–
        value = await self.l2_cache.get(key)
        if value is not None:
            logger.debug(f"L2ç¼“å­˜å‘½ä¸­: {key}")
            # æå‡åˆ°L1
            await self.l1_cache.set(key, value)
            return value

        # 3. ä½¿ç”¨loaderåŠ è½½
        if loader is not None:
            logger.debug(f"ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®æºåŠ è½½: {key}")
            value = await loader() if asyncio.iscoroutinefunction(loader) else loader()
            if value is not None:
                # åŒæ—¶å†™å…¥L1å’ŒL2
                await self.set(key, value)
            return value

        return None

    async def set(
        self,
        key: str,
        value: Any,
        size: int | None = None,
        ttl: float | None = None,
    ) -> None:
        """è®¾ç½®ç¼“å­˜å€¼

        åŒæ—¶å†™å…¥L1å’ŒL2

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            size: æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            ttl: è‡ªå®šä¹‰è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤TTL
        """
        # ä¼°ç®—æ•°æ®å¤§å°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if size is None:
            size = estimate_cache_item_size(value)

        # æ£€æŸ¥å•ä¸ªæ¡ç›®å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
        if size > self.max_item_size_bytes:
            logger.warning(
                f"ç¼“å­˜æ¡ç›®è¿‡å¤§ï¼Œè·³è¿‡ç¼“å­˜: key={key}, "
                f"size={size / (1024 * 1024):.2f}MB, "
                f"limit={self.max_item_size_bytes / (1024 * 1024):.2f}MB"
            )
            return

        # æ ¹æ®TTLå†³å®šå†™å…¥å“ªä¸ªç¼“å­˜å±‚
        if ttl is not None:
            # æœ‰è‡ªå®šä¹‰TTLï¼Œæ ¹æ®TTLå¤§å°å†³å®šå†™å…¥å±‚çº§
            if ttl <= self.l1_cache.ttl:
                # çŸ­TTLï¼Œåªå†™å…¥L1
                await self.l1_cache.set(key, value, size, ttl)
            elif ttl <= self.l2_cache.ttl:
                # ä¸­ç­‰TTLï¼Œå†™å…¥L1å’ŒL2
                await self.l1_cache.set(key, value, size, ttl)
                await self.l2_cache.set(key, value, size, ttl)
            else:
                # é•¿TTLï¼Œåªå†™å…¥L2
                await self.l2_cache.set(key, value, size, ttl)
        else:
            # æ²¡æœ‰è‡ªå®šä¹‰TTLï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼ˆåŒæ—¶å†™å…¥L1å’ŒL2ï¼‰
            await self.l1_cache.set(key, value, size)
            await self.l2_cache.set(key, value, size)

    async def delete(self, key: str) -> None:
        """åˆ é™¤ç¼“å­˜æ¡ç›®

        åŒæ—¶ä»L1å’ŒL2åˆ é™¤

        Args:
            key: ç¼“å­˜é”®
        """
        await self.l1_cache.delete(key)
        await self.l2_cache.delete(key)

    async def clear(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        await self.l1_cache.clear()
        await self.l2_cache.clear()
        logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")

    async def get_stats(self) -> dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜å±‚çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿®å¤ç‰ˆï¼šé¿å…é”åµŒå¥—ï¼Œä½¿ç”¨è¶…æ—¶ï¼‰"""
        # ğŸ”§ ä¿®å¤ï¼šå¹¶è¡Œè·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…é”åµŒå¥—
        l1_stats_task = asyncio.create_task(self._get_cache_stats_safe(self.l1_cache, "L1"))
        l2_stats_task = asyncio.create_task(self._get_cache_stats_safe(self.l2_cache, "L2"))

        # ä½¿ç”¨è¶…æ—¶é¿å…æ­»é”
        results = await asyncio.gather(
            asyncio.wait_for(l1_stats_task, timeout=1.0),
            asyncio.wait_for(l2_stats_task, timeout=1.0),
            return_exceptions=True
        )
        l1_stats = results[0]
        l2_stats = results[1]

        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        if isinstance(l1_stats, BaseException):
            logger.error(f"L1ç»Ÿè®¡è·å–å¤±è´¥: {l1_stats}")
            l1_stats = CacheStats()
        if isinstance(l2_stats, BaseException):
            logger.error(f"L2ç»Ÿè®¡è·å–å¤±è´¥: {l2_stats}")
            l2_stats = CacheStats()

        assert isinstance(l1_stats, CacheStats)
        assert isinstance(l2_stats, CacheStats)

        # ğŸ”§ ä¿®å¤ï¼šå¹¶è¡Œè·å–é”®é›†åˆï¼Œé¿å…é”åµŒå¥—
        l1_keys_task = asyncio.create_task(self._get_cache_keys_safe(self.l1_cache))
        l2_keys_task = asyncio.create_task(self._get_cache_keys_safe(self.l2_cache))

        results = await asyncio.gather(
            asyncio.wait_for(l1_keys_task, timeout=1.0),
            asyncio.wait_for(l2_keys_task, timeout=1.0),
            return_exceptions=True
        )
        l1_keys = results[0]
        l2_keys = results[1]

        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        if isinstance(l1_keys, BaseException):
            logger.warning(f"L1é”®è·å–å¤±è´¥: {l1_keys}")
            l1_keys = set()
        if isinstance(l2_keys, BaseException):
            logger.warning(f"L2é”®è·å–å¤±è´¥: {l2_keys}")
            l2_keys = set()

        assert isinstance(l1_keys, set)
        assert isinstance(l2_keys, set)

        # è®¡ç®—å…±äº«é”®å’Œç‹¬å é”®
        shared_keys = l1_keys & l2_keys
        l1_only_keys = l1_keys - l2_keys
        l2_only_keys = l2_keys - l1_keys

        # ğŸ”§ ä¿®å¤ï¼šå¹¶è¡Œè®¡ç®—å†…å­˜ä½¿ç”¨ï¼Œé¿å…é”åµŒå¥—
        l1_size_task = asyncio.create_task(self._calculate_memory_usage_safe(self.l1_cache, l1_keys))
        l2_size_task = asyncio.create_task(self._calculate_memory_usage_safe(self.l2_cache, l2_keys))

        results = await asyncio.gather(
            asyncio.wait_for(l1_size_task, timeout=1.0),
            asyncio.wait_for(l2_size_task, timeout=1.0),
            return_exceptions=True
        )
        l1_size = results[0]
        l2_size = results[1]

        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        if isinstance(l1_size, BaseException):
            logger.warning(f"L1å†…å­˜è®¡ç®—å¤±è´¥: {l1_size}")
            l1_size = l1_stats.total_size
        if isinstance(l2_size, BaseException):
            logger.warning(f"L2å†…å­˜è®¡ç®—å¤±è´¥: {l2_size}")
            l2_size = l2_stats.total_size

        assert isinstance(l1_size, int)
        assert isinstance(l2_size, int)

        # è®¡ç®—å®é™…æ€»å†…å­˜ï¼ˆé¿å…é‡å¤è®¡æ•°ï¼‰
        actual_total_size = l1_size + l2_size - min(l1_stats.total_size, l2_stats.total_size)

        return {
            "l1": l1_stats,
            "l2": l2_stats,
            "total_memory_mb": actual_total_size / (1024 * 1024),
            "l1_only_mb": l1_size / (1024 * 1024),
            "l2_only_mb": l2_size / (1024 * 1024),
            "shared_mb": min(l1_stats.total_size, l2_stats.total_size) / (1024 * 1024),
            "shared_keys_count": len(shared_keys),
            "dedup_savings_mb": (l1_stats.total_size + l2_stats.total_size - actual_total_size) / (1024 * 1024),
            "max_memory_mb": self.max_memory_bytes / (1024 * 1024),
            "memory_usage_percent": (actual_total_size / self.max_memory_bytes * 100) if self.max_memory_bytes > 0 else 0,
        }

    async def _get_cache_stats_safe(self, cache, cache_name: str) -> CacheStats:
        """å®‰å…¨è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¸¦è¶…æ—¶ï¼‰"""
        try:
            return await asyncio.wait_for(cache.get_stats(), timeout=0.5)
        except asyncio.TimeoutError:
            logger.warning(f"{cache_name}ç»Ÿè®¡è·å–è¶…æ—¶")
            return CacheStats()
        except Exception as e:
            logger.error(f"{cache_name}ç»Ÿè®¡è·å–å¼‚å¸¸: {e}")
            return CacheStats()

    async def _get_cache_keys_safe(self, cache) -> builtins.set[str]:
        """å®‰å…¨è·å–ç¼“å­˜é”®é›†åˆï¼ˆå¸¦è¶…æ—¶ï¼‰"""
        try:
            # å¿«é€Ÿè·å–é”®é›†åˆï¼Œä½¿ç”¨è¶…æ—¶é¿å…æ­»é”
            return await asyncio.wait_for(
                self._extract_keys_with_lock(cache),
                timeout=0.5
            )
        except asyncio.TimeoutError:
            logger.warning(f"ç¼“å­˜é”®è·å–è¶…æ—¶: {cache.name}")
            return set()
        except Exception as e:
            logger.error(f"ç¼“å­˜é”®è·å–å¼‚å¸¸: {e}")
            return set()

    async def _extract_keys_with_lock(self, cache) -> builtins.set[str]:
        """åœ¨é”ä¿æŠ¤ä¸‹æå–é”®é›†åˆ"""
        async with cache._lock:
            return set(cache._cache.keys())

    async def _calculate_memory_usage_safe(self, cache, keys: builtins.set[str]) -> int:
        """å®‰å…¨è®¡ç®—å†…å­˜ä½¿ç”¨ï¼ˆå¸¦è¶…æ—¶ï¼‰"""
        if not keys:
            return 0

        try:
            return await asyncio.wait_for(
                self._calc_memory_with_lock(cache, keys),
                timeout=0.5
            )
        except asyncio.TimeoutError:
            logger.warning(f"å†…å­˜è®¡ç®—è¶…æ—¶: {cache.name}")
            return 0
        except Exception as e:
            logger.error(f"å†…å­˜è®¡ç®—å¼‚å¸¸: {e}")
            return 0

    async def _calc_memory_with_lock(self, cache, keys: builtins.set[str]) -> int:
        """åœ¨é”ä¿æŠ¤ä¸‹è®¡ç®—å†…å­˜ä½¿ç”¨"""
        total_size = 0
        async with cache._lock:
            for key in keys:
                entry = cache._cache.get(key)
                if entry:
                    total_size += entry.size
        return total_size

    async def check_memory_limit(self) -> None:
        """æ£€æŸ¥å¹¶å¼ºåˆ¶æ¸…ç†è¶…å‡ºå†…å­˜é™åˆ¶çš„ç¼“å­˜ï¼ˆä¿®å¤ç‰ˆï¼šé¿å…åµŒå¥—é”ï¼‰"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨è¶…æ—¶è·å–ç»Ÿè®¡ï¼Œé¿å…æ­»é”
            stats = await asyncio.wait_for(self.get_stats(), timeout=2.0)
            total_size = stats["total_memory_mb"] * (1024 * 1024)  # è½¬æ¢å›å­—èŠ‚

            if total_size > self.max_memory_bytes:
                memory_mb = total_size / (1024 * 1024)
                max_mb = self.max_memory_bytes / (1024 * 1024)
                logger.warning(
                    f"ç¼“å­˜å†…å­˜è¶…é™: {memory_mb:.2f}MB / {max_mb:.2f}MB "
                    f"({stats['memory_usage_percent']:.1f}%)ï¼Œå¼€å§‹åˆ†é˜¶æ®µæ¸…ç†"
                )

                # ğŸ”§ ä¿®å¤ï¼šåˆ†é˜¶æ®µæ¸…ç†ï¼Œæ¯é˜¶æ®µéƒ½æœ‰è¶…æ—¶ä¿æŠ¤
                cleanup_success = False

                # é˜¶æ®µ1: æ¸…ç†è¿‡æœŸæ¡ç›®
                try:
                    await asyncio.wait_for(self._clean_expired_entries(), timeout=3.0)

                    # é‡æ–°æ£€æŸ¥å†…å­˜ä½¿ç”¨
                    stats_after_clean = await asyncio.wait_for(self.get_stats(), timeout=1.0)
                    total_after_clean = stats_after_clean["total_memory_mb"] * (1024 * 1024)

                    if total_after_clean <= self.max_memory_bytes:
                        logger.info("æ¸…ç†è¿‡æœŸæ¡ç›®åå†…å­˜ä½¿ç”¨æ­£å¸¸")
                        cleanup_success = True
                except asyncio.TimeoutError:
                    logger.warning("æ¸…ç†è¿‡æœŸæ¡ç›®è¶…æ—¶ï¼Œè·³åˆ°å¼ºåˆ¶æ¸…ç†")

                # é˜¶æ®µ2: å¦‚æœè¿‡æœŸæ¸…ç†ä¸å¤Ÿï¼Œæ¸…ç†L2ç¼“å­˜
                if not cleanup_success:
                    try:
                        logger.info("å¼€å§‹æ¸…ç†L2ç¼“å­˜")
                        await asyncio.wait_for(self.l2_cache.clear(), timeout=2.0)
                        logger.info("L2ç¼“å­˜æ¸…ç†å®Œæˆ")

                        # æ£€æŸ¥L1ç¼“å­˜æ˜¯å¦è¿˜éœ€è¦æ¸…ç†
                        stats_after_l2 = await asyncio.wait_for(self.get_stats(), timeout=1.0)
                        total_after_l2 = stats_after_l2["total_memory_mb"] * (1024 * 1024)

                        if total_after_l2 > self.max_memory_bytes:
                            logger.warning("æ¸…ç†L2åä»è¶…é™ï¼Œç»§ç»­æ¸…ç†L1ç¼“å­˜")
                            await asyncio.wait_for(self.l1_cache.clear(), timeout=2.0)
                            logger.info("L1ç¼“å­˜æ¸…ç†å®Œæˆ")

                    except asyncio.TimeoutError:
                        logger.error("å¼ºåˆ¶æ¸…ç†è¶…æ—¶ï¼Œå†…å­˜å¯èƒ½ä»æœ‰é—®é¢˜")
                    except Exception as e:
                        logger.error(f"å¼ºåˆ¶æ¸…ç†å¤±è´¥: {e}")

                logger.info("ç¼“å­˜å†…å­˜é™åˆ¶æ£€æŸ¥å®Œæˆ")

        except asyncio.TimeoutError:
            logger.warning("å†…å­˜é™åˆ¶æ£€æŸ¥è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æŸ¥")
        except Exception as e:
            logger.error(f"å†…å­˜é™åˆ¶æ£€æŸ¥å¤±è´¥: {e}")

    async def start_cleanup_task(self, interval: float = 60) -> None:
        """å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡

        Args:
            interval: æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        """
        if self._cleanup_task is not None:
            logger.warning("æ¸…ç†ä»»åŠ¡å·²åœ¨è¿è¡Œ")
            return

        async def cleanup_loop():
            while not self._is_closing:
                try:
                    await asyncio.sleep(interval)

                    if self._is_closing:
                        break

                    stats = await self.get_stats()
                    l1_stats = stats["l1"]
                    l2_stats = stats["l2"]
                    logger.info(
                        f"ç¼“å­˜ç»Ÿè®¡ - L1: {l1_stats.item_count}é¡¹, "
                        f"å‘½ä¸­ç‡{l1_stats.hit_rate:.2%} | "
                        f"L2: {l2_stats.item_count}é¡¹, "
                        f"å‘½ä¸­ç‡{l2_stats.hit_rate:.2%} | "
                        f"å†…å­˜: {stats['total_memory_mb']:.2f}MB/{stats['max_memory_mb']:.2f}MB "
                        f"({stats['memory_usage_percent']:.1f}%) | "
                        f"å…±äº«: {stats['shared_keys_count']}é”®/{stats['shared_mb']:.2f}MB "
                        f"(å»é‡èŠ‚çœ{stats['dedup_savings_mb']:.2f}MB)"
                    )

                    # ğŸ”§ æ¸…ç†è¿‡æœŸæ¡ç›®
                    await self._clean_expired_entries()

                    # æ£€æŸ¥å†…å­˜é™åˆ¶
                    await self.check_memory_limit()

                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")

        self._cleanup_task = asyncio.create_task(cleanup_loop())
        logger.info(f"ç¼“å­˜æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš”{interval}ç§’")

    async def stop_cleanup_task(self) -> None:
        """åœæ­¢æ¸…ç†ä»»åŠ¡"""
        self._is_closing = True

        if self._cleanup_task is not None:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
            self._cleanup_task = None
            logger.info("ç¼“å­˜æ¸…ç†ä»»åŠ¡å·²åœæ­¢")

    async def _clean_expired_entries(self) -> None:
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®ï¼ˆä¿®å¤ç‰ˆï¼šå¹¶è¡Œæ¸…ç†ï¼Œé¿å…é”åµŒå¥—ï¼‰"""
        try:
            current_time = time.time()

            # ğŸ”§ ä¿®å¤ï¼šå¹¶è¡Œæ¸…ç† L1 å’Œ L2ï¼Œä½¿ç”¨è¶…æ—¶é¿å…æ­»é”
            async def clean_l1_expired():
                """æ¸…ç†L1è¿‡æœŸæ¡ç›®"""
                try:
                    # ä½¿ç”¨è¶…æ—¶é¿å…é•¿æ—¶é—´æŒé”
                    await asyncio.wait_for(
                        self._clean_cache_layer_expired(self.l1_cache, current_time, "L1"),
                        timeout=2.0
                    )
                except asyncio.TimeoutError:
                    logger.warning("L1ç¼“å­˜æ¸…ç†è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡æ¸…ç†")
                except Exception as e:
                    logger.error(f"L1ç¼“å­˜æ¸…ç†å¼‚å¸¸: {e}")

            async def clean_l2_expired():
                """æ¸…ç†L2è¿‡æœŸæ¡ç›®"""
                try:
                    # ä½¿ç”¨è¶…æ—¶é¿å…é•¿æ—¶é—´æŒé”
                    await asyncio.wait_for(
                        self._clean_cache_layer_expired(self.l2_cache, current_time, "L2"),
                        timeout=2.0
                    )
                except asyncio.TimeoutError:
                    logger.warning("L2ç¼“å­˜æ¸…ç†è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡æ¸…ç†")
                except Exception as e:
                    logger.error(f"L2ç¼“å­˜æ¸…ç†å¼‚å¸¸: {e}")

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¹¶è¡Œæ‰§è¡Œæ¸…ç†ï¼Œé¿å…ä¸²è¡Œç­‰å¾…
            l1_task = asyncio.create_task(clean_l1_expired())
            l2_task = asyncio.create_task(clean_l2_expired())

            # ç­‰å¾…ä¸¤ä¸ªæ¸…ç†ä»»åŠ¡å®Œæˆï¼ˆä½¿ç”¨return_exceptionsé¿å…ä¸€ä¸ªå¤±è´¥å½±å“å¦ä¸€ä¸ªï¼‰
            results = await asyncio.gather(l1_task, l2_task, return_exceptions=True)

            # æ£€æŸ¥æ¸…ç†ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ç¼“å­˜æ¸…ç†ä»»åŠ¡ {'L1' if i == 0 else 'L2'} å¤±è´¥: {result}")
                else:
                    logger.debug(f"ç¼“å­˜æ¸…ç†ä»»åŠ¡ {'L1' if i == 0 else 'L2'} å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸæ¡ç›®å¤±è´¥: {e}")

    async def _clean_cache_layer_expired(self, cache_layer, current_time: float, layer_name: str) -> int:
        """æ¸…ç†å•ä¸ªç¼“å­˜å±‚çš„è¿‡æœŸæ¡ç›®ï¼ˆé¿å…é”åµŒå¥—ï¼‰"""
        expired_keys = []
        cleaned_count = 0

        try:
            # å¿«é€Ÿæ‰«æè¿‡æœŸé”®ï¼ˆçŸ­æš‚æŒé”ï¼‰
            async with cache_layer._lock:
                expired_keys = [
                    key for key, entry in cache_layer._cache.items()
                    if current_time - entry.created_at > cache_layer.ttl
                ]

            # åˆ†æ‰¹åˆ é™¤è¿‡æœŸé”®ï¼Œé¿å…é•¿æ—¶é—´æŒé”
            batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªé”®
            for i in range(0, len(expired_keys), batch_size):
                batch = expired_keys[i:i + batch_size]

                async with cache_layer._lock:
                    for key in batch:
                        entry = cache_layer._cache.pop(key, None)
                        if entry:
                            cache_layer._stats.evictions += 1
                            cache_layer._stats.item_count -= 1
                            cache_layer._stats.total_size -= entry.size
                            cleaned_count += 1

                # åœ¨æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚è®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                if i + batch_size < len(expired_keys):
                    await asyncio.sleep(0.001)  # 1ms

            if cleaned_count > 0:
                logger.debug(f"{layer_name}ç¼“å­˜æ¸…ç†å®Œæˆ: {cleaned_count} ä¸ªè¿‡æœŸæ¡ç›®")

        except Exception as e:
            logger.error(f"{layer_name}ç¼“å­˜å±‚æ¸…ç†å¤±è´¥: {e}")
            raise

        return cleaned_count


# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache: MultiLevelCache | None = None
_cache_lock = asyncio.Lock()


async def get_cache() -> MultiLevelCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹ï¼ˆå•ä¾‹ï¼‰

    ä»é…ç½®æ–‡ä»¶è¯»å–ç¼“å­˜å‚æ•°ï¼Œå¦‚æœé…ç½®æœªåŠ è½½åˆ™ä½¿ç”¨é»˜è®¤å€¼
    å¦‚æœé…ç½®ä¸­ç¦ç”¨äº†ç¼“å­˜ï¼Œè¿”å›ä¸€ä¸ªæœ€å°åŒ–çš„ç¼“å­˜å®ä¾‹ï¼ˆå®¹é‡ä¸º1ï¼‰
    """
    global _global_cache

    if _global_cache is None:
        async with _cache_lock:
            if _global_cache is None:
                # å°è¯•ä»é…ç½®è¯»å–å‚æ•°
                try:
                    from src.config.config import global_config

                    assert global_config is not None
                    db_config = global_config.database

                    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
                    if not db_config.enable_database_cache:
                        logger.info("æ•°æ®åº“ç¼“å­˜å·²ç¦ç”¨ï¼Œä½¿ç”¨æœ€å°åŒ–ç¼“å­˜å®ä¾‹")
                        _global_cache = MultiLevelCache(
                            l1_max_size=1,
                            l1_ttl=1,
                            l2_max_size=1,
                            l2_ttl=1,
                            max_memory_mb=1,
                        )
                        return _global_cache

                    l1_max_size = db_config.cache_l1_max_size
                    l1_ttl = db_config.cache_l1_ttl
                    l2_max_size = db_config.cache_l2_max_size
                    l2_ttl = db_config.cache_l2_ttl
                    max_memory_mb = db_config.cache_max_memory_mb
                    max_item_size_mb = db_config.cache_max_item_size_mb
                    cleanup_interval = db_config.cache_cleanup_interval

                    logger.info(
                        f"ä»é…ç½®åŠ è½½ç¼“å­˜å‚æ•°: L1({l1_max_size}/{l1_ttl}s), "
                        f"L2({l2_max_size}/{l2_ttl}s), å†…å­˜é™åˆ¶({max_memory_mb}MB), "
                        f"å•é¡¹é™åˆ¶({max_item_size_mb}MB)"
                    )
                except Exception as e:
                    # é…ç½®æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    logger.warning(f"æ— æ³•ä»é…ç½®åŠ è½½ç¼“å­˜å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    l1_max_size = 1000
                    l1_ttl = 60
                    l2_max_size = 10000
                    l2_ttl = 300
                    max_memory_mb = 100
                    max_item_size_mb = 1
                    cleanup_interval = 60

                _global_cache = MultiLevelCache(
                    l1_max_size=l1_max_size,
                    l1_ttl=l1_ttl,
                    l2_max_size=l2_max_size,
                    l2_ttl=l2_ttl,
                    max_memory_mb=max_memory_mb,
                    max_item_size_mb=max_item_size_mb,
                )
                await _global_cache.start_cleanup_task(interval=cleanup_interval)

    return _global_cache


async def close_cache() -> None:
    """å…³é—­å…¨å±€ç¼“å­˜"""
    global _global_cache

    if _global_cache is not None:
        await _global_cache.stop_cleanup_task()
        await _global_cache.clear()
        _global_cache = None
        logger.info("å…¨å±€ç¼“å­˜å·²å…³é—­")
