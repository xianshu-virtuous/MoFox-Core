"""å¢å¼ºçš„æ•°æ®åº“æ‰¹é‡è°ƒåº¦å™¨

åœ¨åŸæœ‰æ‰¹å¤„ç†åŠŸèƒ½åŸºç¡€ä¸Šï¼Œå¢åŠ ï¼š
- è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ®æ•°æ®åº“è´Ÿè½½åŠ¨æ€è°ƒæ•´
- ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šæ”¯æŒç´§æ€¥æ“ä½œä¼˜å…ˆæ‰§è¡Œ
- æ€§èƒ½ç›‘æ§ï¼šè¯¦ç»†çš„æ‰§è¡Œç»Ÿè®¡å’Œåˆ†æ
- æ™ºèƒ½åˆå¹¶ï¼šæ›´é«˜æ•ˆçš„æ“ä½œåˆå¹¶ç­–ç•¥
"""

import asyncio
import time
from collections import defaultdict, deque
from collections.abc import Callable
from dataclasses import dataclass, field
from enum import IntEnum
from typing import Any, TypeVar

from sqlalchemy import delete, insert, select, update

from src.common.database.core.session import get_db_session_direct
from src.common.logger import get_logger
from src.common.memory_utils import estimate_size_smart

logger = get_logger("batch_scheduler")

T = TypeVar("T")


class Priority(IntEnum):
    """æ“ä½œä¼˜å…ˆçº§"""
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3


@dataclass
class BatchOperation:
    """æ‰¹é‡æ“ä½œ"""

    operation_type: str  # 'select', 'insert', 'update', 'delete'
    model_class: type
    conditions: dict[str, Any] = field(default_factory=dict)
    data: dict[str, Any] | None = None
    callback: Callable | None = None
    future: asyncio.Future | None = None
    timestamp: float = field(default_factory=time.time)
    priority: Priority = Priority.NORMAL
    timeout: float | None = None  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰


@dataclass
class BatchStats:
    """æ‰¹å¤„ç†ç»Ÿè®¡"""

    total_operations: int = 0
    batched_operations: int = 0
    cache_hits: int = 0
    total_execution_time: float = 0.0
    avg_batch_size: float = 0.0
    avg_wait_time: float = 0.0
    timeout_count: int = 0
    error_count: int = 0

    # è‡ªé€‚åº”ç»Ÿè®¡
    last_batch_duration: float = 0.0
    last_batch_size: int = 0
    congestion_score: float = 0.0  # æ‹¥å¡è¯„åˆ† (0-1)

    # ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç»Ÿè®¡
    cache_size: int = 0  # ç¼“å­˜æ¡ç›®æ•°
    cache_memory_mb: float = 0.0  # ç¼“å­˜å†…å­˜å ç”¨ï¼ˆMBï¼‰


class AdaptiveBatchScheduler:
    """è‡ªé€‚åº”æ‰¹é‡è°ƒåº¦å™¨

    ç‰¹æ€§ï¼š
    - åŠ¨æ€æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´
    - ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šé«˜ä¼˜å…ˆçº§æ“ä½œä¼˜å…ˆæ‰§è¡Œ
    - æ™ºèƒ½ç­‰å¾…ï¼šæ ¹æ®é˜Ÿåˆ—æƒ…å†µåŠ¨æ€è°ƒæ•´ç­‰å¾…æ—¶é—´
    - è¶…æ—¶å¤„ç†ï¼šé˜²æ­¢æ“ä½œé•¿æ—¶é—´é˜»å¡
    """

    def __init__(
        self,
        min_batch_size: int = 10,
        max_batch_size: int = 100,
        base_wait_time: float = 0.05,  # 50ms
        max_wait_time: float = 0.2,  # 200ms
        max_queue_size: int = 1000,
        cache_ttl: float = 30.0,
    ):
        """åˆå§‹åŒ–è°ƒåº¦å™¨

        Args:
            min_batch_size: æœ€å°æ‰¹æ¬¡å¤§å°
            max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            base_wait_time: åŸºç¡€ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_queue_size: æœ€å¤§é˜Ÿåˆ—å¤§å°
            cache_ttl: ç¼“å­˜TTLï¼ˆç§’ï¼‰
        """
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.current_batch_size = min_batch_size
        self.base_wait_time = base_wait_time
        self.max_wait_time = max_wait_time
        self.current_wait_time = base_wait_time
        self.max_queue_size = max_queue_size
        self.cache_ttl = cache_ttl

        # æ“ä½œé˜Ÿåˆ—ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç±»
        self.operation_queues: dict[Priority, deque[BatchOperation]] = {
            priority: deque() for priority in Priority
        }

        # è°ƒåº¦æ§åˆ¶
        self._scheduler_task: asyncio.Task | None = None
        self._is_running = False
        self._lock = asyncio.Lock()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = BatchStats()

        # ğŸ”§ æ”¹è¿›çš„ç»“æœç¼“å­˜ï¼ˆå¸¦å¤§å°é™åˆ¶å’Œå†…å­˜ç»Ÿè®¡ï¼‰
        self._result_cache: dict[str, tuple[Any, float]] = {}
        self._cache_max_size = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self._cache_memory_estimate = 0  # ç¼“å­˜å†…å­˜ä¼°ç®—ï¼ˆå­—èŠ‚ï¼‰
        self._cache_size_map: dict[str, int] = {}  # æ¯ä¸ªç¼“å­˜æ¡ç›®çš„å¤§å°

        logger.info(
            f"è‡ªé€‚åº”æ‰¹é‡è°ƒåº¦å™¨åˆå§‹åŒ–: "
            f"æ‰¹æ¬¡å¤§å°{min_batch_size}-{max_batch_size}, "
            f"ç­‰å¾…æ—¶é—´{base_wait_time*1000:.0f}-{max_wait_time*1000:.0f}ms"
        )

    async def start(self) -> None:
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self._is_running:
            logger.warning("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return

        self._is_running = True
        self._scheduler_task = asyncio.create_task(self._scheduler_loop())
        logger.info("æ‰¹é‡è°ƒåº¦å™¨å·²å¯åŠ¨")

    async def stop(self) -> None:
        """åœæ­¢è°ƒåº¦å™¨"""
        if not self._is_running:
            return

        self._is_running = False

        if self._scheduler_task:
            self._scheduler_task.cancel()
            try:
                await self._scheduler_task
            except asyncio.CancelledError:
                pass

        # å¤„ç†å‰©ä½™æ“ä½œ
        await self._flush_all_queues()
        logger.info("æ‰¹é‡è°ƒåº¦å™¨å·²åœæ­¢")

    async def add_operation(
        self,
        operation: BatchOperation,
    ) -> asyncio.Future:
        """æ·»åŠ æ“ä½œåˆ°é˜Ÿåˆ—

        Args:
            operation: æ‰¹é‡æ“ä½œ

        Returns:
            Futureå¯¹è±¡ï¼Œå¯ç”¨äºè·å–ç»“æœ
        """
        # æ£€æŸ¥ç¼“å­˜
        if operation.operation_type == "select":
            cache_key = self._generate_cache_key(operation)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                future = asyncio.get_event_loop().create_future()
                future.set_result(cached_result)
                return future

        # åˆ›å»ºfuture
        future = asyncio.get_event_loop().create_future()
        operation.future = future

        should_execute_immediately = False
        total_queued = 0

        async with self._lock:
            # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡
            total_queued = sum(len(q) for q in self.operation_queues.values())
            if total_queued >= self.max_queue_size:
                should_execute_immediately = True
            else:
                # æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
                self.operation_queues[operation.priority].append(operation)
                self.stats.total_operations += 1

        # ğŸ”§ ä¿®å¤ï¼šåœ¨é”å¤–æ‰§è¡Œæ“ä½œï¼Œé¿å…æ­»é”
        if should_execute_immediately:
            logger.warning(f"é˜Ÿåˆ—å·²æ»¡({total_queued})ï¼Œç›´æ¥æ‰§è¡Œæ“ä½œ")
            await self._execute_operations([operation])

        return future

    async def _scheduler_loop(self) -> None:
        """è°ƒåº¦å™¨ä¸»å¾ªç¯"""
        while self._is_running:
            try:
                await asyncio.sleep(self.current_wait_time)
                await self._flush_all_queues()
                await self._adjust_parameters()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨å¾ªç¯å¼‚å¸¸: {e}")

    async def _flush_all_queues(self) -> None:
        """åˆ·æ–°æ‰€æœ‰é˜Ÿåˆ—"""
        async with self._lock:
            # æ”¶é›†æ“ä½œï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            operations = []
            for priority in sorted(Priority, reverse=True):
                queue = self.operation_queues[priority]
                count = min(len(queue), self.current_batch_size - len(operations))
                if queue and count > 0:
                    # ä½¿ç”¨ list.extend ä»£æ›¿å¾ªç¯ append
                    operations.extend(queue.popleft() for _ in range(count))

            if not operations:
                return

        # æ‰§è¡Œæ‰¹é‡æ“ä½œ
        await self._execute_operations(operations)

    async def _execute_operations(
        self,
        operations: list[BatchOperation],
    ) -> None:
        """æ‰§è¡Œæ‰¹é‡æ“ä½œ"""
        if not operations:
            return

        start_time = time.time()
        batch_size = len(operations)

        try:
            # æ£€æŸ¥è¶…æ—¶
            valid_operations = []
            for op in operations:
                if op.timeout and (time.time() - op.timestamp) > op.timeout:
                    # è¶…æ—¶ï¼Œè®¾ç½®å¼‚å¸¸
                    if op.future and not op.future.done():
                        op.future.set_exception(TimeoutError("æ“ä½œè¶…æ—¶"))
                    self.stats.timeout_count += 1
                else:
                    valid_operations.append(op)

            if not valid_operations:
                return

            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
            op_groups = defaultdict(list)
            for op in valid_operations:
                key = f"{op.operation_type}_{op.model_class.__name__}"
                op_groups[key].append(op)

            # æ‰§è¡Œå„ç»„æ“ä½œ
            for ops in op_groups.values():
                await self._execute_group(ops)

            # æ›´æ–°ç»Ÿè®¡
            duration = time.time() - start_time
            self.stats.batched_operations += batch_size
            self.stats.total_execution_time += duration
            self.stats.last_batch_duration = duration
            self.stats.last_batch_size = batch_size

            if self.stats.batched_operations > 0:
                self.stats.avg_batch_size = (
                    self.stats.batched_operations /
                    (self.stats.total_execution_time / duration)
                )

            logger.debug(
                f"æ‰¹é‡æ‰§è¡Œå®Œæˆ: {batch_size}ä¸ªæ“ä½œ, è€—æ—¶{duration*1000:.2f}ms"
            )

        except Exception as e:
            logger.error(f"æ‰¹é‡æ“ä½œæ‰§è¡Œå¤±è´¥: {e}")
            self.stats.error_count += 1

            # è®¾ç½®æ‰€æœ‰futureçš„å¼‚å¸¸
            for op in operations:
                if op.future and not op.future.done():
                    op.future.set_exception(e)

    async def _execute_group(self, operations: list[BatchOperation]) -> None:
        """æ‰§è¡ŒåŒç±»æ“ä½œç»„"""
        if not operations:
            return

        op_type = operations[0].operation_type

        try:
            if op_type == "select":
                await self._execute_select_batch(operations)
            elif op_type == "insert":
                await self._execute_insert_batch(operations)
            elif op_type == "update":
                await self._execute_update_batch(operations)
            elif op_type == "delete":
                await self._execute_delete_batch(operations)
            else:
                raise ValueError(f"æœªçŸ¥æ“ä½œç±»å‹: {op_type}")

        except Exception as e:
            logger.error(f"æ‰§è¡Œ{op_type}æ“ä½œç»„å¤±è´¥: {e}")
            for op in operations:
                if op.future and not op.future.done():
                    op.future.set_exception(e)

    async def _execute_select_batch(
        self,
        operations: list[BatchOperation],
    ) -> None:
        """æ‰¹é‡æ‰§è¡ŒæŸ¥è¯¢æ“ä½œ"""
        async with get_db_session_direct() as session:
            for op in operations:
                try:
                    # æ„å»ºæŸ¥è¯¢
                    stmt = select(op.model_class)
                    for key, value in op.conditions.items():
                        attr = getattr(op.model_class, key)
                        if isinstance(value, list | tuple | set):
                            stmt = stmt.where(attr.in_(value))
                        else:
                            stmt = stmt.where(attr == value)

                    # æ‰§è¡ŒæŸ¥è¯¢
                    result = await session.execute(stmt)
                    data = result.scalars().all()

                    # è®¾ç½®ç»“æœ
                    if op.future and not op.future.done():
                        op.future.set_result(data)

                    # ç¼“å­˜ç»“æœ
                    cache_key = self._generate_cache_key(op)
                    self._set_cache(cache_key, data)

                    # æ‰§è¡Œå›è°ƒ
                    if op.callback:
                        try:
                            op.callback(data)
                        except Exception as e:
                            logger.warning(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

                except Exception as e:
                    logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                    if op.future and not op.future.done():
                        op.future.set_exception(e)

    async def _execute_insert_batch(
        self,
        operations: list[BatchOperation],
    ) -> None:
        """æ‰¹é‡æ‰§è¡Œæ’å…¥æ“ä½œ"""
        async with get_db_session_direct() as session:
            try:
                # æ”¶é›†æ•°æ®ï¼Œå¹¶è¿‡æ»¤æ‰ id=None çš„æƒ…å†µï¼ˆè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆï¼‰
                all_data = []
                for op in operations:
                    if op.data:
                        # è¿‡æ»¤æ‰ id ä¸º None çš„é”®ï¼Œè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆä¸»é”®
                        filtered_data = {k: v for k, v in op.data.items() if not (k == "id" and v is None)}
                        all_data.append(filtered_data)
                
                if not all_data:
                    return

                # æ‰¹é‡æ’å…¥
                stmt = insert(operations[0].model_class).values(all_data)
                await session.execute(stmt)
                # æ³¨æ„ï¼šcommit ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†

                # è®¾ç½®ç»“æœ
                for op in operations:
                    if op.future and not op.future.done():
                        op.future.set_result(True)

                    if op.callback:
                        try:
                            op.callback(True)
                        except Exception as e:
                            logger.warning(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

            except Exception as e:
                logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
                # æ³¨æ„ï¼šrollback ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†
                for op in operations:
                    if op.future and not op.future.done():
                        op.future.set_exception(e)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘ rollback

    async def _execute_update_batch(
        self,
        operations: list[BatchOperation],
    ) -> None:
        """æ‰¹é‡æ‰§è¡Œæ›´æ–°æ“ä½œ"""
        async with get_db_session_direct() as session:
            results = []
            try:
                # ğŸ”§ æ”¶é›†æ‰€æœ‰æ“ä½œåä¸€æ¬¡æ€§commitï¼Œè€Œä¸æ˜¯å¾ªç¯ä¸­å¤šæ¬¡commit
                for op in operations:
                    # æ„å»ºæ›´æ–°è¯­å¥
                    stmt = update(op.model_class)
                    for key, value in op.conditions.items():
                        attr = getattr(op.model_class, key)
                        stmt = stmt.where(attr == value)

                    if op.data:
                        stmt = stmt.values(**op.data)

                    # æ‰§è¡Œæ›´æ–°ï¼ˆä½†ä¸commitï¼‰
                    result = await session.execute(stmt)
                    results.append((op, result.rowcount))

                # æ³¨æ„ï¼šcommit ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†

                # è®¾ç½®æ‰€æœ‰æ“ä½œçš„ç»“æœ
                for op, rowcount in results:
                    if op.future and not op.future.done():
                        op.future.set_result(rowcount)

                    if op.callback:
                        try:
                            op.callback(rowcount)
                        except Exception as e:
                            logger.warning(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

            except Exception as e:
                logger.error(f"æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
                # æ³¨æ„ï¼šrollback ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†
                # æ‰€æœ‰æ“ä½œéƒ½å¤±è´¥
                for op in operations:
                    if op.future and not op.future.done():
                        op.future.set_exception(e)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘ rollback

    async def _execute_delete_batch(
        self,
        operations: list[BatchOperation],
    ) -> None:
        """æ‰¹é‡æ‰§è¡Œåˆ é™¤æ“ä½œ"""
        async with get_db_session_direct() as session:
            results = []
            try:
                # ğŸ”§ æ”¶é›†æ‰€æœ‰æ“ä½œåä¸€æ¬¡æ€§commitï¼Œè€Œä¸æ˜¯å¾ªç¯ä¸­å¤šæ¬¡commit
                for op in operations:
                    # æ„å»ºåˆ é™¤è¯­å¥
                    stmt = delete(op.model_class)
                    for key, value in op.conditions.items():
                        attr = getattr(op.model_class, key)
                        stmt = stmt.where(attr == value)

                    # æ‰§è¡Œåˆ é™¤ï¼ˆä½†ä¸commitï¼‰
                    result = await session.execute(stmt)
                    results.append((op, result.rowcount))

                # æ³¨æ„ï¼šcommit ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†

                # è®¾ç½®æ‰€æœ‰æ“ä½œçš„ç»“æœ
                for op, rowcount in results:
                    if op.future and not op.future.done():
                        op.future.set_result(rowcount)

                    if op.callback:
                        try:
                            op.callback(rowcount)
                        except Exception as e:
                            logger.warning(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")

            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
                # æ³¨æ„ï¼šrollback ç”± get_db_session_direct ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†
                # æ‰€æœ‰æ“ä½œéƒ½å¤±è´¥
                for op in operations:
                    if op.future and not op.future.done():
                        op.future.set_exception(e)
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘ rollback

    async def _adjust_parameters(self) -> None:
        """æ ¹æ®æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´å‚æ•°"""
        # è®¡ç®—æ‹¥å¡è¯„åˆ†
        total_queued = sum(len(q) for q in self.operation_queues.values())
        self.stats.congestion_score = min(1.0, total_queued / self.max_queue_size)

        # æ ¹æ®æ‹¥å¡æƒ…å†µè°ƒæ•´æ‰¹æ¬¡å¤§å°
        if self.stats.congestion_score > 0.7:
            # é«˜æ‹¥å¡ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
            self.current_batch_size = min(
                self.max_batch_size,
                int(self.current_batch_size * 1.2),
            )
        elif self.stats.congestion_score < 0.3:
            # ä½æ‹¥å¡ï¼Œå‡å°æ‰¹æ¬¡å¤§å°
            self.current_batch_size = max(
                self.min_batch_size,
                int(self.current_batch_size * 0.9),
            )

        # æ ¹æ®æ‰¹æ¬¡æ‰§è¡Œæ—¶é—´è°ƒæ•´ç­‰å¾…æ—¶é—´
        if self.stats.last_batch_duration > 0:
            if self.stats.last_batch_duration > self.current_wait_time * 2:
                # æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
                self.current_wait_time = min(
                    self.max_wait_time,
                    self.current_wait_time * 1.1,
                )
            elif self.stats.last_batch_duration < self.current_wait_time * 0.5:
                # æ‰§è¡Œå¾ˆå¿«ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                self.current_wait_time = max(
                    self.base_wait_time,
                    self.current_wait_time * 0.9,
                )

    def _generate_cache_key(self, operation: BatchOperation) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_parts = [
            operation.operation_type,
            operation.model_class.__name__,
            str(sorted(operation.conditions.items())),
        ]
        return "|".join(key_parts)

    def _get_from_cache(self, cache_key: str) -> Any | None:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if cache_key in self._result_cache:
            result, timestamp = self._result_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                self.stats.cache_hits += 1
                return result
            else:
                del self._result_cache[cache_key]
        return None

    def _set_cache(self, cache_key: str, result: Any) -> None:
        """è®¾ç½®ç¼“å­˜ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¸¦å¤§å°é™åˆ¶å’Œå†…å­˜ç»Ÿè®¡ï¼‰"""

        # ğŸ”§ æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self._result_cache) >= self._cache_max_size:
            # é¦–å…ˆæ¸…ç†è¿‡æœŸæ¡ç›®
            current_time = time.time()
            expired_keys = [
                k for k, (_, ts) in self._result_cache.items()
                if current_time - ts >= self.cache_ttl
            ]

            for k in expired_keys:
                # æ›´æ–°å†…å­˜ç»Ÿè®¡
                if k in self._cache_size_map:
                    self._cache_memory_estimate -= self._cache_size_map[k]
                    del self._cache_size_map[k]
                del self._result_cache[k]

            # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œæ¸…ç†æœ€è€çš„æ¡ç›®ï¼ˆLRUï¼‰
            if len(self._result_cache) >= self._cache_max_size:
                oldest_key = min(
                    self._result_cache.keys(),
                    key=lambda k: self._result_cache[k][1]
                )
                # æ›´æ–°å†…å­˜ç»Ÿè®¡
                if oldest_key in self._cache_size_map:
                    self._cache_memory_estimate -= self._cache_size_map[oldest_key]
                    del self._cache_size_map[oldest_key]
                del self._result_cache[oldest_key]
                logger.debug(f"ç¼“å­˜å·²æ»¡ï¼Œæ·˜æ±°æœ€è€æ¡ç›®: {oldest_key}")

        # ğŸ”§ ä½¿ç”¨å‡†ç¡®çš„å†…å­˜ä¼°ç®—æ–¹æ³•
        try:
            total_size = estimate_size_smart(cache_key) + estimate_size_smart(result)
            self._cache_size_map[cache_key] = total_size
            self._cache_memory_estimate += total_size
        except Exception as e:
            logger.debug(f"ä¼°ç®—ç¼“å­˜å¤§å°å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼
            self._cache_size_map[cache_key] = 1024
            self._cache_memory_estimate += 1024

        self._result_cache[cache_key] = (result, time.time())

    async def get_stats(self) -> BatchStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¹è¿›ç‰ˆï¼ŒåŒ…å«ç¼“å­˜ç»Ÿè®¡ï¼‰"""
        async with self._lock:
            return BatchStats(
                total_operations=self.stats.total_operations,
                batched_operations=self.stats.batched_operations,
                cache_hits=self.stats.cache_hits,
                total_execution_time=self.stats.total_execution_time,
                avg_batch_size=self.stats.avg_batch_size,
                timeout_count=self.stats.timeout_count,
                error_count=self.stats.error_count,
                last_batch_duration=self.stats.last_batch_duration,
                last_batch_size=self.stats.last_batch_size,
                congestion_score=self.stats.congestion_score,
                # ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç»Ÿè®¡
                cache_size=len(self._result_cache),
                cache_memory_mb=self._cache_memory_estimate / (1024 * 1024),
            )


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_global_scheduler: AdaptiveBatchScheduler | None = None
_scheduler_lock = asyncio.Lock()


async def get_batch_scheduler() -> AdaptiveBatchScheduler:
    """è·å–å…¨å±€æ‰¹é‡è°ƒåº¦å™¨ï¼ˆå•ä¾‹ï¼‰"""
    global _global_scheduler

    if _global_scheduler is None:
        async with _scheduler_lock:
            if _global_scheduler is None:
                _global_scheduler = AdaptiveBatchScheduler()
                await _global_scheduler.start()

    return _global_scheduler


async def close_batch_scheduler() -> None:
    """å…³é—­å…¨å±€æ‰¹é‡è°ƒåº¦å™¨"""
    global _global_scheduler

    if _global_scheduler is not None:
        await _global_scheduler.stop()
        _global_scheduler = None
        logger.info("å…¨å±€æ‰¹é‡è°ƒåº¦å™¨å·²å…³é—­")
