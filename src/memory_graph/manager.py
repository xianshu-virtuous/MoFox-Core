"""
è®°å¿†ç®¡ç†å™¨ - Phase 3

ç»Ÿä¸€çš„è®°å¿†ç³»ç»Ÿç®¡ç†æ¥å£ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
- è®°å¿†åˆ›å»ºã€æ£€ç´¢ã€æ›´æ–°ã€åˆ é™¤
- è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ¿€æ´»ã€é—å¿˜ï¼‰
- è®°å¿†æ•´åˆä¸ç»´æŠ¤
- å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–
"""

import asyncio
import logging
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

from src.config.config import global_config
from src.config.official_configs import MemoryConfig
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import EdgeType, Memory, MemoryEdge, NodeType
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.tools.memory_tools import MemoryTools
from src.memory_graph.utils.embeddings import EmbeddingGenerator
from src.memory_graph.utils.graph_expansion import expand_memories_with_semantic_filter as _expand_graph
from src.memory_graph.utils.similarity import cosine_similarity

if TYPE_CHECKING:
    import numpy as np

logger = logging.getLogger(__name__)


class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨

    æ ¸å¿ƒç®¡ç†ç±»ï¼Œæä¾›è®°å¿†ç³»ç»Ÿçš„ç»Ÿä¸€æ¥å£ï¼š
    - è®°å¿† CRUD æ“ä½œ
    - è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†
    - æ™ºèƒ½æ£€ç´¢ä¸æ¨è
    - è®°å¿†ç»´æŠ¤ä¸ä¼˜åŒ–
    """

    def __init__(
        self,
        data_dir: Path | None = None,
    ):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»global_configè¯»å–ï¼‰
        """
        # ç›´æ¥ä½¿ç”¨ global_config.memory
        if not global_config.memory or not getattr(global_config.memory, "enable", False):
            raise ValueError("è®°å¿†ç³»ç»Ÿæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ [memory] enable = true")

        self.config: MemoryConfig = global_config.memory
        self.data_dir = data_dir or Path(getattr(self.config, "data_dir", "data/memory_graph"))

        # å­˜å‚¨ç»„ä»¶
        self.vector_store: VectorStore | None = None
        self.graph_store: GraphStore | None = None
        self.persistence: PersistenceManager | None = None

        # æ ¸å¿ƒç»„ä»¶
        self.embedding_generator: EmbeddingGenerator | None = None
        self.extractor: MemoryExtractor | None = None
        self.builder: MemoryBuilder | None = None
        self.tools: MemoryTools | None = None

        # çŠ¶æ€
        self._initialized = False
        self._last_maintenance = datetime.now()
        self._maintenance_task: asyncio.Task | None = None
        self._maintenance_interval_hours = getattr(self.config, "consolidation_interval_hours", 1.0)
        self._maintenance_schedule_id: str | None = None  # è°ƒåº¦ä»»åŠ¡ID

        logger.info(f"è®°å¿†ç®¡ç†å™¨å·²åˆ›å»º (data_dir={self.data_dir}, enable={getattr(self.config, 'enable', False)})")

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        æŒ‰ç…§ä¾èµ–é¡ºåºåˆå§‹åŒ–ï¼š
        1. å­˜å‚¨å±‚ï¼ˆå‘é‡å­˜å‚¨ã€å›¾å­˜å‚¨ã€æŒä¹…åŒ–ï¼‰
        2. å·¥å…·å±‚ï¼ˆåµŒå…¥ç”Ÿæˆå™¨ã€æå–å™¨ï¼‰
        3. ç®¡ç†å±‚ï¼ˆæ„å»ºå™¨ã€å·¥å…·æ¥å£ï¼‰
        """
        if self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–")
            return

        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨...")

            # 1. åˆå§‹åŒ–å­˜å‚¨å±‚
            self.data_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å­˜å‚¨é…ç½®
            storage_config = getattr(self.config, "storage", None)
            vector_collection_name = getattr(storage_config, "vector_collection_name", "memory_graph") if storage_config else "memory_graph"

            self.vector_store = VectorStore(
                collection_name=vector_collection_name,
                data_dir=self.data_dir,
            )
            await self.vector_store.initialize()

            self.persistence = PersistenceManager(data_dir=self.data_dir)

            # å°è¯•åŠ è½½ç°æœ‰å›¾æ•°æ®
            self.graph_store = await self.persistence.load_graph_store()
            if not self.graph_store:
                logger.info("æœªæ‰¾åˆ°ç°æœ‰å›¾æ•°æ®ï¼Œåˆ›å»ºæ–°çš„å›¾å­˜å‚¨")
                self.graph_store = GraphStore()
            else:
                stats = self.graph_store.get_statistics()
                logger.info(
                    f"åŠ è½½å›¾æ•°æ®: {stats['total_memories']} æ¡è®°å¿†, "
                    f"{stats['total_nodes']} ä¸ªèŠ‚ç‚¹, {stats['total_edges']} æ¡è¾¹"
                )

            # 2. åˆå§‹åŒ–å·¥å…·å±‚
            self.embedding_generator = EmbeddingGenerator()
            # EmbeddingGenerator ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–

            self.extractor = MemoryExtractor()

            # 3. åˆå§‹åŒ–ç®¡ç†å±‚
            self.builder = MemoryBuilder(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                embedding_generator=self.embedding_generator,
            )

            # æ£€æŸ¥é…ç½®å€¼
            expand_depth = self.config.search_max_expand_depth
            expand_semantic_threshold = self.config.search_expand_semantic_threshold
            logger.info(f"ğŸ“Š é…ç½®æ£€æŸ¥: search_max_expand_depth={expand_depth}, search_expand_semantic_threshold={expand_semantic_threshold}")

            self.tools = MemoryTools(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                persistence_manager=self.persistence,
                embedding_generator=self.embedding_generator,
                max_expand_depth=expand_depth,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•æ·±åº¦
                expand_semantic_threshold=expand_semantic_threshold,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•è¯­ä¹‰é˜ˆå€¼
            )

            self._initialized = True
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

            # å¯åŠ¨åå°ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
            await self.start_maintenance_scheduler()

        except Exception as e:
            logger.error(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def shutdown(self) -> None:
        """
        å…³é—­è®°å¿†ç®¡ç†å™¨

        æ‰§è¡Œæ¸…ç†æ“ä½œï¼š
        - åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        - ä¿å­˜æ‰€æœ‰æ•°æ®
        - å…³é—­å­˜å‚¨ç»„ä»¶
        """
        if not self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— éœ€å…³é—­")
            return

        try:
            logger.info("æ­£åœ¨å…³é—­è®°å¿†ç®¡ç†å™¨...")

            # 1. åœæ­¢è°ƒåº¦ä»»åŠ¡
            await self.stop_maintenance_scheduler()

            # 2. æ‰§è¡Œæœ€åä¸€æ¬¡ç»´æŠ¤ï¼ˆä¿å­˜æ•°æ®ï¼‰
            if self.graph_store and self.persistence:
                logger.info("æ‰§è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜...")
                await self.persistence.save_graph_store(self.graph_store)

            # 3. å…³é—­å­˜å‚¨ç»„ä»¶
            if self.vector_store:
                # VectorStore ä½¿ç”¨ chromadbï¼Œæ— éœ€æ˜¾å¼å…³é—­
                pass

            self._initialized = False
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å…³é—­è®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)

    # ==================== è®°å¿† CRUD æ“ä½œ ====================

    async def create_memory(
        self,
        subject: str,
        memory_type: str,
        topic: str,
        object: str | None = None,
        attributes: dict[str, str] | None = None,
        importance: float = 0.5,
        **kwargs,
    ) -> Memory | None:
        """
        åˆ›å»ºæ–°è®°å¿†

        Args:
            subject: ä¸»ä½“ï¼ˆè°ï¼‰
            memory_type: è®°å¿†ç±»å‹ï¼ˆäº‹ä»¶/è§‚ç‚¹/äº‹å®/å…³ç³»ï¼‰
            topic: ä¸»é¢˜ï¼ˆåšä»€ä¹ˆ/æƒ³ä»€ä¹ˆï¼‰
            object: å®¢ä½“ï¼ˆå¯¹è°/å¯¹ä»€ä¹ˆï¼‰
            attributes: å±æ€§å­—å…¸ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€åŸå› ç­‰ï¼‰
            importance: é‡è¦æ€§ (0.0-1.0)
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åˆ›å»ºçš„è®°å¿†å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.create_memory(
                subject=subject,
                memory_type=memory_type,
                topic=topic,
                object=object,
                attributes=attributes,
                importance=importance,
                **kwargs,
            )

            if result["success"]:
                memory_id = result["memory_id"]
                memory = self.graph_store.get_memory_by_id(memory_id)
                logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory_id}")
                return memory
            else:
                logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
                return None

        except Exception as e:
            logger.error(f"åˆ›å»ºè®°å¿†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return None

    async def get_memory(self, memory_id: str) -> Memory | None:
        """
        æ ¹æ® ID è·å–è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            è®°å¿†å¯¹è±¡ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        return self.graph_store.get_memory_by_id(memory_id)

    async def update_memory(
        self,
        memory_id: str,
        **updates,
    ) -> bool:
        """
        æ›´æ–°è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            **updates: è¦æ›´æ–°çš„å­—æ®µ

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°å…ƒæ•°æ®
            if "importance" in updates:
                memory.importance = updates["importance"]

            if "metadata" in updates:
                memory.metadata.update(updates["metadata"])

            memory.updated_at = datetime.now()

            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†æ›´æ–°æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
            for node in memory.nodes:
                if node.embedding is not None:
                    await self.vector_store.delete_node(node.id)

            # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            self.graph_store.remove_memory(memory_id)

            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†åˆ é™¤æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†æ£€ç´¢æ“ä½œ ====================
    async def search_memories(
        self,
        query: str,
        top_k: int = 10,
        memory_types: list[str] | None = None,
        time_range: tuple[datetime, datetime] | None = None,
        min_importance: float = 0.0,
        include_forgotten: bool = False,
        use_multi_query: bool = True,
        expand_depth: int | None = None,
        context: dict[str, Any] | None = None,
    ) -> list[Memory]:
        """
        æœç´¢è®°å¿†

        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼Œè§£å†³å¤æ‚æŸ¥è¯¢é—®é¢˜ã€‚
        ä¾‹å¦‚ï¼š"æ°ç‘å–µå¦‚ä½•è¯„ä»·æ–°çš„è®°å¿†ç³»ç»Ÿ" ä¼šè¢«åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ï¼Œ
        ç¡®ä¿åŒæ—¶åŒ¹é…"æ°ç‘å–µ"å’Œ"æ–°çš„è®°å¿†ç³»ç»Ÿ"ä¸¤ä¸ªå…³é”®æ¦‚å¿µã€‚

        åŒæ—¶æ”¯æŒå›¾æ‰©å±•ï¼šä»åˆå§‹æ£€ç´¢ç»“æœå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æŸ¥æ‰¾è¯­ä¹‰ç›¸å…³çš„é‚»å±…è®°å¿†ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°
            memory_types: è®°å¿†ç±»å‹è¿‡æ»¤
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ (start, end)
            min_importance: æœ€å°é‡è¦æ€§
            include_forgotten: æ˜¯å¦åŒ…å«å·²é—å¿˜çš„è®°å¿†
            use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆæ¨èï¼Œé»˜è®¤Trueï¼‰
            expand_depth: å›¾æ‰©å±•æ·±åº¦ï¼ˆ0=ç¦ç”¨, 1=æ¨è, 2-3=æ·±åº¦æ¢ç´¢ï¼‰
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¼˜åŒ–ï¼‰

        Returns:
            è®°å¿†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        try:
            # å‡†å¤‡æœç´¢å‚æ•°
            params = {
                "query": query,
                "top_k": top_k,
                "use_multi_query": use_multi_query,
                "expand_depth": expand_depth or global_config.memory.search_max_expand_depth,  # ä¼ é€’å›¾æ‰©å±•æ·±åº¦
                "context": context,
            }

            if memory_types:
                params["memory_types"] = memory_types

            # æ‰§è¡Œæœç´¢
            result = await self.tools.search_memories(**params)

            if not result["success"]:
                logger.error(f"æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}")
                return []

            memories = result.get("results", [])

            # åå¤„ç†è¿‡æ»¤
            filtered_memories = []
            for mem_dict in memories:
                # ä»å­—å…¸é‡å»º Memory å¯¹è±¡
                memory_id = mem_dict.get("memory_id", "")
                if not memory_id:
                    continue

                memory = self.graph_store.get_memory_by_id(memory_id)
                if not memory:
                    continue

                # é‡è¦æ€§è¿‡æ»¤
                if min_importance is not None and memory.importance < min_importance:
                    continue

                # é—å¿˜çŠ¶æ€è¿‡æ»¤
                if not include_forgotten and memory.metadata.get("forgotten", False):
                    continue

                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                if time_range:
                    mem_time = memory.created_at
                    if not (time_range[0] <= mem_time <= time_range[1]):
                        continue

                filtered_memories.append(memory)

            strategy = result.get("strategy", "unknown")
            logger.info(
                f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(filtered_memories)} æ¡è®°å¿† (ç­–ç•¥={strategy})"
            )

            # å¼ºåˆ¶æ¿€æ´»è¢«æ£€ç´¢åˆ°çš„è®°å¿†ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
            if filtered_memories:
                await self._auto_activate_searched_memories(filtered_memories)

            return filtered_memories[:top_k]

        except Exception as e:
            logger.error(f"æœç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    async def link_memories(
        self,
        source_description: str,
        target_description: str,
        relation_type: str,
        importance: float = 0.5,
    ) -> bool:
        """
        å…³è”ä¸¤æ¡è®°å¿†

        Args:
            source_description: æºè®°å¿†æè¿°
            target_description: ç›®æ ‡è®°å¿†æè¿°
            relation_type: å…³ç³»ç±»å‹ï¼ˆå¯¼è‡´/å¼•ç”¨/ç›¸ä¼¼/ç›¸åï¼‰
            importance: å…³ç³»é‡è¦æ€§

        Returns:
            æ˜¯å¦å…³è”æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.link_memories(
                source_memory_description=source_description,
                target_memory_description=target_description,
                relation_type=relation_type,
                importance=importance,
            )

            if result["success"]:
                logger.info(
                    f"è®°å¿†å…³è”æˆåŠŸ: {result['source_memory_id']} -> "
                    f"{result['target_memory_id']} ({relation_type})"
                )
                return True
            else:
                logger.error(f"è®°å¿†å…³è”å¤±è´¥: {result.get('error', 'Unknown error')}")
                return False

        except Exception as e:
            logger.error(f"å…³è”è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

    async def activate_memory(self, memory_id: str, strength: float = 1.0) -> bool:
        """
        æ¿€æ´»è®°å¿†

        æ›´æ–°è®°å¿†çš„æ¿€æ´»åº¦ï¼Œå¹¶ä¼ æ’­åˆ°ç›¸å…³è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            strength: æ¿€æ´»å¼ºåº¦ (0.0-1.0)

        Returns:
            æ˜¯å¦æ¿€æ´»æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°æ¿€æ´»ä¿¡æ¯
            now = datetime.now()
            activation_info = memory.metadata.get("activation", {})

            # æ›´æ–°æ¿€æ´»åº¦ï¼ˆè€ƒè™‘æ—¶é—´è¡°å‡ï¼‰
            last_access = activation_info.get("last_access")
            if last_access:
                # è®¡ç®—æ—¶é—´è¡°å‡
                last_access_dt = datetime.fromisoformat(last_access)
                hours_passed = (now - last_access_dt).total_seconds() / 3600
                decay_rate = getattr(self.config, "activation_decay_rate", 0.95)
                decay_factor = decay_rate ** (hours_passed / 24)
                current_activation = activation_info.get("level", 0.0) * decay_factor
            else:
                current_activation = 0.0

            # æ–°çš„æ¿€æ´»åº¦ = å½“å‰æ¿€æ´»åº¦ + æ¿€æ´»å¼ºåº¦
            new_activation = min(1.0, current_activation + strength)

            activation_info.update({
                "level": new_activation,
                "last_access": now.isoformat(),
                "access_count": activation_info.get("access_count", 0) + 1,
            })

            # åŒæ­¥æ›´æ–° memory.activation å­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            memory.activation = new_activation
            memory.metadata["activation"] = activation_info
            memory.last_accessed = now

            # æ¿€æ´»ä¼ æ’­ï¼šæ¿€æ´»ç›¸å…³è®°å¿†
            if strength > 0.1:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                propagation_depth = getattr(self.config, "activation_propagation_depth", 2)
                related_memories = self._get_related_memories(
                    memory_id,
                    max_depth=propagation_depth
                )
                propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.5)
                propagation_strength = strength * propagation_strength_factor

                max_related = getattr(self.config, "max_related_memories", 5)
                for related_id in related_memories[:max_related]:
                    await self.activate_memory(related_id, propagation_strength)

            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.debug(f"è®°å¿†å·²æ¿€æ´»: {memory_id} (level={new_activation:.3f})")
            return True

        except Exception as e:
            logger.error(f"æ¿€æ´»è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def _auto_activate_searched_memories(self, memories: list[Memory]) -> None:
        """
        è‡ªåŠ¨æ¿€æ´»è¢«æœç´¢åˆ°çš„è®°å¿†

        Args:
            memories: è¢«æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            # è·å–é…ç½®å‚æ•°
            base_strength = getattr(self.config, "auto_activate_base_strength", 0.1)
            max_activate_count = getattr(self.config, "auto_activate_max_count", 5)

            # æ¿€æ´»å¼ºåº¦æ ¹æ®è®°å¿†é‡è¦æ€§è°ƒæ•´
            activate_tasks = []
            for i, memory in enumerate(memories[:max_activate_count]):
                # é‡è¦æ€§è¶Šé«˜ï¼Œæ¿€æ´»å¼ºåº¦è¶Šå¤§
                strength = base_strength * (0.5 + memory.importance)

                # åˆ›å»ºå¼‚æ­¥æ¿€æ´»ä»»åŠ¡
                task = self.activate_memory(memory.id, strength=strength)
                activate_tasks.append(task)

                if i >= max_activate_count - 1:
                    break

            # å¹¶å‘æ‰§è¡Œæ¿€æ´»ä»»åŠ¡ï¼ˆä½†ä¸ç­‰å¾…æ‰€æœ‰å®Œæˆï¼Œé¿å…é˜»å¡æœç´¢ï¼‰
            if activate_tasks:
                import asyncio
                # ä½¿ç”¨ asyncio.gather ä½†è®¾ç½®è¾ƒçŸ­çš„ timeout
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*activate_tasks, return_exceptions=True),
                        timeout=2.0  # 2ç§’è¶…æ—¶ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
                    )
                    logger.debug(f"è‡ªåŠ¨æ¿€æ´» {len(activate_tasks)} æ¡è®°å¿†å®Œæˆ")
                except asyncio.TimeoutError:
                    logger.warning(f"è‡ªåŠ¨æ¿€æ´»è®°å¿†è¶…æ—¶ï¼Œå·²æ¿€æ´»éƒ¨åˆ†è®°å¿†")
                except Exception as e:
                    logger.warning(f"è‡ªåŠ¨æ¿€æ´»è®°å¿†å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"è‡ªåŠ¨æ¿€æ´»æœç´¢è®°å¿†å¤±è´¥: {e}")

    def _get_related_memories(self, memory_id: str, max_depth: int = 1) -> list[str]:
        """
        è·å–ç›¸å…³è®°å¿† ID åˆ—è¡¨ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºæ¿€æ´»ä¼ æ’­ï¼‰

        Args:
            memory_id: è®°å¿† ID
            max_depth: æœ€å¤§éå†æ·±åº¦

        Returns:
            ç›¸å…³è®°å¿† ID åˆ—è¡¨
        """
        memory = self.graph_store.get_memory_by_id(memory_id)
        if not memory:
            return []

        related_ids = set()

        # éå†è®°å¿†çš„èŠ‚ç‚¹
        for node in memory.nodes:
            # è·å–èŠ‚ç‚¹çš„é‚»å±…
            neighbors = list(self.graph_store.graph.neighbors(node.id))

            for neighbor_id in neighbors:
                # è·å–é‚»å±…èŠ‚ç‚¹æ‰€å±çš„è®°å¿†
                neighbor_node = self.graph_store.graph.nodes.get(neighbor_id)
                if neighbor_node:
                    neighbor_memory_ids = neighbor_node.get("memory_ids", [])
                    for mem_id in neighbor_memory_ids:
                        if mem_id != memory_id:
                            related_ids.add(mem_id)

        return list(related_ids)

    async def expand_memories_with_semantic_filter(
        self,
        initial_memory_ids: list[str],
        query_embedding: "np.ndarray",
        max_depth: int = 2,
        semantic_threshold: float = 0.5,
        max_expanded: int = 20
    ) -> list[tuple[str, float]]:
        """
        ä»åˆå§‹è®°å¿†é›†åˆå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æ‰©å±•ï¼Œå¹¶ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤

        è¿™ä¸ªæ–¹æ³•è§£å†³äº†çº¯å‘é‡æœç´¢å¯èƒ½é—æ¼çš„"è¯­ä¹‰ç›¸å…³ä¸”å›¾ç»“æ„ç›¸å…³"çš„è®°å¿†ã€‚

        Args:
            initial_memory_ids: åˆå§‹è®°å¿†IDé›†åˆï¼ˆç”±å‘é‡æœç´¢å¾—åˆ°ï¼‰
            query_embedding: æŸ¥è¯¢å‘é‡
            max_depth: æœ€å¤§æ‰©å±•æ·±åº¦ï¼ˆ1-3æ¨èï¼‰
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5æ¨èï¼‰
            max_expanded: æœ€å¤šæ‰©å±•å¤šå°‘ä¸ªè®°å¿†

        Returns:
            List[(memory_id, relevance_score)] æŒ‰ç›¸å…³åº¦æ’åº
        """
        return await _expand_graph(
            graph_store=self.graph_store,
            vector_store=self.vector_store,
            initial_memory_ids=initial_memory_ids,
            query_embedding=query_embedding,
            max_depth=max_depth,
            semantic_threshold=semantic_threshold,
            max_expanded=max_expanded,
        )

    async def forget_memory(self, memory_id: str) -> bool:
        """
        é—å¿˜è®°å¿†ï¼ˆæ ‡è®°ä¸ºå·²é—å¿˜ï¼Œä¸åˆ é™¤ï¼‰

        Args:
            memory_id: è®°å¿† ID

        Returns:
            æ˜¯å¦é—å¿˜æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            memory.metadata["forgotten"] = True
            memory.metadata["forgotten_at"] = datetime.now().isoformat()

            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†å·²é—å¿˜: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"é—å¿˜è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def auto_forget_memories(self, threshold: float = 0.1) -> int:
        """
        è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦çš„è®°å¿†

        Args:
            threshold: æ¿€æ´»åº¦é˜ˆå€¼

        Returns:
            é—å¿˜çš„è®°å¿†æ•°é‡
        """
        if not self._initialized:
            await self.initialize()

        try:
            forgotten_count = 0
            all_memories = self.graph_store.get_all_memories()

            for memory in all_memories:
                # è·³è¿‡å·²é—å¿˜çš„è®°å¿†
                if memory.metadata.get("forgotten", False):
                    continue

                # è·³è¿‡é«˜é‡è¦æ€§è®°å¿†
                min_importance = getattr(self.config, "forgetting_min_importance", 7.0)
                if memory.importance >= min_importance:
                    continue

                # è®¡ç®—å½“å‰æ¿€æ´»åº¦
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")

                if last_access:
                    last_access_dt = datetime.fromisoformat(last_access)
                    days_passed = (datetime.now() - last_access_dt).days

                    # é•¿æ—¶é—´æœªè®¿é—®çš„è®°å¿†ï¼Œåº”ç”¨æ—¶é—´è¡°å‡
                    decay_factor = 0.9 ** days_passed
                    current_activation = activation_info.get("level", 0.0) * decay_factor

                    # ä½äºé˜ˆå€¼åˆ™é—å¿˜
                    if current_activation < threshold:
                        await self.forget_memory(memory.id)
                        forgotten_count += 1

            logger.info(f"è‡ªåŠ¨é—å¿˜å®Œæˆ: é—å¿˜äº† {forgotten_count} æ¡è®°å¿†")
            return forgotten_count

        except Exception as e:
            logger.error(f"è‡ªåŠ¨é—å¿˜å¤±è´¥: {e}", exc_info=True)
            return 0

    # ==================== ç»Ÿè®¡ä¸ç»´æŠ¤ ====================

    def get_statistics(self) -> dict[str, Any]:
        """
        è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self._initialized or not self.graph_store:
            return {}

        stats = self.graph_store.get_statistics()

        # æ·»åŠ æ¿€æ´»åº¦ç»Ÿè®¡
        all_memories = self.graph_store.get_all_memories()
        activation_levels = []
        forgotten_count = 0

        for memory in all_memories:
            if memory.metadata.get("forgotten", False):
                forgotten_count += 1
            else:
                activation_info = memory.metadata.get("activation", {})
                activation_levels.append(activation_info.get("level", 0.0))

        if activation_levels:
            stats["avg_activation"] = sum(activation_levels) / len(activation_levels)
            stats["max_activation"] = max(activation_levels)
        else:
            stats["avg_activation"] = 0.0
            stats["max_activation"] = 0.0

        stats["forgotten_memories"] = forgotten_count
        stats["active_memories"] = stats["total_memories"] - forgotten_count

        return stats

    async def consolidate_memories(
        self,
        similarity_threshold: float = 0.85,
        time_window_hours: float = 24.0,
        max_batch_size: int = 50,
    ) -> dict[str, Any]:
        """
        æ•´ç†è®°å¿†ï¼šç›´æ¥åˆå¹¶å»é‡ç›¸ä¼¼è®°å¿†ï¼ˆä¸åˆ›å»ºæ–°è¾¹ï¼‰

        æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼š
        1. ä½¿ç”¨ asyncio.create_task åœ¨åå°æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
        2. å‘é‡è®¡ç®—æ‰¹é‡å¤„ç†ï¼Œå‡å°‘é‡å¤è®¡ç®—
        3. å»¶è¿Ÿä¿å­˜ï¼Œæ‰¹é‡å†™å…¥æ•°æ®åº“
        4. æ›´é¢‘ç¹çš„åä½œå¼å¤šä»»åŠ¡è®©å‡º

        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼Œå»ºè®®æé«˜åˆ°0.9å‡å°‘è¯¯åˆ¤ï¼‰
            time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_batch_size: å•æ¬¡æœ€å¤šå¤„ç†çš„è®°å¿†æ•°é‡

        Returns:
            æ•´ç†ç»“æœï¼ˆå¦‚æœæ˜¯å¼‚æ­¥æ‰§è¡Œï¼Œè¿”å›å¯åŠ¨çŠ¶æ€ï¼‰
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info(f"ğŸš€ å¯åŠ¨è®°å¿†æ•´ç†ä»»åŠ¡ (similarity_threshold={similarity_threshold}, time_window={time_window_hours}h, max_batch={max_batch_size})...")

            # åˆ›å»ºåå°ä»»åŠ¡æ‰§è¡Œæ•´ç†
            task = asyncio.create_task(
                self._consolidate_memories_background(
                    similarity_threshold=similarity_threshold,
                    time_window_hours=time_window_hours,
                    max_batch_size=max_batch_size
                )
            )

            # è¿”å›ä»»åŠ¡å¯åŠ¨çŠ¶æ€ï¼Œä¸ç­‰å¾…å®Œæˆ
            return {
                "task_started": True,
                "task_id": id(task),
                "message": "è®°å¿†æ•´ç†ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
            }

        except Exception as e:
            logger.error(f"å¯åŠ¨è®°å¿†æ•´ç†ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "task_started": False}

    async def _consolidate_memories_background(
        self,
        similarity_threshold: float,
        time_window_hours: float,
        max_batch_size: int,
    ) -> None:
        """
        åå°æ‰§è¡Œè®°å¿†æ•´ç†çš„å…·ä½“å®ç°

        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹ä»»åŠ¡ä¸­è¿è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹
        """
        try:
            result = {
                "merged_count": 0,
                "checked_count": 0,
                "skipped_count": 0,
            }

            # è·å–æœ€è¿‘åˆ›å»ºçš„è®°å¿†
            cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= cutoff_time and not mem.metadata.get("forgotten", False)
            ]

            if not recent_memories:
                logger.info("âœ… è®°å¿†æ•´ç†å®Œæˆ: æ²¡æœ‰éœ€è¦æ•´ç†çš„è®°å¿†")
                return

            # é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡
            if len(recent_memories) > max_batch_size:
                logger.info(f"ğŸ“Š è®°å¿†æ•°é‡ {len(recent_memories)} è¶…è¿‡æ‰¹é‡é™åˆ¶ {max_batch_size}ï¼Œä»…å¤„ç†æœ€æ–°çš„ {max_batch_size} æ¡")
                recent_memories = sorted(recent_memories, key=lambda m: m.created_at, reverse=True)[:max_batch_size]
                result["skipped_count"] = len(all_memories) - max_batch_size

            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(recent_memories)} æ¡å¾…æ•´ç†è®°å¿†")
            result["checked_count"] = len(recent_memories)

            # æŒ‰è®°å¿†ç±»å‹åˆ†ç»„ï¼Œå‡å°‘è·¨ç±»å‹æ¯”è¾ƒ
            memories_by_type: dict[str, list[Memory]] = {}
            for mem in recent_memories:
                mem_type = mem.metadata.get("memory_type", "")
                if mem_type not in memories_by_type:
                    memories_by_type[mem_type] = []
                memories_by_type[mem_type].append(mem)

            # è®°å½•éœ€è¦åˆ é™¤çš„è®°å¿†ï¼Œå»¶è¿Ÿæ‰¹é‡åˆ é™¤
            to_delete: list[tuple[Memory, str]] = []  # (memory, reason)
            deleted_ids = set()

            # å¯¹æ¯ä¸ªç±»å‹çš„è®°å¿†è¿›è¡Œç›¸ä¼¼åº¦æ£€æµ‹
            for mem_type, memories in memories_by_type.items():
                if len(memories) < 2:
                    continue

                logger.debug(f"ğŸ” æ£€æŸ¥ç±»å‹ '{mem_type}' çš„ {len(memories)} æ¡è®°å¿†")

                # é¢„æå–æ‰€æœ‰ä¸»é¢˜èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
                embeddings_map: dict[str, "np.ndarray"] = {}
                valid_memories = []

                for mem in memories:
                    topic_node = next((n for n in mem.nodes if n.node_type == NodeType.TOPIC), None)
                    if topic_node and topic_node.embedding is not None:
                        embeddings_map[mem.id] = topic_node.embedding
                        valid_memories.append(mem)

                # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæ¯”é€ä¸ªè®¡ç®—æ›´é«˜æ•ˆï¼‰

                for i in range(len(valid_memories)):
                    # æ›´é¢‘ç¹çš„åä½œå¼å¤šä»»åŠ¡è®©å‡º
                    if i % 5 == 0:
                        await asyncio.sleep(0.001)  # 1msè®©å‡º

                    mem_i = valid_memories[i]
                    if mem_i.id in deleted_ids:
                        continue

                    for j in range(i + 1, len(valid_memories)):
                        if valid_memories[j].id in deleted_ids:
                            continue

                        mem_j = valid_memories[j]

                        # å¿«é€Ÿå‘é‡ç›¸ä¼¼åº¦è®¡ç®—
                        embedding_i = embeddings_map[mem_i.id]
                        embedding_j = embeddings_map[mem_j.id]

                        # ä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
                        similarity = cosine_similarity(embedding_i, embedding_j)

                        if similarity >= similarity_threshold:
                            # å†³å®šä¿ç•™å“ªä¸ªè®°å¿†
                            if mem_i.importance >= mem_j.importance:
                                keep_mem, remove_mem = mem_i, mem_j
                            else:
                                keep_mem, remove_mem = mem_j, mem_i

                            logger.debug(
                                f"ğŸ”„ æ ‡è®°ç›¸ä¼¼è®°å¿† (similarity={similarity:.3f}): "
                                f"ä¿ç•™ {keep_mem.id[:8]}, åˆ é™¤ {remove_mem.id[:8]}"
                            )

                            # å¢å¼ºä¿ç•™è®°å¿†çš„é‡è¦æ€§
                            keep_mem.importance = min(1.0, keep_mem.importance + 0.05)

                            # ç´¯åŠ è®¿é—®æ¬¡æ•°
                            if hasattr(keep_mem, "access_count") and hasattr(remove_mem, "access_count"):
                                keep_mem.access_count += remove_mem.access_count

                            # æ ‡è®°ä¸ºå¾…åˆ é™¤ï¼ˆä¸ç«‹å³åˆ é™¤ï¼‰
                            to_delete.append((remove_mem, f"ä¸è®°å¿† {keep_mem.id[:8]} ç›¸ä¼¼åº¦ {similarity:.3f}"))
                            deleted_ids.add(remove_mem.id)
                            result["merged_count"] += 1

                # æ¯å¤„ç†å®Œä¸€ä¸ªç±»å‹å°±è®©å‡ºæ§åˆ¶æƒ
                await asyncio.sleep(0.005)  # 5msè®©å‡º

            # æ‰¹é‡åˆ é™¤æ ‡è®°çš„è®°å¿†
            if to_delete:
                logger.info(f"ğŸ—‘ï¸ å¼€å§‹æ‰¹é‡åˆ é™¤ {len(to_delete)} æ¡ç›¸ä¼¼è®°å¿†")

                for memory, reason in to_delete:
                    try:
                        # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
                        for node in memory.nodes:
                            if node.embedding is not None:
                                await self.vector_store.delete_node(node.id)

                        # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
                        self.graph_store.remove_memory(memory.id)

                    except Exception as e:
                        logger.warning(f"åˆ é™¤è®°å¿† {memory.id[:8]} å¤±è´¥: {e}")

                # æ‰¹é‡ä¿å­˜ï¼ˆä¸€æ¬¡æ€§å†™å…¥ï¼Œå‡å°‘I/Oï¼‰
                await self.persistence.save_graph_store(self.graph_store)
                logger.info("ğŸ’¾ æ‰¹é‡ä¿å­˜å®Œæˆ")

            logger.info(f"âœ… è®°å¿†æ•´ç†å®Œæˆ: {result}")

        except Exception as e:
            logger.error(f"âŒ è®°å¿†æ•´ç†å¤±è´¥: {e}", exc_info=True)

    async def auto_link_memories(
        self,
        time_window_hours: float | None = None,
        max_candidates: int | None = None,
        min_confidence: float | None = None,
    ) -> dict[str, Any]:
        """
        è‡ªåŠ¨å…³è”è®°å¿†

        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»ï¼Œè‡ªåŠ¨å»ºç«‹å…³è”è¾¹ã€‚

        Args:
            time_window_hours: åˆ†ææ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_candidates: æ¯ä¸ªè®°å¿†æœ€å¤šå…³è”çš„å€™é€‰æ•°
            min_confidence: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            å…³è”ç»“æœç»Ÿè®¡
        """
        if not self._initialized:
            await self.initialize()

        # ä½¿ç”¨é…ç½®å€¼æˆ–å‚æ•°è¦†ç›–
        time_window_hours = time_window_hours if time_window_hours is not None else 24
        max_candidates = max_candidates if max_candidates is not None else getattr(self.config, "auto_link_max_candidates", 10)
        min_confidence = min_confidence if min_confidence is not None else getattr(self.config, "auto_link_min_confidence", 0.7)

        try:
            logger.info(f"å¼€å§‹è‡ªåŠ¨å…³è”è®°å¿† (æ—¶é—´çª—å£={time_window_hours}h)...")

            result = {
                "checked_count": 0,
                "linked_count": 0,
                "relation_stats": {},  # å…³ç³»ç±»å‹ç»Ÿè®¡ {ç±»å‹: æ•°é‡}
                "relations": {},  # è¯¦ç»†å…³ç³» {source_id: [å…³ç³»åˆ—è¡¨]}
            }

            # 1. è·å–æ—¶é—´çª—å£å†…çš„è®°å¿†
            time_threshold = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= time_threshold
                and not mem.metadata.get("forgotten", False)
            ]

            if len(recent_memories) < 2:
                logger.info("è®°å¿†æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è‡ªåŠ¨å…³è”")
                return result

            logger.info(f"æ‰¾åˆ° {len(recent_memories)} æ¡å¾…å…³è”è®°å¿†")

            # 2. ä¸ºæ¯ä¸ªè®°å¿†å¯»æ‰¾å…³è”å€™é€‰
            for memory in recent_memories:
                result["checked_count"] += 1

                # è·³è¿‡å·²ç»æœ‰å¾ˆå¤šè¿æ¥çš„è®°å¿†
                existing_edges = len([
                    e for e in memory.edges
                    if e.edge_type == EdgeType.RELATION
                ])
                if existing_edges >= 10:
                    continue

                # 3. ä½¿ç”¨å‘é‡æœç´¢æ‰¾å€™é€‰è®°å¿†
                candidates = await self._find_link_candidates(
                    memory,
                    exclude_ids={memory.id},
                    max_results=max_candidates
                )

                if not candidates:
                    continue

                # 4. ä½¿ç”¨LLMåˆ†æå…³ç³»
                relations = await self._analyze_memory_relations(
                    source_memory=memory,
                    candidate_memories=candidates,
                    min_confidence=min_confidence
                )

                # 5. å»ºç«‹å…³è”
                for relation in relations:
                    try:
                        # åˆ›å»ºå…³è”è¾¹
                        edge = MemoryEdge(
                            id=f"edge_{uuid.uuid4().hex[:12]}",
                            source_id=memory.subject_id,
                            target_id=relation["target_memory"].subject_id,
                            relation=relation["relation_type"],
                            edge_type=EdgeType.RELATION,
                            importance=relation["confidence"],
                            metadata={
                                "auto_linked": True,
                                "confidence": relation["confidence"],
                                "reasoning": relation["reasoning"],
                                "created_at": datetime.now().isoformat(),
                            }
                        )

                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )

                        # åŒæ—¶æ·»åŠ åˆ°è®°å¿†çš„è¾¹åˆ—è¡¨
                        memory.edges.append(edge)

                        result["linked_count"] += 1

                        # æ›´æ–°ç»Ÿè®¡
                        result["relation_stats"][relation["relation_type"]] = \
                            result["relation_stats"].get(relation["relation_type"], 0) + 1

                        # è®°å½•è¯¦ç»†å…³ç³»
                        if memory.id not in result["relations"]:
                            result["relations"][memory.id] = []
                        result["relations"][memory.id].append({
                            "target_id": relation["target_memory"].id,
                            "relation_type": relation["relation_type"],
                            "confidence": relation["confidence"],
                            "reasoning": relation["reasoning"],
                        })

                        logger.info(
                            f"å»ºç«‹å…³è”: {memory.id[:8]} --[{relation['relation_type']}]--> "
                            f"{relation['target_memory'].id[:8]} "
                            f"(ç½®ä¿¡åº¦={relation['confidence']:.2f})"
                        )

                    except Exception as e:
                        logger.warning(f"å»ºç«‹å…³è”å¤±è´¥: {e}")
                        continue

            # ä¿å­˜æ›´æ–°åçš„å›¾æ•°æ®
            if result["linked_count"] > 0:
                await self.persistence.save_graph_store(self.graph_store)
                logger.info(f"å·²ä¿å­˜ {result['linked_count']} æ¡è‡ªåŠ¨å…³è”è¾¹")

            logger.info(f"è‡ªåŠ¨å…³è”å®Œæˆ: {result}")
            return result

        except Exception as e:
            logger.error(f"è‡ªåŠ¨å…³è”å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "checked_count": 0, "linked_count": 0}

    async def _find_link_candidates(
        self,
        memory: Memory,
        exclude_ids: set[str],
        max_results: int = 5,
    ) -> list[Memory]:
        """
        ä¸ºè®°å¿†å¯»æ‰¾å…³è”å€™é€‰

        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ + æ—¶é—´æ¥è¿‘åº¦æ‰¾åˆ°æ½œåœ¨ç›¸å…³è®°å¿†
        """
        try:
            # è·å–è®°å¿†çš„ä¸»é¢˜
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )

            if not topic_node or not topic_node.content:
                return []

            # ä½¿ç”¨ä¸»é¢˜å†…å®¹æœç´¢ç›¸ä¼¼è®°å¿†
            candidates = await self.search_memories(
                query=topic_node.content,
                top_k=max_results * 2,
                include_forgotten=False,
            )

            # è¿‡æ»¤ï¼šæ’é™¤è‡ªå·±å’Œå·²å…³è”çš„
            existing_targets = {
                e.target_id for e in memory.edges
                if e.edge_type == EdgeType.RELATION
            }

            filtered = [
                c for c in candidates
                if c.id not in exclude_ids
                and c.id not in existing_targets
            ]

            return filtered[:max_results]

        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾å€™é€‰å¤±è´¥: {e}")
            return []

    async def _analyze_memory_relations(
        self,
        source_memory: Memory,
        candidate_memories: list[Memory],
        min_confidence: float = 0.7,
    ) -> list[dict[str, Any]]:
        """
        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»

        Args:
            source_memory: æºè®°å¿†
            candidate_memories: å€™é€‰è®°å¿†åˆ—è¡¨
            min_confidence: æœ€ä½ç½®ä¿¡åº¦

        Returns:
            å…³ç³»åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
            - target_memory: ç›®æ ‡è®°å¿†
            - relation_type: å…³ç³»ç±»å‹
            - confidence: ç½®ä¿¡åº¦
            - reasoning: æ¨ç†è¿‡ç¨‹
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            # æ„å»ºLLMè¯·æ±‚
            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.relation_analysis"
            )

            # æ ¼å¼åŒ–è®°å¿†ä¿¡æ¯
            source_desc = self._format_memory_for_llm(source_memory)
            candidates_desc = "\n\n".join([
                f"è®°å¿†{i+1}:\n{self._format_memory_for_llm(mem)}"
                for i, mem in enumerate(candidate_memories)
            ])

            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†å…³ç³»åˆ†æä¸“å®¶ã€‚è¯·åˆ†ææºè®°å¿†ä¸å€™é€‰è®°å¿†ä¹‹é—´æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„å…³ç³»ã€‚

**å…³ç³»ç±»å‹è¯´æ˜ï¼š**
- å¯¼è‡´: Açš„å‘ç”Ÿå¯¼è‡´äº†Bçš„å‘ç”Ÿï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨: Aæåˆ°æˆ–æ¶‰åŠBï¼ˆå¼•ç”¨å…³ç³»ï¼‰
- ç›¸ä¼¼: Aå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆç›¸ä¼¼å…³ç³»ï¼‰
- ç›¸å: Aå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹ç«‹å…³ç³»ï¼‰
- å…³è”: Aå’ŒBå­˜åœ¨æŸç§å…³è”ä½†ä¸å±äºä»¥ä¸Šç±»å‹ï¼ˆä¸€èˆ¬å…³è”ï¼‰

**æºè®°å¿†ï¼š**
{source_desc}

**å€™é€‰è®°å¿†ï¼š**
{candidates_desc}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. å¯¹æ¯ä¸ªå€™é€‰è®°å¿†ï¼Œåˆ¤æ–­æ˜¯å¦ä¸æºè®°å¿†å­˜åœ¨å…³ç³»
2. å¦‚æœå­˜åœ¨å…³ç³»ï¼ŒæŒ‡å®šå…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦(0.0-1.0)
3. ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±
4. åªè¿”å›ç½®ä¿¡åº¦ >= {min_confidence} çš„å…³ç³»

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
[
  {{
    "candidate_id": 1,
    "has_relation": true,
    "relation_type": "å¯¼è‡´",
    "confidence": 0.85,
    "reasoning": "è®°å¿†1æ˜¯è®°å¿†æºçš„ç»“æœ"
  }},
  {{
    "candidate_id": 2,
    "has_relation": false,
    "reasoning": "ä¸¤è€…æ— æ˜æ˜¾å…³è”"
  }}
]
```

è¯·åˆ†æå¹¶è¾“å‡ºJSONç»“æœï¼š"""

            # è°ƒç”¨LLM
            response, _ = await llm.generate_response_async(
                prompt,
                temperature=0.3,
                max_tokens=1000,
            )

            # è§£æå“åº”
            import json
            import re

            # æå–JSON
            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()

            try:
                analysis_results = json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¿®å¤: {response[:200]}")
                # å°è¯•ç®€å•ä¿®å¤
                json_str = re.sub(r"[\r\n\t]", "", json_str)
                analysis_results = json.loads(json_str)

            # è½¬æ¢ä¸ºç»“æœæ ¼å¼
            relations = []
            for result in analysis_results:
                if not result.get("has_relation", False):
                    continue

                confidence = result.get("confidence", 0.0)
                if confidence < min_confidence:
                    continue

                candidate_id = result.get("candidate_id", 0) - 1
                if 0 <= candidate_id < len(candidate_memories):
                    relations.append({
                        "target_memory": candidate_memories[candidate_id],
                        "relation_type": result.get("relation_type", "å…³è”"),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", ""),
                    })

            logger.debug(f"LLMåˆ†æå®Œæˆ: å‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations

        except Exception as e:
            logger.error(f"LLMå…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)
            return []

    def _format_memory_for_llm(self, memory: Memory) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸ºLLMå¯è¯»çš„æ–‡æœ¬"""
        try:
            # è·å–å…³é”®èŠ‚ç‚¹
            subject_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.SUBJECT),
                None
            )
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )
            object_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.OBJECT),
                None
            )

            parts = []
            parts.append(f"ç±»å‹: {memory.memory_type.value}")

            if subject_node:
                parts.append(f"ä¸»ä½“: {subject_node.content}")

            if topic_node:
                parts.append(f"ä¸»é¢˜: {topic_node.content}")

            if object_node:
                parts.append(f"å¯¹è±¡: {object_node.content}")

            parts.append(f"é‡è¦æ€§: {memory.importance:.2f}")
            parts.append(f"æ—¶é—´: {memory.created_at.strftime('%Y-%m-%d %H:%M')}")

            return " | ".join(parts)

        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–è®°å¿†å¤±è´¥: {e}")
            return f"è®°å¿†ID: {memory.id}"

    async def maintenance(self) -> dict[str, Any]:
        """
        æ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        åŒ…æ‹¬ï¼š
        - è®°å¿†æ•´ç†ï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼‰
        - è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆè½»é‡çº§æ‰§è¡Œï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®

        Returns:
            ç»´æŠ¤ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œè®°å¿†ç³»ç»Ÿç»´æŠ¤ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")

            result = {
                "consolidation_task": "none",
                "linked": 0,
                "forgotten": 0,
                "saved": False,
                "total_time": 0,
            }

            start_time = datetime.now()

            # 1. è®°å¿†æ•´ç†ï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            if getattr(self.config, "consolidation_enabled", False):
                logger.info("ğŸš€ å¯åŠ¨å¼‚æ­¥è®°å¿†æ•´ç†ä»»åŠ¡...")
                consolidate_result = await self.consolidate_memories(
                    similarity_threshold=getattr(self.config, "consolidation_deduplication_threshold", 0.93),
                    time_window_hours=getattr(self.config, "consolidation_time_window_hours", 2.0),  # ç»Ÿä¸€æ—¶é—´çª—å£
                    max_batch_size=getattr(self.config, "consolidation_max_batch_size", 30)
                )

                if consolidate_result.get("task_started"):
                    result["consolidation_task"] = f"background_task_{consolidate_result.get('task_id', 'unknown')}"
                    logger.info("âœ… è®°å¿†æ•´ç†ä»»åŠ¡å·²å¯åŠ¨åˆ°åå°æ‰§è¡Œ")
                else:
                    result["consolidation_task"] = "failed"
                    logger.warning("âŒ è®°å¿†æ•´ç†ä»»åŠ¡å¯åŠ¨å¤±è´¥")

            # 2. è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£ï¼‰
            if getattr(self.config, "consolidation_linking_enabled", True):
                logger.info("ğŸ”— æ‰§è¡Œè½»é‡çº§è‡ªåŠ¨å…³è”...")
                link_result = await self._lightweight_auto_link_memories()
                result["linked"] = link_result.get("linked_count", 0)

            # 3. è‡ªåŠ¨é—å¿˜ï¼ˆå¿«é€Ÿæ‰§è¡Œï¼‰
            if getattr(self.config, "forgetting_enabled", True):
                logger.info("ğŸ—‘ï¸ æ‰§è¡Œè‡ªåŠ¨é—å¿˜...")
                forgotten_count = await self.auto_forget_memories(
                    threshold=getattr(self.config, "forgetting_activation_threshold", 0.1)
                )
                result["forgotten"] = forgotten_count

            # 4. ä¿å­˜æ•°æ®ï¼ˆå¦‚æœè®°å¿†æ•´ç†ä¸åœ¨åå°æ‰§è¡Œï¼‰
            if result["consolidation_task"] == "none":
                await self.persistence.save_graph_store(self.graph_store)
                result["saved"] = True
                logger.info("ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ")

            self._last_maintenance = datetime.now()

            # è®¡ç®—ç»´æŠ¤è€—æ—¶
            total_time = (datetime.now() - start_time).total_seconds()
            result["total_time"] = total_time

            logger.info(f"âœ… ç»´æŠ¤å®Œæˆ (è€—æ—¶ {total_time:.2f}s): {result}")
            return result

        except Exception as e:
            logger.error(f"âŒ ç»´æŠ¤å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "total_time": 0}

    async def _lightweight_auto_link_memories(
        self,
        time_window_hours: float | None = None,  # ä»é…ç½®è¯»å–
        max_candidates: int | None = None,  # ä»é…ç½®è¯»å–
        max_memories: int | None = None,  # ä»é…ç½®è¯»å–
    ) -> dict[str, Any]:
        """
        æ™ºèƒ½è½»é‡çº§è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆä¿ç•™LLMåˆ¤æ–­ï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä»é…ç½®è¯»å–å¤„ç†å‚æ•°ï¼Œå°Šé‡ç”¨æˆ·è®¾ç½®
        2. ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é¢„ç­›é€‰ï¼Œä»…å¯¹é«˜ç›¸ä¼¼åº¦è®°å¿†è°ƒç”¨LLM
        3. æ‰¹é‡LLMè°ƒç”¨ï¼Œå‡å°‘ç½‘ç»œå¼€é”€
        4. å¼‚æ­¥æ‰§è¡Œï¼Œé¿å…é˜»å¡
        """
        try:
            result = {
                "checked_count": 0,
                "linked_count": 0,
                "llm_calls": 0,
            }

            # ä»é…ç½®è¯»å–å‚æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£
            if time_window_hours is None:
                time_window_hours = getattr(self.config, "consolidation_time_window_hours", 2.0)
            if max_candidates is None:
                max_candidates = getattr(self.config, "consolidation_linking_max_candidates", 10)
            if max_memories is None:
                max_memories = getattr(self.config, "consolidation_linking_max_memories", 20)

            # è·å–ç”¨æˆ·é…ç½®æ—¶é—´çª—å£å†…çš„è®°å¿†
            time_threshold = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= time_threshold
                and not mem.metadata.get("forgotten", False)
                and mem.importance >= getattr(self.config, "consolidation_linking_min_importance", 0.5)  # ä»é…ç½®è¯»å–é‡è¦æ€§é˜ˆå€¼
            ]

            if len(recent_memories) > max_memories:
                recent_memories = sorted(recent_memories, key=lambda m: m.created_at, reverse=True)[:max_memories]

            if len(recent_memories) < 2:
                logger.debug("è®°å¿†æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æ™ºèƒ½å…³è”")
                return result

            logger.debug(f"ğŸ§  æ™ºèƒ½å…³è”: æ£€æŸ¥ {len(recent_memories)} æ¡é‡è¦è®°å¿†")

            # ç¬¬ä¸€æ­¥ï¼šå‘é‡ç›¸ä¼¼åº¦é¢„ç­›é€‰ï¼Œæ‰¾åˆ°æ½œåœ¨å…³è”å¯¹
            candidate_pairs = []

            for i, memory in enumerate(recent_memories):
                # è·å–ä¸»é¢˜èŠ‚ç‚¹
                topic_node = next(
                    (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                    None
                )

                if not topic_node or topic_node.embedding is None:
                    continue

                # ä¸å…¶ä»–è®°å¿†è®¡ç®—ç›¸ä¼¼åº¦
                for j, other_memory in enumerate(recent_memories[i+1:], i+1):
                    other_topic = next(
                        (n for n in other_memory.nodes if n.node_type == NodeType.TOPIC),
                        None
                    )

                    if not other_topic or other_topic.embedding is None:
                        continue

                    # å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—
                    similarity = cosine_similarity(
                        topic_node.embedding,
                        other_topic.embedding
                    )

                    # ä½¿ç”¨é…ç½®çš„é¢„ç­›é€‰é˜ˆå€¼
                    pre_filter_threshold = getattr(self.config, "consolidation_linking_pre_filter_threshold", 0.7)
                    if similarity >= pre_filter_threshold:
                        candidate_pairs.append((memory, other_memory, similarity))

                # è®©å‡ºæ§åˆ¶æƒ
                if i % 3 == 0:
                    await asyncio.sleep(0.001)

            logger.debug(f"ğŸ” é¢„ç­›é€‰æ‰¾åˆ° {len(candidate_pairs)} ä¸ªå€™é€‰å…³è”å¯¹")

            if not candidate_pairs:
                return result

            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡LLMåˆ†æï¼ˆä½¿ç”¨é…ç½®çš„æœ€å¤§å€™é€‰å¯¹æ•°ï¼‰
            max_pairs_for_llm = getattr(self.config, "consolidation_linking_max_pairs_for_llm", 5)
            if len(candidate_pairs) <= max_pairs_for_llm:
                link_relations = await self._batch_analyze_memory_relations(candidate_pairs)
                result["llm_calls"] = 1

                # ç¬¬ä¸‰æ­¥ï¼šå»ºç«‹LLMç¡®è®¤çš„å…³è”
                for relation_info in link_relations:
                    try:
                        memory_a, memory_b = relation_info["memory_pair"]
                        relation_type = relation_info["relation_type"]
                        confidence = relation_info["confidence"]

                        # åˆ›å»ºå…³è”è¾¹
                        edge = MemoryEdge(
                            id=f"smart_edge_{uuid.uuid4().hex[:12]}",
                            source_id=memory_a.subject_id,
                            target_id=memory_b.subject_id,
                            relation=relation_type,
                            edge_type=EdgeType.RELATION,
                            importance=confidence,
                            metadata={
                                "auto_linked": True,
                                "method": "llm_analyzed",
                                "vector_similarity": relation_info.get("vector_similarity", 0.0),
                                "confidence": confidence,
                                "reasoning": relation_info.get("reasoning", ""),
                                "created_at": datetime.now().isoformat(),
                            }
                        )

                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )

                        memory_a.edges.append(edge)
                        result["linked_count"] += 1

                        logger.debug(f"ğŸ§  æ™ºèƒ½å…³è”: {memory_a.id[:8]} --[{relation_type}]--> {memory_b.id[:8]} (ç½®ä¿¡åº¦={confidence:.2f})")

                    except Exception as e:
                        logger.warning(f"å»ºç«‹æ™ºèƒ½å…³è”å¤±è´¥: {e}")
                        continue

            # ä¿å­˜å…³è”ç»“æœ
            if result["linked_count"] > 0:
                await self.persistence.save_graph_store(self.graph_store)

            logger.debug(f"âœ… æ™ºèƒ½å…³è”å®Œæˆ: å»ºç«‹äº† {result['linked_count']} ä¸ªå…³è”ï¼ŒLLMè°ƒç”¨ {result['llm_calls']} æ¬¡")
            return result

        except Exception as e:
            logger.error(f"æ™ºèƒ½å…³è”å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "checked_count": 0, "linked_count": 0}

    async def _batch_analyze_memory_relations(
        self,
        candidate_pairs: list[tuple[Memory, Memory, float]]
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æè®°å¿†å…³ç³»ï¼ˆä¼˜åŒ–LLMè°ƒç”¨ï¼‰

        Args:
            candidate_pairs: å€™é€‰è®°å¿†å¯¹åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« (memory_a, memory_b, vector_similarity)

        Returns:
            å…³ç³»åˆ†æç»“æœåˆ—è¡¨
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.batch_relation_analysis"
            )

            # æ ¼å¼åŒ–æ‰€æœ‰å€™é€‰è®°å¿†å¯¹
            candidates_text = ""
            for i, (mem_a, mem_b, similarity) in enumerate(candidate_pairs):
                desc_a = self._format_memory_for_llm(mem_a)
                desc_b = self._format_memory_for_llm(mem_b)
                candidates_text += f"""
å€™é€‰å¯¹ {i+1}:
è®°å¿†A: {desc_a}
è®°å¿†B: {desc_b}
å‘é‡ç›¸ä¼¼åº¦: {similarity:.3f}
"""

            # æ„å»ºæ‰¹é‡åˆ†ææç¤ºè¯ï¼ˆä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
            min_confidence = getattr(self.config, "consolidation_linking_min_confidence", 0.7)

            prompt = f"""ä½ æ˜¯è®°å¿†å…³ç³»åˆ†æä¸“å®¶ã€‚è¯·æ‰¹é‡åˆ†æä»¥ä¸‹å€™é€‰è®°å¿†å¯¹ä¹‹é—´çš„å…³ç³»ã€‚

**å…³ç³»ç±»å‹è¯´æ˜ï¼š**
- å¯¼è‡´: Açš„å‘ç”Ÿå¯¼è‡´äº†Bçš„å‘ç”Ÿï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨: Aæåˆ°æˆ–æ¶‰åŠBï¼ˆå¼•ç”¨å…³ç³»ï¼‰
- ç›¸ä¼¼: Aå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆç›¸ä¼¼å…³ç³»ï¼‰
- ç›¸å: Aå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹ç«‹å…³ç³»ï¼‰
- å…³è”: Aå’ŒBå­˜åœ¨æŸç§å…³è”ä½†ä¸å±äºä»¥ä¸Šç±»å‹ï¼ˆä¸€èˆ¬å…³è”ï¼‰

**å€™é€‰è®°å¿†å¯¹ï¼š**
{candidates_text}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. å¯¹æ¯ä¸ªå€™é€‰å¯¹ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„å…³ç³»
2. å¦‚æœå­˜åœ¨å…³ç³»ï¼ŒæŒ‡å®šå…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦(0.0-1.0)
3. ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±
4. åªè¿”å›ç½®ä¿¡åº¦ >= {min_confidence} çš„å…³ç³»
5. ä¼˜å…ˆè€ƒè™‘å› æœã€å¼•ç”¨ç­‰å¼ºå…³ç³»ï¼Œè°¨æ…å»ºç«‹ç›¸ä¼¼å…³ç³»

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
[
  {{
    "candidate_id": 1,
    "has_relation": true,
    "relation_type": "å¯¼è‡´",
    "confidence": 0.85,
    "reasoning": "è®°å¿†Aæè¿°çš„åŸå› å¯¼è‡´è®°å¿†Bçš„ç»“æœ"
  }},
  {{
    "candidate_id": 2,
    "has_relation": false,
    "reasoning": "ä¸¤è€…æ— æ˜æ˜¾å…³è”"
  }}
]
```

è¯·åˆ†æå¹¶è¾“å‡ºJSONç»“æœï¼š"""

            # è°ƒç”¨LLMï¼ˆä½¿ç”¨é…ç½®çš„å‚æ•°ï¼‰
            llm_temperature = getattr(self.config, "consolidation_linking_llm_temperature", 0.2)
            llm_max_tokens = getattr(self.config, "consolidation_linking_llm_max_tokens", 1500)

            response, _ = await llm.generate_response_async(
                prompt,
                temperature=llm_temperature,
                max_tokens=llm_max_tokens,
            )

            # è§£æå“åº”
            import json
            import re

            # æå–JSON
            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()

            try:
                analysis_results = json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¿®å¤: {response[:200]}")
                # å°è¯•ç®€å•ä¿®å¤
                json_str = re.sub(r"[\r\n\t]", "", json_str)
                analysis_results = json.loads(json_str)

            # è½¬æ¢ä¸ºç»“æœæ ¼å¼
            relations = []
            for result in analysis_results:
                if not result.get("has_relation", False):
                    continue

                confidence = result.get("confidence", 0.0)
                if confidence < min_confidence:  # ä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼
                    continue

                candidate_id = result.get("candidate_id", 0) - 1
                if 0 <= candidate_id < len(candidate_pairs):
                    mem_a, mem_b, vector_similarity = candidate_pairs[candidate_id]
                    relations.append({
                        "memory_pair": (mem_a, mem_b),
                        "relation_type": result.get("relation_type", "å…³è”"),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", ""),
                        "vector_similarity": vector_similarity,
                    })

            logger.debug(f"ğŸ§  LLMæ‰¹é‡åˆ†æå®Œæˆ: å‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations

        except Exception as e:
            logger.error(f"LLMæ‰¹é‡å…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)
            return []

    async def start_maintenance_scheduler(self) -> None:
        """
        å¯åŠ¨è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡

        ä½¿ç”¨ unified_scheduler å®šæœŸæ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼š
        - è®°å¿†æ•´åˆï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®

        é»˜è®¤é—´éš”ï¼š1å°æ—¶
        """
        try:
            from src.schedule.unified_scheduler import TriggerType, unified_scheduler

            # å¦‚æœå·²æœ‰è°ƒåº¦ä»»åŠ¡ï¼Œå…ˆç§»é™¤
            if self._maintenance_schedule_id:
                await unified_scheduler.remove_schedule(self._maintenance_schedule_id)
                logger.info("ç§»é™¤æ—§çš„ç»´æŠ¤è°ƒåº¦ä»»åŠ¡")

            # åˆ›å»ºæ–°çš„è°ƒåº¦ä»»åŠ¡
            interval_seconds = self._maintenance_interval_hours * 3600

            self._maintenance_schedule_id = await unified_scheduler.create_schedule(
                callback=self.maintenance,
                trigger_type=TriggerType.TIME,
                trigger_config={
                    "delay_seconds": interval_seconds,  # é¦–æ¬¡å»¶è¿Ÿï¼ˆå¯åŠ¨å1å°æ—¶ï¼‰
                    "interval_seconds": interval_seconds,  # å¾ªç¯é—´éš”
                },
                is_recurring=True,
                task_name="memory_maintenance",
            )

            logger.info(
                f"âœ… è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å·²å¯åŠ¨ "
                f"(é—´éš”={self._maintenance_interval_hours}å°æ—¶, "
                f"schedule_id={self._maintenance_schedule_id[:8]}...)"
            )

        except ImportError:
            logger.warning("æ— æ³•å¯¼å…¥ unified_schedulerï¼Œç»´æŠ¤è°ƒåº¦åŠŸèƒ½ä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"å¯åŠ¨ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    async def stop_maintenance_scheduler(self) -> None:
        """
        åœæ­¢è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        """
        if not self._maintenance_schedule_id:
            return

        try:
            from src.schedule.unified_scheduler import unified_scheduler

            success = await unified_scheduler.remove_schedule(self._maintenance_schedule_id)
            if success:
                logger.info(f"âœ… è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å·²åœæ­¢ (schedule_id={self._maintenance_schedule_id[:8]}...)")
            else:
                logger.warning(f"åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥ (schedule_id={self._maintenance_schedule_id[:8]}...)")

            self._maintenance_schedule_id = None

        except ImportError:
            logger.warning("æ— æ³•å¯¼å…¥ unified_scheduler")
        except Exception as e:
            logger.error(f"åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
