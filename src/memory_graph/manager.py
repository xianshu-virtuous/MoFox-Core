"""
è®°å¿†ç®¡ç†å™¨ - Phase 3

ç»Ÿä¸€çš„è®°å¿†ç³»ç»Ÿç®¡ç†æ¥å£ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
- è®°å¿†åˆ›å»ºã€æ£€ç´¢ã€æ›´æ–°ã€åˆ é™¤
- è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ¿€æ´»ã€é—å¿˜ï¼‰
- è®°å¿†æ•´åˆä¸ç»´æŠ¤
- å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–
"""

import asyncio
import logging
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

from src.config.config import global_config
from src.config.official_configs import MemoryConfig
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import EdgeType, Memory, MemoryEdge, NodeType
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.tools.memory_tools import MemoryTools
from src.memory_graph.utils.embeddings import EmbeddingGenerator
from src.memory_graph.utils.similarity import cosine_similarity

if TYPE_CHECKING:
    import numpy as np

logger = logging.getLogger(__name__)


class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨

    æ ¸å¿ƒç®¡ç†ç±»ï¼Œæä¾›è®°å¿†ç³»ç»Ÿçš„ç»Ÿä¸€æ¥å£ï¼š
    - è®°å¿† CRUD æ“ä½œ
    - è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†
    - æ™ºèƒ½æ£€ç´¢ä¸æ¨è
    - è®°å¿†ç»´æŠ¤ä¸ä¼˜åŒ–
    """

    def __init__(
        self,
        data_dir: Path | None = None,
    ):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»global_configè¯»å–ï¼‰
        """
        # ç›´æ¥ä½¿ç”¨ global_config.memory
        if not global_config.memory or not getattr(global_config.memory, "enable", False):
            raise ValueError("è®°å¿†ç³»ç»Ÿæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ [memory] enable = true")

        self.config: MemoryConfig = global_config.memory
        self.data_dir = data_dir or Path(getattr(self.config, "data_dir", "data/memory_graph"))

        # å­˜å‚¨ç»„ä»¶
        self.vector_store: VectorStore | None = None
        self.graph_store: GraphStore | None = None
        self.persistence: PersistenceManager | None = None

        # æ ¸å¿ƒç»„ä»¶
        self.embedding_generator: EmbeddingGenerator | None = None
        self.extractor: MemoryExtractor | None = None
        self.builder: MemoryBuilder | None = None
        self.tools: MemoryTools | None = None

        # çŠ¶æ€
        self._initialized = False
        self._last_maintenance = datetime.now()
        self._maintenance_task: asyncio.Task | None = None
        self._maintenance_interval_hours = getattr(self.config, "consolidation_interval_hours", 1.0)
        self._maintenance_running = False  # ç»´æŠ¤ä»»åŠ¡è¿è¡ŒçŠ¶æ€

        logger.info(f"è®°å¿†ç®¡ç†å™¨å·²åˆ›å»º (data_dir={self.data_dir}, enable={getattr(self.config, 'enable', False)})")

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        æŒ‰ç…§ä¾èµ–é¡ºåºåˆå§‹åŒ–ï¼š
        1. å­˜å‚¨å±‚ï¼ˆå‘é‡å­˜å‚¨ã€å›¾å­˜å‚¨ã€æŒä¹…åŒ–ï¼‰
        2. å·¥å…·å±‚ï¼ˆåµŒå…¥ç”Ÿæˆå™¨ã€æå–å™¨ï¼‰
        3. ç®¡ç†å±‚ï¼ˆæ„å»ºå™¨ã€å·¥å…·æ¥å£ï¼‰
        """
        if self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–")
            return

        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨...")

            # 1. åˆå§‹åŒ–å­˜å‚¨å±‚
            self.data_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å­˜å‚¨é…ç½®
            storage_config = getattr(self.config, "storage", None)
            vector_collection_name = getattr(storage_config, "vector_collection_name", "memory_graph") if storage_config else "memory_graph"

            self.vector_store = VectorStore(
                collection_name=vector_collection_name,
                data_dir=self.data_dir,
            )
            await self.vector_store.initialize()

            self.persistence = PersistenceManager(data_dir=self.data_dir)

            # å°è¯•åŠ è½½ç°æœ‰å›¾æ•°æ®
            self.graph_store = await self.persistence.load_graph_store()
            if not self.graph_store:
                logger.info("æœªæ‰¾åˆ°ç°æœ‰å›¾æ•°æ®ï¼Œåˆ›å»ºæ–°çš„å›¾å­˜å‚¨")
                self.graph_store = GraphStore()
            else:
                stats = self.graph_store.get_statistics()
                logger.info(
                    f"åŠ è½½å›¾æ•°æ®: {stats['total_memories']} æ¡è®°å¿†, "
                    f"{stats['total_nodes']} ä¸ªèŠ‚ç‚¹, {stats['total_edges']} æ¡è¾¹"
                )

            # 2. åˆå§‹åŒ–å·¥å…·å±‚
            self.embedding_generator = EmbeddingGenerator()
            # EmbeddingGenerator ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–

            self.extractor = MemoryExtractor()

            # 3. åˆå§‹åŒ–ç®¡ç†å±‚
            self.builder = MemoryBuilder(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                embedding_generator=self.embedding_generator,
            )

            # æ£€æŸ¥é…ç½®å€¼
            expand_depth = self.config.search_max_expand_depth
            expand_semantic_threshold = self.config.search_expand_semantic_threshold
            search_top_k = self.config.search_top_k
            # è¯»å–æƒé‡é…ç½®
            search_vector_weight = self.config.search_vector_weight
            search_importance_weight = self.config.search_importance_weight
            search_recency_weight = self.config.search_recency_weight
            # è¯»å–é˜ˆå€¼è¿‡æ»¤é…ç½®
            search_min_importance = self.config.search_min_importance
            search_similarity_threshold = self.config.search_similarity_threshold

            logger.info(
                f"ğŸ“Š é…ç½®æ£€æŸ¥: search_max_expand_depth={expand_depth}, "
                f"search_expand_semantic_threshold={expand_semantic_threshold}, "
                f"search_top_k={search_top_k}"
            )
            logger.info(
                f"ğŸ“Š æƒé‡é…ç½®: vector={search_vector_weight}, "
                f"importance={search_importance_weight}, "
                f"recency={search_recency_weight}"
            )
            logger.info(
                f"ğŸ“Š é˜ˆå€¼è¿‡æ»¤: min_importance={search_min_importance}, "
                f"similarity_threshold={search_similarity_threshold}"
            )

            self.tools = MemoryTools(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                persistence_manager=self.persistence,
                embedding_generator=self.embedding_generator,
                max_expand_depth=expand_depth,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•æ·±åº¦
                expand_semantic_threshold=expand_semantic_threshold,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•è¯­ä¹‰é˜ˆå€¼
                search_top_k=search_top_k,  # ä»é…ç½®è¯»å–é»˜è®¤ top_k
                search_vector_weight=search_vector_weight,  # ä»é…ç½®è¯»å–å‘é‡æƒé‡
                search_importance_weight=search_importance_weight,  # ä»é…ç½®è¯»å–é‡è¦æ€§æƒé‡
                search_recency_weight=search_recency_weight,  # ä»é…ç½®è¯»å–æ—¶æ•ˆæ€§æƒé‡
                search_min_importance=search_min_importance,  # ä»é…ç½®è¯»å–æœ€å°é‡è¦æ€§é˜ˆå€¼
                search_similarity_threshold=search_similarity_threshold,  # ä»é…ç½®è¯»å–ç›¸ä¼¼åº¦é˜ˆå€¼
            )

            self._initialized = True
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

            # å¯åŠ¨åå°ç»´æŠ¤ä»»åŠ¡
            self._start_maintenance_task()

        except Exception as e:
            logger.error(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def shutdown(self) -> None:
        """
        å…³é—­è®°å¿†ç®¡ç†å™¨

        æ‰§è¡Œæ¸…ç†æ“ä½œï¼š
        - åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        - ä¿å­˜æ‰€æœ‰æ•°æ®
        - å…³é—­å­˜å‚¨ç»„ä»¶
        """
        if not self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— éœ€å…³é—­")
            return

        try:
            logger.info("æ­£åœ¨å…³é—­è®°å¿†ç®¡ç†å™¨...")

            # 1. åœæ­¢ç»´æŠ¤ä»»åŠ¡
            await self._stop_maintenance_task()

            # 2. æ‰§è¡Œæœ€åä¸€æ¬¡ç»´æŠ¤ï¼ˆä¿å­˜æ•°æ®ï¼‰
            if self.graph_store and self.persistence:
                logger.info("æ‰§è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜...")
                await self.persistence.save_graph_store(self.graph_store)

            # 3. å…³é—­å­˜å‚¨ç»„ä»¶
            if self.vector_store:
                # VectorStore ä½¿ç”¨ chromadbï¼Œæ— éœ€æ˜¾å¼å…³é—­
                pass

            self._initialized = False
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å…³é—­è®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)

    # ==================== è®°å¿† CRUD æ“ä½œ ====================

    async def create_memory(
        self,
        subject: str,
        memory_type: str,
        topic: str,
        object: str | None = None,
        attributes: dict[str, str] | None = None,
        importance: float = 0.5,
        **kwargs,
    ) -> Memory | None:
        """
        åˆ›å»ºæ–°è®°å¿†

        Args:
            subject: ä¸»ä½“ï¼ˆè°ï¼‰
            memory_type: è®°å¿†ç±»å‹ï¼ˆäº‹ä»¶/è§‚ç‚¹/äº‹å®/å…³ç³»ï¼‰
            topic: ä¸»é¢˜ï¼ˆåšä»€ä¹ˆ/æƒ³ä»€ä¹ˆï¼‰
            object: å®¢ä½“ï¼ˆå¯¹è°/å¯¹ä»€ä¹ˆï¼‰
            attributes: å±æ€§å­—å…¸ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€åŸå› ç­‰ï¼‰
            importance: é‡è¦æ€§ (0.0-1.0)
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åˆ›å»ºçš„è®°å¿†å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.create_memory(
                subject=subject,
                memory_type=memory_type,
                topic=topic,
                object=object,
                attributes=attributes,
                importance=importance,
                **kwargs,
            )

            if result["success"]:
                memory_id = result["memory_id"]
                memory = self.graph_store.get_memory_by_id(memory_id)
                logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory_id}")
                return memory
            else:
                logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
                return None

        except Exception as e:
            logger.error(f"åˆ›å»ºè®°å¿†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return None

    async def get_memory(self, memory_id: str) -> Memory | None:
        """
        æ ¹æ® ID è·å–è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            è®°å¿†å¯¹è±¡ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        return self.graph_store.get_memory_by_id(memory_id)

    async def update_memory(
        self,
        memory_id: str,
        **updates,
    ) -> bool:
        """
        æ›´æ–°è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            **updates: è¦æ›´æ–°çš„å­—æ®µ

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°å…ƒæ•°æ®
            if "importance" in updates:
                memory.importance = updates["importance"]

            if "metadata" in updates:
                memory.metadata.update(updates["metadata"])

            memory.updated_at = datetime.now()

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("æ›´æ–°è®°å¿†"))
            logger.info(f"è®°å¿†æ›´æ–°æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
            for node in memory.nodes:
                if node.embedding is not None:
                    await self.vector_store.delete_node(node.id)

            # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            self.graph_store.remove_memory(memory_id)

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("åˆ é™¤è®°å¿†"))
            logger.info(f"è®°å¿†åˆ é™¤æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†æ£€ç´¢æ“ä½œ ====================
    async def search_memories(
        self,
        query: str,
        top_k: int | None = None,
        memory_types: list[str] | None = None,
        time_range: tuple[datetime, datetime] | None = None,
        min_importance: float = 0.0,
        include_forgotten: bool = False,
        use_multi_query: bool = True,
        expand_depth: int | None = None,
        context: dict[str, Any] | None = None,
        prefer_node_types: list[str] | None = None,  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹
    ) -> list[Memory]:
        """
        æœç´¢è®°å¿†

        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼Œè§£å†³å¤æ‚æŸ¥è¯¢é—®é¢˜ã€‚
        ä¾‹å¦‚ï¼š"æ°ç‘å–µå¦‚ä½•è¯„ä»·æ–°çš„è®°å¿†ç³»ç»Ÿ" ä¼šè¢«åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ï¼Œ
        ç¡®ä¿åŒæ—¶åŒ¹é…"æ°ç‘å–µ"å’Œ"æ–°çš„è®°å¿†ç³»ç»Ÿ"ä¸¤ä¸ªå…³é”®æ¦‚å¿µã€‚

        åŒæ—¶æ”¯æŒå›¾æ‰©å±•ï¼šä»åˆå§‹æ£€ç´¢ç»“æœå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æŸ¥æ‰¾è¯­ä¹‰ç›¸å…³çš„é‚»å±…è®°å¿†ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°
            memory_types: è®°å¿†ç±»å‹è¿‡æ»¤
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ (start, end)
            min_importance: æœ€å°é‡è¦æ€§
            include_forgotten: æ˜¯å¦åŒ…å«å·²é—å¿˜çš„è®°å¿†
            use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆæ¨èï¼Œé»˜è®¤Trueï¼‰
            expand_depth: å›¾æ‰©å±•æ·±åº¦ï¼ˆ0=ç¦ç”¨, 1=æ¨è, 2-3=æ·±åº¦æ¢ç´¢ï¼‰
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¼˜åŒ–ï¼‰
            prefer_node_types: åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆå¦‚ ["ENTITY", "EVENT"]ï¼‰ğŸ†•

        Returns:
            è®°å¿†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        try:
            # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
            if top_k is None:
                top_k = getattr(self.config, "search_top_k", 10)

            # å‡†å¤‡æœç´¢å‚æ•°
            params = {
                "query": query,
                "top_k": top_k,
                "use_multi_query": use_multi_query,
                "expand_depth": expand_depth or global_config.memory.search_max_expand_depth,  # ä¼ é€’å›¾æ‰©å±•æ·±åº¦
                "context": context,
                "prefer_node_types": prefer_node_types or [],  # ğŸ†• ä¼ é€’åå¥½èŠ‚ç‚¹ç±»å‹
            }

            if memory_types:
                params["memory_types"] = memory_types

            # æ‰§è¡Œæœç´¢
            result = await self.tools.search_memories(**params)

            if not result["success"]:
                logger.error(f"æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}")
                return []

            memories = result.get("results", [])

            # åå¤„ç†è¿‡æ»¤
            filtered_memories = []
            for mem_dict in memories:
                # ä»å­—å…¸é‡å»º Memory å¯¹è±¡
                memory_id = mem_dict.get("memory_id", "")
                if not memory_id:
                    continue

                memory = self.graph_store.get_memory_by_id(memory_id)
                if not memory:
                    continue

                # é‡è¦æ€§è¿‡æ»¤
                if min_importance is not None and memory.importance < min_importance:
                    continue

                # é—å¿˜çŠ¶æ€è¿‡æ»¤
                if not include_forgotten and memory.metadata.get("forgotten", False):
                    continue

                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                if time_range:
                    mem_time = memory.created_at
                    if not (time_range[0] <= mem_time <= time_range[1]):
                        continue

                filtered_memories.append(memory)

            strategy = result.get("strategy", "unknown")
            logger.info(
                f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(filtered_memories)} æ¡è®°å¿† (ç­–ç•¥={strategy})"
            )

            # å¼ºåˆ¶æ¿€æ´»è¢«æ£€ç´¢åˆ°çš„è®°å¿†ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰- ä½¿ç”¨å¿«é€Ÿæ‰¹é‡æ¿€æ´»
            if filtered_memories:
                await self._quick_batch_activate_memories(filtered_memories)

            return filtered_memories[:top_k]

        except Exception as e:
            logger.error(f"æœç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    async def link_memories(
        self,
        source_description: str,
        target_description: str,
        relation_type: str,
        importance: float = 0.5,
    ) -> bool:
        """
        å…³è”ä¸¤æ¡è®°å¿†

        Args:
            source_description: æºè®°å¿†æè¿°
            target_description: ç›®æ ‡è®°å¿†æè¿°
            relation_type: å…³ç³»ç±»å‹ï¼ˆå¯¼è‡´/å¼•ç”¨/ç›¸ä¼¼/ç›¸åï¼‰
            importance: å…³ç³»é‡è¦æ€§

        Returns:
            æ˜¯å¦å…³è”æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.link_memories(
                source_memory_description=source_description,
                target_memory_description=target_description,
                relation_type=relation_type,
                importance=importance,
            )

            if result["success"]:
                logger.info(
                    f"è®°å¿†å…³è”æˆåŠŸ: {result['source_memory_id']} -> "
                    f"{result['target_memory_id']} ({relation_type})"
                )
                return True
            else:
                logger.error(f"è®°å¿†å…³è”å¤±è´¥: {result.get('error', 'Unknown error')}")
                return False

        except Exception as e:
            logger.error(f"å…³è”è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

    async def activate_memory(self, memory_id: str, strength: float = 1.0) -> bool:
        """
        æ¿€æ´»è®°å¿†

        æ›´æ–°è®°å¿†çš„æ¿€æ´»åº¦ï¼Œå¹¶ä¼ æ’­åˆ°ç›¸å…³è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            strength: æ¿€æ´»å¼ºåº¦ (0.0-1.0)

        Returns:
            æ˜¯å¦æ¿€æ´»æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°æ¿€æ´»ä¿¡æ¯
            now = datetime.now()
            activation_info = memory.metadata.get("activation", {})

            # æ›´æ–°æ¿€æ´»åº¦ï¼ˆè€ƒè™‘æ—¶é—´è¡°å‡ï¼‰
            last_access = activation_info.get("last_access")
            if last_access:
                # è®¡ç®—æ—¶é—´è¡°å‡
                last_access_dt = datetime.fromisoformat(last_access)
                hours_passed = (now - last_access_dt).total_seconds() / 3600
                decay_rate = getattr(self.config, "activation_decay_rate", 0.95)
                decay_factor = decay_rate ** (hours_passed / 24)
                current_activation = activation_info.get("level", 0.0) * decay_factor
            else:
                current_activation = 0.0

            # æ–°çš„æ¿€æ´»åº¦ = å½“å‰æ¿€æ´»åº¦ + æ¿€æ´»å¼ºåº¦
            new_activation = min(1.0, current_activation + strength)

            activation_info.update({
                "level": new_activation,
                "last_access": now.isoformat(),
                "access_count": activation_info.get("access_count", 0) + 1,
            })

            # åŒæ­¥æ›´æ–° memory.activation å­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            memory.activation = new_activation
            memory.metadata["activation"] = activation_info
            memory.last_accessed = now

            # æ¿€æ´»ä¼ æ’­ï¼šæ¿€æ´»ç›¸å…³è®°å¿†
            if strength > 0.1:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                propagation_depth = getattr(self.config, "activation_propagation_depth", 2)
                related_memories = self._get_related_memories(
                    memory_id,
                    max_depth=propagation_depth
                )
                propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.5)
                propagation_strength = strength * propagation_strength_factor

                max_related = getattr(self.config, "max_related_memories", 5)
                for related_id in related_memories[:max_related]:
                    await self.activate_memory(related_id, propagation_strength)

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("æ¿€æ´»è®°å¿†"))
            logger.debug(f"è®°å¿†å·²æ¿€æ´»: {memory_id} (level={new_activation:.3f})")
            return True

        except Exception as e:
            logger.error(f"æ¿€æ´»è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def _auto_activate_searched_memories(self, memories: list[Memory]) -> None:
        """
        æ‰¹é‡æ¿€æ´»è¢«æœç´¢åˆ°çš„è®°å¿†

        Args:
            memories: è¢«æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not memories:
                return

            # è·å–é…ç½®å‚æ•°
            base_strength = getattr(self.config, "auto_activate_base_strength", 0.1)
            max_activate_count = getattr(self.config, "auto_activate_max_count", 5)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)
            now = datetime.now()

            # é™åˆ¶å¤„ç†çš„è®°å¿†æ•°é‡
            memories_to_activate = memories[:max_activate_count]

            # æ‰¹é‡æ›´æ–°æ¿€æ´»åº¦
            activation_updates = []
            for memory in memories_to_activate:
                # è®¡ç®—æ¿€æ´»å¼ºåº¦
                strength = base_strength * (0.5 + memory.importance)

                # è·å–å½“å‰æ¿€æ´»åº¦ä¿¡æ¯
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")

                if last_access:
                    # è®¡ç®—æ—¶é—´è¡°å‡
                    last_access_dt = datetime.fromisoformat(last_access)
                    hours_passed = (now - last_access_dt).total_seconds() / 3600
                    decay_factor = decay_rate ** (hours_passed / 24)
                    current_activation = activation_info.get("level", 0.0) * decay_factor
                else:
                    current_activation = 0.0

                # è®¡ç®—æ–°çš„æ¿€æ´»åº¦
                new_activation = min(1.0, current_activation + strength)

                # æ›´æ–°è®°å¿†å¯¹è±¡
                memory.activation = new_activation
                memory.last_accessed = now
                activation_info.update({
                    "level": new_activation,
                    "last_access": now.isoformat(),
                    "access_count": activation_info.get("access_count", 0) + 1,
                })
                memory.metadata["activation"] = activation_info

                activation_updates.append({
                    "memory_id": memory.id,
                    "old_activation": current_activation,
                    "new_activation": new_activation,
                    "strength": strength
                })

            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            if activation_updates:
                asyncio.create_task(self._async_save_graph_store("æ‰¹é‡æ¿€æ´»æ›´æ–°"))

                # æ¿€æ´»ä¼ æ’­ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                asyncio.create_task(self._batch_propagate_activation(memories_to_activate, base_strength))

                logger.debug(f"æ‰¹é‡æ¿€æ´» {len(activation_updates)} æ¡è®°å¿†å®Œæˆ")

        except Exception as e:
            logger.warning(f"æ‰¹é‡æ¿€æ´»æœç´¢è®°å¿†å¤±è´¥: {e}")

    async def _quick_batch_activate_memories(self, memories: list[Memory]) -> None:
        """
        å¿«é€Ÿæ‰¹é‡æ¿€æ´»è®°å¿†ï¼ˆç”¨äºæœç´¢ç»“æœï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰

        ä¸ _auto_activate_searched_memories çš„åŒºåˆ«ï¼š
        - æ›´è½»é‡çº§ï¼Œä¸“æ³¨äºé€Ÿåº¦
        - ç®€åŒ–æ¿€æ´»ä¼ æ’­é€»è¾‘
        - å‡å°‘æ•°æ®åº“å†™å…¥æ¬¡æ•°

        Args:
            memories: éœ€è¦æ¿€æ´»çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not memories:
                return

            # è·å–é…ç½®å‚æ•°
            base_strength = getattr(self.config, "auto_activate_base_strength", 0.1)
            max_activate_count = getattr(self.config, "auto_activate_max_count", 5)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)
            now = datetime.now()

            # é™åˆ¶å¤„ç†çš„è®°å¿†æ•°é‡
            memories_to_activate = memories[:max_activate_count]

            # æ‰¹é‡æ›´æ–°æ¿€æ´»åº¦ï¼ˆå†…å­˜æ“ä½œï¼‰
            for memory in memories_to_activate:
                # è®¡ç®—æ¿€æ´»å¼ºåº¦
                strength = base_strength * (0.5 + memory.importance)

                # å¿«é€Ÿè®¡ç®—æ–°çš„æ¿€æ´»åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")

                if last_access:
                    # ç®€åŒ–çš„æ—¶é—´è¡°å‡è®¡ç®—
                    try:
                        last_access_dt = datetime.fromisoformat(last_access)
                        hours_passed = (now - last_access_dt).total_seconds() / 3600
                        decay_factor = decay_rate ** (hours_passed / 24)
                        current_activation = activation_info.get("level", 0.0) * decay_factor
                    except (ValueError, TypeError):
                        current_activation = activation_info.get("level", 0.0) * 0.9  # é»˜è®¤è¡°å‡
                else:
                    current_activation = 0.0

                # è®¡ç®—æ–°çš„æ¿€æ´»åº¦
                new_activation = min(1.0, current_activation + strength)

                # ç›´æ¥æ›´æ–°è®°å¿†å¯¹è±¡ï¼ˆå†…å­˜ä¸­ï¼‰
                memory.activation = new_activation
                memory.last_accessed = now
                activation_info.update({
                    "level": new_activation,
                    "last_access": now.isoformat(),
                    "access_count": activation_info.get("access_count", 0) + 1,
                })
                memory.metadata["activation"] = activation_info

            # å¼‚æ­¥æ‰¹é‡ä¿å­˜ï¼ˆä¸é˜»å¡æœç´¢ï¼‰
            if memories_to_activate:
                asyncio.create_task(self._background_save_activation(memories_to_activate, base_strength))

            logger.debug(f"å¿«é€Ÿæ‰¹é‡æ¿€æ´» {len(memories_to_activate)} æ¡è®°å¿†")

        except Exception as e:
            logger.warning(f"å¿«é€Ÿæ‰¹é‡æ¿€æ´»è®°å¿†å¤±è´¥: {e}")

    async def _background_save_activation(self, memories: list[Memory], base_strength: float) -> None:
        """
        åå°ä¿å­˜æ¿€æ´»æ›´æ–°å¹¶æ‰§è¡Œä¼ æ’­

        Args:
            memories: å·²æ›´æ–°çš„è®°å¿†åˆ—è¡¨
            base_strength: åŸºç¡€æ¿€æ´»å¼ºåº¦
        """
        try:
            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            asyncio.create_task(self._async_save_graph_store("åå°æ¿€æ´»æ›´æ–°"))

            # ç®€åŒ–çš„æ¿€æ´»ä¼ æ’­ï¼ˆä»…åœ¨å¼ºåº¦è¶³å¤Ÿæ—¶æ‰§è¡Œï¼‰
            if base_strength > 0.08:  # æé«˜ä¼ æ’­é˜ˆå€¼ï¼Œå‡å°‘ä¼ æ’­é¢‘ç‡
                propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.3)  # é™ä½ä¼ æ’­å¼ºåº¦
                max_related = getattr(self.config, "max_related_memories", 3)  # å‡å°‘ä¼ æ’­æ•°é‡

                # åªä¼ æ’­æœ€é‡è¦çš„è®°å¿†çš„æ¿€æ´»
                important_memories = [m for m in memories if m.importance > 0.6][:2]  # æœ€å¤š2ä¸ªé‡è¦è®°å¿†

                for memory in important_memories:
                    related_memories = self._get_related_memories(memory.id, max_depth=1)  # å‡å°‘ä¼ æ’­æ·±åº¦
                    propagation_strength = base_strength * propagation_strength_factor

                    for related_id in related_memories[:max_related]:
                        try:
                            related_memory = self.graph_store.get_memory_by_id(related_id)
                            if related_memory:
                                # ç®€å•çš„æ¿€æ´»åº¦å¢åŠ ï¼ˆä¸è°ƒç”¨å®Œæ•´æ¿€æ´»æ–¹æ³•ï¼‰
                                current_activation = related_memory.metadata.get("activation", {}).get("level", related_memory.activation)
                                new_activation = min(1.0, current_activation + propagation_strength * 0.5)

                                related_memory.activation = new_activation
                                related_memory.metadata["activation"] = {
                                    "level": new_activation,
                                    "last_access": datetime.now().isoformat(),
                                    "access_count": related_memory.metadata.get("activation", {}).get("access_count", 0) + 1,
                                }
                        except Exception as e:
                            logger.debug(f"ä¼ æ’­æ¿€æ´»åˆ°ç›¸å…³è®°å¿† {related_id[:8]} å¤±è´¥: {e}")

                # å†æ¬¡ä¿å­˜ä¼ æ’­åçš„æ›´æ–°
                await self.persistence.save_graph_store(self.graph_store)

            logger.debug(f"åå°ä¿å­˜æ¿€æ´»æ›´æ–°å®Œæˆï¼Œå¤„ç†äº† {len(memories)} æ¡è®°å¿†")

        except Exception as e:
            logger.warning(f"åå°ä¿å­˜æ¿€æ´»æ›´æ–°å¤±è´¥: {e}")

    async def _batch_propagate_activation(self, memories: list[Memory], base_strength: float) -> None:
        """
        æ‰¹é‡ä¼ æ’­æ¿€æ´»åˆ°ç›¸å…³è®°å¿†ï¼ˆåå°æ‰§è¡Œï¼‰

        Args:
            memories: å·²æ¿€æ´»çš„è®°å¿†åˆ—è¡¨
            base_strength: åŸºç¡€æ¿€æ´»å¼ºåº¦
        """
        try:
            propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.5)
            propagation_depth = getattr(self.config, "activation_propagation_depth", 2)
            max_related = getattr(self.config, "max_related_memories", 5)

            # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼ æ’­æ¿€æ´»çš„è®°å¿†ID
            propagation_tasks = []
            for memory in memories:
                if base_strength > 0.05:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                    related_memories = self._get_related_memories(
                        memory.id,
                        max_depth=propagation_depth
                    )
                    propagation_strength = base_strength * propagation_strength_factor

                    for related_id in related_memories[:max_related]:
                        task = self.activate_memory(related_id, propagation_strength)
                        propagation_tasks.append(task)

            # æ‰¹é‡æ‰§è¡Œä¼ æ’­ä»»åŠ¡
            if propagation_tasks:
                import asyncio
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*propagation_tasks, return_exceptions=True),
                        timeout=3.0  # ä¼ æ’­æ“ä½œè¶…æ—¶æ—¶é—´ç¨é•¿
                    )
                    logger.debug(f"æ¿€æ´»ä¼ æ’­å®Œæˆ: {len(propagation_tasks)} ä¸ªç›¸å…³è®°å¿†")
                except asyncio.TimeoutError:
                    logger.warning("æ¿€æ´»ä¼ æ’­è¶…æ—¶ï¼Œéƒ¨åˆ†ç›¸å…³è®°å¿†æœªæ¿€æ´»")
                except Exception as e:
                    logger.warning(f"æ¿€æ´»ä¼ æ’­å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"æ‰¹é‡ä¼ æ’­æ¿€æ´»å¤±è´¥: {e}")

    def _get_related_memories(self, memory_id: str, max_depth: int = 1) -> list[str]:
        """
        è·å–ç›¸å…³è®°å¿† ID åˆ—è¡¨ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºæ¿€æ´»ä¼ æ’­ï¼‰

        Args:
            memory_id: è®°å¿† ID
            max_depth: æœ€å¤§éå†æ·±åº¦

        Returns:
            ç›¸å…³è®°å¿† ID åˆ—è¡¨
        """
        memory = self.graph_store.get_memory_by_id(memory_id)
        if not memory:
            return []

        related_ids = set()

        # éå†è®°å¿†çš„èŠ‚ç‚¹
        for node in memory.nodes:
            # è·å–èŠ‚ç‚¹çš„é‚»å±…
            neighbors = list(self.graph_store.graph.neighbors(node.id))

            for neighbor_id in neighbors:
                # è·å–é‚»å±…èŠ‚ç‚¹æ‰€å±çš„è®°å¿†
                neighbor_node = self.graph_store.graph.nodes.get(neighbor_id)
                if neighbor_node:
                    neighbor_memory_ids = neighbor_node.get("memory_ids", [])
                    for mem_id in neighbor_memory_ids:
                        if mem_id != memory_id:
                            related_ids.add(mem_id)

        return list(related_ids)

    async def forget_memory(self, memory_id: str, cleanup_orphans: bool = True) -> bool:
        """
        é—å¿˜è®°å¿†ï¼ˆç›´æ¥åˆ é™¤ï¼‰

        è¿™ä¸ªæ–¹æ³•ä¼šï¼š
        1. ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
        2. ä»å›¾å­˜å‚¨ä¸­åˆ é™¤è®°å¿†
        3. å¯é€‰ï¼šæ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼ˆå»ºè®®æ‰¹é‡é—å¿˜åç»Ÿä¸€æ¸…ç†ï¼‰
        4. ä¿å­˜æ›´æ–°åçš„æ•°æ®

        Args:
            memory_id: è®°å¿† ID
            cleanup_orphans: æ˜¯å¦ç«‹å³æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼ˆé»˜è®¤Trueï¼Œæ‰¹é‡é—å¿˜æ—¶è®¾ä¸ºFalseï¼‰

        Returns:
            æ˜¯å¦é—å¿˜æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # 1. ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
            deleted_vectors = 0
            for node in memory.nodes:
                if node.embedding is not None:
                    try:
                        await self.vector_store.delete_node(node.id)
                        deleted_vectors += 1
                    except Exception as e:
                        logger.warning(f"åˆ é™¤èŠ‚ç‚¹å‘é‡å¤±è´¥ {node.id}: {e}")

            # 2. ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            success = self.graph_store.remove_memory(memory_id, cleanup_orphans=False)

            if success:
                # 3. å¯é€‰ï¼šæ¸…ç†å­¤ç«‹èŠ‚ç‚¹
                if cleanup_orphans:
                    orphan_nodes, orphan_edges = await self._cleanup_orphan_nodes_and_edges()
                    logger.info(
                        f"è®°å¿†å·²é—å¿˜å¹¶åˆ é™¤: {memory_id} "
                        f"(åˆ é™¤äº† {deleted_vectors} ä¸ªå‘é‡, æ¸…ç†äº† {orphan_nodes} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges} æ¡å­¤ç«‹è¾¹)"
                    )
                else:
                    logger.debug(f"è®°å¿†å·²åˆ é™¤: {memory_id} (åˆ é™¤äº† {deleted_vectors} ä¸ªå‘é‡)")

                # 4. å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
                asyncio.create_task(self._async_save_graph_store("åˆ é™¤ç›¸å…³è®°å¿†"))
                return True
            else:
                logger.error(f"ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†å¤±è´¥: {memory_id}")
                return False

        except Exception as e:
            logger.error(f"é—å¿˜è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def auto_forget_memories(self, threshold: float = 0.1) -> int:
        """
        è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦çš„è®°å¿†ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆï¼‰

        åº”ç”¨æ—¶é—´è¡°å‡å…¬å¼è®¡ç®—å½“å‰æ¿€æ´»åº¦ï¼Œä½äºé˜ˆå€¼åˆ™é—å¿˜ã€‚
        è¡°å‡å…¬å¼ï¼šactivation = base_activation * (decay_rate ^ days_passed)
        
        ä¼˜åŒ–ï¼šæ‰¹é‡åˆ é™¤è®°å¿†åç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼Œå‡å°‘é‡å¤æ£€æŸ¥

        Args:
            threshold: æ¿€æ´»åº¦é˜ˆå€¼

        Returns:
            é—å¿˜çš„è®°å¿†æ•°é‡
        """
        if not self._initialized:
            await self.initialize()

        try:
            forgotten_count = 0
            all_memories = self.graph_store.get_all_memories()

            # è·å–é…ç½®å‚æ•°
            min_importance = getattr(self.config, "forgetting_min_importance", 0.8)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)

            # æ”¶é›†éœ€è¦é—å¿˜çš„è®°å¿†ID
            memories_to_forget = []

            for memory in all_memories:
                # è·³è¿‡å·²é—å¿˜çš„è®°å¿†
                if memory.metadata.get("forgotten", False):
                    continue

                # è·³è¿‡é«˜é‡è¦æ€§è®°å¿†ï¼ˆä¿æŠ¤é‡è¦è®°å¿†ä¸è¢«é—å¿˜ï¼‰
                if memory.importance >= min_importance:
                    continue

                # è®¡ç®—å½“å‰æ¿€æ´»åº¦ï¼ˆåº”ç”¨æ—¶é—´è¡°å‡ï¼‰
                activation_info = memory.metadata.get("activation", {})
                base_activation = activation_info.get("level", memory.activation)
                last_access = activation_info.get("last_access")

                if last_access:
                    try:
                        last_access_dt = datetime.fromisoformat(last_access)
                        days_passed = (datetime.now() - last_access_dt).days

                        # åº”ç”¨æŒ‡æ•°è¡°å‡ï¼šactivation = base * (decay_rate ^ days)
                        current_activation = base_activation * (decay_rate ** days_passed)

                        logger.debug(
                            f"è®°å¿† {memory.id[:8]}: åŸºç¡€æ¿€æ´»åº¦={base_activation:.3f}, "
                            f"ç»è¿‡{days_passed}å¤©è¡°å‡å={current_activation:.3f}"
                        )
                    except (ValueError, TypeError) as e:
                        logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {e}, ä½¿ç”¨åŸºç¡€æ¿€æ´»åº¦")
                        current_activation = base_activation
                else:
                    # æ²¡æœ‰è®¿é—®è®°å½•ï¼Œä½¿ç”¨åŸºç¡€æ¿€æ´»åº¦
                    current_activation = base_activation

                # ä½äºé˜ˆå€¼åˆ™æ ‡è®°ä¸ºå¾…é—å¿˜
                if current_activation < threshold:
                    memories_to_forget.append((memory.id, current_activation))
                    logger.debug(
                        f"æ ‡è®°é—å¿˜ {memory.id[:8]}: æ¿€æ´»åº¦={current_activation:.3f} < é˜ˆå€¼={threshold:.3f}"
                    )

            # æ‰¹é‡é—å¿˜è®°å¿†ï¼ˆä¸ç«‹å³æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼‰
            if memories_to_forget:
                logger.info(f"å¼€å§‹æ‰¹é‡é—å¿˜ {len(memories_to_forget)} æ¡è®°å¿†...")

                for memory_id, activation in memories_to_forget:
                    # cleanup_orphans=Falseï¼šæš‚ä¸æ¸…ç†å­¤ç«‹èŠ‚ç‚¹
                    success = await self.forget_memory(memory_id, cleanup_orphans=False)
                    if success:
                        forgotten_count += 1

                # ç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
                logger.info("æ‰¹é‡é—å¿˜å®Œæˆï¼Œå¼€å§‹ç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹...")
                orphan_nodes, orphan_edges = await self._cleanup_orphan_nodes_and_edges()

                # ä¿å­˜æœ€ç»ˆæ›´æ–°
                await self.persistence.save_graph_store(self.graph_store)

                logger.info(
                    f"âœ… è‡ªåŠ¨é—å¿˜å®Œæˆ: é—å¿˜äº† {forgotten_count} æ¡è®°å¿†, "
                    f"æ¸…ç†äº† {orphan_nodes} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges} æ¡å­¤ç«‹è¾¹"
                )
            else:
                logger.info("âœ… è‡ªåŠ¨é—å¿˜å®Œæˆ: æ²¡æœ‰éœ€è¦é—å¿˜çš„è®°å¿†")

            return forgotten_count

        except Exception as e:
            logger.error(f"è‡ªåŠ¨é—å¿˜å¤±è´¥: {e}", exc_info=True)
            return 0

    async def _cleanup_orphan_nodes_and_edges(self) -> tuple[int, int]:
        """
        æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹

        å­¤ç«‹èŠ‚ç‚¹ï¼šä¸å†å±äºä»»ä½•è®°å¿†çš„èŠ‚ç‚¹
        å­¤ç«‹è¾¹ï¼šè¿æ¥åˆ°å·²åˆ é™¤èŠ‚ç‚¹çš„è¾¹

        Returns:
            (æ¸…ç†çš„å­¤ç«‹èŠ‚ç‚¹æ•°, æ¸…ç†çš„å­¤ç«‹è¾¹æ•°)
        """
        try:
            orphan_nodes_count = 0
            orphan_edges_count = 0

            # 1. æ¸…ç†å­¤ç«‹èŠ‚ç‚¹
            # graph_store.node_to_memories è®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªäº›è®°å¿†
            nodes_to_remove = []

            for node_id, memory_ids in list(self.graph_store.node_to_memories.items()):
                # å¦‚æœèŠ‚ç‚¹ä¸å†å±äºä»»ä½•è®°å¿†ï¼Œæ ‡è®°ä¸ºåˆ é™¤
                if not memory_ids:
                    nodes_to_remove.append(node_id)

            # ä»å›¾ä¸­åˆ é™¤å­¤ç«‹èŠ‚ç‚¹
            for node_id in nodes_to_remove:
                if self.graph_store.graph.has_node(node_id):
                    self.graph_store.graph.remove_node(node_id)
                    orphan_nodes_count += 1

                # ä»æ˜ å°„ä¸­åˆ é™¤
                if node_id in self.graph_store.node_to_memories:
                    del self.graph_store.node_to_memories[node_id]

            # 2. æ¸…ç†å­¤ç«‹è¾¹ï¼ˆæŒ‡å‘å·²åˆ é™¤èŠ‚ç‚¹çš„è¾¹ï¼‰
            edges_to_remove = []

            for source, target, edge_id in self.graph_store.graph.edges(data="edge_id"):
                # æ£€æŸ¥è¾¹çš„æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹æ˜¯å¦è¿˜å­˜åœ¨äºnode_to_memoriesä¸­
                if source not in self.graph_store.node_to_memories or \
                   target not in self.graph_store.node_to_memories:
                    edges_to_remove.append((source, target))

            # åˆ é™¤å­¤ç«‹è¾¹
            for source, target in edges_to_remove:
                try:
                    self.graph_store.graph.remove_edge(source, target)
                    orphan_edges_count += 1
                except Exception as e:
                    logger.debug(f"åˆ é™¤è¾¹å¤±è´¥ {source} -> {target}: {e}")

            if orphan_nodes_count > 0 or orphan_edges_count > 0:
                logger.info(
                    f"æ¸…ç†å®Œæˆ: {orphan_nodes_count} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges_count} æ¡å­¤ç«‹è¾¹"
                )

            return orphan_nodes_count, orphan_edges_count

        except Exception as e:
            logger.error(f"æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹å¤±è´¥: {e}", exc_info=True)
            return 0, 0

    # ==================== ç»Ÿè®¡ä¸ç»´æŠ¤ ====================

    def get_statistics(self) -> dict[str, Any]:
        """
        è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self._initialized or not self.graph_store:
            return {}

        stats = self.graph_store.get_statistics()

        # æ·»åŠ æ¿€æ´»åº¦ç»Ÿè®¡
        all_memories = self.graph_store.get_all_memories()
        activation_levels = []
        forgotten_count = 0

        for memory in all_memories:
            if memory.metadata.get("forgotten", False):
                forgotten_count += 1
            else:
                activation_info = memory.metadata.get("activation", {})
                activation_levels.append(activation_info.get("level", 0.0))

        if activation_levels:
            stats["avg_activation"] = sum(activation_levels) / len(activation_levels)
            stats["max_activation"] = max(activation_levels)
        else:
            stats["avg_activation"] = 0.0
            stats["max_activation"] = 0.0

        stats["forgotten_memories"] = forgotten_count
        stats["active_memories"] = stats["total_memories"] - forgotten_count

        return stats

    async def consolidate_memories(
        self,
        similarity_threshold: float = 0.85,
        time_window_hours: float = 24.0,
        max_batch_size: int = 50,
    ) -> dict[str, Any]:
        """
        æ•´ç†è®°å¿†ï¼šç›´æ¥åˆå¹¶å»é‡ç›¸ä¼¼è®°å¿†ï¼ˆä¸åˆ›å»ºæ–°è¾¹ï¼‰

        æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼š
        1. ä½¿ç”¨ asyncio.create_task åœ¨åå°æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
        2. å‘é‡è®¡ç®—æ‰¹é‡å¤„ç†ï¼Œå‡å°‘é‡å¤è®¡ç®—
        3. å»¶è¿Ÿä¿å­˜ï¼Œæ‰¹é‡å†™å…¥æ•°æ®åº“
        4. æ›´é¢‘ç¹çš„åä½œå¼å¤šä»»åŠ¡è®©å‡º

        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼Œå»ºè®®æé«˜åˆ°0.9å‡å°‘è¯¯åˆ¤ï¼‰
            time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_batch_size: å•æ¬¡æœ€å¤šå¤„ç†çš„è®°å¿†æ•°é‡

        Returns:
            æ•´ç†ç»“æœï¼ˆå¦‚æœæ˜¯å¼‚æ­¥æ‰§è¡Œï¼Œè¿”å›å¯åŠ¨çŠ¶æ€ï¼‰
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info(f"ğŸš€ å¯åŠ¨è®°å¿†æ•´ç†ä»»åŠ¡ (similarity_threshold={similarity_threshold}, time_window={time_window_hours}h, max_batch={max_batch_size})...")

            # åˆ›å»ºåå°ä»»åŠ¡æ‰§è¡Œæ•´ç†
            task = asyncio.create_task(
                self._consolidate_memories_background(
                    similarity_threshold=similarity_threshold,
                    time_window_hours=time_window_hours,
                    max_batch_size=max_batch_size
                )
            )

            # è¿”å›ä»»åŠ¡å¯åŠ¨çŠ¶æ€ï¼Œä¸ç­‰å¾…å®Œæˆ
            return {
                "task_started": True,
                "task_id": id(task),
                "message": "è®°å¿†æ•´ç†ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
            }

        except Exception as e:
            logger.error(f"å¯åŠ¨è®°å¿†æ•´ç†ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "task_started": False}

    async def _consolidate_memories_background(
        self,
        similarity_threshold: float,
        time_window_hours: float,
        max_batch_size: int,
    ) -> None:
        """
        åå°æ‰§è¡Œè®°å¿†æ•´ç†çš„å…·ä½“å®ç° (å®Œæ•´ç‰ˆ)

        æµç¨‹ï¼š
        1. è·å–æ—¶é—´çª—å£å†…çš„è®°å¿†
        2. é‡è¦æ€§è¿‡æ»¤
        3. å‘é‡æ£€ç´¢å…³è”è®°å¿†
        4. åˆ†æ‰¹äº¤ç»™LLMåˆ†æå…³ç³»
        5. ç»Ÿä¸€æ›´æ–°è®°å¿†æ•°æ®

        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹ä»»åŠ¡ä¸­è¿è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹
        """
        try:
            result = {
                "merged_count": 0,
                "checked_count": 0,
                "skipped_count": 0,
                "linked_count": 0,
                "importance_filtered": 0,
            }

            # ===== æ­¥éª¤1: è·å–æ—¶é—´çª—å£å†…çš„è®°å¿† =====
            cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= cutoff_time and not mem.metadata.get("forgotten", False)
            ]

            if not recent_memories:
                logger.info("âœ… è®°å¿†æ•´ç†å®Œæˆ: æ²¡æœ‰éœ€è¦æ•´ç†çš„è®°å¿†")
                return

            logger.info(f"ğŸ“‹ æ­¥éª¤1: æ‰¾åˆ° {len(recent_memories)} æ¡æ—¶é—´çª—å£å†…çš„è®°å¿†")

            # ===== æ­¥éª¤2: é‡è¦æ€§è¿‡æ»¤ =====
            min_importance_for_consolidation = getattr(self.config, "consolidation_min_importance", 0.3)
            important_memories = [
                mem for mem in recent_memories
                if mem.importance >= min_importance_for_consolidation
            ]

            result["importance_filtered"] = len(recent_memories) - len(important_memories)
            logger.info(
                f"ğŸ“Š æ­¥éª¤2: é‡è¦æ€§è¿‡æ»¤ (é˜ˆå€¼={min_importance_for_consolidation:.2f}): "
                f"{len(recent_memories)} â†’ {len(important_memories)} æ¡è®°å¿†"
            )

            if not important_memories:
                logger.info("âœ… è®°å¿†æ•´ç†å®Œæˆ: æ²¡æœ‰é‡è¦çš„è®°å¿†éœ€è¦æ•´ç†")
                return

            # é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡
            if len(important_memories) > max_batch_size:
                logger.info(f"ğŸ“Š è®°å¿†æ•°é‡ {len(important_memories)} è¶…è¿‡æ‰¹é‡é™åˆ¶ {max_batch_size}ï¼Œä»…å¤„ç†æœ€æ–°çš„ {max_batch_size} æ¡")
                important_memories = sorted(important_memories, key=lambda m: m.created_at, reverse=True)[:max_batch_size]
                result["skipped_count"] = len(important_memories) - max_batch_size

            result["checked_count"] = len(important_memories)

            # ===== æ­¥éª¤3: å»é‡ï¼ˆç›¸ä¼¼è®°å¿†åˆå¹¶ï¼‰=====
            # æŒ‰è®°å¿†ç±»å‹åˆ†ç»„ï¼Œå‡å°‘è·¨ç±»å‹æ¯”è¾ƒ
            memories_by_type: dict[str, list[Memory]] = {}
            for mem in important_memories:
                mem_type = mem.metadata.get("memory_type", "")
                if mem_type not in memories_by_type:
                    memories_by_type[mem_type] = []
                memories_by_type[mem_type].append(mem)

            # è®°å½•éœ€è¦åˆ é™¤çš„è®°å¿†ï¼Œå»¶è¿Ÿæ‰¹é‡åˆ é™¤
            to_delete: list[tuple[Memory, str]] = []  # (memory, reason)
            deleted_ids = set()

            # å¯¹æ¯ä¸ªç±»å‹çš„è®°å¿†è¿›è¡Œç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆå»é‡ï¼‰
            logger.info("ğŸ“ æ­¥éª¤3: å¼€å§‹ç›¸ä¼¼è®°å¿†å»é‡...")
            for mem_type, memories in memories_by_type.items():
                if len(memories) < 2:
                    continue

                logger.debug(f"ğŸ” æ£€æŸ¥ç±»å‹ '{mem_type}' çš„ {len(memories)} æ¡è®°å¿†")

                # é¢„æå–æ‰€æœ‰ä¸»é¢˜èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
                embeddings_map: dict[str, "np.ndarray"] = {}
                valid_memories = []

                for mem in memories:
                    topic_node = next((n for n in mem.nodes if n.node_type == NodeType.TOPIC), None)
                    if topic_node and topic_node.embedding is not None:
                        embeddings_map[mem.id] = topic_node.embedding
                        valid_memories.append(mem)

                # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæ¯”é€ä¸ªè®¡ç®—æ›´é«˜æ•ˆï¼‰
                for i in range(len(valid_memories)):
                    # æ›´é¢‘ç¹çš„åä½œå¼å¤šä»»åŠ¡è®©å‡º
                    if i % 5 == 0:
                        await asyncio.sleep(0.001)  # 1msè®©å‡º

                    mem_i = valid_memories[i]
                    if mem_i.id in deleted_ids:
                        continue

                    for j in range(i + 1, len(valid_memories)):
                        if valid_memories[j].id in deleted_ids:
                            continue

                        mem_j = valid_memories[j]

                        # å¿«é€Ÿå‘é‡ç›¸ä¼¼åº¦è®¡ç®—
                        embedding_i = embeddings_map[mem_i.id]
                        embedding_j = embeddings_map[mem_j.id]

                        # ä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
                        similarity = cosine_similarity(embedding_i, embedding_j)

                        if similarity >= similarity_threshold:
                            # å†³å®šä¿ç•™å“ªä¸ªè®°å¿†
                            if mem_i.importance >= mem_j.importance:
                                keep_mem, remove_mem = mem_i, mem_j
                            else:
                                keep_mem, remove_mem = mem_j, mem_i

                            logger.debug(
                                f"ğŸ”„ æ ‡è®°ç›¸ä¼¼è®°å¿† (similarity={similarity:.3f}): "
                                f"ä¿ç•™ {keep_mem.id[:8]}, åˆ é™¤ {remove_mem.id[:8]}"
                            )

                            # å¢å¼ºä¿ç•™è®°å¿†çš„é‡è¦æ€§
                            keep_mem.importance = min(1.0, keep_mem.importance + 0.05)

                            # ç´¯åŠ è®¿é—®æ¬¡æ•°
                            if hasattr(keep_mem, "access_count") and hasattr(remove_mem, "access_count"):
                                keep_mem.access_count += remove_mem.access_count

                            # æ ‡è®°ä¸ºå¾…åˆ é™¤ï¼ˆä¸ç«‹å³åˆ é™¤ï¼‰
                            to_delete.append((remove_mem, f"ä¸è®°å¿† {keep_mem.id[:8]} ç›¸ä¼¼åº¦ {similarity:.3f}"))
                            deleted_ids.add(remove_mem.id)
                            result["merged_count"] += 1

                # æ¯å¤„ç†å®Œä¸€ä¸ªç±»å‹å°±è®©å‡ºæ§åˆ¶æƒ
                await asyncio.sleep(0.005)  # 5msè®©å‡º

            # æ‰¹é‡åˆ é™¤æ ‡è®°çš„è®°å¿†
            if to_delete:
                logger.info(f"ğŸ—‘ï¸ æ‰¹é‡åˆ é™¤ {len(to_delete)} æ¡ç›¸ä¼¼è®°å¿†")

                for memory, reason in to_delete:
                    try:
                        # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
                        for node in memory.nodes:
                            if node.embedding is not None:
                                await self.vector_store.delete_node(node.id)

                        # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
                        self.graph_store.remove_memory(memory.id)

                    except Exception as e:
                        logger.warning(f"åˆ é™¤è®°å¿† {memory.id[:8]} å¤±è´¥: {e}")

                # æ‰¹é‡ä¿å­˜ï¼ˆä¸€æ¬¡æ€§å†™å…¥ï¼Œå‡å°‘I/Oï¼Œå¼‚æ­¥æ‰§è¡Œï¼‰
                asyncio.create_task(self._async_save_graph_store("è®°å¿†å»é‡"))
                logger.info("ğŸ’¾ å»é‡ä¿å­˜ä»»åŠ¡å·²å¯åŠ¨")

            # ===== æ­¥éª¤4: å‘é‡æ£€ç´¢å…³è”è®°å¿† + LLMåˆ†æå…³ç³» =====
            # è¿‡æ»¤æ‰å·²åˆ é™¤çš„è®°å¿†
            remaining_memories = [m for m in important_memories if m.id not in deleted_ids]

            if not remaining_memories:
                logger.info("âœ… è®°å¿†æ•´ç†å®Œæˆ: å»é‡åæ— å‰©ä½™è®°å¿†")
                return

            logger.info(f"ğŸ“ æ­¥éª¤4: å¼€å§‹å…³è”åˆ†æ ({len(remaining_memories)} æ¡è®°å¿†)...")

            # åˆ†æ‰¹å¤„ç†è®°å¿†å…³è”
            llm_batch_size = getattr(self.config, "consolidation_llm_batch_size", 10)
            max_candidates_per_memory = getattr(self.config, "consolidation_max_candidates", 5)
            min_confidence = getattr(self.config, "consolidation_min_confidence", 0.6)

            all_new_edges = []  # æ”¶é›†æ‰€æœ‰æ–°å»ºçš„è¾¹

            for batch_start in range(0, len(remaining_memories), llm_batch_size):
                batch_end = min(batch_start + llm_batch_size, len(remaining_memories))
                batch = remaining_memories[batch_start:batch_end]

                logger.debug(f"å¤„ç†æ‰¹æ¬¡ {batch_start//llm_batch_size + 1}/{(len(remaining_memories)-1)//llm_batch_size + 1}")

                for memory in batch:
                    # è·³è¿‡å·²ç»æœ‰å¾ˆå¤šè¿æ¥çš„è®°å¿†
                    existing_edges = len([
                        e for e in memory.edges
                        if e.edge_type == EdgeType.RELATION
                    ])
                    if existing_edges >= 10:
                        continue

                    # ä½¿ç”¨å‘é‡æœç´¢æ‰¾å€™é€‰å…³è”è®°å¿†
                    candidates = await self._find_link_candidates(
                        memory,
                        exclude_ids={memory.id} | deleted_ids,
                        max_results=max_candidates_per_memory
                    )

                    if not candidates:
                        continue

                    # ä½¿ç”¨LLMåˆ†æå…³ç³»
                    relations = await self._analyze_memory_relations(
                        source_memory=memory,
                        candidate_memories=candidates,
                        min_confidence=min_confidence
                    )

                    # å»ºç«‹å…³è”è¾¹
                    for relation in relations:
                        try:
                            # åˆ›å»ºå…³è”è¾¹
                            edge = MemoryEdge(
                                id=f"edge_{uuid.uuid4().hex[:12]}",
                                source_id=memory.subject_id,
                                target_id=relation["target_memory"].subject_id,
                                relation=relation["relation_type"],
                                edge_type=EdgeType.RELATION,
                                importance=relation["confidence"],
                                metadata={
                                    "auto_linked": True,
                                    "confidence": relation["confidence"],
                                    "reasoning": relation["reasoning"],
                                    "created_at": datetime.now().isoformat(),
                                    "created_by": "consolidation",
                                }
                            )

                            all_new_edges.append((memory, edge, relation))
                            result["linked_count"] += 1

                        except Exception as e:
                            logger.warning(f"åˆ›å»ºå…³è”è¾¹å¤±è´¥: {e}")
                            continue

                # æ¯ä¸ªæ‰¹æ¬¡åè®©å‡ºæ§åˆ¶æƒ
                await asyncio.sleep(0.01)

            # ===== æ­¥éª¤5: ç»Ÿä¸€æ›´æ–°è®°å¿†æ•°æ® =====
            if all_new_edges:
                logger.info(f"ğŸ“ æ­¥éª¤5: ç»Ÿä¸€æ›´æ–° {len(all_new_edges)} æ¡æ–°å…³è”è¾¹...")

                for memory, edge, relation in all_new_edges:
                    try:
                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )

                        # åŒæ—¶æ·»åŠ åˆ°è®°å¿†çš„è¾¹åˆ—è¡¨
                        memory.edges.append(edge)

                        logger.debug(
                            f"âœ“ {memory.id[:8]} --[{relation['relation_type']}]--> "
                            f"{relation['target_memory'].id[:8]} (ç½®ä¿¡åº¦={relation['confidence']:.2f})"
                        )

                    except Exception as e:
                        logger.warning(f"æ·»åŠ è¾¹åˆ°å›¾å¤±è´¥: {e}")

                # æ‰¹é‡ä¿å­˜æ›´æ–°ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
                asyncio.create_task(self._async_save_graph_store("è®°å¿†å…³è”è¾¹"))
                logger.info("ğŸ’¾ å…³è”è¾¹ä¿å­˜ä»»åŠ¡å·²å¯åŠ¨")

            logger.info(f"âœ… è®°å¿†æ•´ç†å®Œæˆ: {result}")

        except Exception as e:
            logger.error(f"âŒ è®°å¿†æ•´ç†å¤±è´¥: {e}", exc_info=True)

    async def auto_link_memories(
        self,
        time_window_hours: float | None = None,
        max_candidates: int | None = None,
        min_confidence: float | None = None,
    ) -> dict[str, Any]:
        """
        è‡ªåŠ¨å…³è”è®°å¿†

        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»ï¼Œè‡ªåŠ¨å»ºç«‹å…³è”è¾¹ã€‚

        Args:
            time_window_hours: åˆ†ææ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_candidates: æ¯ä¸ªè®°å¿†æœ€å¤šå…³è”çš„å€™é€‰æ•°
            min_confidence: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            å…³è”ç»“æœç»Ÿè®¡
        """
        if not self._initialized:
            await self.initialize()

        # ä½¿ç”¨é…ç½®å€¼æˆ–å‚æ•°è¦†ç›–
        time_window_hours = time_window_hours if time_window_hours is not None else 24
        max_candidates = max_candidates if max_candidates is not None else getattr(self.config, "auto_link_max_candidates", 10)
        min_confidence = min_confidence if min_confidence is not None else getattr(self.config, "auto_link_min_confidence", 0.7)

        try:
            logger.info(f"å¼€å§‹è‡ªåŠ¨å…³è”è®°å¿† (æ—¶é—´çª—å£={time_window_hours}h)...")

            result = {
                "checked_count": 0,
                "linked_count": 0,
                "relation_stats": {},  # å…³ç³»ç±»å‹ç»Ÿè®¡ {ç±»å‹: æ•°é‡}
                "relations": {},  # è¯¦ç»†å…³ç³» {source_id: [å…³ç³»åˆ—è¡¨]}
            }

            # 1. è·å–æ—¶é—´çª—å£å†…çš„è®°å¿†
            time_threshold = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= time_threshold
                and not mem.metadata.get("forgotten", False)
            ]

            if len(recent_memories) < 2:
                logger.info("è®°å¿†æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è‡ªåŠ¨å…³è”")
                return result

            logger.info(f"æ‰¾åˆ° {len(recent_memories)} æ¡å¾…å…³è”è®°å¿†")

            # 2. ä¸ºæ¯ä¸ªè®°å¿†å¯»æ‰¾å…³è”å€™é€‰
            for memory in recent_memories:
                result["checked_count"] += 1

                # è·³è¿‡å·²ç»æœ‰å¾ˆå¤šè¿æ¥çš„è®°å¿†
                existing_edges = len([
                    e for e in memory.edges
                    if e.edge_type == EdgeType.RELATION
                ])
                if existing_edges >= 10:
                    continue

                # 3. ä½¿ç”¨å‘é‡æœç´¢æ‰¾å€™é€‰è®°å¿†
                candidates = await self._find_link_candidates(
                    memory,
                    exclude_ids={memory.id},
                    max_results=max_candidates
                )

                if not candidates:
                    continue

                # 4. ä½¿ç”¨LLMåˆ†æå…³ç³»
                relations = await self._analyze_memory_relations(
                    source_memory=memory,
                    candidate_memories=candidates,
                    min_confidence=min_confidence
                )

                # 5. å»ºç«‹å…³è”
                for relation in relations:
                    try:
                        # åˆ›å»ºå…³è”è¾¹
                        edge = MemoryEdge(
                            id=f"edge_{uuid.uuid4().hex[:12]}",
                            source_id=memory.subject_id,
                            target_id=relation["target_memory"].subject_id,
                            relation=relation["relation_type"],
                            edge_type=EdgeType.RELATION,
                            importance=relation["confidence"],
                            metadata={
                                "auto_linked": True,
                                "confidence": relation["confidence"],
                                "reasoning": relation["reasoning"],
                                "created_at": datetime.now().isoformat(),
                            }
                        )

                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )

                        # åŒæ—¶æ·»åŠ åˆ°è®°å¿†çš„è¾¹åˆ—è¡¨
                        memory.edges.append(edge)

                        result["linked_count"] += 1

                        # æ›´æ–°ç»Ÿè®¡
                        result["relation_stats"][relation["relation_type"]] = \
                            result["relation_stats"].get(relation["relation_type"], 0) + 1

                        # è®°å½•è¯¦ç»†å…³ç³»
                        if memory.id not in result["relations"]:
                            result["relations"][memory.id] = []
                        result["relations"][memory.id].append({
                            "target_id": relation["target_memory"].id,
                            "relation_type": relation["relation_type"],
                            "confidence": relation["confidence"],
                            "reasoning": relation["reasoning"],
                        })

                        logger.info(
                            f"å»ºç«‹å…³è”: {memory.id[:8]} --[{relation['relation_type']}]--> "
                            f"{relation['target_memory'].id[:8]} "
                            f"(ç½®ä¿¡åº¦={relation['confidence']:.2f})"
                        )

                    except Exception as e:
                        logger.warning(f"å»ºç«‹å…³è”å¤±è´¥: {e}")
                        continue

            # å¼‚æ­¥ä¿å­˜æ›´æ–°åçš„å›¾æ•°æ®
            if result["linked_count"] > 0:
                asyncio.create_task(self._async_save_graph_store("è‡ªåŠ¨å…³è”"))
                logger.info(f"å·²å¯åŠ¨ä¿å­˜ä»»åŠ¡: {result['linked_count']} æ¡è‡ªåŠ¨å…³è”è¾¹")

            logger.info(f"è‡ªåŠ¨å…³è”å®Œæˆ: {result}")
            return result

        except Exception as e:
            logger.error(f"è‡ªåŠ¨å…³è”å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "checked_count": 0, "linked_count": 0}

    async def _find_link_candidates(
        self,
        memory: Memory,
        exclude_ids: set[str],
        max_results: int = 5,
    ) -> list[Memory]:
        """
        ä¸ºè®°å¿†å¯»æ‰¾å…³è”å€™é€‰

        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ + æ—¶é—´æ¥è¿‘åº¦æ‰¾åˆ°æ½œåœ¨ç›¸å…³è®°å¿†
        """
        try:
            # è·å–è®°å¿†çš„ä¸»é¢˜
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )

            if not topic_node or not topic_node.content:
                return []

            # ä½¿ç”¨ä¸»é¢˜å†…å®¹æœç´¢ç›¸ä¼¼è®°å¿†
            candidates = await self.search_memories(
                query=topic_node.content,
                top_k=max_results * 2,
                include_forgotten=False,
            )

            # è¿‡æ»¤ï¼šæ’é™¤è‡ªå·±å’Œå·²å…³è”çš„
            existing_targets = {
                e.target_id for e in memory.edges
                if e.edge_type == EdgeType.RELATION
            }

            filtered = [
                c for c in candidates
                if c.id not in exclude_ids
                and c.id not in existing_targets
            ]

            return filtered[:max_results]

        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾å€™é€‰å¤±è´¥: {e}")
            return []

    async def _analyze_memory_relations(
        self,
        source_memory: Memory,
        candidate_memories: list[Memory],
        min_confidence: float = 0.7,
    ) -> list[dict[str, Any]]:
        """
        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»

        Args:
            source_memory: æºè®°å¿†
            candidate_memories: å€™é€‰è®°å¿†åˆ—è¡¨
            min_confidence: æœ€ä½ç½®ä¿¡åº¦

        Returns:
            å…³ç³»åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
            - target_memory: ç›®æ ‡è®°å¿†
            - relation_type: å…³ç³»ç±»å‹
            - confidence: ç½®ä¿¡åº¦
            - reasoning: æ¨ç†è¿‡ç¨‹
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            # æ„å»ºLLMè¯·æ±‚
            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.relation_analysis"
            )

            # æ ¼å¼åŒ–è®°å¿†ä¿¡æ¯
            source_desc = self._format_memory_for_llm(source_memory)
            candidates_desc = "\n\n".join([
                f"è®°å¿†{i+1}:\n{self._format_memory_for_llm(mem)}"
                for i, mem in enumerate(candidate_memories)
            ])

            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†å…³ç³»åˆ†æä¸“å®¶ã€‚è¯·åˆ†ææºè®°å¿†ä¸å€™é€‰è®°å¿†ä¹‹é—´æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„å…³ç³»ã€‚

**å…³ç³»ç±»å‹è¯´æ˜ï¼š**
- å¯¼è‡´: Açš„å‘ç”Ÿå¯¼è‡´äº†Bçš„å‘ç”Ÿï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨: Aæåˆ°æˆ–æ¶‰åŠBï¼ˆå¼•ç”¨å…³ç³»ï¼‰
- ç›¸ä¼¼: Aå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆç›¸ä¼¼å…³ç³»ï¼‰
- ç›¸å: Aå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹ç«‹å…³ç³»ï¼‰
- å…³è”: Aå’ŒBå­˜åœ¨æŸç§å…³è”ä½†ä¸å±äºä»¥ä¸Šç±»å‹ï¼ˆä¸€èˆ¬å…³è”ï¼‰

**æºè®°å¿†ï¼š**
{source_desc}

**å€™é€‰è®°å¿†ï¼š**
{candidates_desc}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. å¯¹æ¯ä¸ªå€™é€‰è®°å¿†ï¼Œåˆ¤æ–­æ˜¯å¦ä¸æºè®°å¿†å­˜åœ¨å…³ç³»
2. å¦‚æœå­˜åœ¨å…³ç³»ï¼ŒæŒ‡å®šå…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦(0.0-1.0)
3. ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±
4. åªè¿”å›ç½®ä¿¡åº¦ >= {min_confidence} çš„å…³ç³»

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
[
  {{
    "candidate_id": 1,
    "has_relation": true,
    "relation_type": "å¯¼è‡´",
    "confidence": 0.85,
    "reasoning": "è®°å¿†1æ˜¯è®°å¿†æºçš„ç»“æœ"
  }},
  {{
    "candidate_id": 2,
    "has_relation": false,
    "reasoning": "ä¸¤è€…æ— æ˜æ˜¾å…³è”"
  }}
]
```

è¯·åˆ†æå¹¶è¾“å‡ºJSONç»“æœï¼š"""

            # è°ƒç”¨LLM
            response, _ = await llm.generate_response_async(
                prompt,
                temperature=0.3,
                max_tokens=1000,
            )

            # è§£æå“åº”
            import json
            import re

            # æå–JSON
            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()

            try:
                analysis_results = json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¿®å¤: {response[:200]}")
                # å°è¯•ç®€å•ä¿®å¤
                json_str = re.sub(r"[\r\n\t]", "", json_str)
                analysis_results = json.loads(json_str)

            # è½¬æ¢ä¸ºç»“æœæ ¼å¼
            relations = []
            for result in analysis_results:
                if not result.get("has_relation", False):
                    continue

                confidence = result.get("confidence", 0.0)
                if confidence < min_confidence:
                    continue

                candidate_id = result.get("candidate_id", 0) - 1
                if 0 <= candidate_id < len(candidate_memories):
                    relations.append({
                        "target_memory": candidate_memories[candidate_id],
                        "relation_type": result.get("relation_type", "å…³è”"),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", ""),
                    })

            logger.debug(f"LLMåˆ†æå®Œæˆ: å‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations

        except Exception as e:
            logger.error(f"LLMå…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)
            return []

    def _format_memory_for_llm(self, memory: Memory) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸ºLLMå¯è¯»çš„æ–‡æœ¬"""
        try:
            # è·å–å…³é”®èŠ‚ç‚¹
            subject_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.SUBJECT),
                None
            )
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )
            object_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.OBJECT),
                None
            )

            parts = []
            parts.append(f"ç±»å‹: {memory.memory_type.value}")

            if subject_node:
                parts.append(f"ä¸»ä½“: {subject_node.content}")

            if topic_node:
                parts.append(f"ä¸»é¢˜: {topic_node.content}")

            if object_node:
                parts.append(f"å¯¹è±¡: {object_node.content}")

            parts.append(f"é‡è¦æ€§: {memory.importance:.2f}")
            parts.append(f"æ—¶é—´: {memory.created_at.strftime('%Y-%m-%d %H:%M')}")

            return " | ".join(parts)

        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–è®°å¿†å¤±è´¥: {e}")
            return f"è®°å¿†ID: {memory.id}"

    async def maintenance(self) -> dict[str, Any]:
        """
        æ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        åŒ…æ‹¬ï¼š
        - è®°å¿†æ•´ç†ï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼‰
        - è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆè½»é‡çº§æ‰§è¡Œï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®

        Returns:
            ç»´æŠ¤ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œè®°å¿†ç³»ç»Ÿç»´æŠ¤ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")

            result = {
                "consolidation_task": "none",
                "linked": 0,
                "forgotten": 0,
                "saved": False,
                "total_time": 0,
            }

            start_time = datetime.now()

            # 1. è®°å¿†æ•´ç†ï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            if getattr(self.config, "consolidation_enabled", False):
                logger.info("ğŸš€ å¯åŠ¨å¼‚æ­¥è®°å¿†æ•´ç†ä»»åŠ¡...")
                consolidate_result = await self.consolidate_memories(
                    similarity_threshold=getattr(self.config, "consolidation_deduplication_threshold", 0.93),
                    time_window_hours=getattr(self.config, "consolidation_time_window_hours", 2.0),  # ç»Ÿä¸€æ—¶é—´çª—å£
                    max_batch_size=getattr(self.config, "consolidation_max_batch_size", 30)
                )

                if consolidate_result.get("task_started"):
                    result["consolidation_task"] = f"background_task_{consolidate_result.get('task_id', 'unknown')}"
                    logger.info("âœ… è®°å¿†æ•´ç†ä»»åŠ¡å·²å¯åŠ¨åˆ°åå°æ‰§è¡Œ")
                else:
                    result["consolidation_task"] = "failed"
                    logger.warning("âŒ è®°å¿†æ•´ç†ä»»åŠ¡å¯åŠ¨å¤±è´¥")

            # 2. è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£ï¼‰
            if getattr(self.config, "consolidation_linking_enabled", True):
                logger.info("ğŸ”— æ‰§è¡Œè½»é‡çº§è‡ªåŠ¨å…³è”...")
                link_result = await self._lightweight_auto_link_memories()
                result["linked"] = link_result.get("linked_count", 0)

            # 3. è‡ªåŠ¨é—å¿˜ï¼ˆå¿«é€Ÿæ‰§è¡Œï¼‰
            if getattr(self.config, "forgetting_enabled", True):
                logger.info("ğŸ—‘ï¸ æ‰§è¡Œè‡ªåŠ¨é—å¿˜...")
                forgotten_count = await self.auto_forget_memories(
                    threshold=getattr(self.config, "forgetting_activation_threshold", 0.1)
                )
                result["forgotten"] = forgotten_count

            # 4. ä¿å­˜æ•°æ®ï¼ˆå¦‚æœè®°å¿†æ•´ç†ä¸åœ¨åå°æ‰§è¡Œï¼‰
            if result["consolidation_task"] == "none":
                await self.persistence.save_graph_store(self.graph_store)
                result["saved"] = True
                logger.info("ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ")

            self._last_maintenance = datetime.now()

            # è®¡ç®—ç»´æŠ¤è€—æ—¶
            total_time = (datetime.now() - start_time).total_seconds()
            result["total_time"] = total_time

            logger.info(f"âœ… ç»´æŠ¤å®Œæˆ (è€—æ—¶ {total_time:.2f}s): {result}")
            return result

        except Exception as e:
            logger.error(f"âŒ ç»´æŠ¤å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "total_time": 0}

    async def _lightweight_auto_link_memories(
        self,
        time_window_hours: float | None = None,  # ä»é…ç½®è¯»å–
        max_candidates: int | None = None,  # ä»é…ç½®è¯»å–
        max_memories: int | None = None,  # ä»é…ç½®è¯»å–
    ) -> dict[str, Any]:
        """
        æ™ºèƒ½è½»é‡çº§è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆä¿ç•™LLMåˆ¤æ–­ï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä»é…ç½®è¯»å–å¤„ç†å‚æ•°ï¼Œå°Šé‡ç”¨æˆ·è®¾ç½®
        2. ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é¢„ç­›é€‰ï¼Œä»…å¯¹é«˜ç›¸ä¼¼åº¦è®°å¿†è°ƒç”¨LLM
        3. æ‰¹é‡LLMè°ƒç”¨ï¼Œå‡å°‘ç½‘ç»œå¼€é”€
        4. å¼‚æ­¥æ‰§è¡Œï¼Œé¿å…é˜»å¡
        """
        try:
            result = {
                "checked_count": 0,
                "linked_count": 0,
                "llm_calls": 0,
            }

            # ä»é…ç½®è¯»å–å‚æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ—¶é—´çª—å£
            if time_window_hours is None:
                time_window_hours = getattr(self.config, "consolidation_time_window_hours", 2.0)
            if max_candidates is None:
                max_candidates = getattr(self.config, "consolidation_linking_max_candidates", 10)
            if max_memories is None:
                max_memories = getattr(self.config, "consolidation_linking_max_memories", 20)

            # è·å–ç”¨æˆ·é…ç½®æ—¶é—´çª—å£å†…çš„è®°å¿†
            time_threshold = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()

            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= time_threshold
                and not mem.metadata.get("forgotten", False)
                and mem.importance >= getattr(self.config, "consolidation_linking_min_importance", 0.5)  # ä»é…ç½®è¯»å–é‡è¦æ€§é˜ˆå€¼
            ]

            if len(recent_memories) > max_memories:
                recent_memories = sorted(recent_memories, key=lambda m: m.created_at, reverse=True)[:max_memories]

            if len(recent_memories) < 2:
                logger.debug("è®°å¿†æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æ™ºèƒ½å…³è”")
                return result

            logger.debug(f"ğŸ§  æ™ºèƒ½å…³è”: æ£€æŸ¥ {len(recent_memories)} æ¡é‡è¦è®°å¿†")

            # ç¬¬ä¸€æ­¥ï¼šå‘é‡ç›¸ä¼¼åº¦é¢„ç­›é€‰ï¼Œæ‰¾åˆ°æ½œåœ¨å…³è”å¯¹
            candidate_pairs = []

            for i, memory in enumerate(recent_memories):
                # è·å–ä¸»é¢˜èŠ‚ç‚¹
                topic_node = next(
                    (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                    None
                )

                if not topic_node or topic_node.embedding is None:
                    continue

                # ä¸å…¶ä»–è®°å¿†è®¡ç®—ç›¸ä¼¼åº¦
                for j, other_memory in enumerate(recent_memories[i+1:], i+1):
                    other_topic = next(
                        (n for n in other_memory.nodes if n.node_type == NodeType.TOPIC),
                        None
                    )

                    if not other_topic or other_topic.embedding is None:
                        continue

                    # å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—
                    similarity = cosine_similarity(
                        topic_node.embedding,
                        other_topic.embedding
                    )

                    # ä½¿ç”¨é…ç½®çš„é¢„ç­›é€‰é˜ˆå€¼
                    pre_filter_threshold = getattr(self.config, "consolidation_linking_pre_filter_threshold", 0.7)
                    if similarity >= pre_filter_threshold:
                        candidate_pairs.append((memory, other_memory, similarity))

                # è®©å‡ºæ§åˆ¶æƒ
                if i % 3 == 0:
                    await asyncio.sleep(0.001)

            logger.debug(f"ğŸ” é¢„ç­›é€‰æ‰¾åˆ° {len(candidate_pairs)} ä¸ªå€™é€‰å…³è”å¯¹")

            if not candidate_pairs:
                return result

            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡LLMåˆ†æï¼ˆä½¿ç”¨é…ç½®çš„æœ€å¤§å€™é€‰å¯¹æ•°ï¼‰
            max_pairs_for_llm = getattr(self.config, "consolidation_linking_max_pairs_for_llm", 5)
            if len(candidate_pairs) <= max_pairs_for_llm:
                link_relations = await self._batch_analyze_memory_relations(candidate_pairs)
                result["llm_calls"] = 1

                # ç¬¬ä¸‰æ­¥ï¼šå»ºç«‹LLMç¡®è®¤çš„å…³è”
                for relation_info in link_relations:
                    try:
                        memory_a, memory_b = relation_info["memory_pair"]
                        relation_type = relation_info["relation_type"]
                        confidence = relation_info["confidence"]

                        # åˆ›å»ºå…³è”è¾¹
                        edge = MemoryEdge(
                            id=f"smart_edge_{uuid.uuid4().hex[:12]}",
                            source_id=memory_a.subject_id,
                            target_id=memory_b.subject_id,
                            relation=relation_type,
                            edge_type=EdgeType.RELATION,
                            importance=confidence,
                            metadata={
                                "auto_linked": True,
                                "method": "llm_analyzed",
                                "vector_similarity": relation_info.get("vector_similarity", 0.0),
                                "confidence": confidence,
                                "reasoning": relation_info.get("reasoning", ""),
                                "created_at": datetime.now().isoformat(),
                            }
                        )

                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )

                        memory_a.edges.append(edge)
                        result["linked_count"] += 1

                        logger.debug(f"ğŸ§  æ™ºèƒ½å…³è”: {memory_a.id[:8]} --[{relation_type}]--> {memory_b.id[:8]} (ç½®ä¿¡åº¦={confidence:.2f})")

                    except Exception as e:
                        logger.warning(f"å»ºç«‹æ™ºèƒ½å…³è”å¤±è´¥: {e}")
                        continue

            # ä¿å­˜å…³è”ç»“æœ
            if result["linked_count"] > 0:
                await self.persistence.save_graph_store(self.graph_store)

            logger.debug(f"âœ… æ™ºèƒ½å…³è”å®Œæˆ: å»ºç«‹äº† {result['linked_count']} ä¸ªå…³è”ï¼ŒLLMè°ƒç”¨ {result['llm_calls']} æ¬¡")
            return result

        except Exception as e:
            logger.error(f"æ™ºèƒ½å…³è”å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "checked_count": 0, "linked_count": 0}

    async def _batch_analyze_memory_relations(
        self,
        candidate_pairs: list[tuple[Memory, Memory, float]]
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æè®°å¿†å…³ç³»ï¼ˆä¼˜åŒ–LLMè°ƒç”¨ï¼‰

        Args:
            candidate_pairs: å€™é€‰è®°å¿†å¯¹åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« (memory_a, memory_b, vector_similarity)

        Returns:
            å…³ç³»åˆ†æç»“æœåˆ—è¡¨
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.batch_relation_analysis"
            )

            # æ ¼å¼åŒ–æ‰€æœ‰å€™é€‰è®°å¿†å¯¹
            candidates_text = ""
            for i, (mem_a, mem_b, similarity) in enumerate(candidate_pairs):
                desc_a = self._format_memory_for_llm(mem_a)
                desc_b = self._format_memory_for_llm(mem_b)
                candidates_text += f"""
å€™é€‰å¯¹ {i+1}:
è®°å¿†A: {desc_a}
è®°å¿†B: {desc_b}
å‘é‡ç›¸ä¼¼åº¦: {similarity:.3f}
"""

            # æ„å»ºæ‰¹é‡åˆ†ææç¤ºè¯ï¼ˆä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
            min_confidence = getattr(self.config, "consolidation_linking_min_confidence", 0.7)

            prompt = f"""ä½ æ˜¯è®°å¿†å…³ç³»åˆ†æä¸“å®¶ã€‚è¯·æ‰¹é‡åˆ†æä»¥ä¸‹å€™é€‰è®°å¿†å¯¹ä¹‹é—´çš„å…³ç³»ã€‚

**å…³ç³»ç±»å‹è¯´æ˜ï¼š**
- å¯¼è‡´: Açš„å‘ç”Ÿå¯¼è‡´äº†Bçš„å‘ç”Ÿï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨: Aæåˆ°æˆ–æ¶‰åŠBï¼ˆå¼•ç”¨å…³ç³»ï¼‰
- ç›¸ä¼¼: Aå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆç›¸ä¼¼å…³ç³»ï¼‰
- ç›¸å: Aå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹ç«‹å…³ç³»ï¼‰
- å…³è”: Aå’ŒBå­˜åœ¨æŸç§å…³è”ä½†ä¸å±äºä»¥ä¸Šç±»å‹ï¼ˆä¸€èˆ¬å…³è”ï¼‰

**å€™é€‰è®°å¿†å¯¹ï¼š**
{candidates_text}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. å¯¹æ¯ä¸ªå€™é€‰å¯¹ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„å…³ç³»
2. å¦‚æœå­˜åœ¨å…³ç³»ï¼ŒæŒ‡å®šå…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦(0.0-1.0)
3. ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±
4. åªè¿”å›ç½®ä¿¡åº¦ >= {min_confidence} çš„å…³ç³»
5. ä¼˜å…ˆè€ƒè™‘å› æœã€å¼•ç”¨ç­‰å¼ºå…³ç³»ï¼Œè°¨æ…å»ºç«‹ç›¸ä¼¼å…³ç³»

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
[
  {{
    "candidate_id": 1,
    "has_relation": true,
    "relation_type": "å¯¼è‡´",
    "confidence": 0.85,
    "reasoning": "è®°å¿†Aæè¿°çš„åŸå› å¯¼è‡´è®°å¿†Bçš„ç»“æœ"
  }},
  {{
    "candidate_id": 2,
    "has_relation": false,
    "reasoning": "ä¸¤è€…æ— æ˜æ˜¾å…³è”"
  }}
]
```

è¯·åˆ†æå¹¶è¾“å‡ºJSONç»“æœï¼š"""

            # è°ƒç”¨LLMï¼ˆä½¿ç”¨é…ç½®çš„å‚æ•°ï¼‰
            llm_temperature = getattr(self.config, "consolidation_linking_llm_temperature", 0.2)
            llm_max_tokens = getattr(self.config, "consolidation_linking_llm_max_tokens", 1500)

            response, _ = await llm.generate_response_async(
                prompt,
                temperature=llm_temperature,
                max_tokens=llm_max_tokens,
            )

            # è§£æå“åº”
            import json
            import re

            # æå–JSON
            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()

            try:
                analysis_results = json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¿®å¤: {response[:200]}")
                # å°è¯•ç®€å•ä¿®å¤
                json_str = re.sub(r"[\r\n\t]", "", json_str)
                analysis_results = json.loads(json_str)

            # è½¬æ¢ä¸ºç»“æœæ ¼å¼
            relations = []
            for result in analysis_results:
                if not result.get("has_relation", False):
                    continue

                confidence = result.get("confidence", 0.0)
                if confidence < min_confidence:  # ä½¿ç”¨é…ç½®çš„ç½®ä¿¡åº¦é˜ˆå€¼
                    continue

                candidate_id = result.get("candidate_id", 0) - 1
                if 0 <= candidate_id < len(candidate_pairs):
                    mem_a, mem_b, vector_similarity = candidate_pairs[candidate_id]
                    relations.append({
                        "memory_pair": (mem_a, mem_b),
                        "relation_type": result.get("relation_type", "å…³è”"),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", ""),
                        "vector_similarity": vector_similarity,
                    })

            logger.debug(f"ğŸ§  LLMæ‰¹é‡åˆ†æå®Œæˆ: å‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations

        except Exception as e:
            logger.error(f"LLMæ‰¹é‡å…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)
            return []

    def _start_maintenance_task(self) -> None:
        """
        å¯åŠ¨è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡

        ç›´æ¥åˆ›å»ºasync taskï¼Œé¿å…ä½¿ç”¨scheduleré˜»å¡ä¸»ç¨‹åºï¼š
        - è®°å¿†æ•´åˆï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®

        é»˜è®¤é—´éš”ï¼š1å°æ—¶
        """
        try:
            # å¦‚æœå·²æœ‰ç»´æŠ¤ä»»åŠ¡ï¼Œå…ˆåœæ­¢
            if self._maintenance_task and not self._maintenance_task.done():
                self._maintenance_task.cancel()
                logger.info("å–æ¶ˆæ—§çš„ç»´æŠ¤ä»»åŠ¡")

            # åˆ›å»ºæ–°çš„åå°ç»´æŠ¤ä»»åŠ¡
            self._maintenance_task = asyncio.create_task(
                self._maintenance_loop(),
                name="memory_maintenance_loop"
            )

            logger.info(
                f"âœ… è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡å·²å¯åŠ¨ "
                f"(é—´éš”={self._maintenance_interval_hours}å°æ—¶)"
            )

        except Exception as e:
            logger.error(f"å¯åŠ¨ç»´æŠ¤åå°ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    async def _stop_maintenance_task(self) -> None:
        """
        åœæ­¢è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡
        """
        if not self._maintenance_task or self._maintenance_task.done():
            return

        try:
            self._maintenance_running = False  # è®¾ç½®åœæ­¢æ ‡å¿—
            self._maintenance_task.cancel()

            try:
                await self._maintenance_task
            except asyncio.CancelledError:
                logger.debug("ç»´æŠ¤ä»»åŠ¡å·²å–æ¶ˆ")

            logger.info("âœ… è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡å·²åœæ­¢")
            self._maintenance_task = None

        except Exception as e:
            logger.error(f"åœæ­¢ç»´æŠ¤åå°ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    async def _maintenance_loop(self) -> None:
        """
        è®°å¿†ç»´æŠ¤å¾ªç¯

        åœ¨åå°ç‹¬ç«‹è¿è¡Œï¼Œå®šæœŸæ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼Œé¿å…é˜»å¡ä¸»ç¨‹åº
        """
        self._maintenance_running = True

        try:
            # é¦–æ¬¡æ‰§è¡Œå»¶è¿Ÿï¼ˆå¯åŠ¨å1å°æ—¶ï¼‰
            initial_delay = self._maintenance_interval_hours * 3600
            logger.debug(f"è®°å¿†ç»´æŠ¤ä»»åŠ¡å°†åœ¨ {initial_delay} ç§’åé¦–æ¬¡æ‰§è¡Œ")

            while self._maintenance_running:
                try:
                    # ä½¿ç”¨ asyncio.wait_for æ¥æ”¯æŒå–æ¶ˆ
                    await asyncio.wait_for(
                        asyncio.sleep(initial_delay),
                        timeout=float("inf")  # å…è®¸éšæ—¶å–æ¶ˆ
                    )

                    # æ£€æŸ¥æ˜¯å¦ä»ç„¶éœ€è¦è¿è¡Œ
                    if not self._maintenance_running:
                        break

                    # æ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼ˆä½¿ç”¨try-catché¿å…å´©æºƒï¼‰
                    try:
                        await self.maintenance()
                    except Exception as e:
                        logger.error(f"ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

                    # åç»­æ‰§è¡Œä½¿ç”¨ç›¸åŒé—´éš”
                    initial_delay = self._maintenance_interval_hours * 3600

                except asyncio.CancelledError:
                    logger.debug("ç»´æŠ¤å¾ªç¯è¢«å–æ¶ˆ")
                    break
                except Exception as e:
                    logger.error(f"ç»´æŠ¤å¾ªç¯å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
                    # å¼‚å¸¸åç­‰å¾…è¾ƒçŸ­æ—¶é—´å†é‡è¯•
                    try:
                        await asyncio.sleep(300)  # 5åˆ†é’Ÿåé‡è¯•
                    except asyncio.CancelledError:
                        break

        except asyncio.CancelledError:
            logger.debug("ç»´æŠ¤å¾ªç¯å®Œå…¨é€€å‡º")
        except Exception as e:
            logger.error(f"ç»´æŠ¤å¾ªç¯æ„å¤–ç»“æŸ: {e}", exc_info=True)
        finally:
            self._maintenance_running = False
            logger.debug("ç»´æŠ¤å¾ªç¯å·²æ¸…ç†å®Œæ¯•")

    async def _async_save_graph_store(self, operation_name: str = "æœªçŸ¥æ“ä½œ") -> None:
        """
        å¼‚æ­¥ä¿å­˜å›¾å­˜å‚¨åˆ°ç£ç›˜

        æ­¤æ–¹æ³•è®¾è®¡ä¸ºåœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼ŒåŒ…å«é”™è¯¯å¤„ç†

        Args:
            operation_name: æ“ä½œåç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•
        """
        try:
            # ç¡®ä¿å›¾å­˜å‚¨å­˜åœ¨ä¸”å·²åˆå§‹åŒ–
            if self.graph_store is None:
                logger.warning(f"å›¾å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¼‚æ­¥ä¿å­˜: {operation_name}")
                return

            if self.persistence is None:
                logger.warning(f"æŒä¹…åŒ–ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¼‚æ­¥ä¿å­˜: {operation_name}")
                return

            await self.persistence.save_graph_store(self.graph_store)
            logger.debug(f"å¼‚æ­¥ä¿å­˜å›¾æ•°æ®æˆåŠŸ: {operation_name}")
        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜å›¾æ•°æ®å¤±è´¥ ({operation_name}): {e}", exc_info=True)
            # å¯ä»¥è€ƒè™‘æ·»åŠ é‡è¯•æœºåˆ¶æˆ–è€…é€šçŸ¥æœºåˆ¶
