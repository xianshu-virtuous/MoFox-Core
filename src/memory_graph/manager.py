"""
è®°å¿†ç®¡ç†å™¨ - Phase 3

ç»Ÿä¸€çš„è®°å¿†ç³»ç»Ÿç®¡ç†æ¥å£ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
- è®°å¿†åˆ›å»ºã€æ£€ç´¢ã€æ›´æ–°ã€åˆ é™¤
- è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ¿€æ´»ã€é—å¿˜ï¼‰
- è®°å¿†æ•´åˆä¸ç»´æŠ¤
- å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–
"""

import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from src.config.config import global_config
from src.config.official_configs import MemoryConfig
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import Memory, MemoryEdge, MemoryNode, MemoryType, NodeType, EdgeType
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.tools.memory_tools import MemoryTools
from src.memory_graph.utils.embeddings import EmbeddingGenerator
import uuid

logger = logging.getLogger(__name__)


class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨
    
    æ ¸å¿ƒç®¡ç†ç±»ï¼Œæä¾›è®°å¿†ç³»ç»Ÿçš„ç»Ÿä¸€æ¥å£ï¼š
    - è®°å¿† CRUD æ“ä½œ
    - è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†
    - æ™ºèƒ½æ£€ç´¢ä¸æ¨è
    - è®°å¿†ç»´æŠ¤ä¸ä¼˜åŒ–
    """

    def __init__(
        self,
        data_dir: Optional[Path] = None,
    ):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»global_configè¯»å–ï¼‰
        """
        # ç›´æ¥ä½¿ç”¨ global_config.memory
        if not global_config.memory or not getattr(global_config.memory, 'enable', False):
            raise ValueError("è®°å¿†ç³»ç»Ÿæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ [memory] enable = true")
        
        self.config: MemoryConfig = global_config.memory
        self.data_dir = data_dir or Path(getattr(self.config, 'data_dir', 'data/memory_graph'))
        
        # å­˜å‚¨ç»„ä»¶
        self.vector_store: Optional[VectorStore] = None
        self.graph_store: Optional[GraphStore] = None
        self.persistence: Optional[PersistenceManager] = None
        
        # æ ¸å¿ƒç»„ä»¶
        self.embedding_generator: Optional[EmbeddingGenerator] = None
        self.extractor: Optional[MemoryExtractor] = None
        self.builder: Optional[MemoryBuilder] = None
        self.tools: Optional[MemoryTools] = None
        
        # çŠ¶æ€
        self._initialized = False
        self._last_maintenance = datetime.now()
        self._maintenance_task: Optional[asyncio.Task] = None
        self._maintenance_interval_hours = getattr(self.config, 'consolidation_interval_hours', 1.0)
        self._maintenance_schedule_id: Optional[str] = None  # è°ƒåº¦ä»»åŠ¡ID
        
        logger.info(f"è®°å¿†ç®¡ç†å™¨å·²åˆ›å»º (data_dir={self.data_dir}, enable={getattr(self.config, 'enable', False)})")

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        
        æŒ‰ç…§ä¾èµ–é¡ºåºåˆå§‹åŒ–ï¼š
        1. å­˜å‚¨å±‚ï¼ˆå‘é‡å­˜å‚¨ã€å›¾å­˜å‚¨ã€æŒä¹…åŒ–ï¼‰
        2. å·¥å…·å±‚ï¼ˆåµŒå…¥ç”Ÿæˆå™¨ã€æå–å™¨ï¼‰
        3. ç®¡ç†å±‚ï¼ˆæ„å»ºå™¨ã€å·¥å…·æ¥å£ï¼‰
        """
        if self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–")
            return

        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨...")
            
            # 1. åˆå§‹åŒ–å­˜å‚¨å±‚
            self.data_dir.mkdir(parents=True, exist_ok=True)
            
            # è·å–å­˜å‚¨é…ç½®
            storage_config = getattr(self.config, 'storage', None)
            vector_collection_name = getattr(storage_config, 'vector_collection_name', 'memory_graph') if storage_config else 'memory_graph'
            
            self.vector_store = VectorStore(
                collection_name=vector_collection_name,
                data_dir=self.data_dir,
            )
            await self.vector_store.initialize()
            
            self.persistence = PersistenceManager(data_dir=self.data_dir)
            
            # å°è¯•åŠ è½½ç°æœ‰å›¾æ•°æ®
            self.graph_store = await self.persistence.load_graph_store()
            if not self.graph_store:
                logger.info("æœªæ‰¾åˆ°ç°æœ‰å›¾æ•°æ®ï¼Œåˆ›å»ºæ–°çš„å›¾å­˜å‚¨")
                self.graph_store = GraphStore()
            else:
                stats = self.graph_store.get_statistics()
                logger.info(
                    f"åŠ è½½å›¾æ•°æ®: {stats['total_memories']} æ¡è®°å¿†, "
                    f"{stats['total_nodes']} ä¸ªèŠ‚ç‚¹, {stats['total_edges']} æ¡è¾¹"
                )
            
            # 2. åˆå§‹åŒ–å·¥å…·å±‚
            self.embedding_generator = EmbeddingGenerator()
            # EmbeddingGenerator ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–
            
            self.extractor = MemoryExtractor()
            
            # 3. åˆå§‹åŒ–ç®¡ç†å±‚
            self.builder = MemoryBuilder(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                embedding_generator=self.embedding_generator,
            )
            
            # æ£€æŸ¥é…ç½®å€¼
            expand_depth = self.config.search_max_expand_depth
            logger.info(f"ğŸ“Š é…ç½®æ£€æŸ¥: search_max_expand_depth={expand_depth}")
            
            self.tools = MemoryTools(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                persistence_manager=self.persistence,
                embedding_generator=self.embedding_generator,
                max_expand_depth=expand_depth,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•æ·±åº¦
            )
            
            self._initialized = True
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # å¯åŠ¨åå°ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
            await self.start_maintenance_scheduler()
            
        except Exception as e:
            logger.error(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def shutdown(self) -> None:
        """
        å…³é—­è®°å¿†ç®¡ç†å™¨
        
        æ‰§è¡Œæ¸…ç†æ“ä½œï¼š
        - åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        - ä¿å­˜æ‰€æœ‰æ•°æ®
        - å…³é—­å­˜å‚¨ç»„ä»¶
        """
        if not self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— éœ€å…³é—­")
            return

        try:
            logger.info("æ­£åœ¨å…³é—­è®°å¿†ç®¡ç†å™¨...")
            
            # 1. åœæ­¢è°ƒåº¦ä»»åŠ¡
            await self.stop_maintenance_scheduler()
            
            # 2. æ‰§è¡Œæœ€åä¸€æ¬¡ç»´æŠ¤ï¼ˆä¿å­˜æ•°æ®ï¼‰
            if self.graph_store and self.persistence:
                logger.info("æ‰§è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜...")
                await self.persistence.save_graph_store(self.graph_store)
            
            # 3. å…³é—­å­˜å‚¨ç»„ä»¶
            if self.vector_store:
                # VectorStore ä½¿ç”¨ chromadbï¼Œæ— éœ€æ˜¾å¼å…³é—­
                pass
            
            self._initialized = False
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"å…³é—­è®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)

    # ==================== è®°å¿† CRUD æ“ä½œ ====================

    async def create_memory(
        self,
        subject: str,
        memory_type: str,
        topic: str,
        object: Optional[str] = None,
        attributes: Optional[Dict[str, str]] = None,
        importance: float = 0.5,
        **kwargs,
    ) -> Optional[Memory]:
        """
        åˆ›å»ºæ–°è®°å¿†
        
        Args:
            subject: ä¸»ä½“ï¼ˆè°ï¼‰
            memory_type: è®°å¿†ç±»å‹ï¼ˆäº‹ä»¶/è§‚ç‚¹/äº‹å®/å…³ç³»ï¼‰
            topic: ä¸»é¢˜ï¼ˆåšä»€ä¹ˆ/æƒ³ä»€ä¹ˆï¼‰
            object: å®¢ä½“ï¼ˆå¯¹è°/å¯¹ä»€ä¹ˆï¼‰
            attributes: å±æ€§å­—å…¸ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€åŸå› ç­‰ï¼‰
            importance: é‡è¦æ€§ (0.0-1.0)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            åˆ›å»ºçš„è®°å¿†å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.create_memory(
                subject=subject,
                memory_type=memory_type,
                topic=topic,
                object=object,
                attributes=attributes,
                importance=importance,
                **kwargs,
            )
            
            if result["success"]:
                memory_id = result["memory_id"]
                memory = self.graph_store.get_memory_by_id(memory_id)
                logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory_id}")
                return memory
            else:
                logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
                return None
                
        except Exception as e:
            logger.error(f"åˆ›å»ºè®°å¿†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return None

    async def get_memory(self, memory_id: str) -> Optional[Memory]:
        """
        æ ¹æ® ID è·å–è®°å¿†
        
        Args:
            memory_id: è®°å¿† ID
            
        Returns:
            è®°å¿†å¯¹è±¡ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        return self.graph_store.get_memory_by_id(memory_id)

    async def update_memory(
        self,
        memory_id: str,
        **updates,
    ) -> bool:
        """
        æ›´æ–°è®°å¿†
        
        Args:
            memory_id: è®°å¿† ID
            **updates: è¦æ›´æ–°çš„å­—æ®µ
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False
            
            # æ›´æ–°å…ƒæ•°æ®
            if "importance" in updates:
                memory.importance = updates["importance"]
            
            if "metadata" in updates:
                memory.metadata.update(updates["metadata"])
            
            memory.updated_at = datetime.now()
            
            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†æ›´æ–°æˆåŠŸ: {memory_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤è®°å¿†
        
        Args:
            memory_id: è®°å¿† ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False
            
            # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
            for node in memory.nodes:
                if node.embedding is not None:
                    await self.vector_store.delete_node(node.id)
            
            # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            self.graph_store.remove_memory(memory_id)
            
            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†åˆ é™¤æˆåŠŸ: {memory_id}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†æ£€ç´¢æ“ä½œ ====================

    async def generate_multi_queries(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢è¯­å¥ï¼ˆç”¨äºå¤šè·¯å¬å›ï¼‰
        
        ç®€åŒ–ç‰ˆå¤šæŸ¥è¯¢ç­–ç•¥ï¼šç›´æ¥è®©å°æ¨¡å‹ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢ï¼Œ
        é¿å…å¤æ‚çš„æŸ¥è¯¢åˆ†è§£å’Œç»„åˆé€»è¾‘ã€‚
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆèŠå¤©å†å²ã€å‘è¨€äººã€å‚ä¸è€…ç­‰ï¼‰
            
        Returns:
            List of (query_string, weight) - æŸ¥è¯¢è¯­å¥å’Œæƒé‡
        """
        try:
            from src.llm_models.utils_model import LLMRequest
            from src.config.config import model_config
            
            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.multi_query_generator"
            )
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            chat_history = context.get("chat_history", "") if context else ""
    
            prompt = f"""ä½ æ˜¯è®°å¿†æ£€ç´¢åŠ©æ‰‹ã€‚ä¸ºæé«˜æ£€ç´¢å‡†ç¡®ç‡ï¼Œè¯·ä¸ºæŸ¥è¯¢ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æœç´¢è¯­å¥ã€‚

**æ ¸å¿ƒåŸåˆ™ï¼ˆé‡è¦ï¼ï¼‰ï¼š**
å¯¹äºåŒ…å«å¤šä¸ªæ¦‚å¿µçš„å¤æ‚æŸ¥è¯¢ï¼ˆå¦‚"å°æ˜å¦‚ä½•è¯„ä»·å°ç‹"ï¼‰ï¼Œåº”è¯¥ç”Ÿæˆï¼š
1. å®Œæ•´æŸ¥è¯¢ï¼ˆåŒ…å«æ‰€æœ‰è¦ç´ ï¼‰- æƒé‡1.0
2. æ¯ä¸ªå…³é”®æ¦‚å¿µçš„ç‹¬ç«‹æŸ¥è¯¢ï¼ˆå¦‚"å°æ˜"ã€"å°ç‹"ï¼‰- æƒé‡0.8ï¼Œé¿å…è¢«ä¸»ä½“æ·¹æ²¡ï¼
3. ä¸»ä½“+åŠ¨ä½œç»„åˆï¼ˆå¦‚"å°æ˜ è¯„ä»·"ï¼‰- æƒé‡0.6
4. æ³›åŒ–æŸ¥è¯¢ï¼ˆå¦‚"è¯„ä»·"ï¼‰- æƒé‡0.7

**è¦æ±‚ï¼š**
- ç¬¬ä¸€ä¸ªå¿…é¡»æ˜¯åŸå§‹æŸ¥è¯¢æˆ–åŒä¹‰æ”¹å†™
- è¯†åˆ«æŸ¥è¯¢ä¸­çš„æ‰€æœ‰é‡è¦æ¦‚å¿µï¼Œä¸ºæ¯ä¸ªæ¦‚å¿µç”Ÿæˆç‹¬ç«‹æŸ¥è¯¢
- æŸ¥è¯¢ç®€æ´ï¼ˆ5-20å­—ï¼‰
- ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦æ·»åŠ è¯´æ˜

**å¯¹è¯ä¸Šä¸‹æ–‡ï¼š** {chat_history[-300:] if chat_history else "æ— "}

**è¾“å‡ºJSONæ ¼å¼ï¼š**
```json
{{
    "queries": [
        {{"text": "å®Œæ•´æŸ¥è¯¢", "weight": 1.0}},
        {{"text": "å…³é”®æ¦‚å¿µ1", "weight": 0.8}},
        {{"text": "å…³é”®æ¦‚å¿µ2", "weight": 0.8}},
        {{"text": "ç»„åˆæŸ¥è¯¢", "weight": 0.6}}
    ]
}}
```"""

            response, _ = await llm.generate_response_async(prompt, temperature=0.3, max_tokens=300)
            
            # è§£æJSON
            import json, re
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*$', '', response).strip()
            
            try:
                data = json.loads(response)
                queries = data.get("queries", [])
                
                result = []
                for item in queries:
                    text = item.get("text", "").strip()
                    weight = float(item.get("weight", 0.5))
                    if text:
                        result.append((text, weight))
                
                if result:
                    logger.info(f"ç”Ÿæˆ {len(result)} ä¸ªæŸ¥è¯¢: {[q for q, _ in result]}")
                    return result
                    
            except json.JSONDecodeError as e:
                logger.warning(f"è§£æå¤±è´¥: {e}, response={response[:100]}")
            
        except Exception as e:
            logger.warning(f"å¤šæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
        
        # å›é€€åˆ°åŸå§‹æŸ¥è¯¢
        return [(query, 1.0)]

    async def search_memories(
        self,
        query: str,
        top_k: int = 10,
        memory_types: Optional[List[str]] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        min_importance: float = 0.0,
        include_forgotten: bool = False,
        use_multi_query: bool = True,
        expand_depth: int | None = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> List[Memory]:
        """
        æœç´¢è®°å¿†
        
        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼Œè§£å†³å¤æ‚æŸ¥è¯¢é—®é¢˜ã€‚
        ä¾‹å¦‚ï¼š"æ°ç‘å–µå¦‚ä½•è¯„ä»·æ–°çš„è®°å¿†ç³»ç»Ÿ" ä¼šè¢«åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ï¼Œ
        ç¡®ä¿åŒæ—¶åŒ¹é…"æ°ç‘å–µ"å’Œ"æ–°çš„è®°å¿†ç³»ç»Ÿ"ä¸¤ä¸ªå…³é”®æ¦‚å¿µã€‚
        
        åŒæ—¶æ”¯æŒå›¾æ‰©å±•ï¼šä»åˆå§‹æ£€ç´¢ç»“æœå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æŸ¥æ‰¾è¯­ä¹‰ç›¸å…³çš„é‚»å±…è®°å¿†ã€‚
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°
            memory_types: è®°å¿†ç±»å‹è¿‡æ»¤
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ (start, end)
            min_importance: æœ€å°é‡è¦æ€§
            include_forgotten: æ˜¯å¦åŒ…å«å·²é—å¿˜çš„è®°å¿†
            use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆæ¨èï¼Œé»˜è®¤Trueï¼‰
            expand_depth: å›¾æ‰©å±•æ·±åº¦ï¼ˆ0=ç¦ç”¨, 1=æ¨è, 2-3=æ·±åº¦æ¢ç´¢ï¼‰
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¼˜åŒ–ï¼‰
            
        Returns:
            è®°å¿†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        try:
            # å‡†å¤‡æœç´¢å‚æ•°
            params = {
                "query": query,
                "top_k": top_k,
                "use_multi_query": use_multi_query,
                "expand_depth": expand_depth or global_config.memory.search_max_expand_depth,  # ä¼ é€’å›¾æ‰©å±•æ·±åº¦
                "context": context,
            }
            
            if memory_types:
                params["memory_types"] = memory_types
            
            # æ‰§è¡Œæœç´¢
            result = await self.tools.search_memories(**params)
            
            if not result["success"]:
                logger.error(f"æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}")
                return []
            
            memories = result.get("results", [])
            
            # åå¤„ç†è¿‡æ»¤
            filtered_memories = []
            for mem_dict in memories:
                # ä»å­—å…¸é‡å»º Memory å¯¹è±¡
                memory_id = mem_dict.get("memory_id", "")
                if not memory_id:
                    continue
                    
                memory = self.graph_store.get_memory_by_id(memory_id)
                if not memory:
                    continue
                
                # é‡è¦æ€§è¿‡æ»¤
                if min_importance is not None and memory.importance < min_importance:
                    continue
                
                # é—å¿˜çŠ¶æ€è¿‡æ»¤
                if not include_forgotten and memory.metadata.get("forgotten", False):
                    continue
                
                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                if time_range:
                    mem_time = memory.created_at
                    if not (time_range[0] <= mem_time <= time_range[1]):
                        continue
                
                filtered_memories.append(memory)
            
            strategy = result.get("strategy", "unknown")
            logger.info(
                f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(filtered_memories)} æ¡è®°å¿† (ç­–ç•¥={strategy})"
            )
            return filtered_memories[:top_k]
            
        except Exception as e:
            logger.error(f"æœç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    async def link_memories(
        self,
        source_description: str,
        target_description: str,
        relation_type: str,
        importance: float = 0.5,
    ) -> bool:
        """
        å…³è”ä¸¤æ¡è®°å¿†
        
        Args:
            source_description: æºè®°å¿†æè¿°
            target_description: ç›®æ ‡è®°å¿†æè¿°
            relation_type: å…³ç³»ç±»å‹ï¼ˆå¯¼è‡´/å¼•ç”¨/ç›¸ä¼¼/ç›¸åï¼‰
            importance: å…³ç³»é‡è¦æ€§
            
        Returns:
            æ˜¯å¦å…³è”æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.link_memories(
                source_memory_description=source_description,
                target_memory_description=target_description,
                relation_type=relation_type,
                importance=importance,
            )
            
            if result["success"]:
                logger.info(
                    f"è®°å¿†å…³è”æˆåŠŸ: {result['source_memory_id']} -> "
                    f"{result['target_memory_id']} ({relation_type})"
                )
                return True
            else:
                logger.error(f"è®°å¿†å…³è”å¤±è´¥: {result.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            logger.error(f"å…³è”è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    # ==================== è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

    async def activate_memory(self, memory_id: str, strength: float = 1.0) -> bool:
        """
        æ¿€æ´»è®°å¿†
        
        æ›´æ–°è®°å¿†çš„æ¿€æ´»åº¦ï¼Œå¹¶ä¼ æ’­åˆ°ç›¸å…³è®°å¿†
        
        Args:
            memory_id: è®°å¿† ID
            strength: æ¿€æ´»å¼ºåº¦ (0.0-1.0)
            
        Returns:
            æ˜¯å¦æ¿€æ´»æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False
            
            # æ›´æ–°æ¿€æ´»ä¿¡æ¯
            now = datetime.now()
            activation_info = memory.metadata.get("activation", {})
            
            # æ›´æ–°æ¿€æ´»åº¦ï¼ˆè€ƒè™‘æ—¶é—´è¡°å‡ï¼‰
            last_access = activation_info.get("last_access")
            if last_access:
                # è®¡ç®—æ—¶é—´è¡°å‡
                last_access_dt = datetime.fromisoformat(last_access)
                hours_passed = (now - last_access_dt).total_seconds() / 3600
                decay_rate = getattr(self.config, 'activation_decay_rate', 0.95)
                decay_factor = decay_rate ** (hours_passed / 24)
                current_activation = activation_info.get("level", 0.0) * decay_factor
            else:
                current_activation = 0.0
            
            # æ–°çš„æ¿€æ´»åº¦ = å½“å‰æ¿€æ´»åº¦ + æ¿€æ´»å¼ºåº¦
            new_activation = min(1.0, current_activation + strength)
            
            activation_info.update({
                "level": new_activation,
                "last_access": now.isoformat(),
                "access_count": activation_info.get("access_count", 0) + 1,
            })
            
            memory.metadata["activation"] = activation_info
            memory.last_accessed = now
            
            # æ¿€æ´»ä¼ æ’­ï¼šæ¿€æ´»ç›¸å…³è®°å¿†
            if strength > 0.1:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                propagation_depth = getattr(self.config, 'activation_propagation_depth', 2)
                related_memories = self._get_related_memories(
                    memory_id,
                    max_depth=propagation_depth
                )
                propagation_strength_factor = getattr(self.config, 'activation_propagation_strength', 0.5)
                propagation_strength = strength * propagation_strength_factor
                
                max_related = getattr(self.config, 'max_related_memories', 5)
                for related_id in related_memories[:max_related]:
                    await self.activate_memory(related_id, propagation_strength)
            
            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.debug(f"è®°å¿†å·²æ¿€æ´»: {memory_id} (level={new_activation:.3f})")
            return True
            
        except Exception as e:
            logger.error(f"æ¿€æ´»è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    def _get_related_memories(self, memory_id: str, max_depth: int = 1) -> List[str]:
        """
        è·å–ç›¸å…³è®°å¿† ID åˆ—è¡¨ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºæ¿€æ´»ä¼ æ’­ï¼‰
        
        Args:
            memory_id: è®°å¿† ID
            max_depth: æœ€å¤§éå†æ·±åº¦
            
        Returns:
            ç›¸å…³è®°å¿† ID åˆ—è¡¨
        """
        memory = self.graph_store.get_memory_by_id(memory_id)
        if not memory:
            return []
        
        related_ids = set()
        
        # éå†è®°å¿†çš„èŠ‚ç‚¹
        for node in memory.nodes:
            # è·å–èŠ‚ç‚¹çš„é‚»å±…
            neighbors = list(self.graph_store.graph.neighbors(node.id))
            
            for neighbor_id in neighbors:
                # è·å–é‚»å±…èŠ‚ç‚¹æ‰€å±çš„è®°å¿†
                neighbor_node = self.graph_store.graph.nodes.get(neighbor_id)
                if neighbor_node:
                    neighbor_memory_ids = neighbor_node.get("memory_ids", [])
                    for mem_id in neighbor_memory_ids:
                        if mem_id != memory_id:
                            related_ids.add(mem_id)
        
        return list(related_ids)

    async def expand_memories_with_semantic_filter(
        self,
        initial_memory_ids: List[str],
        query_embedding: "np.ndarray",
        max_depth: int = 2,
        semantic_threshold: float = 0.5,
        max_expanded: int = 20
    ) -> List[Tuple[str, float]]:
        """
        ä»åˆå§‹è®°å¿†é›†åˆå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æ‰©å±•ï¼Œå¹¶ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤
        
        è¿™ä¸ªæ–¹æ³•è§£å†³äº†çº¯å‘é‡æœç´¢å¯èƒ½é—æ¼çš„"è¯­ä¹‰ç›¸å…³ä¸”å›¾ç»“æ„ç›¸å…³"çš„è®°å¿†ã€‚
        
        Args:
            initial_memory_ids: åˆå§‹è®°å¿†IDé›†åˆï¼ˆç”±å‘é‡æœç´¢å¾—åˆ°ï¼‰
            query_embedding: æŸ¥è¯¢å‘é‡
            max_depth: æœ€å¤§æ‰©å±•æ·±åº¦ï¼ˆ1-3æ¨èï¼‰
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5æ¨èï¼‰
            max_expanded: æœ€å¤šæ‰©å±•å¤šå°‘ä¸ªè®°å¿†
            
        Returns:
            List[(memory_id, relevance_score)] æŒ‰ç›¸å…³åº¦æ’åº
        """
        if not initial_memory_ids or query_embedding is None:
            return []
        
        try:
            import numpy as np
            
            # è®°å½•å·²è®¿é—®çš„è®°å¿†ï¼Œé¿å…é‡å¤
            visited_memories = set(initial_memory_ids)
            # è®°å½•æ‰©å±•çš„è®°å¿†åŠå…¶åˆ†æ•°
            expanded_memories: Dict[str, float] = {}
            
            # BFSæ‰©å±•
            current_level = initial_memory_ids
            
            for depth in range(max_depth):
                next_level = []
                
                for memory_id in current_level:
                    memory = self.graph_store.get_memory_by_id(memory_id)
                    if not memory:
                        continue
                    
                    # éå†è¯¥è®°å¿†çš„æ‰€æœ‰èŠ‚ç‚¹
                    for node in memory.nodes:
                        if not node.has_embedding():
                            continue
                        
                        # è·å–é‚»å±…èŠ‚ç‚¹
                        try:
                            neighbors = list(self.graph_store.graph.neighbors(node.id))
                        except:
                            continue
                        
                        for neighbor_id in neighbors:
                            # è·å–é‚»å±…èŠ‚ç‚¹ä¿¡æ¯
                            neighbor_node_data = self.graph_store.graph.nodes.get(neighbor_id)
                            if not neighbor_node_data:
                                continue
                            
                            # è·å–é‚»å±…èŠ‚ç‚¹çš„å‘é‡ï¼ˆä»å‘é‡å­˜å‚¨ï¼‰
                            neighbor_vector_data = await self.vector_store.get_node_by_id(neighbor_id)
                            if not neighbor_vector_data or neighbor_vector_data.get("embedding") is None:
                                continue
                            
                            neighbor_embedding = neighbor_vector_data["embedding"]
                            
                            # è®¡ç®—ä¸æŸ¥è¯¢çš„è¯­ä¹‰ç›¸ä¼¼åº¦
                            semantic_sim = self._cosine_similarity(
                                query_embedding,
                                neighbor_embedding
                            )
                            
                            # è·å–è¾¹çš„æƒé‡
                            try:
                                edge_data = self.graph_store.graph.get_edge_data(node.id, neighbor_id)
                                edge_importance = edge_data.get("importance", 0.5) if edge_data else 0.5
                            except:
                                edge_importance = 0.5
                            
                            # ç»¼åˆè¯„åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦(70%) + å›¾ç»“æ„æƒé‡(20%) + æ·±åº¦è¡°å‡(10%)
                            depth_decay = 1.0 / (depth + 1)  # æ·±åº¦è¶Šæ·±ï¼Œæƒé‡è¶Šä½
                            relevance_score = (
                                semantic_sim * 0.7 + 
                                edge_importance * 0.2 + 
                                depth_decay * 0.1
                            )
                            
                            # åªä¿ç•™è¶…è¿‡é˜ˆå€¼çš„èŠ‚ç‚¹
                            if relevance_score < semantic_threshold:
                                continue
                            
                            # æå–é‚»å±…èŠ‚ç‚¹æ‰€å±çš„è®°å¿†
                            neighbor_memory_ids = neighbor_node_data.get("memory_ids", [])
                            if isinstance(neighbor_memory_ids, str):
                                import json
                                try:
                                    neighbor_memory_ids = json.loads(neighbor_memory_ids)
                                except:
                                    neighbor_memory_ids = [neighbor_memory_ids]
                            
                            for neighbor_mem_id in neighbor_memory_ids:
                                if neighbor_mem_id in visited_memories:
                                    continue
                                
                                # è®°å½•è¿™ä¸ªæ‰©å±•è®°å¿†
                                if neighbor_mem_id not in expanded_memories:
                                    expanded_memories[neighbor_mem_id] = relevance_score
                                    visited_memories.add(neighbor_mem_id)
                                    next_level.append(neighbor_mem_id)
                                else:
                                    # å¦‚æœå·²å­˜åœ¨ï¼Œå–æœ€é«˜åˆ†
                                    expanded_memories[neighbor_mem_id] = max(
                                        expanded_memories[neighbor_mem_id],
                                        relevance_score
                                    )
                
                # å¦‚æœæ²¡æœ‰æ–°èŠ‚ç‚¹æˆ–å·²è¾¾åˆ°æ•°é‡é™åˆ¶ï¼Œæå‰ç»ˆæ­¢
                if not next_level or len(expanded_memories) >= max_expanded:
                    break
                
                current_level = next_level[:max_expanded]  # é™åˆ¶æ¯å±‚çš„æ‰©å±•æ•°é‡
            
            # æ’åºå¹¶è¿”å›
            sorted_results = sorted(
                expanded_memories.items(),
                key=lambda x: x[1],
                reverse=True
            )[:max_expanded]
            
            logger.info(
                f"å›¾æ‰©å±•å®Œæˆ: åˆå§‹{len(initial_memory_ids)}ä¸ª â†’ "
                f"æ‰©å±•{len(sorted_results)}ä¸ªæ–°è®°å¿† "
                f"(æ·±åº¦={max_depth}, é˜ˆå€¼={semantic_threshold:.2f})"
            )
            
            return sorted_results
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰å›¾æ‰©å±•å¤±è´¥: {e}", exc_info=True)
            return []
    
    def _cosine_similarity(self, vec1: "np.ndarray", vec2: "np.ndarray") -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import numpy as np
            
            # ç¡®ä¿æ˜¯numpyæ•°ç»„
            if not isinstance(vec1, np.ndarray):
                vec1 = np.array(vec1)
            if not isinstance(vec2, np.ndarray):
                vec2 = np.array(vec2)
            
            # å½’ä¸€åŒ–
            vec1_norm = np.linalg.norm(vec1)
            vec2_norm = np.linalg.norm(vec2)
            
            if vec1_norm == 0 or vec2_norm == 0:
                return 0.0
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(vec1, vec2) / (vec1_norm * vec2_norm)
            return float(similarity)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    async def forget_memory(self, memory_id: str) -> bool:
        """
        é—å¿˜è®°å¿†ï¼ˆæ ‡è®°ä¸ºå·²é—å¿˜ï¼Œä¸åˆ é™¤ï¼‰
        
        Args:
            memory_id: è®°å¿† ID
            
        Returns:
            æ˜¯å¦é—å¿˜æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False
            
            memory.metadata["forgotten"] = True
            memory.metadata["forgotten_at"] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°
            await self.persistence.save_graph_store(self.graph_store)
            logger.info(f"è®°å¿†å·²é—å¿˜: {memory_id}")
            return True
            
        except Exception as e:
            logger.error(f"é—å¿˜è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return False

    async def auto_forget_memories(self, threshold: float = 0.1) -> int:
        """
        è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦çš„è®°å¿†
        
        Args:
            threshold: æ¿€æ´»åº¦é˜ˆå€¼
            
        Returns:
            é—å¿˜çš„è®°å¿†æ•°é‡
        """
        if not self._initialized:
            await self.initialize()

        try:
            forgotten_count = 0
            all_memories = self.graph_store.get_all_memories()
            
            for memory in all_memories:
                # è·³è¿‡å·²é—å¿˜çš„è®°å¿†
                if memory.metadata.get("forgotten", False):
                    continue
                
                # è·³è¿‡é«˜é‡è¦æ€§è®°å¿†
                min_importance = getattr(self.config, 'forgetting_min_importance', 7.0)
                if memory.importance >= min_importance:
                    continue
                
                # è®¡ç®—å½“å‰æ¿€æ´»åº¦
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")
                
                if last_access:
                    last_access_dt = datetime.fromisoformat(last_access)
                    days_passed = (datetime.now() - last_access_dt).days
                    
                    # é•¿æ—¶é—´æœªè®¿é—®çš„è®°å¿†ï¼Œåº”ç”¨æ—¶é—´è¡°å‡
                    decay_factor = 0.9 ** days_passed
                    current_activation = activation_info.get("level", 0.0) * decay_factor
                    
                    # ä½äºé˜ˆå€¼åˆ™é—å¿˜
                    if current_activation < threshold:
                        await self.forget_memory(memory.id)
                        forgotten_count += 1
            
            logger.info(f"è‡ªåŠ¨é—å¿˜å®Œæˆ: é—å¿˜äº† {forgotten_count} æ¡è®°å¿†")
            return forgotten_count
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨é—å¿˜å¤±è´¥: {e}", exc_info=True)
            return 0

    # ==================== ç»Ÿè®¡ä¸ç»´æŠ¤ ====================

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self._initialized or not self.graph_store:
            return {}

        stats = self.graph_store.get_statistics()
        
        # æ·»åŠ æ¿€æ´»åº¦ç»Ÿè®¡
        all_memories = self.graph_store.get_all_memories()
        activation_levels = []
        forgotten_count = 0
        
        for memory in all_memories:
            if memory.metadata.get("forgotten", False):
                forgotten_count += 1
            else:
                activation_info = memory.metadata.get("activation", {})
                activation_levels.append(activation_info.get("level", 0.0))
        
        if activation_levels:
            stats["avg_activation"] = sum(activation_levels) / len(activation_levels)
            stats["max_activation"] = max(activation_levels)
        else:
            stats["avg_activation"] = 0.0
            stats["max_activation"] = 0.0
        
        stats["forgotten_memories"] = forgotten_count
        stats["active_memories"] = stats["total_memories"] - forgotten_count
        
        return stats

    async def consolidate_memories(
        self,
        similarity_threshold: float = 0.85,
        time_window_hours: float = 24.0,
        max_batch_size: int = 50,
    ) -> Dict[str, Any]:
        """
        æ•´ç†è®°å¿†ï¼šç›´æ¥åˆå¹¶å»é‡ç›¸ä¼¼è®°å¿†ï¼ˆä¸åˆ›å»ºæ–°è¾¹ï¼‰
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. æ·»åŠ æ‰¹é‡é™åˆ¶ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
        2. ç›¸ä¼¼è®°å¿†ç›´æ¥è¦†ç›–åˆå¹¶ï¼Œä¸åˆ›å»ºå…³è”è¾¹
        3. ä½¿ç”¨ asyncio.sleep è®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼Œå»ºè®®æé«˜åˆ°0.9å‡å°‘è¯¯åˆ¤ï¼‰
            time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_batch_size: å•æ¬¡æœ€å¤šå¤„ç†çš„è®°å¿†æ•°é‡
            
        Returns:
            æ•´ç†ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info(f"å¼€å§‹è®°å¿†æ•´ç† (similarity_threshold={similarity_threshold}, time_window={time_window_hours}h, max_batch={max_batch_size})...")
            
            result = {
                "merged_count": 0,
                "checked_count": 0,
                "skipped_count": 0,
            }
            
            # è·å–æœ€è¿‘åˆ›å»ºçš„è®°å¿†
            cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()
            
            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= cutoff_time and not mem.metadata.get("forgotten", False)
            ]
            
            if not recent_memories:
                logger.info("æ²¡æœ‰éœ€è¦æ•´ç†çš„è®°å¿†")
                return result
            
            # é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡
            if len(recent_memories) > max_batch_size:
                logger.info(f"è®°å¿†æ•°é‡ {len(recent_memories)} è¶…è¿‡æ‰¹é‡é™åˆ¶ {max_batch_size}ï¼Œä»…å¤„ç†æœ€æ–°çš„ {max_batch_size} æ¡")
                recent_memories = sorted(recent_memories, key=lambda m: m.created_at, reverse=True)[:max_batch_size]
                result["skipped_count"] = len(all_memories) - max_batch_size
            
            logger.info(f"æ‰¾åˆ° {len(recent_memories)} æ¡å¾…æ•´ç†è®°å¿†")
            result["checked_count"] = len(recent_memories)
            
            # æŒ‰è®°å¿†ç±»å‹åˆ†ç»„
            memories_by_type: Dict[str, List[Memory]] = {}
            for mem in recent_memories:
                mem_type = mem.metadata.get("memory_type", "")
                if mem_type not in memories_by_type:
                    memories_by_type[mem_type] = []
                memories_by_type[mem_type].append(mem)
            
            # è®°å½•å·²åˆ é™¤çš„è®°å¿†IDï¼Œé¿å…é‡å¤å¤„ç†
            deleted_ids = set()
            
            # å¯¹æ¯ä¸ªç±»å‹çš„è®°å¿†è¿›è¡Œç›¸ä¼¼åº¦æ£€æµ‹
            for mem_type, memories in memories_by_type.items():
                if len(memories) < 2:
                    continue
                
                logger.debug(f"æ£€æŸ¥ç±»å‹ '{mem_type}' çš„ {len(memories)} æ¡è®°å¿†")
                
                # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€æµ‹
                for i in range(len(memories)):
                    # è®©å‡ºæ§åˆ¶æƒï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                    if i % 10 == 0:
                        await asyncio.sleep(0)
                    
                    if memories[i].id in deleted_ids:
                        continue
                    
                    for j in range(i + 1, len(memories)):
                        if memories[j].id in deleted_ids:
                            continue
                        
                        mem_i = memories[i]
                        mem_j = memories[j]
                        
                        # è·å–ä¸»é¢˜èŠ‚ç‚¹çš„å‘é‡
                        topic_i = next((n for n in mem_i.nodes if n.node_type == NodeType.TOPIC), None)
                        topic_j = next((n for n in mem_j.nodes if n.node_type == NodeType.TOPIC), None)
                        
                        if not topic_i or not topic_j:
                            continue
                        
                        if topic_i.embedding is None or topic_j.embedding is None:
                            continue
                        
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        import numpy as np
                        similarity = np.dot(topic_i.embedding, topic_j.embedding) / (
                            np.linalg.norm(topic_i.embedding) * np.linalg.norm(topic_j.embedding)
                        )
                        
                        if similarity >= similarity_threshold:
                            # ç›´æ¥å»é‡ï¼šä¿ç•™é‡è¦æ€§é«˜çš„ï¼Œåˆ é™¤å¦ä¸€ä¸ªï¼ˆä¸åˆ›å»ºå…³è”è¾¹ï¼‰
                            if mem_i.importance >= mem_j.importance:
                                keep_mem, remove_mem = mem_i, mem_j
                            else:
                                keep_mem, remove_mem = mem_j, mem_i
                            
                            logger.info(
                                f"å»é‡ç›¸ä¼¼è®°å¿† (similarity={similarity:.3f}): "
                                f"ä¿ç•™ {keep_mem.id}, åˆ é™¤ {remove_mem.id}"
                            )
                            
                            # å¢å¼ºä¿ç•™è®°å¿†çš„é‡è¦æ€§ï¼ˆåˆå¹¶ä¿¡æ¯ä»·å€¼ï¼‰
                            keep_mem.importance = min(1.0, keep_mem.importance + 0.05)
                            keep_mem.activation = min(1.0, keep_mem.activation + 0.05)
                            
                            # å°†è¢«åˆ é™¤è®°å¿†çš„è®¿é—®æ¬¡æ•°ç´¯åŠ åˆ°ä¿ç•™è®°å¿†
                            if hasattr(keep_mem, 'access_count') and hasattr(remove_mem, 'access_count'):
                                keep_mem.access_count += remove_mem.access_count
                            
                            # ç›´æ¥åˆ é™¤ç›¸ä¼¼è®°å¿†ï¼ˆä¸åˆ›å»ºè¾¹ï¼Œç®€åŒ–å›¾ç»“æ„ï¼‰
                            await self.delete_memory(remove_mem.id)
                            deleted_ids.add(remove_mem.id)
                            result["merged_count"] += 1
            
            logger.info(f"è®°å¿†æ•´ç†å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"è®°å¿†æ•´ç†å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "merged_count": 0, "checked_count": 0}

    async def auto_link_memories(
        self,
        time_window_hours: float = None,
        max_candidates: int = None,
        min_confidence: float = None,
    ) -> Dict[str, Any]:
        """
        è‡ªåŠ¨å…³è”è®°å¿†
        
        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»ï¼Œè‡ªåŠ¨å»ºç«‹å…³è”è¾¹ã€‚
        
        Args:
            time_window_hours: åˆ†ææ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_candidates: æ¯ä¸ªè®°å¿†æœ€å¤šå…³è”çš„å€™é€‰æ•°
            min_confidence: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            å…³è”ç»“æœç»Ÿè®¡
        """
        if not self._initialized:
            await self.initialize()

        # ä½¿ç”¨é…ç½®å€¼æˆ–å‚æ•°è¦†ç›–
        time_window_hours = time_window_hours if time_window_hours is not None else 24
        max_candidates = max_candidates if max_candidates is not None else getattr(self.config, 'auto_link_max_candidates', 10)
        min_confidence = min_confidence if min_confidence is not None else getattr(self.config, 'auto_link_min_confidence', 0.7)

        try:
            logger.info(f"å¼€å§‹è‡ªåŠ¨å…³è”è®°å¿† (æ—¶é—´çª—å£={time_window_hours}h)...")
            
            result = {
                "checked_count": 0,
                "linked_count": 0,
                "relation_stats": {},  # å…³ç³»ç±»å‹ç»Ÿè®¡ {ç±»å‹: æ•°é‡}
                "relations": {},  # è¯¦ç»†å…³ç³» {source_id: [å…³ç³»åˆ—è¡¨]}
            }
            
            # 1. è·å–æ—¶é—´çª—å£å†…çš„è®°å¿†
            time_threshold = datetime.now() - timedelta(hours=time_window_hours)
            all_memories = self.graph_store.get_all_memories()
            
            recent_memories = [
                mem for mem in all_memories
                if mem.created_at >= time_threshold
                and not mem.metadata.get("forgotten", False)
            ]
            
            if len(recent_memories) < 2:
                logger.info("è®°å¿†æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è‡ªåŠ¨å…³è”")
                return result
            
            logger.info(f"æ‰¾åˆ° {len(recent_memories)} æ¡å¾…å…³è”è®°å¿†")
            
            # 2. ä¸ºæ¯ä¸ªè®°å¿†å¯»æ‰¾å…³è”å€™é€‰
            for memory in recent_memories:
                result["checked_count"] += 1
                
                # è·³è¿‡å·²ç»æœ‰å¾ˆå¤šè¿æ¥çš„è®°å¿†
                existing_edges = len([
                    e for e in memory.edges
                    if e.edge_type == EdgeType.RELATION
                ])
                if existing_edges >= 10:
                    continue
                
                # 3. ä½¿ç”¨å‘é‡æœç´¢æ‰¾å€™é€‰è®°å¿†
                candidates = await self._find_link_candidates(
                    memory,
                    exclude_ids={memory.id},
                    max_results=max_candidates
                )
                
                if not candidates:
                    continue
                
                # 4. ä½¿ç”¨LLMåˆ†æå…³ç³»
                relations = await self._analyze_memory_relations(
                    source_memory=memory,
                    candidate_memories=candidates,
                    min_confidence=min_confidence
                )
                
                # 5. å»ºç«‹å…³è”
                for relation in relations:
                    try:
                        # åˆ›å»ºå…³è”è¾¹
                        edge = MemoryEdge(
                            id=f"edge_{uuid.uuid4().hex[:12]}",
                            source_id=memory.subject_id,
                            target_id=relation["target_memory"].subject_id,
                            relation=relation["relation_type"],
                            edge_type=EdgeType.RELATION,
                            importance=relation["confidence"],
                            metadata={
                                "auto_linked": True,
                                "confidence": relation["confidence"],
                                "reasoning": relation["reasoning"],
                                "created_at": datetime.now().isoformat(),
                            }
                        )
                        
                        # æ·»åŠ åˆ°å›¾
                        self.graph_store.graph.add_edge(
                            edge.source_id,
                            edge.target_id,
                            edge_id=edge.id,
                            relation=edge.relation,
                            edge_type=edge.edge_type.value,
                            importance=edge.importance,
                            metadata=edge.metadata,
                        )
                        
                        # åŒæ—¶æ·»åŠ åˆ°è®°å¿†çš„è¾¹åˆ—è¡¨
                        memory.edges.append(edge)
                        
                        result["linked_count"] += 1
                        
                        # æ›´æ–°ç»Ÿè®¡
                        result["relation_stats"][relation["relation_type"]] = \
                            result["relation_stats"].get(relation["relation_type"], 0) + 1
                        
                        # è®°å½•è¯¦ç»†å…³ç³»
                        if memory.id not in result["relations"]:
                            result["relations"][memory.id] = []
                        result["relations"][memory.id].append({
                            "target_id": relation["target_memory"].id,
                            "relation_type": relation["relation_type"],
                            "confidence": relation["confidence"],
                            "reasoning": relation["reasoning"],
                        })
                        
                        logger.info(
                            f"å»ºç«‹å…³è”: {memory.id[:8]} --[{relation['relation_type']}]--> "
                            f"{relation['target_memory'].id[:8]} "
                            f"(ç½®ä¿¡åº¦={relation['confidence']:.2f})"
                        )
                        
                    except Exception as e:
                        logger.warning(f"å»ºç«‹å…³è”å¤±è´¥: {e}")
                        continue
            
            # ä¿å­˜æ›´æ–°åçš„å›¾æ•°æ®
            if result["linked_count"] > 0:
                await self.persistence.save_graph_store(self.graph_store)
                logger.info(f"å·²ä¿å­˜ {result['linked_count']} æ¡è‡ªåŠ¨å…³è”è¾¹")
            
            logger.info(f"è‡ªåŠ¨å…³è”å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨å…³è”å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e), "checked_count": 0, "linked_count": 0}

    async def _find_link_candidates(
        self,
        memory: Memory,
        exclude_ids: Set[str],
        max_results: int = 5,
    ) -> List[Memory]:
        """
        ä¸ºè®°å¿†å¯»æ‰¾å…³è”å€™é€‰
        
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ + æ—¶é—´æ¥è¿‘åº¦æ‰¾åˆ°æ½œåœ¨ç›¸å…³è®°å¿†
        """
        try:
            # è·å–è®°å¿†çš„ä¸»é¢˜
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )
            
            if not topic_node or not topic_node.content:
                return []
            
            # ä½¿ç”¨ä¸»é¢˜å†…å®¹æœç´¢ç›¸ä¼¼è®°å¿†
            candidates = await self.search_memories(
                query=topic_node.content,
                top_k=max_results * 2,
                include_forgotten=False,
            )
            
            # è¿‡æ»¤ï¼šæ’é™¤è‡ªå·±å’Œå·²å…³è”çš„
            existing_targets = {
                e.target_id for e in memory.edges
                if e.edge_type == EdgeType.RELATION
            }
            
            filtered = [
                c for c in candidates
                if c.id not in exclude_ids
                and c.id not in existing_targets
            ]
            
            return filtered[:max_results]
            
        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾å€™é€‰å¤±è´¥: {e}")
            return []

    async def _analyze_memory_relations(
        self,
        source_memory: Memory,
        candidate_memories: List[Memory],
        min_confidence: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»
        
        Args:
            source_memory: æºè®°å¿†
            candidate_memories: å€™é€‰è®°å¿†åˆ—è¡¨
            min_confidence: æœ€ä½ç½®ä¿¡åº¦
            
        Returns:
            å…³ç³»åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
            - target_memory: ç›®æ ‡è®°å¿†
            - relation_type: å…³ç³»ç±»å‹
            - confidence: ç½®ä¿¡åº¦
            - reasoning: æ¨ç†è¿‡ç¨‹
        """
        try:
            from src.llm_models.utils_model import LLMRequest
            from src.config.config import model_config
            
            # æ„å»ºLLMè¯·æ±‚
            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.relation_analysis"
            )
            
            # æ ¼å¼åŒ–è®°å¿†ä¿¡æ¯
            source_desc = self._format_memory_for_llm(source_memory)
            candidates_desc = "\n\n".join([
                f"è®°å¿†{i+1}:\n{self._format_memory_for_llm(mem)}"
                for i, mem in enumerate(candidate_memories)
            ])
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†å…³ç³»åˆ†æä¸“å®¶ã€‚è¯·åˆ†ææºè®°å¿†ä¸å€™é€‰è®°å¿†ä¹‹é—´æ˜¯å¦å­˜åœ¨æœ‰æ„ä¹‰çš„å…³ç³»ã€‚

**å…³ç³»ç±»å‹è¯´æ˜ï¼š**
- å¯¼è‡´: Açš„å‘ç”Ÿå¯¼è‡´äº†Bçš„å‘ç”Ÿï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨: Aæåˆ°æˆ–æ¶‰åŠBï¼ˆå¼•ç”¨å…³ç³»ï¼‰
- ç›¸ä¼¼: Aå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆç›¸ä¼¼å…³ç³»ï¼‰
- ç›¸å: Aå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹ç«‹å…³ç³»ï¼‰
- å…³è”: Aå’ŒBå­˜åœ¨æŸç§å…³è”ä½†ä¸å±äºä»¥ä¸Šç±»å‹ï¼ˆä¸€èˆ¬å…³è”ï¼‰

**æºè®°å¿†ï¼š**
{source_desc}

**å€™é€‰è®°å¿†ï¼š**
{candidates_desc}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. å¯¹æ¯ä¸ªå€™é€‰è®°å¿†ï¼Œåˆ¤æ–­æ˜¯å¦ä¸æºè®°å¿†å­˜åœ¨å…³ç³»
2. å¦‚æœå­˜åœ¨å…³ç³»ï¼ŒæŒ‡å®šå…³ç³»ç±»å‹å’Œç½®ä¿¡åº¦(0.0-1.0)
3. ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±
4. åªè¿”å›ç½®ä¿¡åº¦ >= {min_confidence} çš„å…³ç³»

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
[
  {{
    "candidate_id": 1,
    "has_relation": true,
    "relation_type": "å¯¼è‡´",
    "confidence": 0.85,
    "reasoning": "è®°å¿†1æ˜¯è®°å¿†æºçš„ç»“æœ"
  }},
  {{
    "candidate_id": 2,
    "has_relation": false,
    "reasoning": "ä¸¤è€…æ— æ˜æ˜¾å…³è”"
  }}
]
```

è¯·åˆ†æå¹¶è¾“å‡ºJSONç»“æœï¼š"""

            # è°ƒç”¨LLM
            response, _ = await llm.generate_response_async(
                prompt,
                temperature=0.3,
                max_tokens=1000,
            )
            
            # è§£æå“åº”
            import json
            import re
            
            # æå–JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response.strip()
            
            try:
                analysis_results = json.loads(json_str)
            except json.JSONDecodeError:
                logger.warning(f"LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ä¿®å¤: {response[:200]}")
                # å°è¯•ç®€å•ä¿®å¤
                json_str = re.sub(r'[\r\n\t]', '', json_str)
                analysis_results = json.loads(json_str)
            
            # è½¬æ¢ä¸ºç»“æœæ ¼å¼
            relations = []
            for result in analysis_results:
                if not result.get("has_relation", False):
                    continue
                
                confidence = result.get("confidence", 0.0)
                if confidence < min_confidence:
                    continue
                
                candidate_id = result.get("candidate_id", 0) - 1
                if 0 <= candidate_id < len(candidate_memories):
                    relations.append({
                        "target_memory": candidate_memories[candidate_id],
                        "relation_type": result.get("relation_type", "å…³è”"),
                        "confidence": confidence,
                        "reasoning": result.get("reasoning", ""),
                    })
            
            logger.debug(f"LLMåˆ†æå®Œæˆ: å‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations
            
        except Exception as e:
            logger.error(f"LLMå…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)
            return []

    def _format_memory_for_llm(self, memory: Memory) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸ºLLMå¯è¯»çš„æ–‡æœ¬"""
        try:
            # è·å–å…³é”®èŠ‚ç‚¹
            subject_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.SUBJECT),
                None
            )
            topic_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.TOPIC),
                None
            )
            object_node = next(
                (n for n in memory.nodes if n.node_type == NodeType.OBJECT),
                None
            )
            
            parts = []
            parts.append(f"ç±»å‹: {memory.memory_type.value}")
            
            if subject_node:
                parts.append(f"ä¸»ä½“: {subject_node.content}")
            
            if topic_node:
                parts.append(f"ä¸»é¢˜: {topic_node.content}")
            
            if object_node:
                parts.append(f"å¯¹è±¡: {object_node.content}")
            
            parts.append(f"é‡è¦æ€§: {memory.importance:.2f}")
            parts.append(f"æ—¶é—´: {memory.created_at.strftime('%Y-%m-%d %H:%M')}")
            
            return " | ".join(parts)
            
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–è®°å¿†å¤±è´¥: {e}")
            return f"è®°å¿†ID: {memory.id}"

    async def maintenance(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œç»´æŠ¤ä»»åŠ¡
        
        åŒ…æ‹¬ï¼š
        - è®°å¿†æ•´ç†ï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
        - æ¸…ç†è¿‡æœŸè®°å¿†
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®
        
        Returns:
            ç»´æŠ¤ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info("å¼€å§‹æ‰§è¡Œè®°å¿†ç³»ç»Ÿç»´æŠ¤...")
            
            result = {
                "consolidated": 0,
                "forgotten": 0,
                "deleted": 0,
                "saved": False,
            }
            
            # 1. è®°å¿†æ•´ç†ï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
            # é»˜è®¤ç¦ç”¨è‡ªåŠ¨æ•´ç†ï¼Œå› ä¸ºå¯èƒ½é˜»å¡ä¸»æµç¨‹
            # å»ºè®®ï¼šæé«˜é˜ˆå€¼åˆ°0.92ä»¥ä¸Šï¼Œå‡å°‘è¯¯åˆ¤ï¼›é™åˆ¶æ‰¹é‡å¤§å°é¿å…é˜»å¡
            if getattr(self.config, 'consolidation_enabled', False):
                consolidate_result = await self.consolidate_memories(
                    similarity_threshold=getattr(self.config, 'consolidation_similarity_threshold', 0.92),
                    time_window_hours=getattr(self.config, 'consolidation_time_window_hours', 24.0),
                    max_batch_size=getattr(self.config, 'consolidation_max_batch_size', 50)
                )
                result["consolidated"] = consolidate_result.get("merged_count", 0)
            
            # 2. è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆå‘ç°å’Œå»ºç«‹å…³ç³»ï¼‰
            if getattr(self.config, 'auto_link_enabled', True):
                link_result = await self.auto_link_memories()
                result["linked"] = link_result.get("linked_count", 0)
            
            # 3. è‡ªåŠ¨é—å¿˜
            if getattr(self.config, 'forgetting_enabled', True):
                forgotten_count = await self.auto_forget_memories(
                    threshold=getattr(self.config, 'forgetting_activation_threshold', 0.1)
                )
                result["forgotten"] = forgotten_count
            
            # 4. æ¸…ç†éå¸¸æ—§çš„å·²é—å¿˜è®°å¿†ï¼ˆå¯é€‰ï¼‰
            # TODO: å®ç°æ¸…ç†é€»è¾‘
            
            # 5. ä¿å­˜æ•°æ®
            await self.persistence.save_graph_store(self.graph_store)
            result["saved"] = True
            
            self._last_maintenance = datetime.now()
            logger.info(f"ç»´æŠ¤å®Œæˆ: {result}")
            return result
            
        except Exception as e:
            logger.error(f"ç»´æŠ¤å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e)}

    async def start_maintenance_scheduler(self) -> None:
        """
        å¯åŠ¨è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        
        ä½¿ç”¨ unified_scheduler å®šæœŸæ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼š
        - è®°å¿†æ•´åˆï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®
        
        é»˜è®¤é—´éš”ï¼š1å°æ—¶
        """
        try:
            from src.schedule.unified_scheduler import TriggerType, unified_scheduler
            
            # å¦‚æœå·²æœ‰è°ƒåº¦ä»»åŠ¡ï¼Œå…ˆç§»é™¤
            if self._maintenance_schedule_id:
                await unified_scheduler.remove_schedule(self._maintenance_schedule_id)
                logger.info("ç§»é™¤æ—§çš„ç»´æŠ¤è°ƒåº¦ä»»åŠ¡")
            
            # åˆ›å»ºæ–°çš„è°ƒåº¦ä»»åŠ¡
            interval_seconds = self._maintenance_interval_hours * 3600
            
            self._maintenance_schedule_id = await unified_scheduler.create_schedule(
                callback=self.maintenance,
                trigger_type=TriggerType.TIME,
                trigger_config={
                    "delay_seconds": interval_seconds,  # é¦–æ¬¡å»¶è¿Ÿï¼ˆå¯åŠ¨å1å°æ—¶ï¼‰
                    "interval_seconds": interval_seconds,  # å¾ªç¯é—´éš”
                },
                is_recurring=True,
                task_name="memory_maintenance",
            )
            
            logger.info(
                f"âœ… è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å·²å¯åŠ¨ "
                f"(é—´éš”={self._maintenance_interval_hours}å°æ—¶, "
                f"schedule_id={self._maintenance_schedule_id[:8]}...)"
            )
            
        except ImportError:
            logger.warning("æ— æ³•å¯¼å…¥ unified_schedulerï¼Œç»´æŠ¤è°ƒåº¦åŠŸèƒ½ä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"å¯åŠ¨ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

    async def stop_maintenance_scheduler(self) -> None:
        """
        åœæ­¢è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        """
        if not self._maintenance_schedule_id:
            return
        
        try:
            from src.schedule.unified_scheduler import unified_scheduler
            
            success = await unified_scheduler.remove_schedule(self._maintenance_schedule_id)
            if success:
                logger.info(f"âœ… è®°å¿†ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å·²åœæ­¢ (schedule_id={self._maintenance_schedule_id[:8]}...)")
            else:
                logger.warning(f"åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥ (schedule_id={self._maintenance_schedule_id[:8]}...)")
            
            self._maintenance_schedule_id = None
            
        except ImportError:
            logger.warning("æ— æ³•å¯¼å…¥ unified_scheduler")
        except Exception as e:
            logger.error(f"åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
