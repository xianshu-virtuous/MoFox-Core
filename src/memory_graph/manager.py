"""
è®°å¿†ç®¡ç†å™¨ - Phase 3

ç»Ÿä¸€çš„è®°å¿†ç³»ç»Ÿç®¡ç†æ¥å£ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
- è®°å¿†åˆ›å»ºã€æ£€ç´¢ã€æ›´æ–°ã€åˆ é™¤
- è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ¿€æ´»ã€é—å¿˜ï¼‰
- è®°å¿†æ•´åˆä¸ç»´æŠ¤
- å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–
"""

import asyncio
import logging
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

from src.config.config import global_config
from src.config.official_configs import MemoryConfig
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import EdgeType, Memory, MemoryEdge, NodeType
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.tools.memory_tools import MemoryTools
from src.memory_graph.utils.embeddings import EmbeddingGenerator
from src.memory_graph.utils.similarity import cosine_similarity

if TYPE_CHECKING:
    import numpy as np

logger = logging.getLogger(__name__)


class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨

    æ ¸å¿ƒç®¡ç†ç±»ï¼Œæä¾›è®°å¿†ç³»ç»Ÿçš„ç»Ÿä¸€æ¥å£ï¼š
    - è®°å¿† CRUD æ“ä½œ
    - è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†
    - æ™ºèƒ½æ£€ç´¢ä¸æ¨è
    - è®°å¿†ç»´æŠ¤ä¸ä¼˜åŒ–
    """

    def __init__(
        self,
        data_dir: Path | None = None,
    ):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»global_configè¯»å–ï¼‰
        """
        # ç›´æ¥ä½¿ç”¨ global_config.memory
        if not global_config.memory or not getattr(global_config.memory, "enable", False):
            raise ValueError("è®°å¿†ç³»ç»Ÿæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨ [memory] enable = true")

        self.config: MemoryConfig = global_config.memory
        self.data_dir = data_dir or Path(getattr(self.config, "data_dir", "data/memory_graph"))

        # å­˜å‚¨ç»„ä»¶
        self.vector_store: VectorStore | None = None
        self.graph_store: GraphStore | None = None
        self.persistence: PersistenceManager | None = None

        # æ ¸å¿ƒç»„ä»¶
        self.embedding_generator: EmbeddingGenerator | None = None
        self.extractor: MemoryExtractor | None = None
        self.builder: MemoryBuilder | None = None
        self.tools: MemoryTools | None = None

        # çŠ¶æ€
        self._initialized = False
        self._last_maintenance = datetime.now()
        self._maintenance_task: asyncio.Task | None = None
        self._maintenance_interval_hours = getattr(self.config, "consolidation_interval_hours", 1.0)
        self._maintenance_running = False  # ç»´æŠ¤ä»»åŠ¡è¿è¡ŒçŠ¶æ€

        logger.info(f"è®°å¿†ç®¡ç†å™¨å·²åˆ›å»º (data_dir={self.data_dir}, enable={getattr(self.config, 'enable', False)})")

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶

        æŒ‰ç…§ä¾èµ–é¡ºåºåˆå§‹åŒ–ï¼š
        1. å­˜å‚¨å±‚ï¼ˆå‘é‡å­˜å‚¨ã€å›¾å­˜å‚¨ã€æŒä¹…åŒ–ï¼‰
        2. å·¥å…·å±‚ï¼ˆåµŒå…¥ç”Ÿæˆå™¨ã€æå–å™¨ï¼‰
        3. ç®¡ç†å±‚ï¼ˆæ„å»ºå™¨ã€å·¥å…·æ¥å£ï¼‰
        """
        if self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–")
            return

        try:
            logger.debug("å¼€å§‹åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨...")

            # 1. åˆå§‹åŒ–å­˜å‚¨å±‚
            self.data_dir.mkdir(parents=True, exist_ok=True)

            # è·å–å­˜å‚¨é…ç½®
            storage_config = getattr(self.config, "storage", None)
            vector_collection_name = getattr(storage_config, "vector_collection_name", "memory_graph") if storage_config else "memory_graph"

            self.vector_store = VectorStore(
                collection_name=vector_collection_name,
                data_dir=self.data_dir,
            )
            await self.vector_store.initialize()

            self.persistence = PersistenceManager(data_dir=self.data_dir)

            # å°è¯•åŠ è½½ç°æœ‰å›¾æ•°æ®
            self.graph_store = await self.persistence.load_graph_store()
            if not self.graph_store:
                logger.info("æœªæ‰¾åˆ°ç°æœ‰å›¾æ•°æ®ï¼Œåˆ›å»ºæ–°çš„å›¾å­˜å‚¨")
                self.graph_store = GraphStore()
            else:
                stats = self.graph_store.get_statistics()
                logger.info(
                    f"åŠ è½½å›¾æ•°æ®: {stats['total_memories']} æ¡è®°å¿†, "
                    f"{stats['total_nodes']} ä¸ªèŠ‚ç‚¹, {stats['total_edges']} æ¡è¾¹"
                )

            # 2. åˆå§‹åŒ–å·¥å…·å±‚
            self.embedding_generator = EmbeddingGenerator()
            # EmbeddingGenerator ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–

            self.extractor = MemoryExtractor()

            # 3. åˆå§‹åŒ–ç®¡ç†å±‚
            self.builder = MemoryBuilder(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                embedding_generator=self.embedding_generator,
            )

            # æ£€æŸ¥é…ç½®å€¼
            # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœé…ç½®é¡¹ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–æ˜ å°„åˆ°æ–°é…ç½®é¡¹
            expand_depth = getattr(self.config, "path_expansion_max_hops", 2)
            expand_semantic_threshold = getattr(self.config, "search_similarity_threshold", 0.5)
            search_top_k = getattr(self.config, "search_top_k", 10)
            
            # è¯»å–æƒé‡é…ç½®
            search_vector_weight = getattr(self.config, "vector_weight", 0.65)
            # context_weight è¿‘ä¼¼æ˜ å°„ä¸º importance_weight
            search_importance_weight = getattr(self.config, "context_weight", 0.25)
            search_recency_weight = getattr(self.config, "recency_weight", 0.10)
            
            # è¯»å–é˜ˆå€¼è¿‡æ»¤é…ç½®
            search_min_importance = getattr(self.config, "search_min_importance", 0.3)
            search_similarity_threshold = getattr(self.config, "search_similarity_threshold", 0.5)

            self.tools = MemoryTools(
                vector_store=self.vector_store,
                graph_store=self.graph_store,
                persistence_manager=self.persistence,
                embedding_generator=self.embedding_generator,
                max_expand_depth=expand_depth,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•æ·±åº¦
                expand_semantic_threshold=expand_semantic_threshold,  # ä»é…ç½®è¯»å–å›¾æ‰©å±•è¯­ä¹‰é˜ˆå€¼
                search_top_k=search_top_k,  # ä»é…ç½®è¯»å–é»˜è®¤ top_k
                search_vector_weight=search_vector_weight,  # ä»é…ç½®è¯»å–å‘é‡æƒé‡
                search_importance_weight=search_importance_weight,  # ä»é…ç½®è¯»å–é‡è¦æ€§æƒé‡
                search_recency_weight=search_recency_weight,  # ä»é…ç½®è¯»å–æ—¶æ•ˆæ€§æƒé‡
                search_min_importance=search_min_importance,  # ä»é…ç½®è¯»å–æœ€å°é‡è¦æ€§é˜ˆå€¼
                search_similarity_threshold=search_similarity_threshold,  # ä»é…ç½®è¯»å–ç›¸ä¼¼åº¦é˜ˆå€¼
            )

            self._initialized = True
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

            # å¯åŠ¨åå°ç»´æŠ¤ä»»åŠ¡
            self._start_maintenance_task()

        except Exception as e:
            logger.error(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def shutdown(self) -> None:
        """
        å…³é—­è®°å¿†ç®¡ç†å™¨

        æ‰§è¡Œæ¸…ç†æ“ä½œï¼š
        - åœæ­¢ç»´æŠ¤è°ƒåº¦ä»»åŠ¡
        - ä¿å­˜æ‰€æœ‰æ•°æ®
        - å…³é—­å­˜å‚¨ç»„ä»¶
        """
        if not self._initialized:
            logger.warning("è®°å¿†ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— éœ€å…³é—­")
            return

        try:
            logger.info("æ­£åœ¨å…³é—­è®°å¿†ç®¡ç†å™¨...")

            # 1. åœæ­¢ç»´æŠ¤ä»»åŠ¡
            await self._stop_maintenance_task()

            # 2. æ‰§è¡Œæœ€åä¸€æ¬¡ç»´æŠ¤ï¼ˆä¿å­˜æ•°æ®ï¼‰
            if self.graph_store and self.persistence:
                logger.info("æ‰§è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜...")
                await self.persistence.save_graph_store(self.graph_store)

            # 3. å…³é—­å­˜å‚¨ç»„ä»¶
            if self.vector_store:
                # VectorStore ä½¿ç”¨ chromadbï¼Œæ— éœ€æ˜¾å¼å…³é—­
                pass

            self._initialized = False
            logger.info("âœ… è®°å¿†ç®¡ç†å™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å…³é—­è®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}")

    # ==================== è®°å¿† CRUD æ“ä½œ ====================

    async def create_memory(
        self,
        subject: str,
        memory_type: str,
        topic: str,
        object: str | None = None,
        attributes: dict[str, str] | None = None,
        importance: float = 0.5,
        **kwargs,
    ) -> Memory | None:
        """
        åˆ›å»ºæ–°è®°å¿†

        Args:
            subject: ä¸»ä½“ï¼ˆè°ï¼‰
            memory_type: è®°å¿†ç±»å‹ï¼ˆäº‹ä»¶/è§‚ç‚¹/äº‹å®/å…³ç³»ï¼‰
            topic: ä¸»é¢˜ï¼ˆåšä»€ä¹ˆ/æƒ³ä»€ä¹ˆï¼‰
            object: å®¢ä½“ï¼ˆå¯¹è°/å¯¹ä»€ä¹ˆï¼‰
            attributes: å±æ€§å­—å…¸ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€åŸå› ç­‰ï¼‰
            importance: é‡è¦æ€§ (0.0-1.0)
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åˆ›å»ºçš„è®°å¿†å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.create_memory(
                subject=subject,
                memory_type=memory_type,
                topic=topic,
                object=object,
                attributes=attributes,
                importance=importance,
                **kwargs,
            )

            if result["success"]:
                memory_id = result["memory_id"]
                memory = self.graph_store.get_memory_by_id(memory_id)
                logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory_id}")
                return memory
            else:
                logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
                return None

        except Exception as e:
            logger.error(f"åˆ›å»ºè®°å¿†æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return None

    async def get_memory(self, memory_id: str) -> Memory | None:
        """
        æ ¹æ® ID è·å–è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            è®°å¿†å¯¹è±¡ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        return self.graph_store.get_memory_by_id(memory_id)

    async def update_memory(
        self,
        memory_id: str,
        **updates,
    ) -> bool:
        """
        æ›´æ–°è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            **updates: è¦æ›´æ–°çš„å­—æ®µ

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°å…ƒæ•°æ®
            if "importance" in updates:
                memory.importance = updates["importance"]

            if "metadata" in updates:
                memory.metadata.update(updates["metadata"])

            memory.updated_at = datetime.now()

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("æ›´æ–°è®°å¿†"))
            logger.info(f"è®°å¿†æ›´æ–°æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}")
            return False

    async def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤è®°å¿†

        Args:
            memory_id: è®°å¿† ID

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹
            if self.vector_store:
                for node in memory.nodes:
                    if getattr(node, "has_vector", False):
                        await self.vector_store.delete_node(node.id)
                        node.has_vector = False
                        if self.graph_store.graph.has_node(node.id):
                            self.graph_store.graph.nodes[node.id]["has_vector"] = False

            # ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            self.graph_store.remove_memory(memory_id)

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("åˆ é™¤è®°å¿†"))
            logger.info(f"è®°å¿†åˆ é™¤æˆåŠŸ: {memory_id}")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤è®°å¿†å¤±è´¥: {e}")
            return False

    # ==================== è®°å¿†æ£€ç´¢æ“ä½œ ====================
    async def search_memories(
        self,
        query: str,
        top_k: int | None = None,
        memory_types: list[str] | None = None,
        time_range: tuple[datetime, datetime] | None = None,
        min_importance: float = 0.0,
        include_forgotten: bool = False,
        use_multi_query: bool = True,
        expand_depth: int | None = None,
        context: dict[str, Any] | None = None,
        prefer_node_types: list[str] | None = None,  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹
    ) -> list[Memory]:
        """
        æœç´¢è®°å¿†

        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼Œè§£å†³å¤æ‚æŸ¥è¯¢é—®é¢˜ã€‚
        ä¾‹å¦‚ï¼š"æ°ç‘å–µå¦‚ä½•è¯„ä»·æ–°çš„è®°å¿†ç³»ç»Ÿ" ä¼šè¢«åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ï¼Œ
        ç¡®ä¿åŒæ—¶åŒ¹é…"æ°ç‘å–µ"å’Œ"æ–°çš„è®°å¿†ç³»ç»Ÿ"ä¸¤ä¸ªå…³é”®æ¦‚å¿µã€‚

        åŒæ—¶æ”¯æŒå›¾æ‰©å±•ï¼šä»åˆå§‹æ£€ç´¢ç»“æœå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æŸ¥æ‰¾è¯­ä¹‰ç›¸å…³çš„é‚»å±…è®°å¿†ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°
            memory_types: è®°å¿†ç±»å‹è¿‡æ»¤
            time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ (start, end)
            min_importance: æœ€å°é‡è¦æ€§
            include_forgotten: æ˜¯å¦åŒ…å«å·²é—å¿˜çš„è®°å¿†
            use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆæ¨èï¼Œé»˜è®¤Trueï¼‰
            expand_depth: å›¾æ‰©å±•æ·±åº¦ï¼ˆ0=ç¦ç”¨, 1=æ¨è, 2-3=æ·±åº¦æ¢ç´¢ï¼‰
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¼˜åŒ–ï¼‰
            prefer_node_types: åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆå¦‚ ["ENTITY", "EVENT"]ï¼‰ğŸ†•

        Returns:
            è®°å¿†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        try:
            # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
            if top_k is None:
                top_k = getattr(self.config, "search_top_k", 10)

            # å‡†å¤‡æœç´¢å‚æ•°
            params = {
                "query": query,
                "top_k": top_k,
                "use_multi_query": use_multi_query,
                "expand_depth": expand_depth or getattr(global_config.memory, "path_expansion_max_hops", 2),  # ä¼ é€’å›¾æ‰©å±•æ·±åº¦
                "context": context,
                "prefer_node_types": prefer_node_types or [],  # ğŸ†• ä¼ é€’åå¥½èŠ‚ç‚¹ç±»å‹
            }

            if memory_types:
                params["memory_types"] = memory_types

            # æ‰§è¡Œæœç´¢
            result = await self.tools.search_memories(**params)

            if not result["success"]:
                logger.error(f"æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}")
                return []

            memories = result.get("results", [])

            # åå¤„ç†è¿‡æ»¤
            filtered_memories = []
            for mem_dict in memories:
                # ä»å­—å…¸é‡å»º Memory å¯¹è±¡
                memory_id = mem_dict.get("memory_id", "")
                if not memory_id:
                    continue

                memory = self.graph_store.get_memory_by_id(memory_id)
                if not memory:
                    continue

                # é‡è¦æ€§è¿‡æ»¤
                if min_importance is not None and memory.importance < min_importance:
                    continue

                # é—å¿˜çŠ¶æ€è¿‡æ»¤
                if not include_forgotten and memory.metadata.get("forgotten", False):
                    continue

                # æ—¶é—´èŒƒå›´è¿‡æ»¤
                if time_range:
                    mem_time = memory.created_at
                    if not (time_range[0] <= mem_time <= time_range[1]):
                        continue

                filtered_memories.append(memory)

            strategy = result.get("strategy", "unknown")
            logger.info(
                f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(filtered_memories)} æ¡è®°å¿† (ç­–ç•¥={strategy})"
            )

            # å¼ºåˆ¶æ¿€æ´»è¢«æ£€ç´¢åˆ°çš„è®°å¿†ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰- ä½¿ç”¨å¿«é€Ÿæ‰¹é‡æ¿€æ´»
            if filtered_memories:
                await self._quick_batch_activate_memories(filtered_memories)

            return filtered_memories[:top_k]

        except Exception as e:
            logger.error(f"æœç´¢è®°å¿†å¤±è´¥: {e}")
            return []

    async def link_memories(
        self,
        source_description: str,
        target_description: str,
        relation_type: str,
        importance: float = 0.5,
    ) -> bool:
        """
        å…³è”ä¸¤æ¡è®°å¿†

        Args:
            source_description: æºè®°å¿†æè¿°
            target_description: ç›®æ ‡è®°å¿†æè¿°
            relation_type: å…³ç³»ç±»å‹ï¼ˆå¯¼è‡´/å¼•ç”¨/ç›¸ä¼¼/ç›¸åï¼‰
            importance: å…³ç³»é‡è¦æ€§

        Returns:
            æ˜¯å¦å…³è”æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            result = await self.tools.link_memories(
                source_memory_description=source_description,
                target_memory_description=target_description,
                relation_type=relation_type,
                importance=importance,
            )

            if result["success"]:
                logger.info(
                    f"è®°å¿†å…³è”æˆåŠŸ: {result['source_memory_id']} -> "
                    f"{result['target_memory_id']} ({relation_type})"
                )
                return True
            else:
                logger.error(f"è®°å¿†å…³è”å¤±è´¥: {result.get('error', 'Unknown error')}")
                return False

        except Exception as e:
            logger.error(f"å…³è”è®°å¿†å¤±è´¥: {e}")
            return False

    # ==================== è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

    async def activate_memory(self, memory_id: str, strength: float = 1.0) -> bool:
        """
        æ¿€æ´»è®°å¿†

        æ›´æ–°è®°å¿†çš„æ¿€æ´»åº¦ï¼Œå¹¶ä¼ æ’­åˆ°ç›¸å…³è®°å¿†

        Args:
            memory_id: è®°å¿† ID
            strength: æ¿€æ´»å¼ºåº¦ (0.0-1.0)

        Returns:
            æ˜¯å¦æ¿€æ´»æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # æ›´æ–°æ¿€æ´»ä¿¡æ¯
            now = datetime.now()
            activation_info = memory.metadata.get("activation", {})

            # æ›´æ–°æ¿€æ´»åº¦ï¼ˆè€ƒè™‘æ—¶é—´è¡°å‡ï¼‰
            last_access = activation_info.get("last_access")
            if last_access:
                # è®¡ç®—æ—¶é—´è¡°å‡
                last_access_dt = datetime.fromisoformat(last_access)
                hours_passed = (now - last_access_dt).total_seconds() / 3600
                decay_rate = getattr(self.config, "activation_decay_rate", 0.95)
                decay_factor = decay_rate ** (hours_passed / 24)
                current_activation = activation_info.get("level", 0.0) * decay_factor
            else:
                current_activation = 0.0

            # æ–°çš„æ¿€æ´»åº¦ = å½“å‰æ¿€æ´»åº¦ + æ¿€æ´»å¼ºåº¦
            new_activation = min(1.0, current_activation + strength)

            activation_info.update({
                "level": new_activation,
                "last_access": now.isoformat(),
                "access_count": activation_info.get("access_count", 0) + 1,
            })

            # åŒæ­¥æ›´æ–° memory.activation å­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            memory.activation = new_activation
            memory.metadata["activation"] = activation_info
            memory.last_accessed = now

            # æ¿€æ´»ä¼ æ’­ï¼šæ¿€æ´»ç›¸å…³è®°å¿†
            if strength > 0.1:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                propagation_depth = getattr(self.config, "activation_propagation_depth", 2)
                related_memories = self._get_related_memories(
                    memory_id,
                    max_depth=propagation_depth
                )
                propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.5)
                propagation_strength = strength * propagation_strength_factor

                max_related = getattr(self.config, "max_related_memories", 5)
                for related_id in related_memories[:max_related]:
                    await self.activate_memory(related_id, propagation_strength)

            # å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
            asyncio.create_task(self._async_save_graph_store("æ¿€æ´»è®°å¿†"))
            logger.debug(f"è®°å¿†å·²æ¿€æ´»: {memory_id} (level={new_activation:.3f})")
            return True

        except Exception as e:
            logger.error(f"æ¿€æ´»è®°å¿†å¤±è´¥: {e}")
            return False

    async def _auto_activate_searched_memories(self, memories: list[Memory]) -> None:
        """
        æ‰¹é‡æ¿€æ´»è¢«æœç´¢åˆ°çš„è®°å¿†

        Args:
            memories: è¢«æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not memories:
                return

            # è·å–é…ç½®å‚æ•°
            base_strength = getattr(self.config, "auto_activate_base_strength", 0.1)
            max_activate_count = getattr(self.config, "auto_activate_max_count", 5)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)
            now = datetime.now()

            # é™åˆ¶å¤„ç†çš„è®°å¿†æ•°é‡
            memories_to_activate = memories[:max_activate_count]

            # æ‰¹é‡æ›´æ–°æ¿€æ´»åº¦
            activation_updates = []
            for memory in memories_to_activate:
                # è®¡ç®—æ¿€æ´»å¼ºåº¦
                strength = base_strength * (0.5 + memory.importance)

                # è·å–å½“å‰æ¿€æ´»åº¦ä¿¡æ¯
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")

                if last_access:
                    # è®¡ç®—æ—¶é—´è¡°å‡
                    last_access_dt = datetime.fromisoformat(last_access)
                    hours_passed = (now - last_access_dt).total_seconds() / 3600
                    decay_factor = decay_rate ** (hours_passed / 24)
                    current_activation = activation_info.get("level", 0.0) * decay_factor
                else:
                    current_activation = 0.0

                # è®¡ç®—æ–°çš„æ¿€æ´»åº¦
                new_activation = min(1.0, current_activation + strength)

                # æ›´æ–°è®°å¿†å¯¹è±¡
                memory.activation = new_activation
                memory.last_accessed = now
                activation_info.update({
                    "level": new_activation,
                    "last_access": now.isoformat(),
                    "access_count": activation_info.get("access_count", 0) + 1,
                })
                memory.metadata["activation"] = activation_info

                activation_updates.append({
                    "memory_id": memory.id,
                    "old_activation": current_activation,
                    "new_activation": new_activation,
                    "strength": strength
                })

            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            if activation_updates:
                asyncio.create_task(self._async_save_graph_store("æ‰¹é‡æ¿€æ´»æ›´æ–°"))

                # æ¿€æ´»ä¼ æ’­ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                asyncio.create_task(self._batch_propagate_activation(memories_to_activate, base_strength))

                logger.debug(f"æ‰¹é‡æ¿€æ´» {len(activation_updates)} æ¡è®°å¿†å®Œæˆ")

        except Exception as e:
            logger.warning(f"æ‰¹é‡æ¿€æ´»æœç´¢è®°å¿†å¤±è´¥: {e}")

    async def _quick_batch_activate_memories(self, memories: list[Memory]) -> None:
        """
        å¿«é€Ÿæ‰¹é‡æ¿€æ´»è®°å¿†ï¼ˆç”¨äºæœç´¢ç»“æœï¼Œä¼˜åŒ–æ€§èƒ½ï¼‰

        ä¸ _auto_activate_searched_memories çš„åŒºåˆ«ï¼š
        - æ›´è½»é‡çº§ï¼Œä¸“æ³¨äºé€Ÿåº¦
        - ç®€åŒ–æ¿€æ´»ä¼ æ’­é€»è¾‘
        - å‡å°‘æ•°æ®åº“å†™å…¥æ¬¡æ•°

        Args:
            memories: éœ€è¦æ¿€æ´»çš„è®°å¿†åˆ—è¡¨
        """
        try:
            if not memories:
                return

            # è·å–é…ç½®å‚æ•°
            base_strength = getattr(self.config, "auto_activate_base_strength", 0.1)
            max_activate_count = getattr(self.config, "auto_activate_max_count", 5)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)
            now = datetime.now()

            # é™åˆ¶å¤„ç†çš„è®°å¿†æ•°é‡
            memories_to_activate = memories[:max_activate_count]

            # æ‰¹é‡æ›´æ–°æ¿€æ´»åº¦ï¼ˆå†…å­˜æ“ä½œï¼‰
            for memory in memories_to_activate:
                # è®¡ç®—æ¿€æ´»å¼ºåº¦
                strength = base_strength * (0.5 + memory.importance)

                # å¿«é€Ÿè®¡ç®—æ–°çš„æ¿€æ´»åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                activation_info = memory.metadata.get("activation", {})
                last_access = activation_info.get("last_access")

                if last_access:
                    # ç®€åŒ–çš„æ—¶é—´è¡°å‡è®¡ç®—
                    try:
                        last_access_dt = datetime.fromisoformat(last_access)
                        hours_passed = (now - last_access_dt).total_seconds() / 3600
                        decay_factor = decay_rate ** (hours_passed / 24)
                        current_activation = activation_info.get("level", 0.0) * decay_factor
                    except (ValueError, TypeError):
                        current_activation = activation_info.get("level", 0.0) * 0.9  # é»˜è®¤è¡°å‡
                else:
                    current_activation = 0.0

                # è®¡ç®—æ–°çš„æ¿€æ´»åº¦
                new_activation = min(1.0, current_activation + strength)

                # ç›´æ¥æ›´æ–°è®°å¿†å¯¹è±¡ï¼ˆå†…å­˜ä¸­ï¼‰
                memory.activation = new_activation
                memory.last_accessed = now
                activation_info.update({
                    "level": new_activation,
                    "last_access": now.isoformat(),
                    "access_count": activation_info.get("access_count", 0) + 1,
                })
                memory.metadata["activation"] = activation_info

            # å¼‚æ­¥æ‰¹é‡ä¿å­˜ï¼ˆä¸é˜»å¡æœç´¢ï¼‰
            if memories_to_activate:
                asyncio.create_task(self._background_save_activation(memories_to_activate, base_strength))

            logger.debug(f"å¿«é€Ÿæ‰¹é‡æ¿€æ´» {len(memories_to_activate)} æ¡è®°å¿†")

        except Exception as e:
            logger.warning(f"å¿«é€Ÿæ‰¹é‡æ¿€æ´»è®°å¿†å¤±è´¥: {e}")

    async def _background_save_activation(self, memories: list[Memory], base_strength: float) -> None:
        """
        åå°ä¿å­˜æ¿€æ´»æ›´æ–°å¹¶æ‰§è¡Œä¼ æ’­

        Args:
            memories: å·²æ›´æ–°çš„è®°å¿†åˆ—è¡¨
            base_strength: åŸºç¡€æ¿€æ´»å¼ºåº¦
        """
        try:
            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            asyncio.create_task(self._async_save_graph_store("åå°æ¿€æ´»æ›´æ–°"))

            # ç®€åŒ–çš„æ¿€æ´»ä¼ æ’­ï¼ˆä»…åœ¨å¼ºåº¦è¶³å¤Ÿæ—¶æ‰§è¡Œï¼‰
            if base_strength > 0.08:  # æé«˜ä¼ æ’­é˜ˆå€¼ï¼Œå‡å°‘ä¼ æ’­é¢‘ç‡
                propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.3)  # é™ä½ä¼ æ’­å¼ºåº¦
                max_related = getattr(self.config, "max_related_memories", 3)  # å‡å°‘ä¼ æ’­æ•°é‡

                # åªä¼ æ’­æœ€é‡è¦çš„è®°å¿†çš„æ¿€æ´»
                important_memories = [m for m in memories if m.importance > 0.6][:2]  # æœ€å¤š2ä¸ªé‡è¦è®°å¿†

                for memory in important_memories:
                    related_memories = self._get_related_memories(memory.id, max_depth=1)  # å‡å°‘ä¼ æ’­æ·±åº¦
                    propagation_strength = base_strength * propagation_strength_factor

                    for related_id in related_memories[:max_related]:
                        try:
                            related_memory = self.graph_store.get_memory_by_id(related_id)
                            if related_memory:
                                # ç®€å•çš„æ¿€æ´»åº¦å¢åŠ ï¼ˆä¸è°ƒç”¨å®Œæ•´æ¿€æ´»æ–¹æ³•ï¼‰
                                current_activation = related_memory.metadata.get("activation", {}).get("level", related_memory.activation)
                                new_activation = min(1.0, current_activation + propagation_strength * 0.5)

                                related_memory.activation = new_activation
                                related_memory.metadata["activation"] = {
                                    "level": new_activation,
                                    "last_access": datetime.now().isoformat(),
                                    "access_count": related_memory.metadata.get("activation", {}).get("access_count", 0) + 1,
                                }
                        except Exception as e:
                            logger.debug(f"ä¼ æ’­æ¿€æ´»åˆ°ç›¸å…³è®°å¿† {related_id[:8]} å¤±è´¥: {e}")

                # å†æ¬¡ä¿å­˜ä¼ æ’­åçš„æ›´æ–°
                await self.persistence.save_graph_store(self.graph_store)

            logger.debug(f"åå°ä¿å­˜æ¿€æ´»æ›´æ–°å®Œæˆï¼Œå¤„ç†äº† {len(memories)} æ¡è®°å¿†")

        except Exception as e:
            logger.warning(f"åå°ä¿å­˜æ¿€æ´»æ›´æ–°å¤±è´¥: {e}")

    async def _batch_propagate_activation(self, memories: list[Memory], base_strength: float) -> None:
        """
        æ‰¹é‡ä¼ æ’­æ¿€æ´»åˆ°ç›¸å…³è®°å¿†ï¼ˆåå°æ‰§è¡Œï¼‰

        Args:
            memories: å·²æ¿€æ´»çš„è®°å¿†åˆ—è¡¨
            base_strength: åŸºç¡€æ¿€æ´»å¼ºåº¦
        """
        try:
            propagation_strength_factor = getattr(self.config, "activation_propagation_strength", 0.5)
            propagation_depth = getattr(self.config, "activation_propagation_depth", 2)
            max_related = getattr(self.config, "max_related_memories", 5)

            # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼ æ’­æ¿€æ´»çš„è®°å¿†ID
            propagation_tasks = []
            for memory in memories:
                if base_strength > 0.05:  # åªæœ‰è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ‰ä¼ æ’­
                    related_memories = self._get_related_memories(
                        memory.id,
                        max_depth=propagation_depth
                    )
                    propagation_strength = base_strength * propagation_strength_factor

                    for related_id in related_memories[:max_related]:
                        task = self.activate_memory(related_id, propagation_strength)
                        propagation_tasks.append(task)

            # æ‰¹é‡æ‰§è¡Œä¼ æ’­ä»»åŠ¡
            if propagation_tasks:
                import asyncio
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*propagation_tasks, return_exceptions=True),
                        timeout=3.0  # ä¼ æ’­æ“ä½œè¶…æ—¶æ—¶é—´ç¨é•¿
                    )
                    logger.debug(f"æ¿€æ´»ä¼ æ’­å®Œæˆ: {len(propagation_tasks)} ä¸ªç›¸å…³è®°å¿†")
                except asyncio.TimeoutError:
                    logger.warning("æ¿€æ´»ä¼ æ’­è¶…æ—¶ï¼Œéƒ¨åˆ†ç›¸å…³è®°å¿†æœªæ¿€æ´»")
                except Exception as e:
                    logger.warning(f"æ¿€æ´»ä¼ æ’­å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"æ‰¹é‡ä¼ æ’­æ¿€æ´»å¤±è´¥: {e}")

    def _get_related_memories(self, memory_id: str, max_depth: int = 1) -> list[str]:
        """
        è·å–ç›¸å…³è®°å¿† ID åˆ—è¡¨ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºæ¿€æ´»ä¼ æ’­ï¼‰

        Args:
            memory_id: è®°å¿† ID
            max_depth: æœ€å¤§éå†æ·±åº¦

        Returns:
            ç›¸å…³è®°å¿† ID åˆ—è¡¨
        """
        memory = self.graph_store.get_memory_by_id(memory_id)
        if not memory:
            return []

        related_ids = set()

        # éå†è®°å¿†çš„èŠ‚ç‚¹
        for node in memory.nodes:
            # è·å–èŠ‚ç‚¹çš„é‚»å±…
            neighbors = list(self.graph_store.graph.neighbors(node.id))

            for neighbor_id in neighbors:
                # è·å–é‚»å±…èŠ‚ç‚¹æ‰€å±çš„è®°å¿†
                neighbor_node = self.graph_store.graph.nodes.get(neighbor_id)
                if neighbor_node:
                    neighbor_memory_ids = neighbor_node.get("memory_ids", [])
                    for mem_id in neighbor_memory_ids:
                        if mem_id != memory_id:
                            related_ids.add(mem_id)

        return list(related_ids)

    async def forget_memory(self, memory_id: str, cleanup_orphans: bool = True) -> bool:
        """
        é—å¿˜è®°å¿†ï¼ˆç›´æ¥åˆ é™¤ï¼‰

        è¿™ä¸ªæ–¹æ³•ä¼šï¼š
        1. ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
        2. ä»å›¾å­˜å‚¨ä¸­åˆ é™¤è®°å¿†
        3. å¯é€‰ï¼šæ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼ˆå»ºè®®æ‰¹é‡é—å¿˜åç»Ÿä¸€æ¸…ç†ï¼‰
        4. ä¿å­˜æ›´æ–°åçš„æ•°æ®

        Args:
            memory_id: è®°å¿† ID
            cleanup_orphans: æ˜¯å¦ç«‹å³æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼ˆé»˜è®¤Trueï¼Œæ‰¹é‡é—å¿˜æ—¶è®¾ä¸ºFalseï¼‰

        Returns:
            æ˜¯å¦é—å¿˜æˆåŠŸ
        """
        if not self._initialized:
            await self.initialize()

        try:
            memory = self.graph_store.get_memory_by_id(memory_id)
            if not memory:
                logger.warning(f"è®°å¿†ä¸å­˜åœ¨: {memory_id}")
                return False

            # 1. ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹çš„åµŒå…¥å‘é‡
            deleted_vectors = 0
            if self.vector_store:
                for node in memory.nodes:
                    if getattr(node, "has_vector", False):
                        try:
                            await self.vector_store.delete_node(node.id)
                            deleted_vectors += 1
                            node.has_vector = False
                            if self.graph_store.graph.has_node(node.id):
                                self.graph_store.graph.nodes[node.id]["has_vector"] = False
                        except Exception as e:
                            logger.warning(f"åˆ é™¤èŠ‚ç‚¹å‘é‡å¤±è´¥ {node.id}: {e}")

            # 2. ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†
            success = self.graph_store.remove_memory(memory_id, cleanup_orphans=False)

            if success:
                # 3. å¯é€‰ï¼šæ¸…ç†å­¤ç«‹èŠ‚ç‚¹
                if cleanup_orphans:
                    orphan_nodes, orphan_edges = await self._cleanup_orphan_nodes_and_edges()
                    logger.info(
                        f"è®°å¿†å·²é—å¿˜å¹¶åˆ é™¤: {memory_id} "
                        f"(åˆ é™¤äº† {deleted_vectors} ä¸ªå‘é‡, æ¸…ç†äº† {orphan_nodes} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges} æ¡å­¤ç«‹è¾¹)"
                    )
                else:
                    logger.debug(f"è®°å¿†å·²åˆ é™¤: {memory_id} (åˆ é™¤äº† {deleted_vectors} ä¸ªå‘é‡)")

                # 4. å¼‚æ­¥ä¿å­˜æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰æ“ä½œï¼‰
                asyncio.create_task(self._async_save_graph_store("åˆ é™¤ç›¸å…³è®°å¿†"))
                return True
            else:
                logger.error(f"ä»å›¾å­˜å‚¨åˆ é™¤è®°å¿†å¤±è´¥: {memory_id}")
                return False

        except Exception as e:
            logger.error(f"é—å¿˜è®°å¿†å¤±è´¥: {e}")
            return False

    async def auto_forget_memories(self, threshold: float = 0.1) -> int:
        """
        è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦çš„è®°å¿†ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆï¼‰

        åº”ç”¨æ—¶é—´è¡°å‡å…¬å¼è®¡ç®—å½“å‰æ¿€æ´»åº¦ï¼Œä½äºé˜ˆå€¼åˆ™é—å¿˜ã€‚
        è¡°å‡å…¬å¼ï¼šactivation = base_activation * (decay_rate ^ days_passed)
        
        ä¼˜åŒ–ï¼šæ‰¹é‡åˆ é™¤è®°å¿†åç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼Œå‡å°‘é‡å¤æ£€æŸ¥

        Args:
            threshold: æ¿€æ´»åº¦é˜ˆå€¼

        Returns:
            é—å¿˜çš„è®°å¿†æ•°é‡
        """
        if not self._initialized:
            await self.initialize()

        try:
            forgotten_count = 0
            all_memories = self.graph_store.get_all_memories()

            # è·å–é…ç½®å‚æ•°
            min_importance = getattr(self.config, "forgetting_min_importance", 0.8)
            decay_rate = getattr(self.config, "activation_decay_rate", 0.9)

            # æ”¶é›†éœ€è¦é—å¿˜çš„è®°å¿†ID
            memories_to_forget = []

            for memory in all_memories:
                # è·³è¿‡å·²é—å¿˜çš„è®°å¿†
                if memory.metadata.get("forgotten", False):
                    continue

                # è·³è¿‡é«˜é‡è¦æ€§è®°å¿†ï¼ˆä¿æŠ¤é‡è¦è®°å¿†ä¸è¢«é—å¿˜ï¼‰
                if memory.importance >= min_importance:
                    continue

                # è®¡ç®—å½“å‰æ¿€æ´»åº¦ï¼ˆåº”ç”¨æ—¶é—´è¡°å‡ï¼‰
                activation_info = memory.metadata.get("activation", {})
                base_activation = activation_info.get("level", memory.activation)
                last_access = activation_info.get("last_access")

                if last_access:
                    try:
                        last_access_dt = datetime.fromisoformat(last_access)
                        days_passed = (datetime.now() - last_access_dt).days

                        # åº”ç”¨æŒ‡æ•°è¡°å‡ï¼šactivation = base * (decay_rate ^ days)
                        current_activation = base_activation * (decay_rate ** days_passed)

                        logger.debug(
                            f"è®°å¿† {memory.id[:8]}: åŸºç¡€æ¿€æ´»åº¦={base_activation:.3f}, "
                            f"ç»è¿‡{days_passed}å¤©è¡°å‡å={current_activation:.3f}"
                        )
                    except (ValueError, TypeError) as e:
                        logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {e}, ä½¿ç”¨åŸºç¡€æ¿€æ´»åº¦")
                        current_activation = base_activation
                else:
                    # æ²¡æœ‰è®¿é—®è®°å½•ï¼Œä½¿ç”¨åŸºç¡€æ¿€æ´»åº¦
                    current_activation = base_activation

                # ä½äºé˜ˆå€¼åˆ™æ ‡è®°ä¸ºå¾…é—å¿˜
                if current_activation < threshold:
                    memories_to_forget.append((memory.id, current_activation))
                    logger.debug(
                        f"æ ‡è®°é—å¿˜ {memory.id[:8]}: æ¿€æ´»åº¦={current_activation:.3f} < é˜ˆå€¼={threshold:.3f}"
                    )

            # æ‰¹é‡é—å¿˜è®°å¿†ï¼ˆä¸ç«‹å³æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼‰
            if memories_to_forget:
                logger.info(f"å¼€å§‹æ‰¹é‡é—å¿˜ {len(memories_to_forget)} æ¡è®°å¿†...")

                for memory_id, activation in memories_to_forget:
                    # cleanup_orphans=Falseï¼šæš‚ä¸æ¸…ç†å­¤ç«‹èŠ‚ç‚¹
                    success = await self.forget_memory(memory_id, cleanup_orphans=False)
                    if success:
                        forgotten_count += 1

                # ç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
                logger.info("æ‰¹é‡é—å¿˜å®Œæˆï¼Œå¼€å§‹ç»Ÿä¸€æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹...")
                orphan_nodes, orphan_edges = await self._cleanup_orphan_nodes_and_edges()

                # ä¿å­˜æœ€ç»ˆæ›´æ–°
                await self.persistence.save_graph_store(self.graph_store)

                logger.info(
                    f"âœ… è‡ªåŠ¨é—å¿˜å®Œæˆ: é—å¿˜äº† {forgotten_count} æ¡è®°å¿†, "
                    f"æ¸…ç†äº† {orphan_nodes} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges} æ¡å­¤ç«‹è¾¹"
                )
            else:
                logger.info("âœ… è‡ªåŠ¨é—å¿˜å®Œæˆ: æ²¡æœ‰éœ€è¦é—å¿˜çš„è®°å¿†")

            return forgotten_count

        except Exception as e:
            logger.error(f"è‡ªåŠ¨é—å¿˜å¤±è´¥: {e}")
            return 0

    async def _cleanup_orphan_nodes_and_edges(self) -> tuple[int, int]:
        """
        æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹

        å­¤ç«‹èŠ‚ç‚¹ï¼šä¸å†å±äºä»»ä½•è®°å¿†çš„èŠ‚ç‚¹
        å­¤ç«‹è¾¹ï¼šè¿æ¥åˆ°å·²åˆ é™¤èŠ‚ç‚¹çš„è¾¹

        Returns:
            (æ¸…ç†çš„å­¤ç«‹èŠ‚ç‚¹æ•°, æ¸…ç†çš„å­¤ç«‹è¾¹æ•°)
        """
        try:
            orphan_nodes_count = 0
            orphan_edges_count = 0

            # 1. æ¸…ç†å­¤ç«‹èŠ‚ç‚¹
            # graph_store.node_to_memories è®°å½•äº†æ¯ä¸ªèŠ‚ç‚¹å±äºå“ªäº›è®°å¿†
            nodes_to_remove = []

            for node_id, memory_ids in list(self.graph_store.node_to_memories.items()):
                # å¦‚æœèŠ‚ç‚¹ä¸å†å±äºä»»ä½•è®°å¿†ï¼Œæ ‡è®°ä¸ºåˆ é™¤
                if not memory_ids:
                    nodes_to_remove.append(node_id)

            # ä»å›¾ä¸­åˆ é™¤å­¤ç«‹èŠ‚ç‚¹
            for node_id in nodes_to_remove:
                if self.graph_store.graph.has_node(node_id):
                    self.graph_store.graph.remove_node(node_id)
                    orphan_nodes_count += 1

                # ä»æ˜ å°„ä¸­åˆ é™¤
                if node_id in self.graph_store.node_to_memories:
                    del self.graph_store.node_to_memories[node_id]

            # 2. æ¸…ç†å­¤ç«‹è¾¹ï¼ˆæŒ‡å‘å·²åˆ é™¤èŠ‚ç‚¹çš„è¾¹ï¼‰
            edges_to_remove = []

            for source, target, edge_id in self.graph_store.graph.edges(data="edge_id"):
                # æ£€æŸ¥è¾¹çš„æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹æ˜¯å¦è¿˜å­˜åœ¨äºnode_to_memoriesä¸­
                if source not in self.graph_store.node_to_memories or \
                   target not in self.graph_store.node_to_memories:
                    edges_to_remove.append((source, target))

            # åˆ é™¤å­¤ç«‹è¾¹
            for source, target in edges_to_remove:
                try:
                    self.graph_store.graph.remove_edge(source, target)
                    orphan_edges_count += 1
                except Exception as e:
                    logger.debug(f"åˆ é™¤è¾¹å¤±è´¥ {source} -> {target}: {e}")

            if orphan_nodes_count > 0 or orphan_edges_count > 0:
                logger.info(
                    f"æ¸…ç†å®Œæˆ: {orphan_nodes_count} ä¸ªå­¤ç«‹èŠ‚ç‚¹, {orphan_edges_count} æ¡å­¤ç«‹è¾¹"
                )

            return orphan_nodes_count, orphan_edges_count

        except Exception as e:
            logger.error(f"æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹å¤±è´¥: {e}")
            return 0, 0

    # ==================== ç»Ÿè®¡ä¸ç»´æŠ¤ ====================

    def get_statistics(self) -> dict[str, Any]:
        """
        è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self._initialized or not self.graph_store:
            return {}

        stats = self.graph_store.get_statistics()

        # æ·»åŠ æ¿€æ´»åº¦ç»Ÿè®¡
        all_memories = self.graph_store.get_all_memories()
        activation_levels = []
        forgotten_count = 0

        for memory in all_memories:
            if memory.metadata.get("forgotten", False):
                forgotten_count += 1
            else:
                activation_info = memory.metadata.get("activation", {})
                activation_levels.append(activation_info.get("level", 0.0))

        if activation_levels:
            stats["avg_activation"] = sum(activation_levels) / len(activation_levels)
            stats["max_activation"] = max(activation_levels)
        else:
            stats["avg_activation"] = 0.0
            stats["max_activation"] = 0.0

        stats["forgotten_memories"] = forgotten_count
        stats["active_memories"] = stats["total_memories"] - forgotten_count

        return stats

    async def consolidate_memories(
        self,
        similarity_threshold: float = 0.85,
        time_window_hours: float = 24.0,
        max_batch_size: int = 50,
    ) -> dict[str, Any]:
        """
        ç®€åŒ–çš„è®°å¿†æ•´ç†ï¼šä»…æ£€æŸ¥éœ€è¦é—å¿˜çš„è®°å¿†å¹¶æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
        
        åŠŸèƒ½ï¼š
        1. æ£€æŸ¥éœ€è¦é—å¿˜çš„è®°å¿†ï¼ˆä½æ¿€æ´»åº¦ï¼‰
        2. æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹
        
        æ³¨æ„ï¼šè®°å¿†çš„åˆ›å»ºã€åˆå¹¶ã€å…³è”ç­‰æ“ä½œå·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿè‡ªåŠ¨å¤„ç†

        Args:
            similarity_threshold: ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
            time_window_hours: ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
            max_batch_size: ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰

        Returns:
            æ•´ç†ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info("ğŸ§¹ å¼€å§‹è®°å¿†æ•´ç†ï¼šæ£€æŸ¥é—å¿˜ + æ¸…ç†å­¤ç«‹èŠ‚ç‚¹...")

            # æ­¥éª¤1: è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦çš„è®°å¿†
            forgotten_count = await self.auto_forget()

            # æ­¥éª¤2: æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å’Œè¾¹ï¼ˆauto_forgetå†…éƒ¨å·²æ‰§è¡Œï¼Œè¿™é‡Œå†æ¬¡ç¡®ä¿ï¼‰
            orphan_nodes, orphan_edges = await self._cleanup_orphan_nodes_and_edges()

            result = {
                "forgotten_count": forgotten_count,
                "orphan_nodes_cleaned": orphan_nodes,
                "orphan_edges_cleaned": orphan_edges,
                "message": "è®°å¿†æ•´ç†å®Œæˆï¼ˆä»…é—å¿˜å’Œæ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼‰"
            }

            logger.info(f"âœ… è®°å¿†æ•´ç†å®Œæˆ: {result}")
            return result

        except Exception as e:
            logger.error(f"è®°å¿†æ•´ç†å¤±è´¥: {e}")
            return {"error": str(e), "forgotten_count": 0}

    async def _consolidate_memories_background(
        self,
        similarity_threshold: float,
        time_window_hours: float,
        max_batch_size: int,
    ) -> None:
        """
        åå°æ•´ç†ä»»åŠ¡ï¼ˆå·²ç®€åŒ–ä¸ºè°ƒç”¨consolidate_memoriesï¼‰
        
        ä¿ç•™æ­¤æ–¹æ³•ç”¨äºå‘åå…¼å®¹
        """
        await self.consolidate_memories(
            similarity_threshold=similarity_threshold,
            time_window_hours=time_window_hours,
            max_batch_size=max_batch_size
        )

    # ==================== ä»¥ä¸‹æ–¹æ³•å·²åºŸå¼ƒ ====================
    # æ—§çš„è®°å¿†æ•´ç†é€»è¾‘ï¼ˆå»é‡ã€è‡ªåŠ¨å…³è”ç­‰ï¼‰å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£
    # ä¿ç•™æ–¹æ³•ç­¾åç”¨äºå‘åå…¼å®¹ï¼Œä½†ä¸å†æ‰§è¡Œå¤æ‚æ“ä½œ

    async def auto_link_memories(  # å·²åºŸå¼ƒ
        self,
        time_window_hours: float | None = None,
        max_candidates: int | None = None,
        min_confidence: float | None = None,
    ) -> dict[str, Any]:
        """
        è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆå·²åºŸå¼ƒï¼‰

        è¯¥åŠŸèƒ½å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£ã€‚è®°å¿†ä¹‹é—´çš„å…³è”ç°åœ¨é€šè¿‡æ¨¡å‹è‡ªåŠ¨å¤„ç†ã€‚

        Args:
            time_window_hours: åˆ†ææ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_candidates: æ¯ä¸ªè®°å¿†æœ€å¤šå…³è”çš„å€™é€‰æ•°
            min_confidence: æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            ç©ºç»“æœï¼ˆå‘åå…¼å®¹ï¼‰
        """
        logger.warning("auto_link_memories å·²åºŸå¼ƒï¼Œè®°å¿†å…³è”ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿè‡ªåŠ¨å¤„ç†")
        return {"checked_count": 0, "linked_count": 0, "deprecated": True}

    async def _find_link_candidates(  # å·²åºŸå¼ƒ
        self,
        memory: Memory,
        exclude_ids: set[str],
        max_results: int = 5,
    ) -> list[Memory]:
        """
        ä¸ºè®°å¿†å¯»æ‰¾å…³è”å€™é€‰ï¼ˆå·²åºŸå¼ƒï¼‰

        è¯¥åŠŸèƒ½å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£ã€‚
        """
        logger.warning("_find_link_candidates å·²åºŸå¼ƒ")
        return []

    async def _analyze_memory_relations(  # å·²åºŸå¼ƒ
        self,
        source_memory: Memory,
        candidate_memories: list[Memory],
        min_confidence: float = 0.7,
    ) -> list[dict[str, Any]]:
        """
        ä½¿ç”¨LLMåˆ†æè®°å¿†ä¹‹é—´çš„å…³ç³»ï¼ˆå·²åºŸå¼ƒï¼‰

        è¯¥åŠŸèƒ½å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£ã€‚

        Args:
            source_memory: æºè®°å¿†
            candidate_memories: å€™é€‰è®°å¿†åˆ—è¡¨
            min_confidence: æœ€ä½ç½®ä¿¡åº¦

        Returns:
            ç©ºåˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        logger.warning("_analyze_memory_relations å·²åºŸå¼ƒ")
        return []

    def _format_memory_for_llm(self, memory: Memory) -> str:  # å·²åºŸå¼ƒ
        """æ ¼å¼åŒ–è®°å¿†ä¸ºLLMå¯è¯»çš„æ–‡æœ¬ï¼ˆå·²åºŸå¼ƒï¼‰"""
        logger.warning("_format_memory_for_llm å·²åºŸå¼ƒ")
        return f"è®°å¿†ID: {memory.id}"

    async def maintenance(self) -> dict[str, Any]:
        """
        æ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼‰

        åªåŒ…æ‹¬ï¼š
        - ç®€åŒ–çš„è®°å¿†æ•´ç†ï¼ˆæ£€æŸ¥é—å¿˜+æ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼‰
        - ä¿å­˜æ•°æ®

        æ³¨æ„ï¼šè®°å¿†çš„åˆ›å»ºã€åˆå¹¶ã€å…³è”ç­‰æ“ä½œå·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿè‡ªåŠ¨å¤„ç†

        Returns:
            ç»´æŠ¤ç»“æœ
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œè®°å¿†ç³»ç»Ÿç»´æŠ¤...")

            result = {
                "forgotten": 0,
                "orphan_nodes_cleaned": 0,
                "orphan_edges_cleaned": 0,
                "saved": False,
                "total_time": 0,
            }

            start_time = datetime.now()

            # 1. ç®€åŒ–çš„è®°å¿†æ•´ç†ï¼ˆåªæ£€æŸ¥é—å¿˜å’Œæ¸…ç†å­¤ç«‹èŠ‚ç‚¹ï¼‰
            if getattr(self.config, "consolidation_enabled", False):
                consolidate_result = await self.consolidate_memories()
                result["forgotten"] = consolidate_result.get("forgotten_count", 0)
                result["orphan_nodes_cleaned"] = consolidate_result.get("orphan_nodes_cleaned", 0)
                result["orphan_edges_cleaned"] = consolidate_result.get("orphan_edges_cleaned", 0)

            # 2. ä¿å­˜æ•°æ®
            await self.persistence.save_graph_store(self.graph_store)
            result["saved"] = True

            self._last_maintenance = datetime.now()

            # è®¡ç®—ç»´æŠ¤è€—æ—¶
            total_time = (datetime.now() - start_time).total_seconds()
            result["total_time"] = total_time

            logger.info(f"âœ… ç»´æŠ¤å®Œæˆ (è€—æ—¶ {total_time:.2f}s): {result}")
            return result

        except Exception as e:
            logger.error(f"âŒ ç»´æŠ¤å¤±è´¥: {e}")
            return {"error": str(e), "total_time": 0}

    async def _lightweight_auto_link_memories(  # å·²åºŸå¼ƒ
        self,
        time_window_hours: float | None = None,
        max_candidates: int | None = None,
        max_memories: int | None = None,
    ) -> dict[str, Any]:
        """
        æ™ºèƒ½è½»é‡çº§è‡ªåŠ¨å…³è”è®°å¿†ï¼ˆå·²åºŸå¼ƒï¼‰

        è¯¥åŠŸèƒ½å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£ã€‚

        Args:
            time_window_hours: ä»é…ç½®è¯»å–
            max_candidates: ä»é…ç½®è¯»å–
            max_memories: ä»é…ç½®è¯»å–

        Returns:
            ç©ºç»“æœï¼ˆå‘åå…¼å®¹ï¼‰
        """
        logger.warning("_lightweight_auto_link_memories å·²åºŸå¼ƒ")
        return {"checked_count": 0, "linked_count": 0, "deprecated": True}

    async def _batch_analyze_memory_relations(  # å·²åºŸå¼ƒ
        self,
        candidate_pairs: list[tuple[Memory, Memory, float]]
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æè®°å¿†å…³ç³»ï¼ˆå·²åºŸå¼ƒï¼‰

        è¯¥åŠŸèƒ½å·²ç”±ä¸‰çº§è®°å¿†ç³»ç»Ÿå–ä»£ã€‚

        Args:
            candidate_pairs: å€™é€‰è®°å¿†å¯¹åˆ—è¡¨

        Returns:
            ç©ºåˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        logger.warning("_batch_analyze_memory_relations å·²åºŸå¼ƒ")
        return []

    def _start_maintenance_task(self) -> None:
        """
        å¯åŠ¨è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡

        ç›´æ¥åˆ›å»ºasync taskï¼Œé¿å…ä½¿ç”¨scheduleré˜»å¡ä¸»ç¨‹åºï¼š
        - è®°å¿†æ•´åˆï¼ˆåˆå¹¶ç›¸ä¼¼è®°å¿†ï¼‰
        - è‡ªåŠ¨é—å¿˜ä½æ¿€æ´»åº¦è®°å¿†
        - ä¿å­˜æ•°æ®

        é»˜è®¤é—´éš”ï¼š1å°æ—¶
        """
        try:
            # å¦‚æœå·²æœ‰ç»´æŠ¤ä»»åŠ¡ï¼Œå…ˆåœæ­¢
            if self._maintenance_task and not self._maintenance_task.done():
                self._maintenance_task.cancel()
                logger.info("å–æ¶ˆæ—§çš„ç»´æŠ¤ä»»åŠ¡")

            # åˆ›å»ºæ–°çš„åå°ç»´æŠ¤ä»»åŠ¡
            self._maintenance_task = asyncio.create_task(
                self._maintenance_loop(),
                name="memory_maintenance_loop"
            )

            logger.info(
                f"âœ… è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡å·²å¯åŠ¨ "
                f"(é—´éš”={self._maintenance_interval_hours}å°æ—¶)"
            )

        except Exception as e:
            logger.error(f"å¯åŠ¨ç»´æŠ¤åå°ä»»åŠ¡å¤±è´¥: {e}")

    async def _stop_maintenance_task(self) -> None:
        """
        åœæ­¢è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡
        """
        if not self._maintenance_task or self._maintenance_task.done():
            return

        try:
            self._maintenance_running = False  # è®¾ç½®åœæ­¢æ ‡å¿—
            self._maintenance_task.cancel()

            try:
                await self._maintenance_task
            except asyncio.CancelledError:
                logger.debug("ç»´æŠ¤ä»»åŠ¡å·²å–æ¶ˆ")

            logger.info("âœ… è®°å¿†ç»´æŠ¤åå°ä»»åŠ¡å·²åœæ­¢")
            self._maintenance_task = None

        except Exception as e:
            logger.error(f"åœæ­¢ç»´æŠ¤åå°ä»»åŠ¡å¤±è´¥: {e}")

    async def _maintenance_loop(self) -> None:
        """
        è®°å¿†ç»´æŠ¤å¾ªç¯

        åœ¨åå°ç‹¬ç«‹è¿è¡Œï¼Œå®šæœŸæ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼Œé¿å…é˜»å¡ä¸»ç¨‹åº
        """
        self._maintenance_running = True

        try:
            # é¦–æ¬¡æ‰§è¡Œå»¶è¿Ÿï¼ˆå¯åŠ¨å1å°æ—¶ï¼‰
            initial_delay = self._maintenance_interval_hours * 3600
            logger.debug(f"è®°å¿†ç»´æŠ¤ä»»åŠ¡å°†åœ¨ {initial_delay} ç§’åé¦–æ¬¡æ‰§è¡Œ")

            while self._maintenance_running:
                try:
                    # ä½¿ç”¨ asyncio.wait_for æ¥æ”¯æŒå–æ¶ˆ
                    await asyncio.wait_for(
                        asyncio.sleep(initial_delay),
                        timeout=float("inf")  # å…è®¸éšæ—¶å–æ¶ˆ
                    )

                    # æ£€æŸ¥æ˜¯å¦ä»ç„¶éœ€è¦è¿è¡Œ
                    if not self._maintenance_running:
                        break

                    # æ‰§è¡Œç»´æŠ¤ä»»åŠ¡ï¼ˆä½¿ç”¨try-catché¿å…å´©æºƒï¼‰
                    try:
                        await self.maintenance()
                    except Exception as e:
                        logger.error(f"ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

                    # åç»­æ‰§è¡Œä½¿ç”¨ç›¸åŒé—´éš”
                    initial_delay = self._maintenance_interval_hours * 3600

                except asyncio.CancelledError:
                    logger.debug("ç»´æŠ¤å¾ªç¯è¢«å–æ¶ˆ")
                    break
                except Exception as e:
                    logger.error(f"ç»´æŠ¤å¾ªç¯å‘ç”Ÿå¼‚å¸¸: {e}")
                    # å¼‚å¸¸åç­‰å¾…è¾ƒçŸ­æ—¶é—´å†é‡è¯•
                    try:
                        await asyncio.sleep(300)  # 5åˆ†é’Ÿåé‡è¯•
                    except asyncio.CancelledError:
                        break

        except asyncio.CancelledError:
            logger.debug("ç»´æŠ¤å¾ªç¯å®Œå…¨é€€å‡º")
        except Exception as e:
            logger.error(f"ç»´æŠ¤å¾ªç¯æ„å¤–ç»“æŸ: {e}")
        finally:
            self._maintenance_running = False
            logger.debug("ç»´æŠ¤å¾ªç¯å·²æ¸…ç†å®Œæ¯•")

    async def _async_save_graph_store(self, operation_name: str = "æœªçŸ¥æ“ä½œ") -> None:
        """
        å¼‚æ­¥ä¿å­˜å›¾å­˜å‚¨åˆ°ç£ç›˜

        æ­¤æ–¹æ³•è®¾è®¡ä¸ºåœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼ŒåŒ…å«é”™è¯¯å¤„ç†

        Args:
            operation_name: æ“ä½œåç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•
        """
        try:
            # ç¡®ä¿å›¾å­˜å‚¨å­˜åœ¨ä¸”å·²åˆå§‹åŒ–
            if self.graph_store is None:
                logger.warning(f"å›¾å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¼‚æ­¥ä¿å­˜: {operation_name}")
                return

            if self.persistence is None:
                logger.warning(f"æŒä¹…åŒ–ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¼‚æ­¥ä¿å­˜: {operation_name}")
                return

            await self.persistence.save_graph_store(self.graph_store)
            logger.debug(f"å¼‚æ­¥ä¿å­˜å›¾æ•°æ®æˆåŠŸ: {operation_name}")
        except Exception as e:
            logger.error(f"å¼‚æ­¥ä¿å­˜å›¾æ•°æ®å¤±è´¥ ({operation_name}): {e}")
            # å¯ä»¥è€ƒè™‘æ·»åŠ é‡è¯•æœºåˆ¶æˆ–è€…é€šçŸ¥æœºåˆ¶
