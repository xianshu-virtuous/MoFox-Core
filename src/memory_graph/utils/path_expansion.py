"""
è·¯å¾„è¯„åˆ†æ‰©å±•ç®—æ³•

åŸºäºå›¾è·¯å¾„ä¼ æ’­çš„è®°å¿†æ£€ç´¢ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. ä»å‘é‡æœç´¢çš„TopKèŠ‚ç‚¹å‡ºå‘ï¼Œåˆ›å»ºåˆå§‹è·¯å¾„
2. æ²¿è¾¹æ‰©å±•è·¯å¾„ï¼Œåˆ†æ•°é€šè¿‡è¾¹æƒé‡å’ŒèŠ‚ç‚¹ç›¸ä¼¼åº¦ä¼ æ’­
3. è·¯å¾„ç›¸é‡æ—¶åˆå¹¶ï¼Œç›¸äº¤æ—¶å‰ªæ
4. æœ€ç»ˆæ ¹æ®è·¯å¾„è´¨é‡å¯¹è®°å¿†è¯„åˆ†

æ ¸å¿ƒç‰¹æ€§ï¼š
- æŒ‡æ•°è¡°å‡çš„åˆ†æ•°ä¼ æ’­
- åŠ¨æ€åˆ†å‰æ•°é‡æ§åˆ¶
- è·¯å¾„åˆå¹¶ä¸å‰ªæä¼˜åŒ–
- å¤šç»´åº¦æœ€ç»ˆè¯„åˆ†
"""

import asyncio
import heapq
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import TYPE_CHECKING, Any

from src.common.logger import get_logger
from src.memory_graph.utils.similarity import cosine_similarity_async

if TYPE_CHECKING:
    import numpy as np

    from src.memory_graph.storage.graph_store import GraphStore
    from src.memory_graph.storage.vector_store import VectorStore

logger = get_logger(__name__)


@dataclass
class Path:
    """è¡¨ç¤ºä¸€æ¡è·¯å¾„"""

    nodes: list[str] = field(default_factory=list)  # èŠ‚ç‚¹IDåºåˆ—
    edges: list[Any] = field(default_factory=list)  # è¾¹åºåˆ—
    score: float = 0.0  # å½“å‰è·¯å¾„åˆ†æ•°
    depth: int = 0  # è·¯å¾„æ·±åº¦
    parent: "Path | None" = None  # çˆ¶è·¯å¾„ï¼ˆç”¨äºè¿½è¸ªï¼‰
    is_merged: bool = False  # æ˜¯å¦ä¸ºåˆå¹¶è·¯å¾„
    merged_from: list["Path"] = field(default_factory=list)  # åˆå¹¶æ¥æºè·¯å¾„

    def __hash__(self):
        """ä½¿è·¯å¾„å¯å“ˆå¸Œï¼ˆåŸºäºèŠ‚ç‚¹åºåˆ—ï¼‰"""
        return hash(tuple(self.nodes))

    def get_leaf_node(self) -> str | None:
        """è·å–å¶å­èŠ‚ç‚¹ï¼ˆè·¯å¾„ç»ˆç‚¹ï¼‰"""
        return self.nodes[-1] if self.nodes else None

    def contains_node(self, node_id: str) -> bool:
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«æŸä¸ªèŠ‚ç‚¹"""
        return node_id in self.nodes


@dataclass
class PathExpansionConfig:
    """è·¯å¾„æ‰©å±•é…ç½®"""

    max_hops: int = 2  # æœ€å¤§è·³æ•°
    damping_factor: float = 0.85  # è¡°å‡å› å­ï¼ˆPageRanké£æ ¼ï¼‰
    max_branches_per_node: int = 10  # æ¯èŠ‚ç‚¹æœ€å¤§åˆ†å‰æ•°
    path_merge_strategy: str = "weighted_geometric"  # è·¯å¾„åˆå¹¶ç­–ç•¥: weighted_geometric, max_bonus
    pruning_threshold: float = 0.9  # å‰ªæé˜ˆå€¼ï¼ˆæ–°è·¯å¾„åˆ†æ•°éœ€è¾¾åˆ°å·²æœ‰è·¯å¾„çš„90%ï¼‰
    high_score_threshold: float = 0.7  # é«˜åˆ†è·¯å¾„é˜ˆå€¼
    medium_score_threshold: float = 0.4  # ä¸­åˆ†è·¯å¾„é˜ˆå€¼
    max_active_paths: int = 1000  # æœ€å¤§æ´»è·ƒè·¯å¾„æ•°ï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
    top_paths_retain: int = 500  # è¶…é™æ—¶ä¿ç•™çš„topè·¯å¾„æ•°

    # ğŸš€ æ€§èƒ½ä¼˜åŒ–å‚æ•°
    enable_early_stop: bool = True  # å¯ç”¨æ—©åœï¼ˆå¦‚æœè·¯å¾„å¢é•¿å¾ˆå°‘åˆ™æå‰ç»“æŸï¼‰
    early_stop_growth_threshold: float = 0.1  # æ—©åœé˜ˆå€¼ï¼ˆè·¯å¾„å¢é•¿ç‡ä½äº10%åˆ™åœæ­¢ï¼‰
    max_candidate_memories: int = 200  # ğŸ†• æœ€å¤§å€™é€‰è®°å¿†æ•°ï¼ˆåœ¨æœ€ç»ˆè¯„åˆ†å‰è¿‡æ»¤ï¼‰
    min_path_count_for_memory: int = 1  # ğŸ†• è®°å¿†è‡³å°‘éœ€è¦çš„è·¯å¾„æ•°ï¼ˆè¿‡æ»¤å¼±å…³è”è®°å¿†ï¼‰

    # è¾¹ç±»å‹æƒé‡é…ç½®
    edge_type_weights: dict[str, float] = field(
        default_factory=lambda: {
            "REFERENCE": 1.3,  # å¼•ç”¨å…³ç³»æƒé‡æœ€é«˜
            "ATTRIBUTE": 1.2,  # å±æ€§å…³ç³»
            "HAS_PROPERTY": 1.2,  # å±æ€§å…³ç³»
            "RELATION": 0.9,  # ä¸€èˆ¬å…³ç³»é€‚ä¸­é™æƒ
            "TEMPORAL": 0.7,  # æ—¶é—´å…³ç³»é™æƒ
            "DEFAULT": 1.0,  # é»˜è®¤æƒé‡
        }
    )

    # æœ€ç»ˆè¯„åˆ†æƒé‡
    final_scoring_weights: dict[str, float] = field(
        default_factory=lambda: {
            "path_score": 0.50,  # è·¯å¾„åˆ†æ•°å 50%
            "importance": 0.30,  # é‡è¦æ€§å 30%
            "recency": 0.20,  # æ—¶æ•ˆæ€§å 20%
        }
    )


class PathScoreExpansion:
    """è·¯å¾„è¯„åˆ†æ‰©å±•ç®—æ³•å®ç°"""

    def __init__(
        self,
        graph_store: "GraphStore",
        vector_store: "VectorStore",
        config: PathExpansionConfig | None = None,
    ):
        """
        åˆå§‹åŒ–è·¯å¾„æ‰©å±•å™¨

        Args:
            graph_store: å›¾å­˜å‚¨
            vector_store: å‘é‡å­˜å‚¨
            config: æ‰©å±•é…ç½®
        """
        self.graph_store = graph_store
        self.vector_store = vector_store
        self.config = config or PathExpansionConfig()
        self.prefer_node_types: list[str] = []  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé‚»å±…è¾¹ç¼“å­˜
        self._neighbor_cache: dict[str, list[Any]] = {}
        self._node_score_cache: dict[str, float] = {}

        logger.debug(
            f"PathScoreExpansion åˆå§‹åŒ–: max_hops={self.config.max_hops}, "
            f"damping={self.config.damping_factor}, "
            f"merge_strategy={self.config.path_merge_strategy}"
        )

    async def expand_with_path_scoring(
        self,
        initial_nodes: list[tuple[str, float, dict[str, Any]]],  # (node_id, score, metadata)
        query_embedding: "np.ndarray | None",
        top_k: int = 20,
        prefer_node_types: list[str] | None = None,  # ğŸ†• åå¥½èŠ‚ç‚¹ç±»å‹
    ) -> list[tuple[Any, float, list[Path]]]:
        """
        ä½¿ç”¨è·¯å¾„è¯„åˆ†è¿›è¡Œå›¾æ‰©å±•

        Args:
            initial_nodes: åˆå§‹èŠ‚ç‚¹åˆ—è¡¨ï¼ˆæ¥è‡ªå‘é‡æœç´¢ï¼‰
            query_embedding: æŸ¥è¯¢å‘é‡ï¼ˆç”¨äºè®¡ç®—èŠ‚ç‚¹ç›¸ä¼¼åº¦ï¼‰
            top_k: è¿”å›çš„topè®°å¿†æ•°é‡
            prefer_node_types: åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆç”±LLMè¯†åˆ«ï¼‰ï¼Œå¦‚ ["EVENT", "ENTITY"]

        Returns:
            [(Memory, final_score, contributing_paths), ...]
        """
        start_time = time.time()

        if not initial_nodes:
            logger.warning("åˆå§‹èŠ‚ç‚¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè·¯å¾„æ‰©å±•")
            return []

        # ğŸš€ æ¸…ç©ºç¼“å­˜ï¼ˆæ¯æ¬¡æŸ¥è¯¢é‡æ–°å¼€å§‹ï¼‰
        self._neighbor_cache.clear()
        self._node_score_cache.clear()

        # ä¿å­˜åå¥½ç±»å‹
        self.prefer_node_types = prefer_node_types or []
        if self.prefer_node_types:
            logger.debug(f"åå¥½èŠ‚ç‚¹ç±»å‹: {self.prefer_node_types}")

        # 1. åˆå§‹åŒ–è·¯å¾„
        active_paths = []
        best_score_to_node: dict[str, float] = {}  # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„æœ€ä½³åˆ°è¾¾åˆ†æ•°

        for node_id, score, metadata in initial_nodes:
            path = Path(nodes=[node_id], edges=[], score=score, depth=0)
            active_paths.append(path)
            best_score_to_node[node_id] = score

        logger.debug(f"è·¯å¾„æ‰©å±•å¼€å§‹: {len(active_paths)} æ¡åˆå§‹è·¯å¾„")

        # 2. å¤šè·³æ‰©å±•
        hop_stats = []  # æ¯è·³ç»Ÿè®¡ä¿¡æ¯

        for hop in range(self.config.max_hops):
            hop_start = time.time()
            next_paths = []
            branches_created = 0
            paths_merged = 0
            paths_pruned = 0

            # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦è¯„åˆ†çš„å€™é€‰èŠ‚ç‚¹ï¼Œç„¶åæ‰¹é‡è®¡ç®—
            candidate_nodes_for_batch = set()
            path_candidates: list[tuple[Path, Any, str, float]] = []  # (path, edge, next_node, edge_weight)

            # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰å€™é€‰èŠ‚ç‚¹
            for path in active_paths:
                current_node = path.get_leaf_node()
                if not current_node:
                    continue

                # è·å–æ’åºåçš„é‚»å±…è¾¹
                neighbor_edges = await self._get_sorted_neighbor_edges(current_node)

                # åŠ¨æ€è®¡ç®—æœ€å¤§åˆ†å‰æ•°
                max_branches = self._calculate_max_branches(path.score)
                branch_count = 0

                for edge in neighbor_edges[:max_branches]:
                    next_node = edge.target_id if edge.source_id == current_node else edge.source_id

                    # é¿å…ç¯è·¯
                    if path.contains_node(next_node):
                        continue

                    edge_weight = self._get_edge_weight(edge)

                    # è®°å½•å€™é€‰
                    path_candidates.append((path, edge, next_node, edge_weight))
                    candidate_nodes_for_batch.add(next_node)

                    branch_count += 1
                    if branch_count >= max_branches:
                        break

            # ğŸš€ ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰å€™é€‰èŠ‚ç‚¹çš„åˆ†æ•°
            if candidate_nodes_for_batch:
                batch_node_scores = await self._batch_get_node_scores(
                    list(candidate_nodes_for_batch), query_embedding
                )
            else:
                batch_node_scores = {}

            # ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šä½¿ç”¨æ‰¹é‡è®¡ç®—çš„åˆ†æ•°åˆ›å»ºè·¯å¾„
            for path, edge, next_node, edge_weight in path_candidates:
                node_score = batch_node_scores.get(next_node, 0.3)

                new_score = self._calculate_path_score(
                    old_score=path.score,
                    edge_weight=edge_weight,
                    node_score=node_score,
                    depth=hop + 1,
                )

                # å‰ªæï¼šå¦‚æœåˆ°è¾¾è¯¥èŠ‚ç‚¹çš„åˆ†æ•°è¿œä½äºå·²æœ‰æœ€ä¼˜è·¯å¾„ï¼Œè·³è¿‡
                if next_node in best_score_to_node:
                    if new_score < best_score_to_node[next_node] * self.config.pruning_threshold:
                        paths_pruned += 1
                        continue

                # æ›´æ–°æœ€ä½³åˆ†æ•°
                best_score_to_node[next_node] = max(best_score_to_node.get(next_node, 0), new_score)

                # åˆ›å»ºæ–°è·¯å¾„
                new_path = Path(
                    nodes=path.nodes + [next_node],
                    edges=path.edges + [edge],
                    score=new_score,
                    depth=hop + 1,
                    parent=path,
                )

                # å°è¯•è·¯å¾„åˆå¹¶
                merged_path = self._try_merge_paths(new_path, next_paths)
                if merged_path:
                    next_paths.append(merged_path)
                    paths_merged += 1
                else:
                    next_paths.append(new_path)

                branches_created += 1

            # è·¯å¾„æ•°é‡æ§åˆ¶ï¼šå¦‚æœçˆ†ç‚¸æ€§å¢é•¿ï¼Œä¿ç•™é«˜åˆ†è·¯å¾„
            if len(next_paths) > self.config.max_active_paths:
                logger.warning(
                    f"âš ï¸  è·¯å¾„æ•°é‡è¶…é™ ({len(next_paths)} > {self.config.max_active_paths})ï¼Œ"
                    f"ä¿ç•™ top {self.config.top_paths_retain}"
                )
                retain = min(self.config.top_paths_retain, len(next_paths))
                next_paths = heapq.nlargest(retain, next_paths, key=lambda p: p.score)

            # ğŸš€ æ—©åœæ£€æµ‹ï¼šå¦‚æœè·¯å¾„å¢é•¿å¾ˆå°‘ï¼Œæå‰ç»ˆæ­¢
            prev_path_count = len(active_paths)
            active_paths = next_paths

            if self.config.enable_early_stop and prev_path_count > 0:
                growth_rate = (len(active_paths) - prev_path_count) / prev_path_count
                if growth_rate < self.config.early_stop_growth_threshold:
                    logger.debug(
                        f"æ—©åœè§¦å‘: è·¯å¾„å¢é•¿ç‡ {growth_rate:.2%} < {self.config.early_stop_growth_threshold:.0%}, "
                        f"åœ¨ç¬¬ {hop+1}/{self.config.max_hops} è·³åœæ­¢"
                    )
                    hop_time = time.time() - hop_start
                    hop_stats.append(
                        {
                            "hop": hop + 1,
                            "paths": len(active_paths),
                            "branches": branches_created,
                            "merged": paths_merged,
                            "pruned": paths_pruned,
                            "time": hop_time,
                            "early_stopped": True,
                        }
                    )
                    break

            hop_time = time.time() - hop_start
            hop_stats.append(
                {
                    "hop": hop + 1,
                    "paths": len(active_paths),
                    "branches": branches_created,
                    "merged": paths_merged,
                    "pruned": paths_pruned,
                    "time": hop_time,
                }
            )

            logger.debug(
                f"  Hop {hop+1}/{self.config.max_hops}: "
                f"{len(active_paths)} æ¡è·¯å¾„, "
                f"{branches_created} åˆ†å‰, "
                f"{paths_merged} åˆå¹¶, "
                f"{paths_pruned} å‰ªæ, "
                f"{hop_time:.3f}s"
            )

            # æ—©åœï¼šå¦‚æœæ²¡æœ‰æ–°è·¯å¾„
            if not active_paths:
                logger.debug(f"æå‰åœæ­¢ï¼šç¬¬ {hop+1} è·³æ— æ–°è·¯å¾„")
                break

        # 3. æå–å¶å­è·¯å¾„ï¼ˆæœ€å°å­è·¯å¾„ï¼‰
        leaf_paths = self._extract_leaf_paths(active_paths)
        logger.debug(f"æå– {len(leaf_paths)} æ¡å¶å­è·¯å¾„")

        # 4. è·¯å¾„åˆ°è®°å¿†çš„æ˜ å°„
        memory_paths = await self._map_paths_to_memories(leaf_paths)
        logger.debug(f"æ˜ å°„åˆ° {len(memory_paths)} æ¡å€™é€‰è®°å¿†")

        # ğŸš€ 4.5. ç²—æ’è¿‡æ»¤ï¼šåœ¨è¯¦ç»†è¯„åˆ†å‰è¿‡æ»¤æ‰ä½è´¨é‡è®°å¿†
        if len(memory_paths) > self.config.max_candidate_memories:
            # æŒ‰è·¯å¾„æ•°é‡å’Œè·¯å¾„æœ€å¤§åˆ†æ•°è¿›è¡Œç²—æ’
            memory_scores_rough = []
            for mem_id, (memory, paths) in memory_paths.items():
                # ç®€å•è¯„åˆ†ï¼šè·¯å¾„æ•°é‡ Ã— æœ€é«˜è·¯å¾„åˆ†æ•° Ã— é‡è¦æ€§
                max_path_score = max(p.score for p in paths) if paths else 0
                rough_score = len(paths) * max_path_score * memory.importance
                memory_scores_rough.append((mem_id, rough_score))

            # ä¿ç•™topå€™é€‰
            memory_scores_rough.sort(key=lambda x: x[1], reverse=True)
            retained_mem_ids = set(mem_id for mem_id, _ in memory_scores_rough[:self.config.max_candidate_memories])

            # è¿‡æ»¤
            memory_paths = {
                mem_id: (memory, paths)
                for mem_id, (memory, paths) in memory_paths.items()
                if mem_id in retained_mem_ids
            }

            logger.debug(
                f"ç²—æ’è¿‡æ»¤: {len(memory_scores_rough)} â†’ {len(memory_paths)} æ¡å€™é€‰è®°å¿†"
            )

        # 5. æœ€ç»ˆè¯„åˆ†
        scored_memories = await self._final_scoring(memory_paths)

        # 6. æ’åºå¹¶è¿”å›TopK
        scored_memories.sort(key=lambda x: x[1], reverse=True)
        result = scored_memories[:top_k]

        elapsed = time.time() - start_time
        logger.debug(
            f"è·¯å¾„æ‰©å±•å®Œæˆ: {len(initial_nodes)} ä¸ªåˆå§‹èŠ‚ç‚¹ â†’ "
            f"{len(result)} æ¡è®°å¿† (è€—æ—¶ {elapsed:.3f}s)"
        )

        # è¾“å‡ºæ¯è·³ç»Ÿè®¡
        for stat in hop_stats:
            logger.debug(
                f"  ç»Ÿè®¡ Hop{stat['hop']}: {stat['paths']}è·¯å¾„, "
                f"{stat['branches']}åˆ†å‰, {stat['merged']}åˆå¹¶, "
                f"{stat['pruned']}å‰ªæ, {stat['time']:.3f}s"
            )

        return result

    async def _get_sorted_neighbor_edges(self, node_id: str) -> list[Any]:
        """
        è·å–èŠ‚ç‚¹çš„æ’åºé‚»å±…è¾¹ï¼ˆæŒ‰è¾¹æƒé‡æ’åºï¼‰- å¸¦ç¼“å­˜ä¼˜åŒ–

        Args:
            node_id: èŠ‚ç‚¹ID

        Returns:
            æ’åºåçš„è¾¹åˆ—è¡¨
        """
        # ğŸš€ ç¼“å­˜æ£€æŸ¥
        if node_id in self._neighbor_cache:
            return self._neighbor_cache[node_id]

        edges = self.graph_store.get_edges_for_node(node_id)

        if not edges:
            self._neighbor_cache[node_id] = []
            return []

        # æŒ‰è¾¹æƒé‡æ’åº
        unique_edges = sorted(edges, key=lambda e: self._get_edge_weight(e), reverse=True)

        # ğŸš€ å­˜å…¥ç¼“å­˜
        self._neighbor_cache[node_id] = unique_edges

        return unique_edges

    def _get_edge_weight(self, edge: Any) -> float:
        """
        è·å–è¾¹çš„æƒé‡

        Args:
            edge: è¾¹å¯¹è±¡

        Returns:
            è¾¹æƒé‡
        """
        # åŸºç¡€æƒé‡ï¼šè¾¹è‡ªèº«çš„é‡è¦æ€§
        base_weight = getattr(edge, "importance", 0.5)

        # è¾¹ç±»å‹æƒé‡
        edge_type_str = edge.edge_type.value if hasattr(edge.edge_type, "value") else str(edge.edge_type)
        type_weight = self.config.edge_type_weights.get(edge_type_str, self.config.edge_type_weights["DEFAULT"])

        # ç»¼åˆæƒé‡
        return base_weight * type_weight

    async def _get_node_score(self, node_id: str, query_embedding: "np.ndarray | None") -> float:
        """
        è·å–èŠ‚ç‚¹åˆ†æ•°ï¼ˆåŸºäºä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦ + åå¥½ç±»å‹åŠ æˆï¼‰

        Args:
            node_id: èŠ‚ç‚¹ID
            query_embedding: æŸ¥è¯¢å‘é‡

        Returns:
            èŠ‚ç‚¹åˆ†æ•°ï¼ˆ0.0-1.0ï¼Œåå¥½ç±»å‹èŠ‚ç‚¹å¯èƒ½è¶…è¿‡1.0ï¼‰
        """
        # ä»å‘é‡å­˜å‚¨è·å–èŠ‚ç‚¹æ•°æ®
        node_data = await self.vector_store.get_node_by_id(node_id)

        if query_embedding is None:
            base_score = 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
        else:
            if not node_data or "embedding" not in node_data:
                base_score = 0.3  # æ— å‘é‡çš„èŠ‚ç‚¹ç»™ä½åˆ†
            else:
                node_embedding = node_data["embedding"]
                similarity = await cosine_similarity_async(query_embedding, node_embedding)
                base_score = max(0.0, min(1.0, similarity))  # é™åˆ¶åœ¨[0, 1]

        # ğŸ†• åå¥½ç±»å‹åŠ æˆ
        if self.prefer_node_types and node_data:
            metadata = node_data.get("metadata", {})
            node_type = metadata.get("node_type")
            if node_type and node_type in self.prefer_node_types:
                # ç»™äºˆ20%çš„åˆ†æ•°åŠ æˆ
                bonus = base_score * 0.2
                logger.debug(f"èŠ‚ç‚¹ {node_id[:8]} ç±»å‹ {node_type} åŒ¹é…åå¥½ï¼ŒåŠ æˆ {bonus:.3f}")
                return base_score + bonus

        return base_score

    async def _batch_get_node_scores(
        self, node_ids: list[str], query_embedding: "np.ndarray | None"
    ) -> dict[str, float]:
        """
        æ‰¹é‡è·å–èŠ‚ç‚¹åˆ†æ•°ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            node_ids: èŠ‚ç‚¹IDåˆ—è¡¨
            query_embedding: æŸ¥è¯¢å‘é‡

        Returns:
            {node_id: score} å­—å…¸
        """
        import numpy as np

        scores = {}

        if query_embedding is None:
            # æ— æŸ¥è¯¢å‘é‡æ—¶ï¼Œè¿”å›é»˜è®¤åˆ†æ•°
            return dict.fromkeys(node_ids, 0.5)

        # æ‰¹é‡è·å–èŠ‚ç‚¹æ•°æ®
        node_data_list = await asyncio.gather(
            *[self.vector_store.get_node_by_id(nid) for nid in node_ids],
            return_exceptions=True
        )

        # æ”¶é›†æœ‰æ•ˆçš„åµŒå…¥å‘é‡
        valid_embeddings = []
        valid_node_ids = []
        node_metadata_map = {}

        for nid, node_data in zip(node_ids, node_data_list):
            if isinstance(node_data, Exception):
                scores[nid] = 0.3
                continue

            # ç±»å‹å®ˆå«ï¼šç¡®ä¿ node_data æ˜¯å­—å…¸
            if not node_data or not isinstance(node_data, dict) or "embedding" not in node_data:
                scores[nid] = 0.3
            else:
                valid_embeddings.append(node_data["embedding"])
                valid_node_ids.append(nid)
                node_metadata_map[nid] = node_data.get("metadata", {})

        if valid_embeddings:
            # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨çŸ©é˜µè¿ç®—ï¼‰- ç§»è‡³to_threadæ‰§è¡Œ
            similarities = await asyncio.to_thread(self._batch_compute_similarities, valid_embeddings, query_embedding)

            # åº”ç”¨åå¥½ç±»å‹åŠ æˆ
            for nid, sim in zip(valid_node_ids, similarities):
                base_score = float(sim)

                # åå¥½ç±»å‹åŠ æˆ
                if self.prefer_node_types and nid in node_metadata_map:
                    node_type = node_metadata_map[nid].get("node_type")
                    if node_type and node_type in self.prefer_node_types:
                        bonus = base_score * 0.2
                        scores[nid] = base_score + bonus
                    else:
                        scores[nid] = base_score
                else:
                    scores[nid] = base_score

        return scores

    def _calculate_path_score(self, old_score: float, edge_weight: float, node_score: float, depth: int) -> float:
        """
        è®¡ç®—è·¯å¾„åˆ†æ•°ï¼ˆæ ¸å¿ƒå…¬å¼ï¼‰

        ä½¿ç”¨æŒ‡æ•°è¡°å‡ + è¾¹æƒé‡ä¼ æ’­ + èŠ‚ç‚¹åˆ†æ•°æ³¨å…¥

        Args:
            old_score: æ—§è·¯å¾„åˆ†æ•°
            edge_weight: è¾¹æƒé‡
            node_score: æ–°èŠ‚ç‚¹åˆ†æ•°
            depth: å½“å‰æ·±åº¦

        Returns:
            æ–°è·¯å¾„åˆ†æ•°
        """
        # æŒ‡æ•°è¡°å‡å› å­
        decay = self.config.damping_factor**depth

        # ä¼ æ’­åˆ†æ•°ï¼šæ—§åˆ†æ•° Ã— è¾¹æƒé‡ Ã— è¡°å‡
        propagated_score = old_score * edge_weight * decay

        # æ–°é²œåˆ†æ•°ï¼šèŠ‚ç‚¹åˆ†æ•° Ã— (1 - è¡°å‡)
        fresh_score = node_score * (1 - decay)

        return propagated_score + fresh_score

    def _calculate_max_branches(self, path_score: float) -> int:
        """
        åŠ¨æ€è®¡ç®—æœ€å¤§åˆ†å‰æ•°

        Args:
            path_score: è·¯å¾„åˆ†æ•°

        Returns:
            æœ€å¤§åˆ†å‰æ•°
        """
        if path_score > self.config.high_score_threshold:
            return int(self.config.max_branches_per_node * 1.5)  # é«˜åˆ†è·¯å¾„å¤šæ¢ç´¢
        elif path_score > self.config.medium_score_threshold:
            return self.config.max_branches_per_node
        else:
            return int(self.config.max_branches_per_node * 0.5)  # ä½åˆ†è·¯å¾„å°‘æ¢ç´¢

    def _try_merge_paths(self, new_path: Path, existing_paths: list[Path]) -> Path | None:
        """
        å°è¯•è·¯å¾„åˆå¹¶ï¼ˆç«¯ç‚¹ç›¸é‡ï¼‰

        Args:
            new_path: æ–°è·¯å¾„
            existing_paths: ç°æœ‰è·¯å¾„åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„è·¯å¾„ï¼Œå¦‚æœä¸åˆå¹¶åˆ™è¿”å› None
        """
        endpoint = new_path.get_leaf_node()
        if not endpoint:
            return None

        for existing in existing_paths:
            if existing.get_leaf_node() == endpoint and not existing.is_merged:
                # ç«¯ç‚¹ç›¸é‡ï¼Œåˆå¹¶è·¯å¾„
                merged_score = self._merge_score(new_path.score, existing.score)

                merged_path = Path(
                    nodes=new_path.nodes,  # ä¿ç•™æ–°è·¯å¾„çš„èŠ‚ç‚¹åºåˆ—
                    edges=new_path.edges,
                    score=merged_score,
                    depth=new_path.depth,
                    parent=new_path.parent,
                    is_merged=True,
                    merged_from=[new_path, existing],
                )

                # ä»ç°æœ‰åˆ—è¡¨ä¸­ç§»é™¤è¢«åˆå¹¶çš„è·¯å¾„
                existing_paths.remove(existing)

                logger.debug(f"ğŸ”€ è·¯å¾„åˆå¹¶: {new_path.score:.3f} + {existing.score:.3f} â†’ {merged_score:.3f}")

                return merged_path

        return None

    def _merge_score(self, score1: float, score2: float) -> float:
        """
        åˆå¹¶ä¸¤æ¡è·¯å¾„çš„åˆ†æ•°

        Args:
            score1: è·¯å¾„1åˆ†æ•°
            score2: è·¯å¾„2åˆ†æ•°

        Returns:
            åˆå¹¶ååˆ†æ•°
        """
        if self.config.path_merge_strategy == "weighted_geometric":
            # å‡ ä½•å¹³å‡ Ã— 1.2 åŠ æˆ
            return (score1 * score2) ** 0.5 * 1.2
        elif self.config.path_merge_strategy == "max_bonus":
            # å–æœ€å¤§å€¼ Ã— 1.3 åŠ æˆ
            return max(score1, score2) * 1.3
        else:
            # é»˜è®¤ï¼šç®—æœ¯å¹³å‡ Ã— 1.15 åŠ æˆ
            return (score1 + score2) / 2 * 1.15

    def _extract_leaf_paths(self, all_paths: list[Path]) -> list[Path]:
        """
        æå–å¶å­è·¯å¾„ï¼ˆæœ€å°å­è·¯å¾„ï¼‰

        Args:
            all_paths: æ‰€æœ‰è·¯å¾„

        Returns:
            å¶å­è·¯å¾„åˆ—è¡¨
        """
        # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
        children_map: dict[int, list[Path]] = {}
        for path in all_paths:
            if path.parent:
                parent_id = id(path.parent)
                if parent_id not in children_map:
                    children_map[parent_id] = []
                children_map[parent_id].append(path)

        # æå–æ²¡æœ‰å­è·¯å¾„çš„è·¯å¾„
        leaf_paths = [p for p in all_paths if id(p) not in children_map]

        return leaf_paths

    async def _map_paths_to_memories(self, paths: list[Path]) -> dict[str, tuple[Any, list[Path]]]:
        """
        å°†è·¯å¾„æ˜ å°„åˆ°è®°å¿† - ä¼˜åŒ–ç‰ˆ

        Args:
            paths: è·¯å¾„åˆ—è¡¨

        Returns:
            {memory_id: (Memory, [contributing_paths])}
        """
        # ä½¿ç”¨ä¸´æ—¶å­—å…¸å­˜å‚¨è·¯å¾„åˆ—è¡¨
        temp_paths: dict[str, list[Path]] = {}
        temp_memories: dict[str, Any] = {}  # å­˜å‚¨ Memory å¯¹è±¡

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦è·å–çš„è®°å¿†IDï¼Œç„¶åæ‰¹é‡è·å–
        all_memory_ids = set()
        path_to_memory_ids: dict[int, set[str]] = {}  # pathå¯¹è±¡id -> è®°å¿†IDé›†åˆ

        for path in paths:
            memory_ids_in_path = set()

            # æ”¶é›†è·¯å¾„ä¸­æ‰€æœ‰èŠ‚ç‚¹æ¶‰åŠçš„è®°å¿†
            for node_id in path.nodes:
                memory_ids = self.graph_store.node_to_memories.get(node_id, [])
                memory_ids_in_path.update(memory_ids)

            all_memory_ids.update(memory_ids_in_path)
            path_to_memory_ids[id(path)] = memory_ids_in_path

        # ğŸš€ æ‰¹é‡è·å–è®°å¿†å¯¹è±¡ï¼ˆå¦‚æœgraph_storeæ”¯æŒæ‰¹é‡è·å–ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾é€ä¸ªè·å–ï¼Œå¦‚æœæœ‰æ‰¹é‡APIå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–
        memory_cache: dict[str, Any] = self.graph_store.get_memories_by_ids(all_memory_ids)

        # æ„å»ºæ˜ å°„å…³ç³»
        for path in paths:
            memory_ids_in_path = path_to_memory_ids[id(path)]

            for mem_id in memory_ids_in_path:
                if mem_id in memory_cache:
                    if mem_id not in temp_paths:
                        temp_paths[mem_id] = []
                        temp_memories[mem_id] = memory_cache[mem_id]
                    temp_paths[mem_id].append(path)

        # æ„å»ºæœ€ç»ˆè¿”å›çš„å­—å…¸
        memory_paths: dict[str, tuple[Any, list[Path]]] = {
            mem_id: (temp_memories[mem_id], paths_list)
            for mem_id, paths_list in temp_paths.items()
        }

        return memory_paths

    async def _final_scoring(
        self, memory_paths: dict[str, tuple[Any, list[Path]]]
    ) -> list[tuple[Any, float, list[Path]]]:
        """
        æœ€ç»ˆè¯„åˆ†ï¼ˆç»“åˆè·¯å¾„åˆ†æ•°ã€é‡è¦æ€§ã€æ—¶æ•ˆæ€§ + åå¥½ç±»å‹åŠ æˆï¼‰- ä¼˜åŒ–ç‰ˆ

        Args:
            memory_paths: è®°å¿†IDåˆ°(è®°å¿†, è·¯å¾„åˆ—è¡¨)çš„æ˜ å°„

        Returns:
            [(Memory, final_score, paths), ...]
        """
        scored_memories = []

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æœéœ€è¦åå¥½ç±»å‹åŠ æˆï¼Œæ‰¹é‡é¢„åŠ è½½æ‰€æœ‰èŠ‚ç‚¹çš„ç±»å‹ä¿¡æ¯
        node_type_cache: dict[str, str | None] = {}

        if self.prefer_node_types:
            # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„èŠ‚ç‚¹IDï¼Œå¹¶è®°å½•è®°å¿†ä¸­çš„ç±»å‹æç¤º
            all_node_ids: set[str] = set()
            node_type_hints: dict[str, str | None] = {}
            for memory, _ in memory_paths.values():
                memory_nodes = getattr(memory, "nodes", [])
                for node in memory_nodes:
                    node_id = node.id if hasattr(node, "id") else str(node)
                    all_node_ids.add(node_id)
                    if node_id not in node_type_hints:
                        node_obj_type = getattr(node, "node_type", None)
                        if node_obj_type is not None:
                            node_type_hints[node_id] = getattr(node_obj_type, "value", str(node_obj_type))

            if all_node_ids:
                logger.info(f"ğŸ§  é¢„å¤„ç† {len(all_node_ids)} ä¸ªèŠ‚ç‚¹çš„ç±»å‹ä¿¡æ¯")
                for nid in all_node_ids:
                    node_attrs = self.graph_store.graph.nodes.get(nid, {}) if hasattr(self.graph_store, "graph") else {}
                    metadata = node_attrs.get("metadata", {}) if isinstance(node_attrs, dict) else {}
                    node_type = metadata.get("node_type") or node_attrs.get("node_type")

                    if not node_type:
                        # å›é€€åˆ°è®°å¿†ä¸­çš„èŠ‚ç‚¹å®šä¹‰
                        node_type = node_type_hints.get(nid)

                    node_type_cache[nid] = node_type
        # éå†æ‰€æœ‰è®°å¿†è¿›è¡Œè¯„åˆ†
        for mem_id, (memory, paths) in memory_paths.items():
            # 1. èšåˆè·¯å¾„åˆ†æ•°
            path_score = self._aggregate_path_scores(paths)

            # 2. è®¡ç®—é‡è¦æ€§åˆ†æ•°
            importance_score = memory.importance

            # 3. è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°
            recency_score = self._calculate_recency(memory)

            # 4. ç»¼åˆè¯„åˆ†
            weights = self.config.final_scoring_weights
            base_final_score = (
                path_score * weights["path_score"]
                + importance_score * weights["importance"]
                + recency_score * weights["recency"]
            )

            # ğŸ†• 5. åå¥½ç±»å‹åŠ æˆï¼ˆä½¿ç”¨ç¼“å­˜çš„ç±»å‹ä¿¡æ¯ï¼‰
            type_bonus = 0.0
            if self.prefer_node_types and node_type_cache:
                memory_nodes = getattr(memory, "nodes", [])
                if memory_nodes:
                    # ä½¿ç”¨ç¼“å­˜å¿«é€Ÿè®¡ç®—åŒ¹é…æ•°
                    matched_count = 0
                    for node in memory_nodes:
                        node_id = node.id if hasattr(node, "id") else str(node)
                        node_type = node_type_cache.get(node_id)
                        if node_type and node_type in self.prefer_node_types:
                            matched_count += 1

                    if matched_count > 0:
                        match_ratio = matched_count / len(memory_nodes)
                        # æ ¹æ®åŒ¹é…æ¯”ä¾‹ç»™äºˆåŠ æˆï¼ˆæœ€é«˜10%ï¼‰
                        type_bonus = base_final_score * match_ratio * 0.1
                        logger.debug(
                            f"è®°å¿† {mem_id[:8]} åŒ…å« {matched_count}/{len(memory_nodes)} ä¸ªåå¥½ç±»å‹èŠ‚ç‚¹ï¼Œ"
                            f"åŠ æˆ {type_bonus:.3f}"
                        )

            final_score = base_final_score + type_bonus
            scored_memories.append((memory, final_score, paths))

        return scored_memories

    def _aggregate_path_scores(self, paths: list[Path]) -> float:
        """
        èšåˆå¤šæ¡è·¯å¾„çš„åˆ†æ•°

        Args:
            paths: è·¯å¾„åˆ—è¡¨

        Returns:
            èšåˆåˆ†æ•°
        """
        if not paths:
            return 0.0

        # æ–¹æ¡ˆA: æ€»åˆ†ï¼ˆè·¯å¾„è¶Šå¤šï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
        total_score = sum(p.score for p in paths)

        # æ–¹æ¡ˆB: Top-Kå¹³å‡ï¼ˆå…³æ³¨æœ€ä¼˜è·¯å¾„ï¼‰
        top3 = sorted(paths, key=lambda p: p.score, reverse=True)[:3]
        avg_top = sum(p.score for p in top3) / len(top3) if top3 else 0.0

        # ç»„åˆï¼š40% æ€»åˆ† + 60% Topå‡åˆ†
        return total_score * 0.4 + avg_top * 0.6

    def _calculate_recency(self, memory: Any) -> float:
        """
        è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°

        Args:
            memory: è®°å¿†å¯¹è±¡

        Returns:
            æ—¶æ•ˆæ€§åˆ†æ•° [0, 1]
        """
        now = datetime.now(timezone.utc)

        # ç¡®ä¿æ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯
        if memory.created_at.tzinfo is None:
            memory_time = memory.created_at.replace(tzinfo=timezone.utc)
        else:
            memory_time = memory.created_at

        # è®¡ç®—å¤©æ•°å·®
        age_days = (now - memory_time).total_seconds() / 86400

        # 30å¤©åŠè¡°æœŸ
        recency_score = 1.0 / (1.0 + age_days / 30)

        return recency_score

    def _batch_compute_similarities(
        self,
        valid_embeddings: list["np.ndarray"],
        query_embedding: "np.ndarray"
    ) -> "np.ndarray":
        """
        æ‰¹é‡è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼ˆCPUå¯†é›†å‹æ“ä½œï¼Œç§»è‡³to_threadä¸­æ‰§è¡Œï¼‰

        Args:
            valid_embeddings: æœ‰æ•ˆçš„åµŒå…¥å‘é‡åˆ—è¡¨
            query_embedding: æŸ¥è¯¢å‘é‡

        Returns:
            ç›¸ä¼¼åº¦æ•°ç»„
        """
        import numpy as np

        # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨çŸ©é˜µè¿ç®—ï¼‰
        embeddings_matrix = np.array(valid_embeddings)
        query_norm = np.linalg.norm(query_embedding)
        embeddings_norms = np.linalg.norm(embeddings_matrix, axis=1)

        # å‘é‡åŒ–è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = np.dot(embeddings_matrix, query_embedding) / (embeddings_norms * query_norm + 1e-8)
        similarities = np.clip(similarities, 0.0, 1.0)

        return similarities


__all__ = ["Path", "PathExpansionConfig", "PathScoreExpansion"]
