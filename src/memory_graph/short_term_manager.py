"""
çŸ­æœŸè®°å¿†å±‚ç®¡ç†å™¨ (Short-term Memory Manager)

è´Ÿè´£ç®¡ç†çŸ­æœŸè®°å¿†ï¼š
- ä»æ¿€æ´»çš„æ„ŸçŸ¥è®°å¿†å—æå–ç»“æ„åŒ–è®°å¿†
- LLM å†³ç­–ï¼šåˆå¹¶ã€æ›´æ–°ã€åˆ›å»ºã€ä¸¢å¼ƒ
- å®¹é‡ç®¡ç†å’Œè½¬ç§»åˆ°é•¿æœŸè®°å¿†
"""

import asyncio
import json
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np

from src.common.logger import get_logger
from src.memory_graph.models import (
    MemoryBlock,
    ShortTermDecision,
    ShortTermMemory,
    ShortTermOperation,
)
from src.memory_graph.utils.embeddings import EmbeddingGenerator
from src.memory_graph.utils.similarity import cosine_similarity

logger = get_logger(__name__)


class ShortTermMemoryManager:
    """
    çŸ­æœŸè®°å¿†å±‚ç®¡ç†å™¨

    ç®¡ç†æ´»è·ƒçš„ç»“æ„åŒ–è®°å¿†ï¼Œä»‹äºæ„ŸçŸ¥è®°å¿†å’Œé•¿æœŸè®°å¿†ä¹‹é—´ã€‚
    """

    def __init__(
        self,
        data_dir: Path | None = None,
        max_memories: int = 30,
        transfer_importance_threshold: float = 0.6,
        llm_temperature: float = 0.2,
    ):
        """
        åˆå§‹åŒ–çŸ­æœŸè®°å¿†å±‚ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            max_memories: æœ€å¤§çŸ­æœŸè®°å¿†æ•°é‡
            transfer_importance_threshold: è½¬ç§»åˆ°é•¿æœŸè®°å¿†çš„é‡è¦æ€§é˜ˆå€¼
            llm_temperature: LLM å†³ç­–çš„æ¸©åº¦å‚æ•°
        """
        self.data_dir = data_dir or Path("data/memory_graph/three_tier")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # é…ç½®å‚æ•°
        self.max_memories = max_memories
        self.transfer_importance_threshold = transfer_importance_threshold
        self.llm_temperature = llm_temperature

        # æ ¸å¿ƒæ•°æ®
        self.memories: list[ShortTermMemory] = []
        self.embedding_generator: EmbeddingGenerator | None = None

        # çŠ¶æ€
        self._initialized = False
        self._save_lock = asyncio.Lock()

        logger.info(
            f"çŸ­æœŸè®°å¿†ç®¡ç†å™¨å·²åˆ›å»º (max_memories={max_memories}, "
            f"transfer_threshold={transfer_importance_threshold:.2f})"
        )

    async def initialize(self) -> None:
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        if self._initialized:
            logger.warning("çŸ­æœŸè®°å¿†ç®¡ç†å™¨å·²ç»åˆå§‹åŒ–")
            return

        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–çŸ­æœŸè®°å¿†ç®¡ç†å™¨...")

            # åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨
            self.embedding_generator = EmbeddingGenerator()

            # å°è¯•åŠ è½½ç°æœ‰æ•°æ®
            await self._load_from_disk()

            self._initialized = True
            logger.info(f"âœ… çŸ­æœŸè®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ (å·²åŠ è½½ {len(self.memories)} æ¡è®°å¿†)")

        except Exception as e:
            logger.error(f"çŸ­æœŸè®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def add_from_block(self, block: MemoryBlock) -> ShortTermMemory | None:
        """
        ä»æ¿€æ´»çš„æ„ŸçŸ¥è®°å¿†å—åˆ›å»ºçŸ­æœŸè®°å¿†

        æµç¨‹ï¼š
        1. ä½¿ç”¨ LLM ä»è®°å¿†å—æå–ç»“æ„åŒ–ä¿¡æ¯
        2. ä¸ç°æœ‰çŸ­æœŸè®°å¿†æ¯”è¾ƒï¼Œå†³å®šå¦‚ä½•å¤„ç†ï¼ˆMERGE/UPDATE/CREATE_NEW/DISCARDï¼‰
        3. æ‰§è¡Œå†³ç­–
        4. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å®¹é‡ä¸Šé™

        Args:
            block: å·²æ¿€æ´»çš„è®°å¿†å—

        Returns:
            æ–°åˆ›å»ºæˆ–æ›´æ–°çš„çŸ­æœŸè®°å¿†ï¼Œå¤±è´¥æˆ–ä¸¢å¼ƒè¿”å› None
        """
        if not self._initialized:
            await self.initialize()

        try:
            logger.info(f"å¼€å§‹å¤„ç†è®°å¿†å—: {block.id}")

            # æ­¥éª¤1: ä½¿ç”¨ LLM æå–ç»“æ„åŒ–è®°å¿†
            extracted_memory = await self._extract_structured_memory(block)
            if not extracted_memory:
                logger.warning(f"è®°å¿†å— {block.id} æå–å¤±è´¥ï¼Œè·³è¿‡")
                return None

            # æ­¥éª¤2: å†³ç­–å¦‚ä½•å¤„ç†æ–°è®°å¿†
            decision = await self._decide_memory_operation(extracted_memory)
            logger.info(f"LLM å†³ç­–: {decision}")

            # æ­¥éª¤3: æ‰§è¡Œå†³ç­–
            result_memory = await self._execute_decision(extracted_memory, decision)

            # æ­¥éª¤4: æ£€æŸ¥å®¹é‡å¹¶å¯èƒ½è§¦å‘è½¬ç§»
            if len(self.memories) >= self.max_memories:
                logger.warning(
                    f"çŸ­æœŸè®°å¿†å·²è¾¾ä¸Šé™ ({len(self.memories)}/{self.max_memories})ï¼Œ"
                    f"éœ€è¦è½¬ç§»åˆ°é•¿æœŸè®°å¿†"
                )
                # æ³¨æ„ï¼šå®é™…è½¬ç§»ç”±å¤–éƒ¨è°ƒç”¨ transfer_to_long_term()

            # å¼‚æ­¥ä¿å­˜
            asyncio.create_task(self._save_to_disk())

            return result_memory

        except Exception as e:
            logger.error(f"æ·»åŠ çŸ­æœŸè®°å¿†å¤±è´¥: {e}", exc_info=True)
            return None

    async def _extract_structured_memory(self, block: MemoryBlock) -> ShortTermMemory | None:
        """
        ä½¿ç”¨ LLM ä»è®°å¿†å—æå–ç»“æ„åŒ–ä¿¡æ¯

        Args:
            block: è®°å¿†å—

        Returns:
            æå–çš„çŸ­æœŸè®°å¿†ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ç‰‡æ®µä¸­æå–ä¸€æ¡ç»“æ„åŒ–çš„è®°å¿†ã€‚

**å¯¹è¯å†…å®¹ï¼š**
```
{block.combined_text}
```

**ä»»åŠ¡è¦æ±‚ï¼š**
1. æå–å¯¹è¯çš„æ ¸å¿ƒä¿¡æ¯ï¼Œå½¢æˆä¸€æ¡ç®€æ´çš„è®°å¿†æè¿°
2. è¯†åˆ«è®°å¿†çš„ä¸»ä½“ï¼ˆsubjectï¼‰ã€ä¸»é¢˜ï¼ˆtopicï¼‰ã€å®¢ä½“ï¼ˆobjectï¼‰
3. åˆ¤æ–­è®°å¿†ç±»å‹ï¼ˆevent/fact/opinion/relationï¼‰
4. è¯„ä¼°é‡è¦æ€§ï¼ˆ0.0-1.0ï¼‰

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
{{
  "content": "è®°å¿†çš„å®Œæ•´æè¿°",
  "subject": "ä¸»ä½“",
  "topic": "ä¸»é¢˜/åŠ¨ä½œ",
  "object": "å®¢ä½“",
  "memory_type": "event/fact/opinion/relation",
  "importance": 0.7,
  "attributes": {{
    "time": "æ—¶é—´ä¿¡æ¯",
    "location": "åœ°ç‚¹ä¿¡æ¯"
  }}
}}
```

è¯·è¾“å‡ºJSONï¼š"""

            # è°ƒç”¨çŸ­æœŸè®°å¿†æ„å»ºæ¨¡å‹
            llm = LLMRequest(
                model_set=model_config.model_task_config.memory_short_term_builder,
                request_type="short_term_memory.extract",
            )

            response, _ = await llm.generate_response_async(
                prompt,
                temperature=self.llm_temperature,
                max_tokens=800,
            )

            # è§£æå“åº”
            data = self._parse_json_response(response)
            if not data:
                logger.error(f"LLM å“åº”è§£æå¤±è´¥: {response[:200]}")
                return None

            # ç”Ÿæˆå‘é‡
            content = data.get("content", "")
            embedding = await self._generate_embedding(content)

            # åˆ›å»ºçŸ­æœŸè®°å¿†
            memory = ShortTermMemory(
                id=f"stm_{uuid.uuid4().hex[:12]}",
                content=content,
                embedding=embedding,
                importance=data.get("importance", 0.5),
                source_block_ids=[block.id],
                subject=data.get("subject"),
                topic=data.get("topic"),
                object=data.get("object"),
                memory_type=data.get("memory_type"),
                attributes=data.get("attributes", {}),
            )

            logger.info(f"âœ… æå–ç»“æ„åŒ–è®°å¿†: {memory.content[:50]}...")
            return memory

        except Exception as e:
            logger.error(f"æå–ç»“æ„åŒ–è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return None

    async def _decide_memory_operation(self, new_memory: ShortTermMemory) -> ShortTermDecision:
        """
        ä½¿ç”¨ LLM å†³å®šå¦‚ä½•å¤„ç†æ–°è®°å¿†

        Args:
            new_memory: æ–°æå–çš„çŸ­æœŸè®°å¿†

        Returns:
            å†³ç­–ç»“æœ
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            # æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰è®°å¿†
            similar_memories = await self._find_similar_memories(new_memory, top_k=5)

            # å¦‚æœæ²¡æœ‰ç›¸ä¼¼è®°å¿†ï¼Œç›´æ¥åˆ›å»ºæ–°è®°å¿†
            if not similar_memories:
                return ShortTermDecision(
                    operation=ShortTermOperation.CREATE_NEW,
                    reasoning="æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„ç°æœ‰è®°å¿†ï¼Œä½œä¸ºæ–°è®°å¿†ä¿å­˜",
                    confidence=1.0,
                )

            # æ„å»ºæç¤ºè¯
            existing_memories_desc = "\n\n".join(
                [
                    f"è®°å¿†{i+1} (ID: {mem.id}, é‡è¦æ€§: {mem.importance:.2f}, ç›¸ä¼¼åº¦: {sim:.2f}):\n{mem.content}"
                    for i, (mem, sim) in enumerate(similar_memories)
                ]
            )

            prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†ç®¡ç†ä¸“å®¶ã€‚ç°åœ¨æœ‰ä¸€æ¡æ–°è®°å¿†éœ€è¦å¤„ç†ï¼Œè¯·å†³å®šå¦‚ä½•æ“ä½œã€‚

**æ–°è®°å¿†ï¼š**
{new_memory.content}

**ç°æœ‰ç›¸ä¼¼è®°å¿†ï¼š**
{existing_memories_desc}

**æ“ä½œé€‰é¡¹ï¼š**
1. merge - åˆå¹¶åˆ°ç°æœ‰è®°å¿†ï¼ˆå†…å®¹é«˜åº¦é‡å æˆ–äº’è¡¥ï¼‰
2. update - æ›´æ–°ç°æœ‰è®°å¿†ï¼ˆæ–°ä¿¡æ¯ä¿®æ­£æˆ–è¡¥å……æ—§ä¿¡æ¯ï¼‰
3. create_new - åˆ›å»ºæ–°è®°å¿†ï¼ˆä¸ç°æœ‰è®°å¿†ä¸åŒçš„ç‹¬ç«‹ä¿¡æ¯ï¼‰
4. discard - ä¸¢å¼ƒï¼ˆä»·å€¼è¿‡ä½æˆ–å®Œå…¨é‡å¤ï¼‰
5. keep_separate - æš‚ä¿æŒç‹¬ç«‹ï¼ˆç›¸å…³ä½†ç‹¬ç«‹çš„ä¿¡æ¯ï¼‰

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
{{
  "operation": "merge/update/create_new/discard/keep_separate",
  "target_memory_id": "ç›®æ ‡è®°å¿†çš„IDï¼ˆmerge/updateæ—¶éœ€è¦ï¼‰",
  "merged_content": "åˆå¹¶/æ›´æ–°åçš„å®Œæ•´å†…å®¹",
  "reasoning": "å†³ç­–ç†ç”±",
  "confidence": 0.85,
  "updated_importance": 0.7
}}
```

è¯·è¾“å‡ºJSONï¼š"""

            # è°ƒç”¨çŸ­æœŸè®°å¿†å†³ç­–æ¨¡å‹
            llm = LLMRequest(
                model_set=model_config.model_task_config.memory_short_term_decider,
                request_type="short_term_memory.decide",
            )

            response, _ = await llm.generate_response_async(
                prompt,
                temperature=self.llm_temperature,
                max_tokens=1000,
            )

            # è§£æå“åº”
            data = self._parse_json_response(response)
            if not data:
                logger.error(f"LLM å†³ç­–å“åº”è§£æå¤±è´¥: {response[:200]}")
                # é»˜è®¤åˆ›å»ºæ–°è®°å¿†
                return ShortTermDecision(
                    operation=ShortTermOperation.CREATE_NEW,
                    reasoning="LLM å“åº”è§£æå¤±è´¥ï¼Œé»˜è®¤åˆ›å»ºæ–°è®°å¿†",
                    confidence=0.5,
                )

            # åˆ›å»ºå†³ç­–å¯¹è±¡
            # å°† LLM è¿”å›çš„å¤§å†™æ“ä½œåè½¬æ¢ä¸ºå°å†™ï¼ˆé€‚é…æšä¸¾å®šä¹‰ï¼‰
            operation_str = data.get("operation", "CREATE_NEW").lower()
            
            decision = ShortTermDecision(
                operation=ShortTermOperation(operation_str),
                target_memory_id=data.get("target_memory_id"),
                merged_content=data.get("merged_content"),
                reasoning=data.get("reasoning", ""),
                confidence=data.get("confidence", 0.5),
                updated_importance=data.get("updated_importance"),
            )

            logger.info(f"LLM å†³ç­–å®Œæˆ: {decision}")
            return decision

        except Exception as e:
            logger.error(f"LLM å†³ç­–å¤±è´¥: {e}", exc_info=True)
            # é»˜è®¤åˆ›å»ºæ–°è®°å¿†
            return ShortTermDecision(
                operation=ShortTermOperation.CREATE_NEW,
                reasoning=f"LLM å†³ç­–å¤±è´¥: {e}",
                confidence=0.3,
            )

    async def _execute_decision(
        self, new_memory: ShortTermMemory, decision: ShortTermDecision
    ) -> ShortTermMemory | None:
        """
        æ‰§è¡Œ LLM çš„å†³ç­–

        Args:
            new_memory: æ–°è®°å¿†
            decision: å†³ç­–ç»“æœ

        Returns:
            æœ€ç»ˆçš„è®°å¿†å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯æ–°å»ºæˆ–æ›´æ–°çš„ï¼‰ï¼Œå¤±è´¥æˆ–ä¸¢å¼ƒè¿”å› None
        """
        try:
            if decision.operation == ShortTermOperation.CREATE_NEW:
                # åˆ›å»ºæ–°è®°å¿†
                self.memories.append(new_memory)
                logger.info(f"âœ… åˆ›å»ºæ–°çŸ­æœŸè®°å¿†: {new_memory.id}")
                return new_memory

            elif decision.operation == ShortTermOperation.MERGE:
                # åˆå¹¶åˆ°ç°æœ‰è®°å¿†
                target = self._find_memory_by_id(decision.target_memory_id)
                if not target:
                    logger.warning(f"ç›®æ ‡è®°å¿†ä¸å­˜åœ¨ï¼Œæ”¹ä¸ºåˆ›å»ºæ–°è®°å¿†: {decision.target_memory_id}")
                    self.memories.append(new_memory)
                    return new_memory

                # æ›´æ–°å†…å®¹
                target.content = decision.merged_content or f"{target.content}\n{new_memory.content}"
                target.source_block_ids.extend(new_memory.source_block_ids)

                # æ›´æ–°é‡è¦æ€§
                if decision.updated_importance is not None:
                    target.importance = decision.updated_importance

                # é‡æ–°ç”Ÿæˆå‘é‡
                target.embedding = await self._generate_embedding(target.content)
                target.update_access()

                logger.info(f"âœ… åˆå¹¶è®°å¿†åˆ°: {target.id}")
                return target

            elif decision.operation == ShortTermOperation.UPDATE:
                # æ›´æ–°ç°æœ‰è®°å¿†
                target = self._find_memory_by_id(decision.target_memory_id)
                if not target:
                    logger.warning(f"ç›®æ ‡è®°å¿†ä¸å­˜åœ¨ï¼Œæ”¹ä¸ºåˆ›å»ºæ–°è®°å¿†: {decision.target_memory_id}")
                    self.memories.append(new_memory)
                    return new_memory

                # æ›´æ–°å†…å®¹
                if decision.merged_content:
                    target.content = decision.merged_content
                    target.embedding = await self._generate_embedding(target.content)

                # æ›´æ–°é‡è¦æ€§
                if decision.updated_importance is not None:
                    target.importance = decision.updated_importance

                target.source_block_ids.extend(new_memory.source_block_ids)
                target.update_access()

                logger.info(f"âœ… æ›´æ–°è®°å¿†: {target.id}")
                return target

            elif decision.operation == ShortTermOperation.DISCARD:
                # ä¸¢å¼ƒ
                logger.info(f"ğŸ—‘ï¸ ä¸¢å¼ƒä½ä»·å€¼è®°å¿†: {decision.reasoning}")
                return None

            elif decision.operation == ShortTermOperation.KEEP_SEPARATE:
                # ä¿æŒç‹¬ç«‹
                self.memories.append(new_memory)
                logger.info(f"âœ… ä¿æŒç‹¬ç«‹è®°å¿†: {new_memory.id}")
                return new_memory

            else:
                logger.warning(f"æœªçŸ¥æ“ä½œç±»å‹: {decision.operation}ï¼Œé»˜è®¤åˆ›å»ºæ–°è®°å¿†")
                self.memories.append(new_memory)
                return new_memory

        except Exception as e:
            logger.error(f"æ‰§è¡Œå†³ç­–å¤±è´¥: {e}", exc_info=True)
            return None

    async def _find_similar_memories(
        self, memory: ShortTermMemory, top_k: int = 5
    ) -> list[tuple[ShortTermMemory, float]]:
        """
        æŸ¥æ‰¾ä¸ç»™å®šè®°å¿†ç›¸ä¼¼çš„ç°æœ‰è®°å¿†

        Args:
            memory: ç›®æ ‡è®°å¿†
            top_k: è¿”å›çš„æœ€å¤§æ•°é‡

        Returns:
            (è®°å¿†, ç›¸ä¼¼åº¦) åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åº
        """
        if memory.embedding is None or len(memory.embedding) == 0 or not self.memories:
            return []

        try:
            scored = []
            for existing_mem in self.memories:
                if existing_mem.embedding is None:
                    continue

                similarity = cosine_similarity(memory.embedding, existing_mem.embedding)
                scored.append((existing_mem, similarity))

            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
            scored.sort(key=lambda x: x[1], reverse=True)

            return scored[:top_k]

        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    def _find_memory_by_id(self, memory_id: str | None) -> ShortTermMemory | None:
        """æ ¹æ®IDæŸ¥æ‰¾è®°å¿†"""
        if not memory_id:
            return None

        for mem in self.memories:
            if mem.id == memory_id:
                return mem

        return None

    async def _generate_embedding(self, text: str) -> np.ndarray | None:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡"""
        try:
            if not self.embedding_generator:
                logger.error("åµŒå…¥ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
                return None

            embedding = await self.embedding_generator.generate(text)
            return embedding

        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘é‡å¤±è´¥: {e}", exc_info=True)
            return None

    async def _generate_embeddings_batch(self, texts: list[str]) -> list[np.ndarray | None]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            å‘é‡åˆ—è¡¨ï¼Œä¸è¾“å…¥ä¸€ä¸€å¯¹åº”
        """
        try:
            if not self.embedding_generator:
                logger.error("åµŒå…¥ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
                return [None] * len(texts)

            embeddings = await self.embedding_generator.generate_batch(texts)
            return embeddings

        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆå‘é‡å¤±è´¥: {e}", exc_info=True)
            return [None] * len(texts)

    def _parse_json_response(self, response: str) -> dict[str, Any] | None:
        """è§£æ LLM çš„ JSON å“åº”"""
        try:
            # å°è¯•æå– JSON ä»£ç å—
            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£æ
                json_str = response.strip()

            # ç§»é™¤å¯èƒ½çš„æ³¨é‡Š
            json_str = re.sub(r"//.*", "", json_str)
            json_str = re.sub(r"/\*.*?\*/", "", json_str, flags=re.DOTALL)

            data = json.loads(json_str)
            return data

        except json.JSONDecodeError as e:
            logger.warning(f"JSON è§£æå¤±è´¥: {e}, å“åº”: {response[:200]}")
            return None

    async def search_memories(
        self, query_text: str, top_k: int = 5, similarity_threshold: float = 0.5
    ) -> list[ShortTermMemory]:
        """
        æ£€ç´¢ç›¸å…³çš„çŸ­æœŸè®°å¿†

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æœ€å¤§æ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        if not self._initialized:
            await self.initialize()

        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self._generate_embedding(query_text)
            if query_embedding is None or len(query_embedding) == 0:
                return []

            # è®¡ç®—ç›¸ä¼¼åº¦
            scored = []
            for memory in self.memories:
                if memory.embedding is None:
                    continue

                similarity = cosine_similarity(query_embedding, memory.embedding)
                if similarity >= similarity_threshold:
                    scored.append((memory, similarity))

            # æ’åºå¹¶å– TopK
            scored.sort(key=lambda x: x[1], reverse=True)
            results = [mem for mem, _ in scored[:top_k]]

            # æ›´æ–°è®¿é—®è®°å½•
            for mem in results:
                mem.update_access()

            logger.info(f"æ£€ç´¢åˆ° {len(results)} æ¡çŸ­æœŸè®°å¿†")
            return results

        except Exception as e:
            logger.error(f"æ£€ç´¢çŸ­æœŸè®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    def get_memories_for_transfer(self) -> list[ShortTermMemory]:
        """
        è·å–éœ€è¦è½¬ç§»åˆ°é•¿æœŸè®°å¿†çš„è®°å¿†

        é€»è¾‘ï¼š
        1. ä¼˜å…ˆé€‰æ‹©é‡è¦æ€§ >= é˜ˆå€¼çš„è®°å¿†
        2. å¦‚æœå‰©ä½™è®°å¿†æ•°é‡ä»è¶…è¿‡ max_memoriesï¼Œç›´æ¥æ¸…ç†æœ€æ—©çš„ä½é‡è¦æ€§è®°å¿†ç›´åˆ°ä½äºä¸Šé™
        """
        # 1. æ­£å¸¸ç­›é€‰ï¼šé‡è¦æ€§è¾¾æ ‡çš„è®°å¿†
        candidates = [mem for mem in self.memories if mem.importance >= self.transfer_importance_threshold]
        candidate_ids = {mem.id for mem in candidates}
        
        # 2. æ£€æŸ¥ä½é‡è¦æ€§è®°å¿†æ˜¯å¦ç§¯å‹
        # å‰©ä½™çš„éƒ½æ˜¯ä½é‡è¦æ€§è®°å¿†
        low_importance_memories = [mem for mem in self.memories if mem.id not in candidate_ids]
        
        # å¦‚æœä½é‡è¦æ€§è®°å¿†æ•°é‡è¶…è¿‡äº†ä¸Šé™ï¼ˆè¯´æ˜ç§¯å‹ä¸¥é‡ï¼‰
        # æˆ‘ä»¬éœ€è¦æ¸…ç†æ‰ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯è½¬ç§»å®ƒä»¬
        if len(low_importance_memories) > self.max_memories:
            # ç›®æ ‡ä¿ç•™æ•°é‡ï¼ˆé™è‡³ä¸Šé™çš„ 90%ï¼‰
            target_keep_count = int(self.max_memories * 0.9)
            num_to_remove = len(low_importance_memories) - target_keep_count
            
            if num_to_remove > 0:
                # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—©çš„
                low_importance_memories.sort(key=lambda x: x.created_at)
                to_remove = low_importance_memories[:num_to_remove]
                
                for mem in to_remove:
                    if mem in self.memories:
                        self.memories.remove(mem)
                        
                logger.info(
                    f"çŸ­æœŸè®°å¿†æ¸…ç†: ç§»é™¤äº† {len(to_remove)} æ¡ä½é‡è¦æ€§è®°å¿† "
                    f"(ä¿ç•™ {len(self.memories)} æ¡)"
                )
                
                # è§¦å‘ä¿å­˜
                asyncio.create_task(self._save_to_disk())
            
        return candidates

    async def clear_transferred_memories(self, memory_ids: list[str]) -> None:
        """
        æ¸…é™¤å·²è½¬ç§»åˆ°é•¿æœŸè®°å¿†çš„è®°å¿†

        Args:
            memory_ids: å·²è½¬ç§»çš„è®°å¿†IDåˆ—è¡¨
        """
        try:
            self.memories = [mem for mem in self.memories if mem.id not in memory_ids]
            logger.info(f"æ¸…é™¤ {len(memory_ids)} æ¡å·²è½¬ç§»çš„çŸ­æœŸè®°å¿†")

            # å¼‚æ­¥ä¿å­˜
            asyncio.create_task(self._save_to_disk())

        except Exception as e:
            logger.error(f"æ¸…é™¤å·²è½¬ç§»è®°å¿†å¤±è´¥: {e}", exc_info=True)

    def get_statistics(self) -> dict[str, Any]:
        """è·å–çŸ­æœŸè®°å¿†å±‚ç»Ÿè®¡ä¿¡æ¯"""
        if not self._initialized:
            return {}

        total_access = sum(mem.access_count for mem in self.memories)
        avg_importance = sum(mem.importance for mem in self.memories) / len(self.memories) if self.memories else 0

        return {
            "total_memories": len(self.memories),
            "max_memories": self.max_memories,
            "total_access_count": total_access,
            "avg_importance": avg_importance,
            "transferable_count": len(self.get_memories_for_transfer()),
            "transfer_threshold": self.transfer_importance_threshold,
        }

    async def _save_to_disk(self) -> None:
        """ä¿å­˜çŸ­æœŸè®°å¿†åˆ°ç£ç›˜"""
        async with self._save_lock:
            try:
                import orjson

                save_path = self.data_dir / "short_term_memory.json"
                data = {
                    "memories": [mem.to_dict() for mem in self.memories],
                    "max_memories": self.max_memories,
                    "transfer_threshold": self.transfer_importance_threshold,
                }

                save_path.write_bytes(orjson.dumps(data, option=orjson.OPT_INDENT_2))

                logger.debug(f"çŸ­æœŸè®°å¿†å·²ä¿å­˜åˆ° {save_path}")

            except Exception as e:
                logger.error(f"ä¿å­˜çŸ­æœŸè®°å¿†å¤±è´¥: {e}", exc_info=True)

    async def _load_from_disk(self) -> None:
        """ä»ç£ç›˜åŠ è½½çŸ­æœŸè®°å¿†"""
        try:
            import orjson

            load_path = self.data_dir / "short_term_memory.json"

            if not load_path.exists():
                logger.info("æœªæ‰¾åˆ°çŸ­æœŸè®°å¿†æ•°æ®æ–‡ä»¶")
                return

            data = orjson.loads(load_path.read_bytes())
            self.memories = [ShortTermMemory.from_dict(m) for m in data.get("memories", [])]

            # é‡æ–°ç”Ÿæˆå‘é‡
            await self._reload_embeddings()

            logger.info(f"çŸ­æœŸè®°å¿†å·²ä» {load_path} åŠ è½½ ({len(self.memories)} æ¡)")

        except Exception as e:
            logger.error(f"åŠ è½½çŸ­æœŸè®°å¿†å¤±è´¥: {e}", exc_info=True)

    async def _reload_embeddings(self) -> None:
        """é‡æ–°ç”Ÿæˆè®°å¿†çš„å‘é‡"""
        logger.info("é‡æ–°ç”ŸæˆçŸ­æœŸè®°å¿†å‘é‡...")

        memories_to_process = []
        texts_to_process = []

        for memory in self.memories:
            if memory.embedding is None and memory.content and memory.content.strip():
                memories_to_process.append(memory)
                texts_to_process.append(memory.content)

        if not memories_to_process:
            logger.info("æ²¡æœ‰éœ€è¦é‡æ–°ç”Ÿæˆå‘é‡çš„çŸ­æœŸè®°å¿†")
            return

        logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(memories_to_process)} æ¡çŸ­æœŸè®°å¿†çš„å‘é‡...")

        embeddings = await self._generate_embeddings_batch(texts_to_process)

        success_count = 0
        for memory, embedding in zip(memories_to_process, embeddings):
            if embedding is not None:
                memory.embedding = embedding
                success_count += 1

        logger.info(f"âœ… å‘é‡é‡æ–°ç”Ÿæˆå®Œæˆï¼ˆæˆåŠŸ: {success_count}/{len(memories_to_process)}ï¼‰")

    async def shutdown(self) -> None:
        """å…³é—­ç®¡ç†å™¨"""
        if not self._initialized:
            return

        try:
            logger.info("æ­£åœ¨å…³é—­çŸ­æœŸè®°å¿†ç®¡ç†å™¨...")

            # æœ€åä¸€æ¬¡ä¿å­˜
            await self._save_to_disk()

            self._initialized = False
            logger.info("âœ… çŸ­æœŸè®°å¿†ç®¡ç†å™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å…³é—­çŸ­æœŸè®°å¿†ç®¡ç†å™¨å¤±è´¥: {e}", exc_info=True)


# å…¨å±€å•ä¾‹
_short_term_manager_instance: ShortTermMemoryManager | None = None


def get_short_term_manager() -> ShortTermMemoryManager:
    """è·å–çŸ­æœŸè®°å¿†ç®¡ç†å™¨å•ä¾‹"""
    global _short_term_manager_instance
    if _short_term_manager_instance is None:
        _short_term_manager_instance = ShortTermMemoryManager()
    return _short_term_manager_instance
