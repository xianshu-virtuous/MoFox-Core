"""
LLM å·¥å…·æ¥å£ï¼šå®šä¹‰è®°å¿†ç³»ç»Ÿçš„å·¥å…· schema å’Œæ‰§è¡Œé€»è¾‘
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple

from src.common.logger import get_logger
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import Memory, MemoryStatus
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.utils.embeddings import EmbeddingGenerator

logger = get_logger(__name__)


class MemoryTools:
    """
    è®°å¿†ç³»ç»Ÿå·¥å…·é›†
    
    æä¾›ç»™ LLM ä½¿ç”¨çš„å·¥å…·æ¥å£ï¼š
    1. create_memory: åˆ›å»ºæ–°è®°å¿†
    2. link_memories: å…³è”ä¸¤ä¸ªè®°å¿†
    3. search_memories: æœç´¢è®°å¿†
    """

    def __init__(
        self,
        vector_store: VectorStore,
        graph_store: GraphStore,
        persistence_manager: PersistenceManager,
        embedding_generator: Optional[EmbeddingGenerator] = None,
        max_expand_depth: int = 1,
    ):
        """
        åˆå§‹åŒ–å·¥å…·é›†
        
        Args:
            vector_store: å‘é‡å­˜å‚¨
            graph_store: å›¾å­˜å‚¨
            persistence_manager: æŒä¹…åŒ–ç®¡ç†å™¨
            embedding_generator: åµŒå…¥ç”Ÿæˆå™¨ï¼ˆå¯é€‰ï¼‰
            max_expand_depth: å›¾æ‰©å±•æ·±åº¦çš„é»˜è®¤å€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
        """
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.persistence_manager = persistence_manager
        self._initialized = False
        self.max_expand_depth = max_expand_depth  # ä¿å­˜é…ç½®çš„é»˜è®¤å€¼
        
        logger.info(f"MemoryTools åˆå§‹åŒ–: max_expand_depth={max_expand_depth}")

        # åˆå§‹åŒ–ç»„ä»¶
        self.extractor = MemoryExtractor()
        self.builder = MemoryBuilder(
            vector_store=vector_store,
            graph_store=graph_store,
            embedding_generator=embedding_generator,
        )

    async def _ensure_initialized(self):
        """ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–"""
        if not self._initialized:
            await self.vector_store.initialize()
            self._initialized = True

    @staticmethod
    def get_create_memory_schema() -> Dict[str, Any]:
        """
        è·å– create_memory å·¥å…·çš„ JSON schema
        
        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "create_memory",
            "description": """åˆ›å»ºä¸€ä¸ªæ–°çš„è®°å¿†èŠ‚ç‚¹ï¼Œè®°å½•å¯¹è¯ä¸­æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

ğŸ¯ **æ ¸å¿ƒåŸåˆ™**ï¼šä¸»åŠ¨è®°å½•ã€ç§¯ææ„å»ºã€ä¸°å¯Œç»†èŠ‚

âœ… **ä¼˜å…ˆåˆ›å»ºè®°å¿†çš„åœºæ™¯**ï¼ˆé¼“åŠ±è®°å½•ï¼‰ï¼š
1. **ä¸ªäººä¿¡æ¯**ï¼šå§“åã€æ˜µç§°ã€å¹´é¾„ã€èŒä¸šã€èº«ä»½ã€æ‰€åœ¨åœ°ã€è”ç³»æ–¹å¼ç­‰
2. **å…´è¶£çˆ±å¥½**ï¼šå–œæ¬¢/ä¸å–œæ¬¢çš„äº‹ç‰©ã€å¨±ä¹åå¥½ã€è¿åŠ¨çˆ±å¥½ã€é¥®é£Ÿå£å‘³ç­‰
3. **ç”Ÿæ´»çŠ¶æ€**ï¼šå·¥ä½œå­¦ä¹ çŠ¶æ€ã€ç”Ÿæ´»ä¹ æƒ¯ã€ä½œæ¯æ—¶é—´ã€æ—¥å¸¸å®‰æ’ç­‰
4. **ç»å†äº‹ä»¶**ï¼šæ­£åœ¨åšçš„äº‹ã€å®Œæˆçš„ä»»åŠ¡ã€å‚ä¸çš„æ´»åŠ¨ã€é‡åˆ°çš„é—®é¢˜ç­‰
5. **è§‚ç‚¹æ€åº¦**ï¼šå¯¹äº‹ç‰©çš„çœ‹æ³•ã€ä»·å€¼è§‚ã€æƒ…ç»ªè¡¨è¾¾ã€è¯„ä»·æ„è§ç­‰
6. **è®¡åˆ’ç›®æ ‡**ï¼šæœªæ¥æ‰“ç®—ã€å­¦ä¹ è®¡åˆ’ã€å·¥ä½œç›®æ ‡ã€å¾…åŠäº‹é¡¹ç­‰
7. **äººé™…å…³ç³»**ï¼šæåˆ°çš„æœ‹å‹ã€å®¶äººã€åŒäº‹ã€è®¤è¯†çš„äººç­‰
8. **æŠ€èƒ½çŸ¥è¯†**ï¼šæŒæ¡çš„æŠ€èƒ½ã€å­¦ä¹ çš„çŸ¥è¯†ã€ä¸“ä¸šé¢†åŸŸã€ä½¿ç”¨çš„å·¥å…·ç­‰
9. **ç‰©å“èµ„æº**ï¼šæ‹¥æœ‰çš„ç‰©å“ã€ä½¿ç”¨çš„è®¾å¤‡ã€å–œæ¬¢çš„å“ç‰Œç­‰
10. **æ—¶é—´åœ°ç‚¹**ï¼šé‡è¦æ—¶é—´èŠ‚ç‚¹ã€å¸¸å»çš„åœ°ç‚¹ã€æ´»åŠ¨åœºæ‰€ç­‰

âš ï¸ **æš‚ä¸åˆ›å»ºçš„æƒ…å†µ**ï¼ˆä»…é™ä»¥ä¸‹ï¼‰ï¼š
- çº¯ç²¹çš„æ‹›å‘¼è¯­ï¼ˆå•çº¯çš„"ä½ å¥½"ã€"å†è§"ï¼‰
- å®Œå…¨æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼ˆå•çº¯çš„"å“¦"ã€"å—¯"ï¼‰
- æ˜ç¡®çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¦‚"åˆ‡æ¢æ¨¡å¼"ã€"é‡å¯"ï¼‰

ï¿½ **è®°å¿†æ‹†åˆ†å»ºè®®**ï¼š
- ä¸€å¥è¯åŒ…å«å¤šä¸ªä¿¡æ¯ç‚¹ â†’ æ‹†æˆå¤šæ¡è®°å¿†ï¼ˆæ›´åˆ©äºåç»­æ£€ç´¢ï¼‰
- ä¾‹å¦‚ï¼š"æˆ‘æœ€è¿‘åœ¨å­¦Pythonå’Œæœºå™¨å­¦ä¹ ï¼Œæƒ³æ‰¾å·¥ä½œ"
  â†’ æ‹†æˆ3æ¡ï¼š
  1. "ç”¨æˆ·æ­£åœ¨å­¦ä¹ Python"ï¼ˆäº‹ä»¶ï¼‰
  2. "ç”¨æˆ·æ­£åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ "ï¼ˆäº‹ä»¶ï¼‰
  3. "ç”¨æˆ·æƒ³æ‰¾å·¥ä½œ"ï¼ˆäº‹ä»¶/ç›®æ ‡ï¼‰

ğŸ“Œ **è®°å¿†è´¨é‡å»ºè®®**ï¼š
- è®°å½•æ—¶å°½é‡è¡¥å……æ—¶é—´ï¼ˆ"ä»Šå¤©"ã€"æœ€è¿‘"ã€"æ˜¨å¤©"ç­‰ï¼‰
- åŒ…å«å…·ä½“ç»†èŠ‚ï¼ˆè¶Šå…·ä½“è¶Šå¥½ï¼‰
- ä¸»ä½“æ˜ç¡®ï¼ˆä¼˜å…ˆä½¿ç”¨"ç”¨æˆ·"æˆ–å…·ä½“äººåï¼Œé¿å…"æˆ‘"ï¼‰

è®°å¿†ç»“æ„ï¼šä¸»ä½“ + ç±»å‹ + ä¸»é¢˜ + å®¢ä½“ï¼ˆå¯é€‰ï¼‰+ å±æ€§ï¼ˆè¶Šè¯¦ç»†è¶Šå¥½ï¼‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "subject": {
                        "type": "string",
                        "description": "è®°å¿†çš„ä¸»ä½“ï¼ˆè°çš„ä¿¡æ¯ï¼‰ï¼š\n- å¯¹è¯ä¸­çš„ç”¨æˆ·ç»Ÿä¸€ä½¿ç”¨'ç”¨æˆ·'\n- æåˆ°çš„å…·ä½“äººç‰©ä½¿ç”¨å…¶åå­—ï¼ˆå¦‚'å°æ˜'ã€'å¼ ä¸‰'ï¼‰\n- é¿å…ä½¿ç”¨'æˆ‘'ã€'ä»–'ç­‰ä»£è¯",
                    },
                    "memory_type": {
                        "type": "string",
                        "enum": ["äº‹ä»¶", "äº‹å®", "å…³ç³»", "è§‚ç‚¹"],
                        "description": "é€‰æ‹©æœ€åˆé€‚çš„è®°å¿†ç±»å‹ï¼š\n\nã€äº‹ä»¶ã€‘æ—¶é—´ç›¸å…³çš„åŠ¨ä½œæˆ–å‘ç”Ÿçš„äº‹ï¼ˆç”¨'æ­£åœ¨'ã€'å®Œæˆäº†'ã€'å‚åŠ 'ç­‰åŠ¨è¯ï¼‰\n  ä¾‹ï¼šæ­£åœ¨å­¦ä¹ Pythonã€å®Œæˆäº†é¡¹ç›®ã€å‚åŠ ä¼šè®®ã€å»æ—…è¡Œ\n\nã€äº‹å®ã€‘ç›¸å¯¹ç¨³å®šçš„å®¢è§‚ä¿¡æ¯ï¼ˆç”¨'æ˜¯'ã€'æœ‰'ã€'åœ¨'ç­‰æè¿°çŠ¶æ€ï¼‰\n  ä¾‹ï¼šèŒä¸šæ˜¯å·¥ç¨‹å¸ˆã€ä½åœ¨åŒ—äº¬ã€æœ‰ä¸€åªçŒ«ã€ä¼šè¯´è‹±è¯­\n\nã€è§‚ç‚¹ã€‘ä¸»è§‚çœ‹æ³•ã€å–œå¥½ã€æ€åº¦ï¼ˆç”¨'å–œæ¬¢'ã€'è®¤ä¸º'ã€'è§‰å¾—'ç­‰ï¼‰\n  ä¾‹ï¼šå–œæ¬¢Pythonã€è®¤ä¸ºAIå¾ˆé‡è¦ã€è§‰å¾—ç´¯ã€è®¨åŒåŠ ç­\n\nã€å…³ç³»ã€‘äººä¸äººä¹‹é—´çš„å…³ç³»\n  ä¾‹ï¼šè®¤è¯†äº†æœ‹å‹ã€æ˜¯åŒäº‹ã€å®¶äººå…³ç³»",
                    },
                    "topic": {
                        "type": "string",
                        "description": "è®°å¿†çš„æ ¸å¿ƒå†…å®¹ï¼ˆåšä»€ä¹ˆ/æ˜¯ä»€ä¹ˆ/å…³äºä»€ä¹ˆï¼‰ï¼š\n- å°½é‡å…·ä½“æ˜ç¡®ï¼ˆ'å­¦ä¹ Pythonç¼–ç¨‹' ä¼˜äº 'å­¦ä¹ 'ï¼‰\n- åŒ…å«å…³é”®åŠ¨è¯æˆ–æ ¸å¿ƒæ¦‚å¿µ\n- å¯ä»¥åŒ…å«æ—¶é—´çŠ¶æ€ï¼ˆ'æ­£åœ¨å­¦ä¹ 'ã€'å·²å®Œæˆ'ã€'è®¡åˆ’åš'ï¼‰",
                    },
                    "object": {
                        "type": "string",
                        "description": "å¯é€‰ï¼šè®°å¿†æ¶‰åŠçš„å¯¹è±¡æˆ–ç›®æ ‡ï¼š\n- äº‹ä»¶çš„å¯¹è±¡ï¼ˆå­¦ä¹ çš„æ˜¯ä»€ä¹ˆã€è´­ä¹°çš„æ˜¯ä»€ä¹ˆï¼‰\n- è§‚ç‚¹çš„å¯¹è±¡ï¼ˆå–œæ¬¢çš„æ˜¯ä»€ä¹ˆã€è®¨åŒçš„æ˜¯ä»€ä¹ˆï¼‰\n- å¯ä»¥ç•™ç©ºï¼ˆå¦‚æœtopicå·²ç»è¶³å¤Ÿå®Œæ•´ï¼‰",
                    },
                    "attributes": {
                        "type": "object",
                        "description": "è®°å¿†çš„è¯¦ç»†å±æ€§ï¼ˆå»ºè®®å°½é‡å¡«å†™ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼‰ï¼š",
                        "properties": {
                            "æ—¶é—´": {
                                "type": "string",
                                "description": "æ—¶é—´ä¿¡æ¯ï¼ˆå¼ºçƒˆå»ºè®®å¡«å†™ï¼‰ï¼š\n- å…·ä½“æ—¥æœŸï¼š'2025-11-05'ã€'2025å¹´11æœˆ'\n- ç›¸å¯¹æ—¶é—´ï¼š'ä»Šå¤©'ã€'æ˜¨å¤©'ã€'ä¸Šå‘¨'ã€'æœ€è¿‘'ã€'3å¤©å‰'\n- æ—¶é—´æ®µï¼š'ä»Šå¤©ä¸‹åˆ'ã€'ä¸Šä¸ªæœˆ'ã€'è¿™å­¦æœŸ'",
                            },
                            "åœ°ç‚¹": {
                                "type": "string", 
                                "description": "åœ°ç‚¹ä¿¡æ¯ï¼ˆå¦‚æ¶‰åŠï¼‰ï¼š\n- å…·ä½“åœ°å€ã€åŸå¸‚åã€å›½å®¶\n- åœºæ‰€ç±»å‹ï¼š'åœ¨å®¶'ã€'å…¬å¸'ã€'å­¦æ ¡'ã€'å’–å•¡åº—'"
                            },
                            "åŸå› ": {
                                "type": "string", 
                                "description": "ä¸ºä»€ä¹ˆè¿™æ ·åš/è¿™æ ·æƒ³ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "æ–¹å¼": {
                                "type": "string", 
                                "description": "æ€ä¹ˆåšçš„/é€šè¿‡ä»€ä¹ˆæ–¹å¼ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "ç»“æœ": {
                                "type": "string",
                                "description": "ç»“æœå¦‚ä½•/äº§ç”Ÿä»€ä¹ˆå½±å“ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "çŠ¶æ€": {
                                "type": "string",
                                "description": "å½“å‰è¿›å±•ï¼š'è¿›è¡Œä¸­'ã€'å·²å®Œæˆ'ã€'è®¡åˆ’ä¸­'ã€'æš‚åœ'ç­‰"
                            },
                            "ç¨‹åº¦": {
                                "type": "string",
                                "description": "ç¨‹åº¦æè¿°ï¼ˆå¦‚'éå¸¸'ã€'æ¯”è¾ƒ'ã€'æœ‰ç‚¹'ã€'ä¸å¤ª'ï¼‰"
                            },
                        },
                        "additionalProperties": True,
                    },
                    "importance": {
                        "type": "number",
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "é‡è¦æ€§è¯„åˆ†ï¼ˆé»˜è®¤0.5ï¼Œæ—¥å¸¸å¯¹è¯å»ºè®®0.5-0.7ï¼‰ï¼š\n\n0.3-0.4: æ¬¡è¦ç»†èŠ‚ï¼ˆå¶ç„¶æåŠçš„çäº‹ï¼‰\n0.5-0.6: æ—¥å¸¸ä¿¡æ¯ï¼ˆä¸€èˆ¬æ€§çš„åˆ†äº«ã€æ™®é€šçˆ±å¥½ï¼‰â† æ¨èé»˜è®¤å€¼\n0.7-0.8: é‡è¦ä¿¡æ¯ï¼ˆæ˜ç¡®çš„åå¥½ã€é‡è¦è®¡åˆ’ã€æ ¸å¿ƒçˆ±å¥½ï¼‰\n0.9-1.0: å…³é”®ä¿¡æ¯ï¼ˆèº«ä»½ä¿¡æ¯ã€é‡å¤§å†³å®šã€å¼ºçƒˆæƒ…æ„Ÿï¼‰\n\nğŸ’¡ å»ºè®®ï¼šæ—¥å¸¸å¯¹è¯ä¸­å¤§éƒ¨åˆ†è®°å¿†ä½¿ç”¨0.5-0.6ï¼Œé™¤éç”¨æˆ·ç‰¹åˆ«å¼ºè°ƒ",
                    },
                },
                "required": ["subject", "memory_type", "topic"],
            },
        }

    @staticmethod
    def get_link_memories_schema() -> Dict[str, Any]:
        """
        è·å– link_memories å·¥å…·çš„ JSON schema
        
        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "link_memories",
            "description": """æ‰‹åŠ¨å…³è”ä¸¤ä¸ªå·²å­˜åœ¨çš„è®°å¿†ã€‚

âš ï¸ ä½¿ç”¨å»ºè®®ï¼š
- ç³»ç»Ÿä¼šè‡ªåŠ¨å‘ç°è®°å¿†é—´çš„å…³è”å…³ç³»ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨æ­¤å·¥å…·
- ä»…åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨ï¼š
  1. ç”¨æˆ·æ˜ç¡®æŒ‡å‡ºä¸¤ä¸ªè®°å¿†ä¹‹é—´çš„å…³ç³»
  2. å‘ç°æ˜æ˜¾çš„å› æœå…³ç³»ä½†ç³»ç»Ÿæœªè‡ªåŠ¨å…³è”
  3. éœ€è¦å»ºç«‹ç‰¹æ®Šçš„å¼•ç”¨å…³ç³»

å…³ç³»ç±»å‹è¯´æ˜ï¼š
- å¯¼è‡´ï¼šAäº‹ä»¶/è¡Œä¸ºå¯¼è‡´Bäº‹ä»¶/ç»“æœï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨ï¼šAè®°å¿†å¼•ç”¨/åŸºäºBè®°å¿†ï¼ˆçŸ¥è¯†å…³è”ï¼‰
- ç›¸ä¼¼ï¼šAå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆä¸»é¢˜ç›¸ä¼¼ï¼‰
- ç›¸åï¼šAå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹æ¯”å…³ç³»ï¼‰
- å…³è”ï¼šAå’ŒBå­˜åœ¨ä¸€èˆ¬æ€§å…³è”ï¼ˆå…¶ä»–å…³ç³»ï¼‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "source_memory_description": {
                        "type": "string",
                        "description": "æºè®°å¿†çš„å…³é”®æè¿°ï¼ˆç”¨äºæœç´¢å®šä½ï¼Œéœ€è¦è¶³å¤Ÿå…·ä½“ï¼‰",
                    },
                    "target_memory_description": {
                        "type": "string",
                        "description": "ç›®æ ‡è®°å¿†çš„å…³é”®æè¿°ï¼ˆç”¨äºæœç´¢å®šä½ï¼Œéœ€è¦è¶³å¤Ÿå…·ä½“ï¼‰",
                    },
                    "relation_type": {
                        "type": "string",
                        "enum": ["å¯¼è‡´", "å¼•ç”¨", "ç›¸ä¼¼", "ç›¸å", "å…³è”"],
                        "description": "å…³ç³»ç±»å‹ï¼ˆä»ä¸Šè¿°5ç§ç±»å‹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ï¼‰",
                    },
                    "importance": {
                        "type": "number",
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "å…³ç³»çš„é‡è¦æ€§ï¼ˆ0.0-1.0ï¼‰ï¼š\n- 0.5-0.6: ä¸€èˆ¬å…³è”\n- 0.7-0.8: é‡è¦å…³è”\n- 0.9-1.0: å…³é”®å…³è”\né»˜è®¤0.6",
                    },
                },
                "required": [
                    "source_memory_description",
                    "target_memory_description",
                    "relation_type",
                ],
            },
        }

    @staticmethod
    def get_search_memories_schema() -> Dict[str, Any]:
        """
        è·å– search_memories å·¥å…·çš„ JSON schema
        
        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "search_memories",
            "description": """æœç´¢ç›¸å…³çš„è®°å¿†ï¼Œç”¨äºå›å¿†å’ŒæŸ¥æ‰¾å†å²ä¿¡æ¯ã€‚

ä½¿ç”¨åœºæ™¯ï¼š
- ç”¨æˆ·è¯¢é—®ä¹‹å‰çš„å¯¹è¯å†…å®¹
- éœ€è¦å›å¿†ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ã€åå¥½ã€ç»å†
- æŸ¥æ‰¾ç›¸å…³çš„å†å²äº‹ä»¶æˆ–è§‚ç‚¹
- åŸºäºä¸Šä¸‹æ–‡è¡¥å……ä¿¡æ¯

æœç´¢ç‰¹æ€§ï¼š
- è¯­ä¹‰æœç´¢ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦åŒ¹é…
- å›¾éå†ï¼šè‡ªåŠ¨æ‰©å±•ç›¸å…³è”çš„è®°å¿†
- æ—¶é—´è¿‡æ»¤ï¼šæŒ‰æ—¶é—´èŒƒå›´ç­›é€‰
- ç±»å‹è¿‡æ»¤ï¼šæŒ‰è®°å¿†ç±»å‹ç­›é€‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°è¦æŸ¥æ‰¾çš„å†…å®¹ï¼Œå¦‚'ç”¨æˆ·çš„èŒä¸š'ã€'æœ€è¿‘çš„é¡¹ç›®'ã€'Pythonç›¸å…³çš„è®°å¿†'ï¼‰",
                    },
                    "memory_types": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": ["äº‹ä»¶", "äº‹å®", "å…³ç³»", "è§‚ç‚¹"],
                        },
                        "description": "è®°å¿†ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œç•™ç©ºè¡¨ç¤ºæœç´¢æ‰€æœ‰ç±»å‹ï¼‰",
                    },
                    "time_range": {
                        "type": "object",
                        "properties": {
                            "start": {
                                "type": "string",
                                "description": "å¼€å§‹æ—¶é—´ï¼ˆå¦‚'3å¤©å‰'ã€'ä¸Šå‘¨'ã€'2025-11-01'ï¼‰",
                            },
                            "end": {
                                "type": "string",
                                "description": "ç»“æŸæ—¶é—´ï¼ˆå¦‚'ä»Šå¤©'ã€'ç°åœ¨'ã€'2025-11-05'ï¼‰",
                            },
                        },
                        "description": "æ—¶é—´èŒƒå›´ï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥æ‰¾ç‰¹å®šæ—¶é—´æ®µçš„è®°å¿†ï¼‰",
                    },
                    "top_k": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 50,
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œé»˜è®¤10ï¼‰ã€‚æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼š\n- å¿«é€ŸæŸ¥æ‰¾ï¼š3-5æ¡\n- ä¸€èˆ¬æœç´¢ï¼š10æ¡\n- å…¨é¢äº†è§£ï¼š20-30æ¡",
                    },
                    "expand_depth": {
                        "type": "integer",
                        "minimum": 0,
                        "maximum": 3,
                        "description": "å›¾æ‰©å±•æ·±åº¦ï¼ˆ0-3ï¼Œé»˜è®¤1ï¼‰ï¼š\n- 0: ä»…è¿”å›ç›´æ¥åŒ¹é…çš„è®°å¿†\n- 1: åŒ…å«ä¸€åº¦ç›¸å…³çš„è®°å¿†ï¼ˆæ¨èï¼‰\n- 2-3: åŒ…å«æ›´å¤šé—´æ¥ç›¸å…³çš„è®°å¿†ï¼ˆç”¨äºæ·±åº¦æ¢ç´¢ï¼‰",
                    },
                },
                "required": ["query"],
            },
        }

    async def create_memory(self, **params) -> Dict[str, Any]:
        """
        æ‰§è¡Œ create_memory å·¥å…·
        
        Args:
            **params: å·¥å…·å‚æ•°
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f"åˆ›å»ºè®°å¿†: {params.get('subject')} - {params.get('topic')}")

            # 0. ç¡®ä¿åˆå§‹åŒ–
            await self._ensure_initialized()

            # 1. æå–å‚æ•°
            extracted = self.extractor.extract_from_tool_params(params)

            # 2. æ„å»ºè®°å¿†
            memory = await self.builder.build_memory(extracted)

            # 3. æ·»åŠ åˆ°å­˜å‚¨ï¼ˆæš‚å­˜çŠ¶æ€ï¼‰
            await self._add_memory_to_stores(memory)

            # 4. ä¿å­˜åˆ°ç£ç›˜
            await self.persistence_manager.save_graph_store(self.graph_store)

            logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory.id}")

            return {
                "success": True,
                "memory_id": memory.id,
                "message": f"è®°å¿†å·²åˆ›å»º: {extracted['subject']} - {extracted['topic']}",
                "nodes_count": len(memory.nodes),
                "edges_count": len(memory.edges),
            }

        except Exception as e:
            logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†åˆ›å»ºå¤±è´¥",
            }

    async def link_memories(self, **params) -> Dict[str, Any]:
        """
        æ‰§è¡Œ link_memories å·¥å…·
        
        Args:
            **params: å·¥å…·å‚æ•°
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(
                f"å…³è”è®°å¿†: {params.get('source_memory_description')} -> "
                f"{params.get('target_memory_description')}"
            )

            # 1. æå–å‚æ•°
            extracted = self.extractor.extract_link_params(params)

            # 2. æŸ¥æ‰¾æºè®°å¿†å’Œç›®æ ‡è®°å¿†
            source_memory = await self._find_memory_by_description(
                extracted["source_description"]
            )
            target_memory = await self._find_memory_by_description(
                extracted["target_description"]
            )

            if not source_memory:
                return {
                    "success": False,
                    "error": "æ‰¾ä¸åˆ°æºè®°å¿†",
                    "message": f"æœªæ‰¾åˆ°åŒ¹é…çš„æºè®°å¿†: {extracted['source_description']}",
                }

            if not target_memory:
                return {
                    "success": False,
                    "error": "æ‰¾ä¸åˆ°ç›®æ ‡è®°å¿†",
                    "message": f"æœªæ‰¾åˆ°åŒ¹é…çš„ç›®æ ‡è®°å¿†: {extracted['target_description']}",
                }

            # 3. åˆ›å»ºå…³è”è¾¹
            edge = await self.builder.link_memories(
                source_memory=source_memory,
                target_memory=target_memory,
                relation_type=extracted["relation_type"],
                importance=extracted["importance"],
            )

            # 4. æ·»åŠ è¾¹åˆ°å›¾å­˜å‚¨
            self.graph_store.graph.add_edge(
                edge.source_id,
                edge.target_id,
                relation=edge.relation,
                edge_type=edge.edge_type.value,
                importance=edge.importance,
                **edge.metadata
            )

            # 5. ä¿å­˜
            await self.persistence_manager.save_graph_store(self.graph_store)

            logger.info(f"è®°å¿†å…³è”æˆåŠŸ: {source_memory.id} -> {target_memory.id}")

            return {
                "success": True,
                "message": f"è®°å¿†å·²å…³è”: {extracted['relation_type']}",
                "source_memory_id": source_memory.id,
                "target_memory_id": target_memory.id,
                "relation_type": extracted["relation_type"],
            }

        except Exception as e:
            logger.error(f"è®°å¿†å…³è”å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†å…³è”å¤±è´¥",
            }

    async def search_memories(self, **params) -> Dict[str, Any]:
        """
        æ‰§è¡Œ search_memories å·¥å…·
        
        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼š
        1. æŸ¥è¯¢åˆ†è§£ï¼ˆè¯†åˆ«ä¸»è¦å®ä½“å’Œæ¦‚å¿µï¼‰
        2. å¤šæŸ¥è¯¢å¹¶è¡Œæ£€ç´¢
        3. ç»“æœèåˆå’Œé‡æ’
        
        Args:
            **params: å·¥å…·å‚æ•°
                - query: æŸ¥è¯¢å­—ç¬¦ä¸²
                - top_k: è¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤10ï¼‰
                - expand_depth: æ‰©å±•æ·±åº¦ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
                - use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆé»˜è®¤Trueï¼‰
                - context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            query = params.get("query", "")
            top_k = params.get("top_k", 10)
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼è€Œä¸æ˜¯ç¡¬ç¼–ç çš„ 1
            expand_depth = params.get("expand_depth", self.max_expand_depth)
            use_multi_query = params.get("use_multi_query", True)
            context = params.get("context", None)

            logger.info(f"æœç´¢è®°å¿†: {query} (top_k={top_k}, expand_depth={expand_depth}, multi_query={use_multi_query})")

            # 0. ç¡®ä¿åˆå§‹åŒ–
            await self._ensure_initialized()

            # 1. æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢æ–¹å¼
            if use_multi_query:
                # å¤šæŸ¥è¯¢ç­–ç•¥
                similar_nodes = await self._multi_query_search(query, top_k, context)
            else:
                # ä¼ ç»Ÿå•æŸ¥è¯¢ç­–ç•¥
                similar_nodes = await self._single_query_search(query, top_k)

            # 2. æå–åˆå§‹è®°å¿†IDï¼ˆæ¥è‡ªå‘é‡æœç´¢ï¼‰
            initial_memory_ids = set()
            memory_scores = {}  # è®°å½•æ¯ä¸ªè®°å¿†çš„åˆå§‹åˆ†æ•°
            
            for node_id, similarity, metadata in similar_nodes:
                if "memory_ids" in metadata:
                    ids = metadata["memory_ids"]
                    # ç¡®ä¿æ˜¯åˆ—è¡¨
                    if isinstance(ids, str):
                        import orjson
                        try:
                            ids = orjson.loads(ids)
                        except:
                            ids = [ids]
                    if isinstance(ids, list):
                        for mem_id in ids:
                            initial_memory_ids.add(mem_id)
                            # è®°å½•æœ€é«˜åˆ†æ•°
                            if mem_id not in memory_scores or similarity > memory_scores[mem_id]:
                                memory_scores[mem_id] = similarity

            # 3. å›¾æ‰©å±•ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰expand_depthï¼‰
            expanded_memory_scores = {}
            if expand_depth > 0 and initial_memory_ids:
                logger.info(f"å¼€å§‹å›¾æ‰©å±•: åˆå§‹è®°å¿†{len(initial_memory_ids)}ä¸ª, æ·±åº¦={expand_depth}")
                
                # è·å–æŸ¥è¯¢çš„embeddingç”¨äºè¯­ä¹‰è¿‡æ»¤
                if self.builder.embedding_generator:
                    try:
                        query_embedding = await self.builder.embedding_generator.generate(query)
                        
                        # ç›´æ¥ä½¿ç”¨å›¾æ‰©å±•é€»è¾‘ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
                        expanded_results = await self._expand_with_semantic_filter(
                            initial_memory_ids=list(initial_memory_ids),
                            query_embedding=query_embedding,
                            max_depth=expand_depth,
                            semantic_threshold=0.5,
                            max_expanded=top_k * 2
                        )
                        
                        # æ—§ä»£ç ï¼ˆå¦‚æœéœ€è¦ä½¿ç”¨Managerï¼‰ï¼š
                        # from src.memory_graph.manager import MemoryManager
                        # manager = MemoryManager.get_instance()
                        # expanded_results = await manager.expand_memories_with_semantic_filter(
                        #     initial_memory_ids=list(initial_memory_ids),
                        #     query_embedding=query_embedding,
                        #     max_depth=expand_depth,
                        #     semantic_threshold=0.5,
                        #     max_expanded=top_k * 2
                        # )
                        
                        # åˆå¹¶æ‰©å±•ç»“æœ
                        for mem_id, score in expanded_results:
                            expanded_memory_scores[mem_id] = score
                        
                        logger.info(f"å›¾æ‰©å±•å®Œæˆ: æ–°å¢{len(expanded_memory_scores)}ä¸ªç›¸å…³è®°å¿†")
                        
                    except Exception as e:
                        logger.warning(f"å›¾æ‰©å±•å¤±è´¥: {e}")

            # 4. åˆå¹¶åˆå§‹è®°å¿†å’Œæ‰©å±•è®°å¿†
            all_memory_ids = set(initial_memory_ids) | set(expanded_memory_scores.keys())
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼šåˆå§‹è®°å¿†ä¿æŒåŸåˆ†æ•°ï¼Œæ‰©å±•è®°å¿†ä½¿ç”¨æ‰©å±•åˆ†æ•°
            final_scores = {}
            for mem_id in all_memory_ids:
                if mem_id in memory_scores:
                    # åˆå§‹è®°å¿†ï¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                    final_scores[mem_id] = memory_scores[mem_id]
                elif mem_id in expanded_memory_scores:
                    # æ‰©å±•è®°å¿†ï¼šä½¿ç”¨å›¾æ‰©å±•åˆ†æ•°ï¼ˆç¨å¾®é™æƒï¼‰
                    final_scores[mem_id] = expanded_memory_scores[mem_id] * 0.8
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_memory_ids = sorted(
                final_scores.keys(),
                key=lambda x: final_scores[x],
                reverse=True
            )[:top_k * 2]  # å–2å€æ•°é‡ç”¨äºåç»­è¿‡æ»¤

            # 5. è·å–å®Œæ•´è®°å¿†å¹¶è¿›è¡Œæœ€ç»ˆæ’åº
            memories_with_scores = []
            for memory_id in sorted_memory_ids:
                memory = self.graph_store.get_memory_by_id(memory_id)
                if memory:
                    # ç»¼åˆè¯„åˆ†ï¼šç›¸ä¼¼åº¦(60%) + é‡è¦æ€§(30%) + æ—¶æ•ˆæ€§(10%)
                    similarity_score = final_scores[memory_id]
                    importance_score = memory.importance
                    
                    # è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°ï¼ˆæœ€è¿‘çš„è®°å¿†å¾—åˆ†æ›´é«˜ï¼‰
                    from datetime import datetime, timezone
                    now = datetime.now(timezone.utc)
                    # ç¡®ä¿ memory.created_at æœ‰æ—¶åŒºä¿¡æ¯
                    if memory.created_at.tzinfo is None:
                        memory_time = memory.created_at.replace(tzinfo=timezone.utc)
                    else:
                        memory_time = memory.created_at
                    age_days = (now - memory_time).total_seconds() / 86400
                    recency_score = 1.0 / (1.0 + age_days / 30)  # 30å¤©åŠè¡°æœŸ
                    
                    # ç»¼åˆåˆ†æ•°
                    final_score = (
                        similarity_score * 0.6 +
                        importance_score * 0.3 +
                        recency_score * 0.1
                    )
                    
                    memories_with_scores.append((memory, final_score))
            
            # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
            memories_with_scores.sort(key=lambda x: x[1], reverse=True)
            memories = [mem for mem, _ in memories_with_scores[:top_k]]

            # 6. æ ¼å¼åŒ–ç»“æœ
            results = []
            for memory in memories:
                result = {
                    "memory_id": memory.id,
                    "importance": memory.importance,
                    "created_at": memory.created_at.isoformat(),
                    "summary": self._summarize_memory(memory),
                }
                results.append(result)

            logger.info(
                f"æœç´¢å®Œæˆ: åˆå§‹{len(initial_memory_ids)}ä¸ª â†’ "
                f"æ‰©å±•{len(expanded_memory_scores)}ä¸ª â†’ "
                f"æœ€ç»ˆè¿”å›{len(results)}æ¡è®°å¿†"
            )

            return {
                "success": True,
                "results": results,
                "total": len(results),
                "query": query,
                "strategy": "multi_query" if use_multi_query else "single_query",
                "expanded_count": len(expanded_memory_scores),
                "expand_depth": expand_depth,
            }

        except Exception as e:
            logger.error(f"è®°å¿†æœç´¢å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†æœç´¢å¤±è´¥",
                "results": [],
            }

    async def _generate_multi_queries_simple(
        self, query: str, context: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[str, float]]:
        """
        ç®€åŒ–ç‰ˆå¤šæŸ¥è¯¢ç”Ÿæˆï¼ˆç›´æ¥åœ¨ Tools å±‚å®ç°ï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼‰
        
        è®©å°æ¨¡å‹ç›´æ¥ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢è¯­å¥ã€‚
        """
        try:
            from src.llm_models.utils_model import LLMRequest
            from src.config.config import model_config

            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.multi_query"
            )

            # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            participants = context.get("participants", []) if context else []
            chat_history = context.get("chat_history", "") if context else ""
            sender = context.get("sender", "") if context else ""

            # å¤„ç†èŠå¤©å†å²ï¼Œæå–æœ€è¿‘5æ¡å·¦å³çš„å¯¹è¯
            recent_chat = ""
            if chat_history:
                lines = chat_history.strip().split('\n')
                # å–æœ€è¿‘5æ¡æ¶ˆæ¯
                recent_lines = lines[-5:] if len(lines) > 5 else lines
                recent_chat = '\n'.join(recent_lines)

            prompt = f"""åŸºäºèŠå¤©ä¸Šä¸‹æ–‡ä¸ºæŸ¥è¯¢ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æœç´¢è¯­å¥ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

**å½“å‰æŸ¥è¯¢ï¼š** {query}
**å‘é€è€…ï¼š** {sender if sender else 'æœªçŸ¥'}
**å‚ä¸è€…ï¼š** {', '.join(participants) if participants else 'æ— '}

**æœ€è¿‘èŠå¤©è®°å½•ï¼ˆæœ€è¿‘5æ¡ï¼‰ï¼š**
{recent_chat if recent_chat else 'æ— èŠå¤©å†å²'}

**åˆ†æåŸåˆ™ï¼š**
1. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šæ ¹æ®èŠå¤©å†å²ç†è§£æŸ¥è¯¢çš„çœŸå®æ„å›¾
2. **æŒ‡ä»£æ¶ˆè§£**ï¼šè¯†åˆ«å¹¶ä»£æ¢"ä»–"ã€"å¥¹"ã€"å®ƒ"ã€"é‚£ä¸ª"ç­‰æŒ‡ä»£è¯
3. **è¯é¢˜å…³è”**ï¼šç»“åˆæœ€è¿‘è®¨è®ºçš„è¯é¢˜ç”Ÿæˆæ›´ç²¾å‡†çš„æŸ¥è¯¢
4. **æŸ¥è¯¢åˆ†è§£**ï¼šå¯¹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢

**ç”Ÿæˆç­–ç•¥ï¼š**
1. **å®Œæ•´æŸ¥è¯¢**ï¼ˆæƒé‡1.0ï¼‰ï¼šç»“åˆä¸Šä¸‹æ–‡çš„å®Œæ•´æŸ¥è¯¢ï¼ŒåŒ…å«æŒ‡ä»£æ¶ˆè§£
2. **å…³é”®æ¦‚å¿µæŸ¥è¯¢**ï¼ˆæƒé‡0.8ï¼‰ï¼šæŸ¥è¯¢ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯èŠå¤©ä¸­æåˆ°çš„å®ä½“
3. **è¯é¢˜æ‰©å±•æŸ¥è¯¢**ï¼ˆæƒé‡0.7ï¼‰ï¼šåŸºäºæœ€è¿‘èŠå¤©è¯é¢˜çš„ç›¸å…³æŸ¥è¯¢
4. **åŠ¨ä½œ/æƒ…æ„ŸæŸ¥è¯¢**ï¼ˆæƒé‡0.6ï¼‰ï¼šå¦‚æœæ¶‰åŠæƒ…æ„Ÿæˆ–åŠ¨ä½œï¼Œç”Ÿæˆç›¸å…³æŸ¥è¯¢

**è¾“å‡ºJSONæ ¼å¼ï¼š**
```json
{{"queries": [{{"text": "æŸ¥è¯¢è¯­å¥", "weight": 1.0}}, {{"text": "æŸ¥è¯¢è¯­å¥", "weight": 0.8}}]}}
```

**ç¤ºä¾‹ï¼š**
- æŸ¥è¯¢ï¼š"ä»–æ€ä¹ˆæ ·äº†ï¼Ÿ" + èŠå¤©ä¸­æåˆ°"å°æ˜ç”Ÿç—…äº†" â†’ "å°æ˜èº«ä½“æ¢å¤æƒ…å†µ"
- æŸ¥è¯¢ï¼š"é‚£ä¸ªé¡¹ç›®" + èŠå¤©ä¸­è®¨è®º"è®°å¿†ç³»ç»Ÿå¼€å‘" â†’ "è®°å¿†ç³»ç»Ÿé¡¹ç›®è¿›å±•"
"""

            response, _ = await llm.generate_response_async(prompt, temperature=0.3, max_tokens=250)
            
            import orjson, re
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*$', '', response).strip()
            
            data = orjson.loads(response)
            queries = data.get("queries", [])
            
            result = [(item.get("text", "").strip(), float(item.get("weight", 0.5))) 
                     for item in queries if item.get("text", "").strip()]
            
            if result:
                logger.info(f"ç”ŸæˆæŸ¥è¯¢: {[q for q, _ in result]}")
                return result
                
        except Exception as e:
            logger.warning(f"å¤šæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
        
        return [(query, 1.0)]

    async def _single_query_search(
        self, query: str, top_k: int
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        ä¼ ç»Ÿçš„å•æŸ¥è¯¢æœç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°
            
        Returns:
            ç›¸ä¼¼èŠ‚ç‚¹åˆ—è¡¨ [(node_id, similarity, metadata), ...]
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        if self.builder.embedding_generator:
            query_embedding = await self.builder.embedding_generator.generate(query)
        else:
            logger.warning("æœªé…ç½®åµŒå…¥ç”Ÿæˆå™¨ï¼Œä½¿ç”¨éšæœºå‘é‡")
            import numpy as np
            query_embedding = np.random.rand(384).astype(np.float32)

        # å‘é‡æœç´¢
        similar_nodes = await self.vector_store.search_similar_nodes(
            query_embedding=query_embedding,
            limit=top_k * 2,  # å¤šå–ä¸€äº›ï¼Œåç»­è¿‡æ»¤
        )

        return similar_nodes

    async def _multi_query_search(
        self, query: str, top_k: int, context: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        å¤šæŸ¥è¯¢ç­–ç•¥æœç´¢ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        ç›´æ¥ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ï¼Œæ— éœ€å¤æ‚çš„åˆ†è§£å’Œç»„åˆã€‚
        
        æ­¥éª¤ï¼š
        1. è®©å°æ¨¡å‹ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
        2. ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”ŸæˆåµŒå…¥
        3. å¹¶è¡Œæœç´¢å¹¶èåˆç»“æœ
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            
        Returns:
            èåˆåçš„ç›¸ä¼¼èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            # 1. ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢
            multi_queries = await self._generate_multi_queries_simple(query, context)
            
            logger.debug(f"ç”Ÿæˆ {len(multi_queries)} ä¸ªæŸ¥è¯¢: {multi_queries}")

            # 2. ç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢çš„åµŒå…¥
            if not self.builder.embedding_generator:
                logger.warning("æœªé…ç½®åµŒå…¥ç”Ÿæˆå™¨ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼")
                return await self._single_query_search(query, top_k)

            query_embeddings = []
            query_weights = []

            for sub_query, weight in multi_queries:
                embedding = await self.builder.embedding_generator.generate(sub_query)
                query_embeddings.append(embedding)
                query_weights.append(weight)

            # 3. å¤šæŸ¥è¯¢èåˆæœç´¢
            similar_nodes = await self.vector_store.search_with_multiple_queries(
                query_embeddings=query_embeddings,
                query_weights=query_weights,
                limit=top_k * 2,  # å¤šå–ä¸€äº›ï¼Œåç»­è¿‡æ»¤
                fusion_strategy="weighted_max",
            )

            logger.info(f"å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆ: {len(similar_nodes)} ä¸ªèŠ‚ç‚¹")

            return similar_nodes

        except Exception as e:
            logger.warning(f"å¤šæŸ¥è¯¢æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼: {e}", exc_info=True)
            return await self._single_query_search(query, top_k)

    async def _add_memory_to_stores(self, memory: Memory):
        """å°†è®°å¿†æ·»åŠ åˆ°å­˜å‚¨"""
        # 1. æ·»åŠ åˆ°å›¾å­˜å‚¨
        self.graph_store.add_memory(memory)

        # 2. æ·»åŠ æœ‰åµŒå…¥çš„èŠ‚ç‚¹åˆ°å‘é‡å­˜å‚¨
        for node in memory.nodes:
            if node.embedding is not None:
                await self.vector_store.add_node(node)

    async def _find_memory_by_description(self, description: str) -> Optional[Memory]:
        """
        é€šè¿‡æè¿°æŸ¥æ‰¾è®°å¿†
        
        Args:
            description: è®°å¿†æè¿°
            
        Returns:
            æ‰¾åˆ°çš„è®°å¿†ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        # ä½¿ç”¨è¯­ä¹‰æœç´¢æŸ¥æ‰¾æœ€ç›¸å…³çš„è®°å¿†
        if self.builder.embedding_generator:
            query_embedding = await self.builder.embedding_generator.generate(description)
        else:
            import numpy as np
            query_embedding = np.random.rand(384).astype(np.float32)

        # æœç´¢ç›¸ä¼¼èŠ‚ç‚¹
        similar_nodes = await self.vector_store.search_similar_nodes(
            query_embedding=query_embedding,
            limit=5,
        )

        if not similar_nodes:
            return None

        # è·å–æœ€ç›¸ä¼¼èŠ‚ç‚¹å…³è”çš„è®°å¿†
        node_id, similarity, metadata = similar_nodes[0]
        
        if "memory_ids" not in metadata or not metadata["memory_ids"]:
            return None
        
        ids = metadata["memory_ids"]
        
        # ç¡®ä¿æ˜¯åˆ—è¡¨
        if isinstance(ids, str):
            import orjson
            try:
                ids = orjson.loads(ids)
            except Exception as e:
                logger.warning(f"JSON è§£æå¤±è´¥: {e}")
                ids = [ids]
        
        if isinstance(ids, list) and ids:
            memory_id = ids[0]
            return self.graph_store.get_memory_by_id(memory_id)
        
        return None

    def _summarize_memory(self, memory: Memory) -> str:
        """ç”Ÿæˆè®°å¿†æ‘˜è¦"""
        if not memory.metadata:
            return "æœªçŸ¥è®°å¿†"

        subject = memory.metadata.get("subject", "")
        topic = memory.metadata.get("topic", "")
        memory_type = memory.metadata.get("memory_type", "")

        return f"{subject} - {memory_type}: {topic}"

    async def _expand_with_semantic_filter(
        self,
        initial_memory_ids: List[str],
        query_embedding,
        max_depth: int = 2,
        semantic_threshold: float = 0.5,
        max_expanded: int = 20
    ) -> List[Tuple[str, float]]:
        """
        ä»åˆå§‹è®°å¿†é›†åˆå‡ºå‘ï¼Œæ²¿å›¾ç»“æ„æ‰©å±•ï¼Œå¹¶ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤
        
        Args:
            initial_memory_ids: åˆå§‹è®°å¿†IDé›†åˆ
            query_embedding: æŸ¥è¯¢å‘é‡
            max_depth: æœ€å¤§æ‰©å±•æ·±åº¦
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
            max_expanded: æœ€å¤šæ‰©å±•å¤šå°‘ä¸ªè®°å¿†
            
        Returns:
            List[(memory_id, relevance_score)]
        """
        if not initial_memory_ids or query_embedding is None:
            return []
        
        try:
            import numpy as np
            
            visited_memories = set(initial_memory_ids)
            expanded_memories: Dict[str, float] = {}
            
            current_level = initial_memory_ids
            
            for depth in range(max_depth):
                next_level = []
                
                for memory_id in current_level:
                    memory = self.graph_store.get_memory_by_id(memory_id)
                    if not memory:
                        continue
                    
                    for node in memory.nodes:
                        if not node.has_embedding():
                            continue
                        
                        try:
                            neighbors = list(self.graph_store.graph.neighbors(node.id))
                        except:
                            continue
                        
                        for neighbor_id in neighbors:
                            neighbor_node_data = self.graph_store.graph.nodes.get(neighbor_id)
                            if not neighbor_node_data:
                                continue
                            
                            neighbor_vector_data = await self.vector_store.get_node_by_id(neighbor_id)
                            if neighbor_vector_data is None:
                                continue
                            
                            neighbor_embedding = neighbor_vector_data.get("embedding")
                            if neighbor_embedding is None:
                                continue
                            
                            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                            semantic_sim = self._cosine_similarity(
                                query_embedding,
                                neighbor_embedding
                            )
                            
                            # è·å–è¾¹æƒé‡
                            try:
                                edge_data = self.graph_store.graph.get_edge_data(node.id, neighbor_id)
                                edge_importance = edge_data.get("importance", 0.5) if edge_data else 0.5
                            except:
                                edge_importance = 0.5
                            
                            # ç»¼åˆè¯„åˆ†
                            depth_decay = 1.0 / (depth + 1)
                            relevance_score = (
                                semantic_sim * 0.7 + 
                                edge_importance * 0.2 + 
                                depth_decay * 0.1
                            )
                            
                            if relevance_score < semantic_threshold:
                                continue
                            
                            # æå–è®°å¿†ID
                            neighbor_memory_ids = neighbor_node_data.get("memory_ids", [])
                            if isinstance(neighbor_memory_ids, str):
                                import orjson
                                try:
                                    neighbor_memory_ids = orjson.loads(neighbor_memory_ids)
                                except:
                                    neighbor_memory_ids = [neighbor_memory_ids]
                            
                            for neighbor_mem_id in neighbor_memory_ids:
                                if neighbor_mem_id in visited_memories:
                                    continue
                                
                                if neighbor_mem_id not in expanded_memories:
                                    expanded_memories[neighbor_mem_id] = relevance_score
                                    visited_memories.add(neighbor_mem_id)
                                    next_level.append(neighbor_mem_id)
                                else:
                                    expanded_memories[neighbor_mem_id] = max(
                                        expanded_memories[neighbor_mem_id],
                                        relevance_score
                                    )
                
                if not next_level or len(expanded_memories) >= max_expanded:
                    break
                
                current_level = next_level[:max_expanded]
            
            sorted_results = sorted(
                expanded_memories.items(),
                key=lambda x: x[1],
                reverse=True
            )[:max_expanded]
            
            return sorted_results
            
        except Exception as e:
            logger.error(f"å›¾æ‰©å±•å¤±è´¥: {e}", exc_info=True)
            return []
    
    def _cosine_similarity(self, vec1, vec2) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            import numpy as np
            
            if not isinstance(vec1, np.ndarray):
                vec1 = np.array(vec1)
            if not isinstance(vec2, np.ndarray):
                vec2 = np.array(vec2)
            
            vec1_norm = np.linalg.norm(vec1)
            vec2_norm = np.linalg.norm(vec2)
            
            if vec1_norm == 0 or vec2_norm == 0:
                return 0.0
            
            similarity = np.dot(vec1, vec2) / (vec1_norm * vec2_norm)
            return float(similarity)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    @staticmethod
    def get_all_tool_schemas() -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥å…·çš„ schema
        
        Returns:
            å·¥å…· schema åˆ—è¡¨
        """
        return [
            MemoryTools.get_create_memory_schema(),
            MemoryTools.get_link_memories_schema(),
            MemoryTools.get_search_memories_schema(),
        ]
