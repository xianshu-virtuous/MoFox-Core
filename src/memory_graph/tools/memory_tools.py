"""
LLM å·¥å…·æ¥å£ï¼šå®šä¹‰è®°å¿†ç³»ç»Ÿçš„å·¥å…· schema å’Œæ‰§è¡Œé€»è¾‘
"""

from __future__ import annotations

from typing import Any

from src.common.logger import get_logger
from src.memory_graph.core.builder import MemoryBuilder
from src.memory_graph.core.extractor import MemoryExtractor
from src.memory_graph.models import Memory
from src.memory_graph.storage.graph_store import GraphStore
from src.memory_graph.storage.persistence import PersistenceManager
from src.memory_graph.storage.vector_store import VectorStore
from src.memory_graph.utils.embeddings import EmbeddingGenerator
from src.memory_graph.utils.graph_expansion import expand_memories_with_semantic_filter

logger = get_logger(__name__)


class MemoryTools:
    """
    è®°å¿†ç³»ç»Ÿå·¥å…·é›†

    æä¾›ç»™ LLM ä½¿ç”¨çš„å·¥å…·æ¥å£ï¼š
    1. create_memory: åˆ›å»ºæ–°è®°å¿†
    2. link_memories: å…³è”ä¸¤ä¸ªè®°å¿†
    3. search_memories: æœç´¢è®°å¿†
    """

    def __init__(
        self,
        vector_store: VectorStore,
        graph_store: GraphStore,
        persistence_manager: PersistenceManager,
        embedding_generator: EmbeddingGenerator | None = None,
        max_expand_depth: int = 1,
        expand_semantic_threshold: float = 0.3,
        search_top_k: int = 10,
        # æ–°å¢ï¼šæœç´¢æƒé‡é…ç½®
        search_vector_weight: float = 0.65,
        search_importance_weight: float = 0.25,
        search_recency_weight: float = 0.10,
        # æ–°å¢ï¼šé˜ˆå€¼è¿‡æ»¤é…ç½®
        search_min_importance: float = 0.3,
        search_similarity_threshold: float = 0.5,
    ):
        """
        åˆå§‹åŒ–å·¥å…·é›†

        Args:
            vector_store: å‘é‡å­˜å‚¨
            graph_store: å›¾å­˜å‚¨
            persistence_manager: æŒä¹…åŒ–ç®¡ç†å™¨
            embedding_generator: åµŒå…¥ç”Ÿæˆå™¨ï¼ˆå¯é€‰ï¼‰
            max_expand_depth: å›¾æ‰©å±•æ·±åº¦çš„é»˜è®¤å€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
            expand_semantic_threshold: å›¾æ‰©å±•æ—¶è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_top_k: é»˜è®¤æ£€ç´¢è¿”å›æ•°é‡ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_vector_weight: å‘é‡ç›¸ä¼¼åº¦æƒé‡ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_importance_weight: é‡è¦æ€§æƒé‡ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_recency_weight: æ—¶æ•ˆæ€§æƒé‡ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_min_importance: æœ€å°é‡è¦æ€§é˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
            search_similarity_threshold: å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼‰
        """
        self.vector_store = vector_store
        self.graph_store = graph_store
        self.persistence_manager = persistence_manager
        self._initialized = False
        self.max_expand_depth = max_expand_depth
        self.expand_semantic_threshold = expand_semantic_threshold
        self.search_top_k = search_top_k
        
        # ä¿å­˜æƒé‡é…ç½®
        self.base_vector_weight = search_vector_weight
        self.base_importance_weight = search_importance_weight
        self.base_recency_weight = search_recency_weight
        
        # ä¿å­˜é˜ˆå€¼è¿‡æ»¤é…ç½®
        self.search_min_importance = search_min_importance
        self.search_similarity_threshold = search_similarity_threshold

        logger.info(
            f"MemoryTools åˆå§‹åŒ–: max_expand_depth={max_expand_depth}, "
            f"expand_semantic_threshold={expand_semantic_threshold}, "
            f"search_top_k={search_top_k}, "
            f"æƒé‡é…ç½®: vector={search_vector_weight}, importance={search_importance_weight}, recency={search_recency_weight}, "
            f"é˜ˆå€¼è¿‡æ»¤: min_importance={search_min_importance}, similarity_threshold={search_similarity_threshold}"
        )

        # åˆå§‹åŒ–ç»„ä»¶
        self.extractor = MemoryExtractor()
        self.builder = MemoryBuilder(
            vector_store=vector_store,
            graph_store=graph_store,
            embedding_generator=embedding_generator,
        )

    async def _ensure_initialized(self):
        """ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–"""
        if not self._initialized:
            await self.vector_store.initialize()
            self._initialized = True

    @staticmethod
    def get_create_memory_schema() -> dict[str, Any]:
        """
        è·å– create_memory å·¥å…·çš„ JSON schema

        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "create_memory",
            "description": """åˆ›å»ºä¸€ä¸ªæ–°çš„è®°å¿†èŠ‚ç‚¹ï¼Œè®°å½•å¯¹è¯ä¸­æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

ğŸ¯ **æ ¸å¿ƒåŸåˆ™**ï¼šä¸»åŠ¨è®°å½•ã€ç§¯ææ„å»ºã€ä¸°å¯Œç»†èŠ‚

âœ… **ä¼˜å…ˆåˆ›å»ºè®°å¿†çš„åœºæ™¯**ï¼ˆé¼“åŠ±è®°å½•ï¼‰ï¼š
1. **ä¸ªäººä¿¡æ¯**ï¼šå§“åã€æ˜µç§°ã€å¹´é¾„ã€èŒä¸šã€èº«ä»½ã€æ‰€åœ¨åœ°ã€è”ç³»æ–¹å¼ç­‰
2. **å…´è¶£çˆ±å¥½**ï¼šå–œæ¬¢/ä¸å–œæ¬¢çš„äº‹ç‰©ã€å¨±ä¹åå¥½ã€è¿åŠ¨çˆ±å¥½ã€é¥®é£Ÿå£å‘³ç­‰
3. **ç”Ÿæ´»çŠ¶æ€**ï¼šå·¥ä½œå­¦ä¹ çŠ¶æ€ã€ç”Ÿæ´»ä¹ æƒ¯ã€ä½œæ¯æ—¶é—´ã€æ—¥å¸¸å®‰æ’ç­‰
4. **ç»å†äº‹ä»¶**ï¼šæ­£åœ¨åšçš„äº‹ã€å®Œæˆçš„ä»»åŠ¡ã€å‚ä¸çš„æ´»åŠ¨ã€é‡åˆ°çš„é—®é¢˜ç­‰
5. **è§‚ç‚¹æ€åº¦**ï¼šå¯¹äº‹ç‰©çš„çœ‹æ³•ã€ä»·å€¼è§‚ã€æƒ…ç»ªè¡¨è¾¾ã€è¯„ä»·æ„è§ç­‰
6. **è®¡åˆ’ç›®æ ‡**ï¼šæœªæ¥æ‰“ç®—ã€å­¦ä¹ è®¡åˆ’ã€å·¥ä½œç›®æ ‡ã€å¾…åŠäº‹é¡¹ç­‰
7. **äººé™…å…³ç³»**ï¼šæåˆ°çš„æœ‹å‹ã€å®¶äººã€åŒäº‹ã€è®¤è¯†çš„äººç­‰
8. **æŠ€èƒ½çŸ¥è¯†**ï¼šæŒæ¡çš„æŠ€èƒ½ã€å­¦ä¹ çš„çŸ¥è¯†ã€ä¸“ä¸šé¢†åŸŸã€ä½¿ç”¨çš„å·¥å…·ç­‰
9. **ç‰©å“èµ„æº**ï¼šæ‹¥æœ‰çš„ç‰©å“ã€ä½¿ç”¨çš„è®¾å¤‡ã€å–œæ¬¢çš„å“ç‰Œç­‰
10. **æ—¶é—´åœ°ç‚¹**ï¼šé‡è¦æ—¶é—´èŠ‚ç‚¹ã€å¸¸å»çš„åœ°ç‚¹ã€æ´»åŠ¨åœºæ‰€ç­‰

âš ï¸ **æš‚ä¸åˆ›å»ºçš„æƒ…å†µ**ï¼ˆä»…é™ä»¥ä¸‹ï¼‰ï¼š
- çº¯ç²¹çš„æ‹›å‘¼è¯­ï¼ˆå•çº¯çš„"ä½ å¥½"ã€"å†è§"ï¼‰
- å®Œå…¨æ— æ„ä¹‰çš„è¯­æ°”è¯ï¼ˆå•çº¯çš„"å“¦"ã€"å—¯"ï¼‰
- æ˜ç¡®çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆå¦‚"åˆ‡æ¢æ¨¡å¼"ã€"é‡å¯"ï¼‰

ï¿½ **è®°å¿†æ‹†åˆ†å»ºè®®**ï¼š
- ä¸€å¥è¯åŒ…å«å¤šä¸ªä¿¡æ¯ç‚¹ â†’ æ‹†æˆå¤šæ¡è®°å¿†ï¼ˆæ›´åˆ©äºåç»­æ£€ç´¢ï¼‰
- ä¾‹å¦‚ï¼š"æˆ‘æœ€è¿‘åœ¨å­¦Pythonå’Œæœºå™¨å­¦ä¹ ï¼Œæƒ³æ‰¾å·¥ä½œ"
  â†’ æ‹†æˆ3æ¡ï¼š
  1. "ç”¨æˆ·æ­£åœ¨å­¦ä¹ Python"ï¼ˆäº‹ä»¶ï¼‰
  2. "ç”¨æˆ·æ­£åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ "ï¼ˆäº‹ä»¶ï¼‰
  3. "ç”¨æˆ·æƒ³æ‰¾å·¥ä½œ"ï¼ˆäº‹ä»¶/ç›®æ ‡ï¼‰

ğŸ“Œ **è®°å¿†è´¨é‡å»ºè®®**ï¼š
- è®°å½•æ—¶å°½é‡è¡¥å……æ—¶é—´ï¼ˆ"ä»Šå¤©"ã€"æœ€è¿‘"ã€"æ˜¨å¤©"ç­‰ï¼‰
- åŒ…å«å…·ä½“ç»†èŠ‚ï¼ˆè¶Šå…·ä½“è¶Šå¥½ï¼‰
- ä¸»ä½“æ˜ç¡®ï¼ˆä¼˜å…ˆä½¿ç”¨"ç”¨æˆ·"æˆ–å…·ä½“äººåï¼Œé¿å…"æˆ‘"ï¼‰

è®°å¿†ç»“æ„ï¼šä¸»ä½“ + ç±»å‹ + ä¸»é¢˜ + å®¢ä½“ï¼ˆå¯é€‰ï¼‰+ å±æ€§ï¼ˆè¶Šè¯¦ç»†è¶Šå¥½ï¼‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "subject": {
                        "type": "string",
                        "description": "è®°å¿†çš„ä¸»ä½“ï¼ˆè°çš„ä¿¡æ¯ï¼‰ï¼š\n- å¯¹è¯ä¸­çš„ç”¨æˆ·ç»Ÿä¸€ä½¿ç”¨'ç”¨æˆ·'\n- æåˆ°çš„å…·ä½“äººç‰©ä½¿ç”¨å…¶åå­—ï¼ˆå¦‚'å°æ˜'ã€'å¼ ä¸‰'ï¼‰\n- é¿å…ä½¿ç”¨'æˆ‘'ã€'ä»–'ç­‰ä»£è¯",
                    },
                    "memory_type": {
                        "type": "string",
                        "enum": ["äº‹ä»¶", "äº‹å®", "å…³ç³»", "è§‚ç‚¹"],
                        "description": "é€‰æ‹©æœ€åˆé€‚çš„è®°å¿†ç±»å‹ï¼š\n\nã€äº‹ä»¶ã€‘æ—¶é—´ç›¸å…³çš„åŠ¨ä½œæˆ–å‘ç”Ÿçš„äº‹ï¼ˆç”¨'æ­£åœ¨'ã€'å®Œæˆäº†'ã€'å‚åŠ 'ç­‰åŠ¨è¯ï¼‰\n  ä¾‹ï¼šæ­£åœ¨å­¦ä¹ Pythonã€å®Œæˆäº†é¡¹ç›®ã€å‚åŠ ä¼šè®®ã€å»æ—…è¡Œ\n\nã€äº‹å®ã€‘ç›¸å¯¹ç¨³å®šçš„å®¢è§‚ä¿¡æ¯ï¼ˆç”¨'æ˜¯'ã€'æœ‰'ã€'åœ¨'ç­‰æè¿°çŠ¶æ€ï¼‰\n  ä¾‹ï¼šèŒä¸šæ˜¯å·¥ç¨‹å¸ˆã€ä½åœ¨åŒ—äº¬ã€æœ‰ä¸€åªçŒ«ã€ä¼šè¯´è‹±è¯­\n\nã€è§‚ç‚¹ã€‘ä¸»è§‚çœ‹æ³•ã€å–œå¥½ã€æ€åº¦ï¼ˆç”¨'å–œæ¬¢'ã€'è®¤ä¸º'ã€'è§‰å¾—'ç­‰ï¼‰\n  ä¾‹ï¼šå–œæ¬¢Pythonã€è®¤ä¸ºAIå¾ˆé‡è¦ã€è§‰å¾—ç´¯ã€è®¨åŒåŠ ç­\n\nã€å…³ç³»ã€‘äººä¸äººä¹‹é—´çš„å…³ç³»\n  ä¾‹ï¼šè®¤è¯†äº†æœ‹å‹ã€æ˜¯åŒäº‹ã€å®¶äººå…³ç³»",
                    },
                    "topic": {
                        "type": "string",
                        "description": "è®°å¿†çš„æ ¸å¿ƒå†…å®¹ï¼ˆåšä»€ä¹ˆ/æ˜¯ä»€ä¹ˆ/å…³äºä»€ä¹ˆï¼‰ï¼š\n- å°½é‡å…·ä½“æ˜ç¡®ï¼ˆ'å­¦ä¹ Pythonç¼–ç¨‹' ä¼˜äº 'å­¦ä¹ 'ï¼‰\n- åŒ…å«å…³é”®åŠ¨è¯æˆ–æ ¸å¿ƒæ¦‚å¿µ\n- å¯ä»¥åŒ…å«æ—¶é—´çŠ¶æ€ï¼ˆ'æ­£åœ¨å­¦ä¹ 'ã€'å·²å®Œæˆ'ã€'è®¡åˆ’åš'ï¼‰",
                    },
                    "object": {
                        "type": "string",
                        "description": "å¯é€‰ï¼šè®°å¿†æ¶‰åŠçš„å¯¹è±¡æˆ–ç›®æ ‡ï¼š\n- äº‹ä»¶çš„å¯¹è±¡ï¼ˆå­¦ä¹ çš„æ˜¯ä»€ä¹ˆã€è´­ä¹°çš„æ˜¯ä»€ä¹ˆï¼‰\n- è§‚ç‚¹çš„å¯¹è±¡ï¼ˆå–œæ¬¢çš„æ˜¯ä»€ä¹ˆã€è®¨åŒçš„æ˜¯ä»€ä¹ˆï¼‰\n- å¯ä»¥ç•™ç©ºï¼ˆå¦‚æœtopicå·²ç»è¶³å¤Ÿå®Œæ•´ï¼‰",
                    },
                    "attributes": {
                        "type": "object",
                        "description": "è®°å¿†çš„è¯¦ç»†å±æ€§ï¼ˆå»ºè®®å°½é‡å¡«å†™ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼‰ï¼š",
                        "properties": {
                            "æ—¶é—´": {
                                "type": "string",
                                "description": "æ—¶é—´ä¿¡æ¯ï¼ˆå¼ºçƒˆå»ºè®®å¡«å†™ï¼‰ï¼š\n- å…·ä½“æ—¥æœŸï¼š'2025-11-05'ã€'2025å¹´11æœˆ'\n- ç›¸å¯¹æ—¶é—´ï¼š'ä»Šå¤©'ã€'æ˜¨å¤©'ã€'ä¸Šå‘¨'ã€'æœ€è¿‘'ã€'3å¤©å‰'\n- æ—¶é—´æ®µï¼š'ä»Šå¤©ä¸‹åˆ'ã€'ä¸Šä¸ªæœˆ'ã€'è¿™å­¦æœŸ'",
                            },
                            "åœ°ç‚¹": {
                                "type": "string",
                                "description": "åœ°ç‚¹ä¿¡æ¯ï¼ˆå¦‚æ¶‰åŠï¼‰ï¼š\n- å…·ä½“åœ°å€ã€åŸå¸‚åã€å›½å®¶\n- åœºæ‰€ç±»å‹ï¼š'åœ¨å®¶'ã€'å…¬å¸'ã€'å­¦æ ¡'ã€'å’–å•¡åº—'"
                            },
                            "åŸå› ": {
                                "type": "string",
                                "description": "ä¸ºä»€ä¹ˆè¿™æ ·åš/è¿™æ ·æƒ³ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "æ–¹å¼": {
                                "type": "string",
                                "description": "æ€ä¹ˆåšçš„/é€šè¿‡ä»€ä¹ˆæ–¹å¼ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "ç»“æœ": {
                                "type": "string",
                                "description": "ç»“æœå¦‚ä½•/äº§ç”Ÿä»€ä¹ˆå½±å“ï¼ˆå¦‚æ˜ç¡®æåˆ°ï¼‰"
                            },
                            "çŠ¶æ€": {
                                "type": "string",
                                "description": "å½“å‰è¿›å±•ï¼š'è¿›è¡Œä¸­'ã€'å·²å®Œæˆ'ã€'è®¡åˆ’ä¸­'ã€'æš‚åœ'ç­‰"
                            },
                            "ç¨‹åº¦": {
                                "type": "string",
                                "description": "ç¨‹åº¦æè¿°ï¼ˆå¦‚'éå¸¸'ã€'æ¯”è¾ƒ'ã€'æœ‰ç‚¹'ã€'ä¸å¤ª'ï¼‰"
                            },
                        },
                        "additionalProperties": True,
                    },
                    "importance": {
                        "type": "number",
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "é‡è¦æ€§è¯„åˆ†ï¼ˆé»˜è®¤0.5ï¼Œæ—¥å¸¸å¯¹è¯å»ºè®®0.5-0.7ï¼‰ï¼š\n\n0.3-0.4: æ¬¡è¦ç»†èŠ‚ï¼ˆå¶ç„¶æåŠçš„çäº‹ï¼‰\n0.5-0.6: æ—¥å¸¸ä¿¡æ¯ï¼ˆä¸€èˆ¬æ€§çš„åˆ†äº«ã€æ™®é€šçˆ±å¥½ï¼‰â† æ¨èé»˜è®¤å€¼\n0.7-0.8: é‡è¦ä¿¡æ¯ï¼ˆæ˜ç¡®çš„åå¥½ã€é‡è¦è®¡åˆ’ã€æ ¸å¿ƒçˆ±å¥½ï¼‰\n0.9-1.0: å…³é”®ä¿¡æ¯ï¼ˆèº«ä»½ä¿¡æ¯ã€é‡å¤§å†³å®šã€å¼ºçƒˆæƒ…æ„Ÿï¼‰\n\nğŸ’¡ å»ºè®®ï¼šæ—¥å¸¸å¯¹è¯ä¸­å¤§éƒ¨åˆ†è®°å¿†ä½¿ç”¨0.5-0.6ï¼Œé™¤éç”¨æˆ·ç‰¹åˆ«å¼ºè°ƒ",
                    },
                },
                "required": ["subject", "memory_type", "topic"],
            },
        }

    @staticmethod
    def get_link_memories_schema() -> dict[str, Any]:
        """
        è·å– link_memories å·¥å…·çš„ JSON schema

        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "link_memories",
            "description": """æ‰‹åŠ¨å…³è”ä¸¤ä¸ªå·²å­˜åœ¨çš„è®°å¿†ã€‚

âš ï¸ ä½¿ç”¨å»ºè®®ï¼š
- ç³»ç»Ÿä¼šè‡ªåŠ¨å‘ç°è®°å¿†é—´çš„å…³è”å…³ç³»ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨æ­¤å·¥å…·
- ä»…åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨ï¼š
  1. ç”¨æˆ·æ˜ç¡®æŒ‡å‡ºä¸¤ä¸ªè®°å¿†ä¹‹é—´çš„å…³ç³»
  2. å‘ç°æ˜æ˜¾çš„å› æœå…³ç³»ä½†ç³»ç»Ÿæœªè‡ªåŠ¨å…³è”
  3. éœ€è¦å»ºç«‹ç‰¹æ®Šçš„å¼•ç”¨å…³ç³»

å…³ç³»ç±»å‹è¯´æ˜ï¼š
- å¯¼è‡´ï¼šAäº‹ä»¶/è¡Œä¸ºå¯¼è‡´Bäº‹ä»¶/ç»“æœï¼ˆå› æœå…³ç³»ï¼‰
- å¼•ç”¨ï¼šAè®°å¿†å¼•ç”¨/åŸºäºBè®°å¿†ï¼ˆçŸ¥è¯†å…³è”ï¼‰
- ç›¸ä¼¼ï¼šAå’ŒBæè¿°ç›¸ä¼¼çš„å†…å®¹ï¼ˆä¸»é¢˜ç›¸ä¼¼ï¼‰
- ç›¸åï¼šAå’ŒBè¡¨è¾¾ç›¸åçš„è§‚ç‚¹ï¼ˆå¯¹æ¯”å…³ç³»ï¼‰
- å…³è”ï¼šAå’ŒBå­˜åœ¨ä¸€èˆ¬æ€§å…³è”ï¼ˆå…¶ä»–å…³ç³»ï¼‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "source_memory_description": {
                        "type": "string",
                        "description": "æºè®°å¿†çš„å…³é”®æè¿°ï¼ˆç”¨äºæœç´¢å®šä½ï¼Œéœ€è¦è¶³å¤Ÿå…·ä½“ï¼‰",
                    },
                    "target_memory_description": {
                        "type": "string",
                        "description": "ç›®æ ‡è®°å¿†çš„å…³é”®æè¿°ï¼ˆç”¨äºæœç´¢å®šä½ï¼Œéœ€è¦è¶³å¤Ÿå…·ä½“ï¼‰",
                    },
                    "relation_type": {
                        "type": "string",
                        "enum": ["å¯¼è‡´", "å¼•ç”¨", "ç›¸ä¼¼", "ç›¸å", "å…³è”"],
                        "description": "å…³ç³»ç±»å‹ï¼ˆä»ä¸Šè¿°5ç§ç±»å‹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ï¼‰",
                    },
                    "importance": {
                        "type": "number",
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "å…³ç³»çš„é‡è¦æ€§ï¼ˆ0.0-1.0ï¼‰ï¼š\n- 0.5-0.6: ä¸€èˆ¬å…³è”\n- 0.7-0.8: é‡è¦å…³è”\n- 0.9-1.0: å…³é”®å…³è”\né»˜è®¤0.6",
                    },
                },
                "required": [
                    "source_memory_description",
                    "target_memory_description",
                    "relation_type",
                ],
            },
        }

    @staticmethod
    def get_search_memories_schema() -> dict[str, Any]:
        """
        è·å– search_memories å·¥å…·çš„ JSON schema

        Returns:
            å·¥å…· schema å®šä¹‰
        """
        return {
            "name": "search_memories",
            "description": """æœç´¢ç›¸å…³çš„è®°å¿†ï¼Œç”¨äºå›å¿†å’ŒæŸ¥æ‰¾å†å²ä¿¡æ¯ã€‚

ä½¿ç”¨åœºæ™¯ï¼š
- ç”¨æˆ·è¯¢é—®ä¹‹å‰çš„å¯¹è¯å†…å®¹
- éœ€è¦å›å¿†ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ã€åå¥½ã€ç»å†
- æŸ¥æ‰¾ç›¸å…³çš„å†å²äº‹ä»¶æˆ–è§‚ç‚¹
- åŸºäºä¸Šä¸‹æ–‡è¡¥å……ä¿¡æ¯

æœç´¢ç‰¹æ€§ï¼š
- è¯­ä¹‰æœç´¢ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦åŒ¹é…
- å›¾éå†ï¼šè‡ªåŠ¨æ‰©å±•ç›¸å…³è”çš„è®°å¿†
- æ—¶é—´è¿‡æ»¤ï¼šæŒ‰æ—¶é—´èŒƒå›´ç­›é€‰
- ç±»å‹è¿‡æ»¤ï¼šæŒ‰è®°å¿†ç±»å‹ç­›é€‰""",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢ï¼ˆç”¨è‡ªç„¶è¯­è¨€æè¿°è¦æŸ¥æ‰¾çš„å†…å®¹ï¼Œå¦‚'ç”¨æˆ·çš„èŒä¸š'ã€'æœ€è¿‘çš„é¡¹ç›®'ã€'Pythonç›¸å…³çš„è®°å¿†'ï¼‰",
                    },
                    "memory_types": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": ["äº‹ä»¶", "äº‹å®", "å…³ç³»", "è§‚ç‚¹"],
                        },
                        "description": "è®°å¿†ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œç•™ç©ºè¡¨ç¤ºæœç´¢æ‰€æœ‰ç±»å‹ï¼‰",
                    },
                    "time_range": {
                        "type": "object",
                        "properties": {
                            "start": {
                                "type": "string",
                                "description": "å¼€å§‹æ—¶é—´ï¼ˆå¦‚'3å¤©å‰'ã€'ä¸Šå‘¨'ã€'2025-11-01'ï¼‰",
                            },
                            "end": {
                                "type": "string",
                                "description": "ç»“æŸæ—¶é—´ï¼ˆå¦‚'ä»Šå¤©'ã€'ç°åœ¨'ã€'2025-11-05'ï¼‰",
                            },
                        },
                        "description": "æ—¶é—´èŒƒå›´ï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥æ‰¾ç‰¹å®šæ—¶é—´æ®µçš„è®°å¿†ï¼‰",
                    },
                    "top_k": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 50,
                        "description": "è¿”å›ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨ç³»ç»Ÿé…ç½®ï¼‰ã€‚æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼š\n- å¿«é€ŸæŸ¥æ‰¾ï¼š3-5æ¡\n- ä¸€èˆ¬æœç´¢ï¼š10-15æ¡\n- å…¨é¢äº†è§£ï¼š20-30æ¡\n- æ·±åº¦æ¢ç´¢ï¼š40-50æ¡\nå»ºè®®ï¼šé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œå¦åˆ™ä¸æŒ‡å®šæ­¤å‚æ•°ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨å†³å®šã€‚",
                    },
                    "expand_depth": {
                        "type": "integer",
                        "minimum": 0,
                        "maximum": 3,
                        "description": "å›¾æ‰©å±•æ·±åº¦ï¼ˆ0-3ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨ç³»ç»Ÿé…ç½®ï¼Œé€šå¸¸ä¸º2ï¼‰ï¼š\n- 0: ä»…è¿”å›ç›´æ¥åŒ¹é…çš„è®°å¿†\n- 1: åŒ…å«ä¸€åº¦ç›¸å…³çš„è®°å¿†\n- 2: åŒ…å«äºŒåº¦ç›¸å…³çš„è®°å¿†ï¼ˆæ¨èï¼‰\n- 3: åŒ…å«ä¸‰åº¦ç›¸å…³çš„è®°å¿†ï¼ˆæ·±åº¦æ¢ç´¢ï¼‰\nå»ºè®®ï¼šé€šå¸¸ä¸éœ€è¦æŒ‡å®šï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ·±åº¦ã€‚",
                    },
                    "prefer_node_types": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": ["ATTRIBUTE", "REFERENCE", "ENTITY", "EVENT", "RELATION"],
                        },
                        "description": "ä¼˜å…ˆå¬å›çš„èŠ‚ç‚¹ç±»å‹ï¼ˆå¯é€‰ï¼‰ï¼š\n- ATTRIBUTE: å±æ€§ä¿¡æ¯ï¼ˆå¦‚é…ç½®ã€å‚æ•°ï¼‰\n- REFERENCE: å¼•ç”¨ä¿¡æ¯ï¼ˆå¦‚æ–‡æ¡£åœ°å€ã€é“¾æ¥ï¼‰\n- ENTITY: å®ä½“ä¿¡æ¯ï¼ˆå¦‚äººç‰©ã€ç»„ç»‡ï¼‰\n- EVENT: äº‹ä»¶ä¿¡æ¯ï¼ˆå¦‚æ´»åŠ¨ã€å¯¹è¯ï¼‰\n- RELATION: å…³ç³»ä¿¡æ¯ï¼ˆå¦‚äººé™…å…³ç³»ï¼‰",
                    },
                },
                "required": ["query"],
            },
        }

    async def create_memory(self, **params) -> dict[str, Any]:
        """
        æ‰§è¡Œ create_memory å·¥å…·

        Args:
            **params: å·¥å…·å‚æ•°

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f"åˆ›å»ºè®°å¿†: {params.get('subject')} - {params.get('topic')}")

            # 0. ç¡®ä¿åˆå§‹åŒ–
            await self._ensure_initialized()

            # 1. æå–å‚æ•°
            extracted = self.extractor.extract_from_tool_params(params)

            # 2. æ„å»ºè®°å¿†
            memory = await self.builder.build_memory(extracted)

            # 3. æ·»åŠ åˆ°å­˜å‚¨ï¼ˆæš‚å­˜çŠ¶æ€ï¼‰
            await self._add_memory_to_stores(memory)

            # 4. ä¿å­˜åˆ°ç£ç›˜
            await self.persistence_manager.save_graph_store(self.graph_store)

            logger.info(f"è®°å¿†åˆ›å»ºæˆåŠŸ: {memory.id}")

            return {
                "success": True,
                "memory_id": memory.id,
                "message": f"è®°å¿†å·²åˆ›å»º: {extracted['subject']} - {extracted['topic']}",
                "nodes_count": len(memory.nodes),
                "edges_count": len(memory.edges),
            }

        except Exception as e:
            logger.error(f"è®°å¿†åˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†åˆ›å»ºå¤±è´¥",
            }

    async def link_memories(self, **params) -> dict[str, Any]:
        """
        æ‰§è¡Œ link_memories å·¥å…·

        Args:
            **params: å·¥å…·å‚æ•°

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            logger.info(
                f"å…³è”è®°å¿†: {params.get('source_memory_description')} -> "
                f"{params.get('target_memory_description')}"
            )

            # 1. æå–å‚æ•°
            extracted = self.extractor.extract_link_params(params)

            # 2. æŸ¥æ‰¾æºè®°å¿†å’Œç›®æ ‡è®°å¿†
            source_memory = await self._find_memory_by_description(
                extracted["source_description"]
            )
            target_memory = await self._find_memory_by_description(
                extracted["target_description"]
            )

            if not source_memory:
                return {
                    "success": False,
                    "error": "æ‰¾ä¸åˆ°æºè®°å¿†",
                    "message": f"æœªæ‰¾åˆ°åŒ¹é…çš„æºè®°å¿†: {extracted['source_description']}",
                }

            if not target_memory:
                return {
                    "success": False,
                    "error": "æ‰¾ä¸åˆ°ç›®æ ‡è®°å¿†",
                    "message": f"æœªæ‰¾åˆ°åŒ¹é…çš„ç›®æ ‡è®°å¿†: {extracted['target_description']}",
                }

            # 3. åˆ›å»ºå…³è”è¾¹
            edge = await self.builder.link_memories(
                source_memory=source_memory,
                target_memory=target_memory,
                relation_type=extracted["relation_type"],
                importance=extracted["importance"],
            )

            # 4. æ·»åŠ è¾¹åˆ°å›¾å­˜å‚¨
            self.graph_store.graph.add_edge(
                edge.source_id,
                edge.target_id,
                relation=edge.relation,
                edge_type=edge.edge_type.value,
                importance=edge.importance,
                **edge.metadata
            )

            # 5. ä¿å­˜
            await self.persistence_manager.save_graph_store(self.graph_store)

            logger.info(f"è®°å¿†å…³è”æˆåŠŸ: {source_memory.id} -> {target_memory.id}")

            return {
                "success": True,
                "message": f"è®°å¿†å·²å…³è”: {extracted['relation_type']}",
                "source_memory_id": source_memory.id,
                "target_memory_id": target_memory.id,
                "relation_type": extracted["relation_type"],
            }

        except Exception as e:
            logger.error(f"è®°å¿†å…³è”å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†å…³è”å¤±è´¥",
            }

    async def search_memories(self, **params) -> dict[str, Any]:
        """
        æ‰§è¡Œ search_memories å·¥å…·

        ä½¿ç”¨å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–ï¼š
        1. æŸ¥è¯¢åˆ†è§£ï¼ˆè¯†åˆ«ä¸»è¦å®ä½“å’Œæ¦‚å¿µï¼‰
        2. å¤šæŸ¥è¯¢å¹¶è¡Œæ£€ç´¢
        3. ç»“æœèåˆå’Œé‡æ’

        Args:
            **params: å·¥å…·å‚æ•°
                - query: æŸ¥è¯¢å­—ç¬¦ä¸²
                - top_k: è¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤10ï¼‰
                - expand_depth: æ‰©å±•æ·±åº¦ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
                - use_multi_query: æ˜¯å¦ä½¿ç”¨å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆé»˜è®¤Trueï¼‰
                - prefer_node_types: ä¼˜å…ˆå¬å›çš„èŠ‚ç‚¹ç±»å‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                - context: æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            æœç´¢ç»“æœ
        """
        try:
            query = params.get("query", "")
            top_k = params.get("top_k", self.search_top_k)  # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
            expand_depth = params.get("expand_depth", self.max_expand_depth)
            use_multi_query = params.get("use_multi_query", True)
            prefer_node_types = params.get("prefer_node_types", [])  # ğŸ†• ä¼˜å…ˆèŠ‚ç‚¹ç±»å‹
            context = params.get("context", None)

            logger.info(
                f"æœç´¢è®°å¿†: {query} (top_k={top_k}, expand_depth={expand_depth}, "
                f"multi_query={use_multi_query}, prefer_types={prefer_node_types})"
            )

            # 0. ç¡®ä¿åˆå§‹åŒ–
            await self._ensure_initialized()

            # 1. æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢æ–¹å¼
            llm_prefer_types = []  # LLMè¯†åˆ«çš„åå¥½èŠ‚ç‚¹ç±»å‹
            
            if use_multi_query:
                # å¤šæŸ¥è¯¢ç­–ç•¥ï¼ˆè¿”å›èŠ‚ç‚¹åˆ—è¡¨ + åå¥½ç±»å‹ï¼‰
                similar_nodes, llm_prefer_types = await self._multi_query_search(query, top_k, context)
            else:
                # ä¼ ç»Ÿå•æŸ¥è¯¢ç­–ç•¥
                similar_nodes = await self._single_query_search(query, top_k)
            
            # åˆå¹¶ç”¨æˆ·æŒ‡å®šçš„åå¥½ç±»å‹å’ŒLLMè¯†åˆ«çš„åå¥½ç±»å‹
            all_prefer_types = list(set(prefer_node_types + llm_prefer_types))
            if all_prefer_types:
                logger.info(f"æœ€ç»ˆåå¥½èŠ‚ç‚¹ç±»å‹: {all_prefer_types} (ç”¨æˆ·æŒ‡å®š: {prefer_node_types}, LLMè¯†åˆ«: {llm_prefer_types})")
                # æ›´æ–°prefer_node_typesç”¨äºåç»­è¯„åˆ†
                prefer_node_types = all_prefer_types

            # 2. æå–åˆå§‹è®°å¿†IDï¼ˆæ¥è‡ªå‘é‡æœç´¢ï¼‰
            initial_memory_ids = set()
            memory_scores = {}  # è®°å½•æ¯ä¸ªè®°å¿†çš„åˆå§‹åˆ†æ•°

            for node_id, similarity, metadata in similar_nodes:
                if "memory_ids" in metadata:
                    ids = metadata["memory_ids"]
                    # ç¡®ä¿æ˜¯åˆ—è¡¨
                    if isinstance(ids, str):
                        import orjson
                        try:
                            ids = orjson.loads(ids)
                        except Exception:
                            ids = [ids]
                    if isinstance(ids, list):
                        for mem_id in ids:
                            initial_memory_ids.add(mem_id)
                            # è®°å½•æœ€é«˜åˆ†æ•°
                            if mem_id not in memory_scores or similarity > memory_scores[mem_id]:
                                memory_scores[mem_id] = similarity
            
            # ğŸ”¥ è¯¦ç»†æ—¥å¿—ï¼šæ£€æŸ¥åˆå§‹å¬å›æƒ…å†µ
            logger.info(
                f"åˆå§‹å‘é‡æœç´¢: è¿”å›{len(similar_nodes)}ä¸ªèŠ‚ç‚¹ â†’ "
                f"æå–{len(initial_memory_ids)}æ¡è®°å¿†"
            )
            if len(initial_memory_ids) == 0:
                logger.warning(
                    f"âš ï¸ å‘é‡æœç´¢æœªæ‰¾åˆ°ä»»ä½•è®°å¿†ï¼"
                    f"å¯èƒ½åŸå› ï¼š1) åµŒå…¥æ¨¡å‹ç†è§£é—®é¢˜ 2) è®°å¿†èŠ‚ç‚¹æœªå»ºç«‹ç´¢å¼• 3) æŸ¥è¯¢è¡¨è¾¾ä¸å­˜å‚¨å†…å®¹å·®å¼‚è¿‡å¤§"
                )
                # è¾“å‡ºç›¸ä¼¼èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
                if similar_nodes:
                    logger.debug(f"å‘é‡æœç´¢è¿”å›çš„èŠ‚ç‚¹å…ƒæ•°æ®æ ·ä¾‹: {similar_nodes[0][2] if len(similar_nodes) > 0 else 'None'}")
            elif len(initial_memory_ids) < 3:
                logger.warning(f"âš ï¸ åˆå§‹å¬å›è®°å¿†æ•°é‡è¾ƒå°‘({len(initial_memory_ids)}æ¡)ï¼Œå¯èƒ½å½±å“ç»“æœè´¨é‡")

            # 3. å›¾æ‰©å±•ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰expand_depthï¼‰
            expanded_memory_scores = {}
            if expand_depth > 0 and initial_memory_ids:
                logger.info(f"å¼€å§‹å›¾æ‰©å±•: åˆå§‹è®°å¿†{len(initial_memory_ids)}ä¸ª, æ·±åº¦={expand_depth}")

                # è·å–æŸ¥è¯¢çš„embeddingç”¨äºè¯­ä¹‰è¿‡æ»¤
                if self.builder.embedding_generator:
                    try:
                        query_embedding = await self.builder.embedding_generator.generate(query)

                        # åªæœ‰åœ¨åµŒå…¥ç”ŸæˆæˆåŠŸæ—¶æ‰è¿›è¡Œè¯­ä¹‰æ‰©å±•
                        if query_embedding is not None:
                            # ä½¿ç”¨å…±äº«çš„å›¾æ‰©å±•å·¥å…·å‡½æ•°
                            expanded_results = await expand_memories_with_semantic_filter(
                                graph_store=self.graph_store,
                                vector_store=self.vector_store,
                                initial_memory_ids=list(initial_memory_ids),
                                query_embedding=query_embedding,
                                max_depth=expand_depth,
                                semantic_threshold=self.expand_semantic_threshold,  # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
                                max_expanded=top_k * 2
                            )

                            # åˆå¹¶æ‰©å±•ç»“æœ
                            expanded_memory_scores.update(dict(expanded_results))

                            logger.info(f"å›¾æ‰©å±•å®Œæˆ: æ–°å¢{len(expanded_memory_scores)}ä¸ªç›¸å…³è®°å¿†")

                    except Exception as e:
                        logger.warning(f"å›¾æ‰©å±•å¤±è´¥: {e}")

            # 4. åˆå¹¶åˆå§‹è®°å¿†å’Œæ‰©å±•è®°å¿†
            all_memory_ids = set(initial_memory_ids) | set(expanded_memory_scores.keys())

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼šåˆå§‹è®°å¿†ä¿æŒåŸåˆ†æ•°ï¼Œæ‰©å±•è®°å¿†ä½¿ç”¨æ‰©å±•åˆ†æ•°
            final_scores = {}
            for mem_id in all_memory_ids:
                if mem_id in memory_scores:
                    # åˆå§‹è®°å¿†ï¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                    final_scores[mem_id] = memory_scores[mem_id]
                elif mem_id in expanded_memory_scores:
                    # æ‰©å±•è®°å¿†ï¼šä½¿ç”¨å›¾æ‰©å±•åˆ†æ•°ï¼ˆç¨å¾®é™æƒï¼‰
                    final_scores[mem_id] = expanded_memory_scores[mem_id] * 0.8

            # æŒ‰åˆ†æ•°æ’åºï¼ˆå…ˆç²—æ’ï¼Œç¨åä¼šç”¨è¯¦ç»†è¯„åˆ†é‡æ–°æ’åºï¼‰
            sorted_memory_ids = sorted(
                final_scores.keys(),
                key=lambda x: final_scores[x],
                reverse=True
            )  # ğŸ”¥ ä¸å†æå‰æˆªæ–­ï¼Œè®©æ‰€æœ‰å€™é€‰å‚ä¸è¯¦ç»†è¯„åˆ†
            
            # ğŸ” ç»Ÿè®¡åˆå§‹è®°å¿†çš„ç›¸ä¼¼åº¦åˆ†å¸ƒï¼ˆç”¨äºè¯Šæ–­ï¼‰
            if memory_scores:
                similarities = list(memory_scores.values())
                logger.info(
                    f"ğŸ“Š å‘é‡ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€é«˜={max(similarities):.3f}, "
                    f"æœ€ä½={min(similarities):.3f}, "
                    f"å¹³å‡={sum(similarities)/len(similarities):.3f}, "
                    f">0.3: {len([s for s in similarities if s > 0.3])}/{len(similarities)}, "
                    f">0.2: {len([s for s in similarities if s > 0.2])}/{len(similarities)}"
                )

            # 5. è·å–å®Œæ•´è®°å¿†å¹¶è¿›è¡Œæœ€ç»ˆæ’åºï¼ˆä¼˜åŒ–åçš„åŠ¨æ€æƒé‡ç³»ç»Ÿï¼‰
            memories_with_scores = []
            filter_stats = {"importance": 0, "similarity": 0, "total_checked": 0}  # è¿‡æ»¤ç»Ÿè®¡
            
            for memory_id in sorted_memory_ids:  # éå†æ‰€æœ‰å€™é€‰
                memory = self.graph_store.get_memory_by_id(memory_id)
                if memory:
                    filter_stats["total_checked"] += 1
                    # åŸºç¡€åˆ†æ•°
                    similarity_score = final_scores[memory_id]
                    importance_score = memory.importance
                    
                    # ğŸ†• åŒºåˆ†è®°å¿†æ¥æºï¼ˆç”¨äºè¿‡æ»¤ï¼‰
                    is_initial_memory = memory_id in memory_scores  # æ˜¯å¦æ¥è‡ªåˆå§‹å‘é‡æœç´¢
                    true_similarity = memory_scores.get(memory_id, 0.0) if is_initial_memory else None

                    # è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°ï¼ˆæœ€è¿‘çš„è®°å¿†å¾—åˆ†æ›´é«˜ï¼‰
                    from datetime import datetime, timezone
                    now = datetime.now(timezone.utc)
                    # ç¡®ä¿ memory.created_at æœ‰æ—¶åŒºä¿¡æ¯
                    if memory.created_at.tzinfo is None:
                        memory_time = memory.created_at.replace(tzinfo=timezone.utc)
                    else:
                        memory_time = memory.created_at
                    age_days = (now - memory_time).total_seconds() / 86400
                    recency_score = 1.0 / (1.0 + age_days / 30)  # 30å¤©åŠè¡°æœŸ

                    # è·å–æ¿€æ´»åº¦åˆ†æ•°
                    activation_info = memory.metadata.get("activation", {})
                    activation_score = activation_info.get("level", memory.activation)
                    if activation_score == 0.0 and memory.activation > 0.0:
                        activation_score = memory.activation

                    # ğŸ†• åŠ¨æ€æƒé‡è®¡ç®—ï¼šä½¿ç”¨é…ç½®çš„åŸºç¡€æƒé‡ + æ ¹æ®è®°å¿†ç±»å‹å¾®è°ƒ
                    memory_type = memory.memory_type.value if hasattr(memory.memory_type, 'value') else str(memory.memory_type)
                    
                    # æ£€æµ‹è®°å¿†çš„ä¸»è¦èŠ‚ç‚¹ç±»å‹
                    node_types_count = {}
                    for node in memory.nodes:
                        nt = node.node_type.value if hasattr(node.node_type, 'value') else str(node.node_type)
                        node_types_count[nt] = node_types_count.get(nt, 0) + 1
                    
                    dominant_node_type = max(node_types_count.items(), key=lambda x: x[1])[0] if node_types_count else "unknown"
                    
                    # æ ¹æ®è®°å¿†ç±»å‹å’ŒèŠ‚ç‚¹ç±»å‹è®¡ç®—è°ƒæ•´ç³»æ•°ï¼ˆåœ¨é…ç½®æƒé‡åŸºç¡€ä¸Šå¾®è°ƒï¼‰
                    if dominant_node_type in ["ATTRIBUTE", "REFERENCE"] or memory_type == "FACT":
                        # äº‹å®æ€§è®°å¿†ï¼šæå‡ç›¸ä¼¼åº¦æƒé‡ï¼Œé™ä½æ—¶æ•ˆæ€§æƒé‡
                        type_adjustments = {
                            "similarity": 1.08,    # ç›¸ä¼¼åº¦æå‡ 8%
                            "importance": 1.0,     # é‡è¦æ€§ä¿æŒ
                            "recency": 0.5,        # æ—¶æ•ˆæ€§é™ä½ 50%ï¼ˆäº‹å®ä¸éšæ—¶é—´å¤±æ•ˆï¼‰
                        }
                    elif memory_type in ["CONVERSATION", "EPISODIC"] or dominant_node_type == "EVENT":
                        # å¯¹è¯/äº‹ä»¶è®°å¿†ï¼šæå‡æ—¶æ•ˆæ€§æƒé‡
                        type_adjustments = {
                            "similarity": 0.85,    # ç›¸ä¼¼åº¦é™ä½ 15%
                            "importance": 0.8,     # é‡è¦æ€§é™ä½ 20%
                            "recency": 2.5,        # æ—¶æ•ˆæ€§æå‡ 150%
                        }
                    elif dominant_node_type == "ENTITY" or memory_type == "SEMANTIC":
                        # å®ä½“/è¯­ä¹‰è®°å¿†ï¼šå¹³è¡¡è°ƒæ•´
                        type_adjustments = {
                            "similarity": 0.92,    # ç›¸ä¼¼åº¦å¾®é™ 8%
                            "importance": 1.2,     # é‡è¦æ€§æå‡ 20%
                            "recency": 1.0,        # æ—¶æ•ˆæ€§ä¿æŒ
                        }
                    else:
                        # é»˜è®¤ä¸è°ƒæ•´
                        type_adjustments = {
                            "similarity": 1.0,
                            "importance": 1.0,
                            "recency": 1.0,
                        }
                    
                    # åº”ç”¨è°ƒæ•´åçš„æƒé‡ï¼ˆåŸºäºé…ç½®çš„åŸºç¡€æƒé‡ï¼‰
                    weights = {
                        "similarity": self.base_vector_weight * type_adjustments["similarity"],
                        "importance": self.base_importance_weight * type_adjustments["importance"],
                        "recency": self.base_recency_weight * type_adjustments["recency"],
                    }
                    
                    # å½’ä¸€åŒ–æƒé‡ï¼ˆç¡®ä¿æ€»å’Œä¸º1.0ï¼‰
                    total_weight = sum(weights.values())
                    if total_weight > 0:
                        weights = {k: v / total_weight for k, v in weights.items()}
                    
                    # ç»¼åˆåˆ†æ•°è®¡ç®—ï¼ˆğŸ”¥ ç§»é™¤æ¿€æ´»åº¦å½±å“ï¼‰
                    final_score = (
                        similarity_score * weights["similarity"] +
                        importance_score * weights["importance"] +
                        recency_score * weights["recency"]
                    )
                    
                    # ğŸ†• é˜ˆå€¼è¿‡æ»¤ç­–ç•¥ï¼š
                    # 1. é‡è¦æ€§è¿‡æ»¤ï¼šåº”ç”¨äºæ‰€æœ‰è®°å¿†ï¼ˆè¿‡æ»¤æä½è´¨é‡ï¼‰
                    if memory.importance < self.search_min_importance:
                        filter_stats["importance"] += 1
                        logger.debug(f"âŒ è¿‡æ»¤ {memory.id[:8]}: é‡è¦æ€§ {memory.importance:.2f} < é˜ˆå€¼ {self.search_min_importance}")
                        continue
                    
                    # 2. ç›¸ä¼¼åº¦è¿‡æ»¤ï¼šä¸å†å¯¹åˆå§‹å‘é‡æœç´¢ç»“æœè¿‡æ»¤ï¼ˆä¿¡ä»»å‘é‡æœç´¢çš„æ’åºï¼‰
                    # ç†ç”±ï¼šå‘é‡æœç´¢å·²ç»æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›çš„éƒ½æ˜¯æœ€ç›¸å…³ç»“æœ
                    # å¦‚æœå†ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œä¼šå¯¼è‡´"æœ€ç›¸å…³çš„ä¹Ÿä¸å¤Ÿç›¸å…³"çš„çŸ›ç›¾
                    # 
                    # æ³¨æ„ï¼šå¦‚æœæœªæ¥éœ€è¦å¯¹æ‰©å±•è®°å¿†è¿‡æ»¤ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é€»è¾‘
                    # if not is_initial_memory and some_score < threshold:
                    #     continue
                    
                    # è®°å½•é€šè¿‡è¿‡æ»¤çš„è®°å¿†ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    if is_initial_memory:
                        logger.debug(
                            f"âœ… ä¿ç•™ {memory.id[:8]} [åˆå§‹]: ç›¸ä¼¼åº¦={true_similarity:.3f}, "
                            f"é‡è¦æ€§={memory.importance:.2f}, ç»¼åˆåˆ†æ•°={final_score:.4f}"
                        )
                    else:
                        logger.debug(
                            f"âœ… ä¿ç•™ {memory.id[:8]} [æ‰©å±•]: é‡è¦æ€§={memory.importance:.2f}, "
                            f"ç»¼åˆåˆ†æ•°={final_score:.4f}"
                        )
                    
                    # ğŸ†• èŠ‚ç‚¹ç±»å‹åŠ æƒï¼šå¯¹REFERENCE/ATTRIBUTEèŠ‚ç‚¹é¢å¤–åŠ åˆ†ï¼ˆä¿ƒè¿›äº‹å®æ€§ä¿¡æ¯å¬å›ï¼‰
                    if "REFERENCE" in node_types_count or "ATTRIBUTE" in node_types_count:
                        final_score *= 1.1  # 10% åŠ æˆ
                    
                    # ğŸ†• ç”¨æˆ·æŒ‡å®šçš„ä¼˜å…ˆèŠ‚ç‚¹ç±»å‹é¢å¤–åŠ æƒ
                    if prefer_node_types:
                        for prefer_type in prefer_node_types:
                            if prefer_type in node_types_count:
                                final_score *= 1.15  # 15% é¢å¤–åŠ æˆ
                                logger.debug(f"è®°å¿† {memory.id[:8]} åŒ…å«ä¼˜å…ˆèŠ‚ç‚¹ç±»å‹ {prefer_type}ï¼ŒåŠ æƒååˆ†æ•°: {final_score:.4f}")
                                break
                    
                    memories_with_scores.append((memory, final_score, dominant_node_type))

            # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
            memories_with_scores.sort(key=lambda x: x[1], reverse=True)
            memories = [mem for mem, _, _ in memories_with_scores[:top_k]]

            # ç»Ÿè®¡è¿‡æ»¤æƒ…å†µ
            total_candidates = len(all_memory_ids)
            filtered_count = total_candidates - len(memories_with_scores)
            
            # 6. æ ¼å¼åŒ–ç»“æœï¼ˆåŒ…å«è°ƒè¯•ä¿¡æ¯ï¼‰
            results = []
            for memory, score, node_type in memories_with_scores[:top_k]:
                result = {
                    "memory_id": memory.id,
                    "importance": memory.importance,
                    "created_at": memory.created_at.isoformat(),
                    "summary": self._summarize_memory(memory),
                    "score": round(score, 4),  # ğŸ†• æš´éœ²æœ€ç»ˆåˆ†æ•°ï¼Œä¾¿äºè°ƒè¯•
                    "dominant_node_type": node_type,  # ğŸ†• æš´éœ²èŠ‚ç‚¹ç±»å‹
                }
                results.append(result)

            logger.info(
                f"æœç´¢å®Œæˆ: åˆå§‹{len(initial_memory_ids)}ä¸ª â†’ "
                f"æ‰©å±•{len(expanded_memory_scores)}ä¸ª â†’ "
                f"å€™é€‰{total_candidates}ä¸ª â†’ "
                f"è¿‡æ»¤{filtered_count}ä¸ª (é‡è¦æ€§è¿‡æ»¤) â†’ "
                f"æœ€ç»ˆè¿”å›{len(results)}æ¡è®°å¿†"
            )
            
            # å¦‚æœè¿‡æ»¤ç‡è¿‡é«˜ï¼Œå‘å‡ºè­¦å‘Š
            if total_candidates > 0:
                filter_rate = filtered_count / total_candidates
                if filter_rate > 0.5:  # é™ä½è­¦å‘Šé˜ˆå€¼åˆ°50%
                    logger.warning(
                        f"âš ï¸ è¿‡æ»¤ç‡è¾ƒé«˜ ({filter_rate*100:.1f}%)ï¼"
                        f"åŸå› ï¼š{filter_stats['importance']}ä¸ªè®°å¿†é‡è¦æ€§ < {self.search_min_importance}ã€‚"
                        f"å»ºè®®ï¼š1) é™ä½ min_importance é˜ˆå€¼ï¼Œæˆ– 2) æ£€æŸ¥è®°å¿†è´¨é‡è¯„åˆ†"
                    )

            return {
                "success": True,
                "results": results,
                "total": len(results),
                "query": query,
                "strategy": "multi_query" if use_multi_query else "single_query",
                "expanded_count": len(expanded_memory_scores),
                "expand_depth": expand_depth,
            }

        except Exception as e:
            logger.error(f"è®°å¿†æœç´¢å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "message": "è®°å¿†æœç´¢å¤±è´¥",
                "results": [],
            }

    async def _generate_multi_queries_simple(
        self, query: str, context: dict[str, Any] | None = None
    ) -> tuple[list[tuple[str, float]], list[str]]:
        """
        ç®€åŒ–ç‰ˆå¤šæŸ¥è¯¢ç”Ÿæˆï¼ˆç›´æ¥åœ¨ Tools å±‚å®ç°ï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼‰

        è®©å°æ¨¡å‹ç›´æ¥ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢è¯­å¥ï¼Œå¹¶è¯†åˆ«åå¥½çš„èŠ‚ç‚¹ç±»å‹ã€‚
        
        Returns:
            (æŸ¥è¯¢åˆ—è¡¨, åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨)
        """
        try:
            from src.config.config import model_config
            from src.llm_models.utils_model import LLMRequest

            llm = LLMRequest(
                model_set=model_config.model_task_config.utils_small,
                request_type="memory.multi_query"
            )

            # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            participants = context.get("participants", []) if context else []
            chat_history = context.get("chat_history", "") if context else ""
            sender = context.get("sender", "") if context else ""

            # å¤„ç†èŠå¤©å†å²ï¼Œæå–æœ€è¿‘5æ¡å·¦å³çš„å¯¹è¯
            recent_chat = ""
            if chat_history:
                lines = chat_history.strip().split("\n")
                # å–æœ€è¿‘5æ¡æ¶ˆæ¯
                recent_lines = lines[-5:] if len(lines) > 5 else lines
                recent_chat = "\n".join(recent_lines)

            prompt = f"""åŸºäºèŠå¤©ä¸Šä¸‹æ–‡ä¸ºæŸ¥è¯¢ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æœç´¢è¯­å¥ï¼Œå¹¶è¯†åˆ«æŸ¥è¯¢æ„å›¾å¯¹åº”çš„è®°å¿†ç±»å‹ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

**å½“å‰æŸ¥è¯¢ï¼š** {query}
**å‘é€è€…ï¼š** {sender if sender else 'æœªçŸ¥'}
**å‚ä¸è€…ï¼š** {', '.join(participants) if participants else 'æ— '}
**å½“å‰æ—¶é—´ï¼š** {__import__('datetime').datetime.now().__str__()}

**æœ€è¿‘èŠå¤©è®°å½•ï¼ˆæœ€è¿‘5æ¡ï¼‰ï¼š**
{recent_chat if recent_chat else 'æ— èŠå¤©å†å²'}

---

## ç¬¬ä¸€æ­¥ï¼šåˆ†ææŸ¥è¯¢æ„å›¾ä¸è®°å¿†ç±»å‹

### è®°å¿†ç±»å‹è¯†åˆ«è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§åˆ¤æ–­ï¼‰

| æŸ¥è¯¢ç‰¹å¾ | åå¥½èŠ‚ç‚¹ç±»å‹ | ç¤ºä¾‹ |
|---------|-------------|------|
| ğŸ”— **æŸ¥æ‰¾é“¾æ¥/åœ°å€/URL/ç½‘å€/æ–‡æ¡£ä½ç½®** | `REFERENCE` | "xxxçš„æ–‡æ¡£åœ°å€"ã€"é‚£ä¸ªç½‘ç«™é“¾æ¥" |
| âš™ï¸ **æŸ¥è¯¢é…ç½®/å‚æ•°/è®¾ç½®/å±æ€§å€¼** | `ATTRIBUTE` | "Pythonç‰ˆæœ¬æ˜¯å¤šå°‘"ã€"æ•°æ®åº“é…ç½®" |
| ğŸ‘¤ **è¯¢é—®äººç‰©/ç»„ç»‡/å®ä½“èº«ä»½** | `ENTITY` | "æ‹¾é£æ˜¯è°"ã€"MoFoxå›¢é˜Ÿæˆå‘˜" |
| ğŸ”„ **è¯¢é—®å…³ç³»/äººé™…/äº¤äº’** | `RELATION` | "æˆ‘å’Œæœºå™¨äººçš„å…³ç³»"ã€"è°è®¤è¯†è°" |
| ğŸ“… **å›å¿†äº‹ä»¶/å¯¹è¯/æ´»åŠ¨** | `EVENT` | "ä¸Šæ¬¡èŠäº†ä»€ä¹ˆ"ã€"æ˜¨å¤©çš„ä¼šè®®" |
| ğŸ’¡ **æŸ¥è¯¢æ¦‚å¿µ/å®šä¹‰/çŸ¥è¯†** | æ— ç‰¹å®šåå¥½ | "ä»€ä¹ˆæ˜¯è®°å¿†å›¾è°±" |

### åˆ¤æ–­è§„åˆ™
- å¦‚æœæŸ¥è¯¢åŒ…å«"åœ°å€"ã€"é“¾æ¥"ã€"URL"ã€"ç½‘å€"ã€"æ–‡æ¡£"ç­‰å…³é”®è¯ â†’ `REFERENCE`
- å¦‚æœæŸ¥è¯¢åŒ…å«"é…ç½®"ã€"å‚æ•°"ã€"è®¾ç½®"ã€"ç‰ˆæœ¬"ã€"å±æ€§"ç­‰å…³é”®è¯ â†’ `ATTRIBUTE`
- å¦‚æœæŸ¥è¯¢è¯¢é—®"æ˜¯è°"ã€"ä»€ä¹ˆäºº"ã€"å›¢é˜Ÿ"ã€"ç»„ç»‡"ç­‰ â†’ `ENTITY`
- å¦‚æœæŸ¥è¯¢è¯¢é—®"å…³ç³»"ã€"æœ‹å‹"ã€"è®¤è¯†"ç­‰ â†’ `RELATION`
- å¦‚æœæŸ¥è¯¢å›å¿†"ä¸Šæ¬¡"ã€"ä¹‹å‰"ã€"è®¨è®ºè¿‡"ã€"èŠè¿‡"ç­‰ â†’ `EVENT`
- å¦‚æœæ— æ˜ç¡®ç‰¹å¾ â†’ ä¸æŒ‡å®šç±»å‹ï¼ˆç©ºåˆ—è¡¨ï¼‰

---

## ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¤šè§’åº¦æŸ¥è¯¢

### åˆ†æåŸåˆ™
1. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šæ ¹æ®èŠå¤©å†å²ç†è§£æŸ¥è¯¢çš„çœŸå®æ„å›¾
2. **æŒ‡ä»£æ¶ˆè§£**ï¼šè¯†åˆ«å¹¶ä»£æ¢"ä»–"ã€"å¥¹"ã€"å®ƒ"ã€"é‚£ä¸ª"ç­‰æŒ‡ä»£è¯ä¸ºå…·ä½“å®ä½“å
3. **è¯é¢˜å…³è”**ï¼šç»“åˆæœ€è¿‘è®¨è®ºçš„è¯é¢˜ç”Ÿæˆæ›´ç²¾å‡†çš„æŸ¥è¯¢
4. **æŸ¥è¯¢åˆ†è§£**ï¼šå¯¹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢
5. **å®ä½“æå–**ï¼šæ˜¾å¼æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“ï¼ˆäººåã€é¡¹ç›®åã€ç»„ç»‡åç­‰ï¼‰

### ç”Ÿæˆç­–ç•¥ï¼ˆæŒ‰é¡ºåºï¼‰
1. **å®Œæ•´æŸ¥è¯¢**ï¼ˆæƒé‡1.0ï¼‰ï¼šç»“åˆä¸Šä¸‹æ–‡çš„å®Œæ•´æŸ¥è¯¢ï¼ŒåŒ…å«æŒ‡ä»£æ¶ˆè§£åçš„å®ä½“å
2. **å…³é”®å®ä½“æŸ¥è¯¢**ï¼ˆæƒé‡0.9ï¼‰ï¼šåªåŒ…å«æ ¸å¿ƒå®ä½“ï¼Œå»é™¤ä¿®é¥°è¯ï¼ˆå¦‚"xxxçš„"â†’"xxx"ï¼‰
3. **åŒä¹‰è¡¨è¾¾æŸ¥è¯¢**ï¼ˆæƒé‡0.8ï¼‰ï¼šç”¨ä¸åŒè¡¨è¾¾æ–¹å¼é‡è¿°æŸ¥è¯¢æ„å›¾
4. **è¯é¢˜æ‰©å±•æŸ¥è¯¢**ï¼ˆæƒé‡0.7ï¼‰ï¼šåŸºäºæœ€è¿‘èŠå¤©è¯é¢˜çš„ç›¸å…³æŸ¥è¯¢
5. **æ—¶é—´èŒƒå›´æŸ¥è¯¢**ï¼ˆæƒé‡0.6ï¼Œå¦‚é€‚ç”¨ï¼‰ï¼šå¦‚æœæ¶‰åŠæ—¶é—´ï¼Œç”Ÿæˆå…·ä½“æ—¶é—´èŒƒå›´

---

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "prefer_node_types": ["REFERENCE", "ATTRIBUTE"],
  "queries": [
    {{"text": "å®Œæ•´æŸ¥è¯¢ï¼ˆå·²æ¶ˆè§£æŒ‡ä»£ï¼‰", "weight": 1.0}},
    {{"text": "æ ¸å¿ƒå®ä½“æŸ¥è¯¢", "weight": 0.9}},
    {{"text": "åŒä¹‰è¡¨è¾¾æŸ¥è¯¢", "weight": 0.8}}
  ]
}}
```

**å­—æ®µè¯´æ˜**ï¼š
- `prefer_node_types`: åå¥½çš„èŠ‚ç‚¹ç±»å‹æ•°ç»„ï¼Œå¯é€‰å€¼ï¼š`REFERENCE`ã€`ATTRIBUTE`ã€`ENTITY`ã€`RELATION`ã€`EVENT`ï¼Œå¦‚æ— æ˜ç¡®ç‰¹å¾åˆ™ä¸ºç©ºæ•°ç»„`[]`
- `queries`: æŸ¥è¯¢æ•°ç»„ï¼Œæ¯ä¸ªæŸ¥è¯¢åŒ…å«`text`ï¼ˆæŸ¥è¯¢æ–‡æœ¬ï¼‰å’Œ`weight`ï¼ˆæƒé‡0.5-1.0ï¼‰

---

## ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæŸ¥è¯¢æ–‡æ¡£åœ°å€
**è¾“å…¥**ï¼š
- æŸ¥è¯¢ï¼š"ä½ çŸ¥é“MoFox-Botçš„æ–‡æ¡£åœ°å€å—ï¼Ÿ"
- èŠå¤©å†å²ï¼šæ— 

**è¾“å‡º**ï¼š
```json
{{
  "prefer_node_types": ["REFERENCE"],
  "queries": [
    {{"text": "MoFox-Botæ–‡æ¡£åœ°å€", "weight": 1.0}},
    {{"text": "MoFox-Bot", "weight": 0.9}},
    {{"text": "MoFox-Botå®˜æ–¹æ–‡æ¡£URL", "weight": 0.8}}
  ]
}}
```

### ç¤ºä¾‹2ï¼šæŸ¥è¯¢äººç‰©å…³ç³»
**è¾“å…¥**ï¼š
- æŸ¥è¯¢ï¼š"æ‹¾é£æ˜¯è°ï¼Ÿ"
- èŠå¤©å†å²ï¼šæåˆ°è¿‡"æ‹¾é£å’Œæ°ç‘å–µ"

**è¾“å‡º**ï¼š
```json
{{
  "prefer_node_types": ["ENTITY", "RELATION"],
  "queries": [
    {{"text": "æ‹¾é£èº«ä»½ä¿¡æ¯", "weight": 1.0}},
    {{"text": "æ‹¾é£", "weight": 0.9}},
    {{"text": "æ‹¾é£å’Œæ°ç‘å–µçš„å…³ç³»", "weight": 0.8}}
  ]
}}
```

### ç¤ºä¾‹3ï¼šæŸ¥è¯¢é…ç½®å‚æ•°
**è¾“å…¥**ï¼š
- æŸ¥è¯¢ï¼š"Pythonç‰ˆæœ¬æ˜¯å¤šå°‘ï¼Ÿ"
- èŠå¤©å†å²ï¼šè®¨è®ºè¿‡"é¡¹ç›®ç¯å¢ƒé…ç½®"

**è¾“å‡º**ï¼š
```json
{{
  "prefer_node_types": ["ATTRIBUTE"],
  "queries": [
    {{"text": "Pythonç‰ˆæœ¬å·", "weight": 1.0}},
    {{"text": "Pythoné…ç½®", "weight": 0.9}},
    {{"text": "é¡¹ç›®Pythonç¯å¢ƒç‰ˆæœ¬", "weight": 0.8}}
  ]
}}
```

### ç¤ºä¾‹4ï¼šå›å¿†å¯¹è¯ï¼ˆæ— æ˜ç¡®ç±»å‹ï¼‰
**è¾“å…¥**ï¼š
- æŸ¥è¯¢ï¼š"æˆ‘ä»¬ä¸Šæ¬¡èŠäº†ä»€ä¹ˆï¼Ÿ"
- èŠå¤©å†å²ï¼šæœ€è¿‘è®¨è®º"è®°å¿†ç³»ç»Ÿä¼˜åŒ–"

**è¾“å‡º**ï¼š
```json
{{
  "prefer_node_types": ["EVENT"],
  "queries": [
    {{"text": "æœ€è¿‘å¯¹è¯å†…å®¹", "weight": 1.0}},
    {{"text": "è®°å¿†ç³»ç»Ÿä¼˜åŒ–è®¨è®º", "weight": 0.9}},
    {{"text": "ä¸Šæ¬¡èŠå¤©è®°å½•", "weight": 0.8}}
  ]
}}
```

---

**ç°åœ¨è¯·æ ¹æ®ä¸Šè¿°è§„åˆ™ç”Ÿæˆè¾“å‡ºï¼ˆä»…è¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š**
"""

            response, _ = await llm.generate_response_async(prompt, temperature=0.3, max_tokens=300)

            import re
            import orjson
            
            # æ¸…ç†Markdownä»£ç å—
            response = re.sub(r"```json\s*", "", response)
            response = re.sub(r"```\s*$", "", response).strip()

            # è§£æJSON
            data = orjson.loads(response)
            
            # æå–æŸ¥è¯¢åˆ—è¡¨
            queries = data.get("queries", [])
            result_queries = [(item.get("text", "").strip(), float(item.get("weight", 0.5)))
                             for item in queries if item.get("text", "").strip()]
            
            # æå–åå¥½èŠ‚ç‚¹ç±»å‹
            prefer_node_types = data.get("prefer_node_types", [])
            # ç¡®ä¿ç±»å‹æ­£ç¡®ä¸”æœ‰æ•ˆ
            valid_types = {"REFERENCE", "ATTRIBUTE", "ENTITY", "RELATION", "EVENT"}
            prefer_node_types = [t for t in prefer_node_types if t in valid_types]

            if result_queries:
                logger.info(
                    f"ç”ŸæˆæŸ¥è¯¢: {[q for q, _ in result_queries]} "
                    f"(åå¥½ç±»å‹: {prefer_node_types if prefer_node_types else 'æ— '})"
                )
                return result_queries, prefer_node_types

        except Exception as e:
            logger.warning(f"å¤šæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")

        # é™çº§ï¼šè¿”å›åŸå§‹æŸ¥è¯¢å’Œç©ºçš„èŠ‚ç‚¹ç±»å‹åˆ—è¡¨
        return [(query, 1.0)], []

    async def _single_query_search(
        self, query: str, top_k: int
    ) -> list[tuple[str, float, dict[str, Any]]]:
        """
        ä¼ ç»Ÿçš„å•æŸ¥è¯¢æœç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°

        Returns:
            ç›¸ä¼¼èŠ‚ç‚¹åˆ—è¡¨ [(node_id, similarity, metadata), ...]
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = None
        if self.builder.embedding_generator:
            query_embedding = await self.builder.embedding_generator.generate(query)

        # å¦‚æœåµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå‘é‡æœç´¢
        if query_embedding is None:
            logger.warning("åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡èŠ‚ç‚¹æœç´¢")
            return []

        # å‘é‡æœç´¢ï¼ˆå¢åŠ è¿”å›æ•°é‡ä»¥æé«˜å¬å›ç‡ï¼‰
        similar_nodes = await self.vector_store.search_similar_nodes(
            query_embedding=query_embedding,
            limit=top_k * 5,  # ğŸ”¥ ä»2å€æå‡åˆ°5å€ï¼Œæé«˜åˆå§‹å¬å›ç‡
            min_similarity=0.0,  # ä¸åœ¨è¿™é‡Œè¿‡æ»¤ï¼Œäº¤ç»™åç»­è¯„åˆ†
        )
        
        logger.debug(f"å•æŸ¥è¯¢å‘é‡æœç´¢: æŸ¥è¯¢='{query}', è¿”å›èŠ‚ç‚¹æ•°={len(similar_nodes)}")
        if similar_nodes:
            logger.debug(f"Top 3ç›¸ä¼¼åº¦: {[f'{sim:.3f}' for _, sim, _ in similar_nodes[:3]]}")

        return similar_nodes

    async def _multi_query_search(
        self, query: str, top_k: int, context: dict[str, Any] | None = None
    ) -> tuple[list[tuple[str, float, dict[str, Any]]], list[str]]:
        """
        å¤šæŸ¥è¯¢ç­–ç•¥æœç´¢ï¼ˆç®€åŒ–ç‰ˆ + èŠ‚ç‚¹ç±»å‹è¯†åˆ«ï¼‰

        ç›´æ¥ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ï¼Œå¹¶è¯†åˆ«æŸ¥è¯¢æ„å›¾å¯¹åº”çš„åå¥½èŠ‚ç‚¹ç±»å‹ã€‚

        æ­¥éª¤ï¼š
        1. è®©å°æ¨¡å‹ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢ + è¯†åˆ«åå¥½èŠ‚ç‚¹ç±»å‹
        2. ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”ŸæˆåµŒå…¥
        3. å¹¶è¡Œæœç´¢å¹¶èåˆç»“æœ

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡

        Returns:
            (èåˆåçš„ç›¸ä¼¼èŠ‚ç‚¹åˆ—è¡¨, åå¥½èŠ‚ç‚¹ç±»å‹åˆ—è¡¨)
        """
        try:
            # 1. ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ + èŠ‚ç‚¹ç±»å‹è¯†åˆ«
            multi_queries, prefer_node_types = await self._generate_multi_queries_simple(query, context)

            logger.debug(f"ç”Ÿæˆ {len(multi_queries)} ä¸ªæŸ¥è¯¢: {multi_queries}, åå¥½ç±»å‹: {prefer_node_types}")

            # 2. ç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢çš„åµŒå…¥
            if not self.builder.embedding_generator:
                logger.warning("æœªé…ç½®åµŒå…¥ç”Ÿæˆå™¨ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼")
                single_results = await self._single_query_search(query, top_k)
                return single_results, prefer_node_types

            query_embeddings = []
            query_weights = []

            for sub_query, weight in multi_queries:
                embedding = await self.builder.embedding_generator.generate(sub_query)
                if embedding is not None:
                    query_embeddings.append(embedding)
                    query_weights.append(weight)

            # å¦‚æœæ‰€æœ‰åµŒå…¥éƒ½ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼
            if not query_embeddings:
                logger.warning("æ‰€æœ‰æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼")
                single_results = await self._single_query_search(query, top_k)
                return single_results, prefer_node_types

            # 3. å¤šæŸ¥è¯¢èåˆæœç´¢
            similar_nodes = await self.vector_store.search_with_multiple_queries(
                query_embeddings=query_embeddings,
                query_weights=query_weights,
                limit=top_k * 5,  # ğŸ”¥ ä»2å€æå‡åˆ°5å€ï¼Œæé«˜åˆå§‹å¬å›ç‡
                fusion_strategy="weighted_max",
            )

            logger.info(f"å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆ: {len(similar_nodes)} ä¸ªèŠ‚ç‚¹ (åå¥½ç±»å‹: {prefer_node_types})")
            if similar_nodes:
                logger.debug(f"Top 5èåˆç›¸ä¼¼åº¦: {[f'{sim:.3f}' for _, sim, _ in similar_nodes[:5]]}")

            return similar_nodes, prefer_node_types

        except Exception as e:
            logger.warning(f"å¤šæŸ¥è¯¢æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å•æŸ¥è¯¢æ¨¡å¼: {e}", exc_info=True)
            single_results = await self._single_query_search(query, top_k)
            return single_results, []

    async def _add_memory_to_stores(self, memory: Memory):
        """å°†è®°å¿†æ·»åŠ åˆ°å­˜å‚¨"""
        # 1. æ·»åŠ åˆ°å›¾å­˜å‚¨
        self.graph_store.add_memory(memory)

        # 2. æ·»åŠ æœ‰åµŒå…¥çš„èŠ‚ç‚¹åˆ°å‘é‡å­˜å‚¨
        for node in memory.nodes:
            if node.embedding is not None:
                await self.vector_store.add_node(node)

    async def _find_memory_by_description(self, description: str) -> Memory | None:
        """
        é€šè¿‡æè¿°æŸ¥æ‰¾è®°å¿†

        Args:
            description: è®°å¿†æè¿°

        Returns:
            æ‰¾åˆ°çš„è®°å¿†ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        # ä½¿ç”¨è¯­ä¹‰æœç´¢æŸ¥æ‰¾æœ€ç›¸å…³çš„è®°å¿†
        query_embedding = None
        if self.builder.embedding_generator:
            query_embedding = await self.builder.embedding_generator.generate(description)

        # å¦‚æœåµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æœç´¢
        if query_embedding is None:
            logger.debug("åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡æè¿°æœç´¢")
            return None

        # æœç´¢ç›¸ä¼¼èŠ‚ç‚¹
        similar_nodes = await self.vector_store.search_similar_nodes(
            query_embedding=query_embedding,
            limit=5,
        )

        if not similar_nodes:
            return None

        # è·å–æœ€ç›¸ä¼¼èŠ‚ç‚¹å…³è”çš„è®°å¿†
        _node_id, _similarity, metadata = similar_nodes[0]

        if "memory_ids" not in metadata or not metadata["memory_ids"]:
            return None

        ids = metadata["memory_ids"]

        # ç¡®ä¿æ˜¯åˆ—è¡¨
        if isinstance(ids, str):
            import orjson
            try:
                ids = orjson.loads(ids)
            except Exception as e:
                logger.warning(f"JSON è§£æå¤±è´¥: {e}")
                ids = [ids]

        if isinstance(ids, list) and ids:
            memory_id = ids[0]
            return self.graph_store.get_memory_by_id(memory_id)

        return None

    def _summarize_memory(self, memory: Memory) -> str:
        """ç”Ÿæˆè®°å¿†æ‘˜è¦"""
        if not memory.metadata:
            return "æœªçŸ¥è®°å¿†"

        subject = memory.metadata.get("subject", "")
        topic = memory.metadata.get("topic", "")
        memory_type = memory.metadata.get("memory_type", "")

        return f"{subject} - {memory_type}: {topic}"

    @staticmethod
    def get_all_tool_schemas() -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å·¥å…·çš„ schema

        Returns:
            å·¥å…· schema åˆ—è¡¨
        """
        return [
            MemoryTools.get_create_memory_schema(),
            MemoryTools.get_link_memories_schema(),
            MemoryTools.get_search_memories_schema(),
        ]
