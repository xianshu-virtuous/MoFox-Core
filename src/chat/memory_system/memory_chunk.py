"""
ç»“æž„åŒ–è®°å¿†å•å…ƒè®¾è®¡
å®žçŽ°é«˜è´¨é‡ã€ç»“æž„åŒ–çš„è®°å¿†å•å…ƒï¼Œç¬¦åˆæ–‡æ¡£è®¾è®¡è§„èŒƒ
"""

import hashlib
import time
import uuid
from collections.abc import Iterable
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

import numpy as np
import orjson

from src.common.logger import get_logger

logger = get_logger(__name__)


class MemoryType(Enum):
    """è®°å¿†ç±»åž‹åˆ†ç±»"""

    PERSONAL_FACT = "personal_fact"  # ä¸ªäººäº‹å®žï¼ˆå§“åã€èŒä¸šã€ä½å€ç­‰ï¼‰
    EVENT = "event"  # äº‹ä»¶ï¼ˆé‡è¦ç»åŽ†ã€çº¦ä¼šç­‰ï¼‰
    PREFERENCE = "preference"  # åå¥½ï¼ˆå–œå¥½ã€ä¹ æƒ¯ç­‰ï¼‰
    OPINION = "opinion"  # è§‚ç‚¹ï¼ˆå¯¹äº‹ç‰©çš„çœ‹æ³•ï¼‰
    RELATIONSHIP = "relationship"  # å…³ç³»ï¼ˆä¸Žä»–äººçš„å…³ç³»ï¼‰
    EMOTION = "emotion"  # æƒ…æ„ŸçŠ¶æ€
    KNOWLEDGE = "knowledge"  # çŸ¥è¯†ä¿¡æ¯
    SKILL = "skill"  # æŠ€èƒ½èƒ½åŠ›
    GOAL = "goal"  # ç›®æ ‡è®¡åˆ’
    EXPERIENCE = "experience"  # ç»éªŒæ•™è®­
    CONTEXTUAL = "contextual"  # ä¸Šä¸‹æ–‡ä¿¡æ¯


class ConfidenceLevel(Enum):
    """ç½®ä¿¡åº¦ç­‰çº§"""

    LOW = 1  # ä½Žç½®ä¿¡åº¦ï¼Œå¯èƒ½ä¸å‡†ç¡®
    MEDIUM = 2  # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæœ‰ä¸€å®šä¾æ®
    HIGH = 3  # é«˜ç½®ä¿¡åº¦ï¼Œæœ‰æ˜Žç¡®æ¥æº
    VERIFIED = 4  # å·²éªŒè¯ï¼Œéžå¸¸å¯é 


class ImportanceLevel(Enum):
    """é‡è¦æ€§ç­‰çº§"""

    LOW = 1  # ä½Žé‡è¦æ€§ï¼Œæ™®é€šä¿¡æ¯
    NORMAL = 2  # ä¸€èˆ¬é‡è¦æ€§ï¼Œæ—¥å¸¸ä¿¡æ¯
    HIGH = 3  # é«˜é‡è¦æ€§ï¼Œé‡è¦ä¿¡æ¯
    CRITICAL = 4  # å…³é”®é‡è¦æ€§ï¼Œæ ¸å¿ƒä¿¡æ¯


@dataclass
class ContentStructure:
    """ä¸»è°“å®¾ç»“æž„ï¼ŒåŒ…å«è‡ªç„¶è¯­è¨€æè¿°"""

    subject: str | list[str]
    predicate: str
    object: str | dict
    display: str = ""

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {"subject": self.subject, "predicate": self.predicate, "object": self.object, "display": self.display}

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ContentStructure":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        return cls(
            subject=data.get("subject", ""),
            predicate=data.get("predicate", ""),
            object=data.get("object", ""),
            display=data.get("display", ""),
        )

    def to_subject_list(self) -> list[str]:
        """å°†ä¸»è¯­è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼"""
        if isinstance(self.subject, list):
            return [s for s in self.subject if isinstance(s, str) and s.strip()]
        if isinstance(self.subject, str) and self.subject.strip():
            return [self.subject.strip()]
        return []

    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        if self.display:
            return self.display
        subjects = "ã€".join(self.to_subject_list()) or str(self.subject)
        object_str = self.object if isinstance(self.object, str) else str(self.object)
        return f"{subjects} {self.predicate} {object_str}".strip()


@dataclass
class MemoryMetadata:
    """è®°å¿†å…ƒæ•°æ® - ç®€åŒ–ç‰ˆæœ¬"""

    # åŸºç¡€ä¿¡æ¯
    memory_id: str  # å”¯ä¸€æ ‡è¯†ç¬¦
    user_id: str  # ç”¨æˆ·ID
    chat_id: str | None = None  # èŠå¤©IDï¼ˆç¾¤èŠæˆ–ç§èŠï¼‰

    # æ—¶é—´ä¿¡æ¯
    created_at: float = 0.0  # åˆ›å»ºæ—¶é—´æˆ³
    last_accessed: float = 0.0  # æœ€åŽè®¿é—®æ—¶é—´
    last_modified: float = 0.0  # æœ€åŽä¿®æ”¹æ—¶é—´

    # æ¿€æ´»é¢‘çŽ‡ç®¡ç†
    last_activation_time: float = 0.0  # æœ€åŽæ¿€æ´»æ—¶é—´
    activation_frequency: int = 0  # æ¿€æ´»é¢‘çŽ‡ï¼ˆå•ä½æ—¶é—´å†…çš„æ¿€æ´»æ¬¡æ•°ï¼‰
    total_activations: int = 0  # æ€»æ¿€æ´»æ¬¡æ•°

    # ç»Ÿè®¡ä¿¡æ¯
    access_count: int = 0  # è®¿é—®æ¬¡æ•°
    relevance_score: float = 0.0  # ç›¸å…³åº¦è¯„åˆ†

    # ä¿¡å¿ƒå’Œé‡è¦æ€§ï¼ˆæ ¸å¿ƒå­—æ®µï¼‰
    confidence: ConfidenceLevel = ConfidenceLevel.MEDIUM
    importance: ImportanceLevel = ImportanceLevel.NORMAL

    # é—å¿˜æœºåˆ¶ç›¸å…³
    forgetting_threshold: float = 0.0  # é—å¿˜é˜ˆå€¼ï¼ˆåŠ¨æ€è®¡ç®—ï¼‰
    last_forgetting_check: float = 0.0  # ä¸Šæ¬¡é—å¿˜æ£€æŸ¥æ—¶é—´

    # æ¥æºä¿¡æ¯
    source_context: str | None = None  # æ¥æºä¸Šä¸‹æ–‡ç‰‡æ®µ
    # å…¼å®¹æ—§å­—æ®µ: ä¸€äº›ä»£ç æˆ–æ—§ç‰ˆæœ¬å¯èƒ½ç›´æŽ¥è®¿é—® metadata.source
    source: str | None = None

    def __post_init__(self):
        """åŽåˆå§‹åŒ–å¤„ç†"""
        if not self.memory_id:
            self.memory_id = str(uuid.uuid4())

        current_time = time.time()

        if self.created_at == 0:
            self.created_at = current_time

        if self.last_accessed == 0:
            self.last_accessed = current_time

        if self.last_modified == 0:
            self.last_modified = current_time

        if self.last_activation_time == 0:
            self.last_activation_time = current_time

        if self.last_forgetting_check == 0:
            self.last_forgetting_check = current_time

        # å…¼å®¹æ€§ï¼šå¦‚æžœæ—§å­—æ®µ source è¢«ä½¿ç”¨ï¼Œä¿è¯ source ä¸Ž source_context åŒæ­¥
        if not getattr(self, "source", None) and getattr(self, "source_context", None):
            try:
                self.source = str(self.source_context)
            except Exception:
                self.source = None
        # å¦‚æžœæœ‰ source å­—æ®µä½† source_context ä¸ºç©ºï¼Œä¹ŸåŒæ­¥å›žåŽ»
        if not getattr(self, "source_context", None) and getattr(self, "source", None):
            try:
                self.source_context = str(self.source)
            except Exception:
                self.source_context = None

    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        current_time = time.time()
        self.last_accessed = current_time
        self.access_count += 1
        self.total_activations += 1

        # æ›´æ–°æ¿€æ´»é¢‘çŽ‡
        self._update_activation_frequency(current_time)

    def _update_activation_frequency(self, current_time: float):
        """æ›´æ–°æ¿€æ´»é¢‘çŽ‡ï¼ˆ24å°æ—¶å†…çš„æ¿€æ´»æ¬¡æ•°ï¼‰"""

        # å¦‚æžœè¶…è¿‡24å°æ—¶ï¼Œé‡ç½®æ¿€æ´»é¢‘çŽ‡
        if current_time - self.last_activation_time > 86400:  # 24å°æ—¶ = 86400ç§’
            self.activation_frequency = 1
        else:
            self.activation_frequency += 1

        self.last_activation_time = current_time

    def update_relevance(self, new_score: float):
        """æ›´æ–°ç›¸å…³åº¦è¯„åˆ†"""
        self.relevance_score = max(0.0, min(1.0, new_score))
        self.last_modified = time.time()

    def calculate_forgetting_threshold(self) -> float:
        """è®¡ç®—é—å¿˜é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰"""
        # åŸºç¡€å¤©æ•°
        base_days = 30.0

        # é‡è¦æ€§æƒé‡ (1-4 -> 0-3)
        importance_weight = (self.importance.value - 1) * 15  # 0, 15, 30, 45

        # ç½®ä¿¡åº¦æƒé‡ (1-4 -> 0-3)
        confidence_weight = (self.confidence.value - 1) * 10  # 0, 10, 20, 30

        # æ¿€æ´»é¢‘çŽ‡æƒé‡ï¼ˆæ¯5æ¬¡æ¿€æ´»å¢žåŠ 1å¤©ï¼‰
        frequency_weight = min(self.activation_frequency, 20) * 0.5  # æœ€å¤š10å¤©

        # è®¡ç®—æœ€ç»ˆé˜ˆå€¼
        threshold = base_days + importance_weight + confidence_weight + frequency_weight

        # è®¾ç½®æœ€å°å’Œæœ€å¤§é˜ˆå€¼
        return max(7.0, min(threshold, 365.0))  # 7å¤©åˆ°1å¹´ä¹‹é—´

    def should_forget(self, current_time: float | None = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é—å¿˜"""
        if current_time is None:
            current_time = time.time()

        # è®¡ç®—é—å¿˜é˜ˆå€¼
        self.forgetting_threshold = self.calculate_forgetting_threshold()

        # è®¡ç®—è·ç¦»æœ€åŽæ¿€æ´»çš„æ—¶é—´
        days_since_activation = (current_time - self.last_activation_time) / 86400

        return days_since_activation > self.forgetting_threshold

    def is_dormant(self, current_time: float | None = None, inactive_days: int = 90) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¤„äºŽä¼‘çœ çŠ¶æ€ï¼ˆé•¿æœŸæœªæ¿€æ´»ï¼‰"""
        if current_time is None:
            current_time = time.time()

        days_since_last_access = (current_time - self.last_accessed) / 86400
        return days_since_last_access > inactive_days

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "memory_id": self.memory_id,
            "user_id": self.user_id,
            "chat_id": self.chat_id,
            "created_at": self.created_at,
            "last_accessed": self.last_accessed,
            "last_modified": self.last_modified,
            "last_activation_time": self.last_activation_time,
            "activation_frequency": self.activation_frequency,
            "total_activations": self.total_activations,
            "access_count": self.access_count,
            "relevance_score": self.relevance_score,
            "confidence": self.confidence.value,
            "importance": self.importance.value,
            "forgetting_threshold": self.forgetting_threshold,
            "last_forgetting_check": self.last_forgetting_check,
            "source_context": self.source_context,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "MemoryMetadata":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        return cls(
            memory_id=data.get("memory_id", ""),
            user_id=data.get("user_id", ""),
            chat_id=data.get("chat_id"),
            created_at=data.get("created_at", 0),
            last_accessed=data.get("last_accessed", 0),
            last_modified=data.get("last_modified", 0),
            last_activation_time=data.get("last_activation_time", 0),
            activation_frequency=data.get("activation_frequency", 0),
            total_activations=data.get("total_activations", 0),
            access_count=data.get("access_count", 0),
            relevance_score=data.get("relevance_score", 0.0),
            confidence=ConfidenceLevel(data.get("confidence", ConfidenceLevel.MEDIUM.value)),
            importance=ImportanceLevel(data.get("importance", ImportanceLevel.NORMAL.value)),
            forgetting_threshold=data.get("forgetting_threshold", 0.0),
            last_forgetting_check=data.get("last_forgetting_check", 0),
            source_context=data.get("source_context"),
        )


@dataclass
class MemoryChunk:
    """ç»“æž„åŒ–è®°å¿†å•å…ƒ - æ ¸å¿ƒæ•°æ®ç»“æž„"""

    # å…ƒæ•°æ®
    metadata: MemoryMetadata

    # å†…å®¹ç»“æž„
    content: ContentStructure  # ä¸»è°“å®¾ç»“æž„
    memory_type: MemoryType  # è®°å¿†ç±»åž‹

    # æ‰©å±•ä¿¡æ¯
    keywords: list[str] = field(default_factory=list)  # å…³é”®è¯åˆ—è¡¨
    tags: list[str] = field(default_factory=list)  # æ ‡ç­¾åˆ—è¡¨
    categories: list[str] = field(default_factory=list)  # åˆ†ç±»åˆ—è¡¨

    # è¯­ä¹‰ä¿¡æ¯
    embedding: list[float] | None = None  # è¯­ä¹‰å‘é‡
    semantic_hash: str | None = None  # è¯­ä¹‰å“ˆå¸Œå€¼

    # å…³è”ä¿¡æ¯
    related_memories: list[str] = field(default_factory=list)  # å…³è”è®°å¿†IDåˆ—è¡¨
    temporal_context: dict[str, Any] | None = None  # æ—¶é—´ä¸Šä¸‹æ–‡

    def __post_init__(self):
        """åŽåˆå§‹åŒ–å¤„ç†"""
        if self.embedding and len(self.embedding) > 0:
            self._generate_semantic_hash()

    def _generate_semantic_hash(self):
        """ç”Ÿæˆè¯­ä¹‰å“ˆå¸Œå€¼"""
        if not self.embedding:
            return

        try:
            # ä½¿ç”¨å‘é‡å’Œå†…å®¹ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œ
            content_str = f"{self.content.subject}:{self.content.predicate}:{self.content.object!s}"
            embedding_str = ",".join(map(str, [round(x, 6) for x in self.embedding]))

            hash_input = f"{content_str}|{embedding_str}"
            hash_object = hashlib.sha256(hash_input.encode("utf-8"))
            self.semantic_hash = hash_object.hexdigest()[:16]

        except Exception as e:
            logger.warning(f"ç”Ÿæˆè¯­ä¹‰å“ˆå¸Œå¤±è´¥: {e}")
            self.semantic_hash = str(uuid.uuid4())[:16]

    @property
    def memory_id(self) -> str:
        """èŽ·å–è®°å¿†ID"""
        return self.metadata.memory_id

    @property
    def user_id(self) -> str:
        """èŽ·å–ç”¨æˆ·ID"""
        return self.metadata.user_id

    @property
    def text_content(self) -> str:
        """èŽ·å–æ–‡æœ¬å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨displayï¼‰"""
        return str(self.content)

    @property
    def display(self) -> str:
        """èŽ·å–å±•ç¤ºæ–‡æœ¬"""
        return self.content.display or str(self.content)

    @property
    def subjects(self) -> list[str]:
        """èŽ·å–ä¸»è¯­åˆ—è¡¨"""
        return self.content.to_subject_list()

    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.metadata.update_access()

    def update_relevance(self, new_score: float):
        """æ›´æ–°ç›¸å…³åº¦è¯„åˆ†"""
        self.metadata.update_relevance(new_score)

    def should_forget(self, current_time: float | None = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é—å¿˜"""
        return self.metadata.should_forget(current_time)

    def is_dormant(self, current_time: float | None = None, inactive_days: int = 90) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¤„äºŽä¼‘çœ çŠ¶æ€ï¼ˆé•¿æœŸæœªæ¿€æ´»ï¼‰"""
        return self.metadata.is_dormant(current_time, inactive_days)

    def calculate_forgetting_threshold(self) -> float:
        """è®¡ç®—é—å¿˜é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰"""
        return self.metadata.calculate_forgetting_threshold()

    def add_keyword(self, keyword: str):
        """æ·»åŠ å…³é”®è¯"""
        if keyword and keyword not in self.keywords:
            self.keywords.append(keyword.strip())

    def add_tag(self, tag: str):
        """æ·»åŠ æ ‡ç­¾"""
        if tag and tag not in self.tags:
            self.tags.append(tag.strip())

    def add_category(self, category: str):
        """æ·»åŠ åˆ†ç±»"""
        if category and category not in self.categories:
            self.categories.append(category.strip())

    def add_related_memory(self, memory_id: str):
        """æ·»åŠ å…³è”è®°å¿†"""
        if memory_id and memory_id not in self.related_memories:
            self.related_memories.append(memory_id)

    def set_embedding(self, embedding: list[float]):
        """è®¾ç½®è¯­ä¹‰å‘é‡"""
        self.embedding = embedding
        self._generate_semantic_hash()

    def calculate_similarity(self, other: "MemoryChunk") -> float:
        """è®¡ç®—ä¸Žå¦ä¸€ä¸ªè®°å¿†å—çš„ç›¸ä¼¼åº¦"""
        if not self.embedding or not other.embedding:
            return 0.0

        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            v1 = np.array(self.embedding)
            v2 = np.array(other.embedding)

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return max(0.0, min(1.0, similarity))

        except Exception as e:
            logger.warning(f"è®¡ç®—è®°å¿†ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå®Œæ•´çš„å­—å…¸æ ¼å¼"""
        return {
            "metadata": self.metadata.to_dict(),
            "content": self.content.to_dict(),
            "memory_type": self.memory_type.value,
            "keywords": self.keywords,
            "tags": self.tags,
            "categories": self.categories,
            "embedding": self.embedding,
            "semantic_hash": self.semantic_hash,
            "related_memories": self.related_memories,
            "temporal_context": self.temporal_context,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "MemoryChunk":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        metadata = MemoryMetadata.from_dict(data.get("metadata", {}))
        content = ContentStructure.from_dict(data.get("content", {}))

        chunk = cls(
            metadata=metadata,
            content=content,
            memory_type=MemoryType(data.get("memory_type", MemoryType.CONTEXTUAL.value)),
            keywords=data.get("keywords", []),
            tags=data.get("tags", []),
            categories=data.get("categories", []),
            embedding=data.get("embedding"),
            semantic_hash=data.get("semantic_hash"),
            related_memories=data.get("related_memories", []),
            temporal_context=data.get("temporal_context"),
        )

        return chunk

    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        return orjson.dumps(self.to_dict()).decode("utf-8")

    @classmethod
    def from_json(cls, json_str: str) -> "MemoryChunk":
        """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå®žä¾‹"""
        try:
            data = orjson.loads(json_str)
            return cls.from_dict(data)
        except Exception as e:
            logger.error(f"ä»ŽJSONåˆ›å»ºè®°å¿†å—å¤±è´¥: {e}")
            raise

    def is_similar_to(self, other: "MemoryChunk", threshold: float = 0.8) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸Žå¦ä¸€ä¸ªè®°å¿†å—ç›¸ä¼¼"""
        if self.semantic_hash and other.semantic_hash:
            return self.semantic_hash == other.semantic_hash

        return self.calculate_similarity(other) >= threshold

    def merge_with(self, other: "MemoryChunk") -> bool:
        """ä¸Žå¦ä¸€ä¸ªè®°å¿†å—åˆå¹¶ï¼ˆå¦‚æžœç›¸ä¼¼ï¼‰"""
        if not self.is_similar_to(other):
            return False

        try:
            # åˆå¹¶å…³é”®è¯
            for keyword in other.keywords:
                self.add_keyword(keyword)

            # åˆå¹¶æ ‡ç­¾
            for tag in other.tags:
                self.add_tag(tag)

            # åˆå¹¶åˆ†ç±»
            for category in other.categories:
                self.add_category(category)

            # åˆå¹¶å…³è”è®°å¿†
            for memory_id in other.related_memories:
                self.add_related_memory(memory_id)

            # æ›´æ–°å…ƒæ•°æ®
            self.metadata.last_modified = time.time()
            self.metadata.access_count += other.metadata.access_count
            self.metadata.relevance_score = max(self.metadata.relevance_score, other.metadata.relevance_score)

            # æ›´æ–°ç½®ä¿¡åº¦
            if other.metadata.confidence.value > self.metadata.confidence.value:
                self.metadata.confidence = other.metadata.confidence

            # æ›´æ–°é‡è¦æ€§
            if other.metadata.importance.value > self.metadata.importance.value:
                self.metadata.importance = other.metadata.importance

            logger.debug(f"è®°å¿†å— {self.memory_id} åˆå¹¶äº†è®°å¿†å— {other.memory_id}")
            return True

        except Exception as e:
            logger.error(f"åˆå¹¶è®°å¿†å—å¤±è´¥: {e}")
            return False

    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        type_emoji = {
            MemoryType.PERSONAL_FACT: "ðŸ‘¤",
            MemoryType.EVENT: "ðŸ“…",
            MemoryType.PREFERENCE: "â¤ï¸",
            MemoryType.OPINION: "ðŸ’­",
            MemoryType.RELATIONSHIP: "ðŸ‘¥",
            MemoryType.EMOTION: "ðŸ˜Š",
            MemoryType.KNOWLEDGE: "ðŸ“š",
            MemoryType.SKILL: "ðŸ› ï¸",
            MemoryType.GOAL: "ðŸŽ¯",
            MemoryType.EXPERIENCE: "ðŸ’¡",
            MemoryType.CONTEXTUAL: "ðŸ“",
        }

        emoji = type_emoji.get(self.memory_type, "ðŸ“")
        confidence_icon = "â—" * self.metadata.confidence.value
        importance_icon = "â˜…" * self.metadata.importance.value

        return f"{emoji} [{self.memory_type.value}] {self.display} {confidence_icon} {importance_icon}"

    def __repr__(self) -> str:
        """è°ƒè¯•è¡¨ç¤º"""
        return f"MemoryChunk(id={self.memory_id[:8]}..., type={self.memory_type.value}, user={self.user_id})"


def _build_display_text(subjects: Iterable[str], predicate: str, obj: str | dict) -> str:
    """æ ¹æ®ä¸»è°“å®¾ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°"""
    subjects_clean = [s.strip() for s in subjects if s and isinstance(s, str)]
    subject_part = "ã€".join(subjects_clean) if subjects_clean else "å¯¹è¯å‚ä¸Žè€…"

    if isinstance(obj, dict):
        object_candidates = []
        for key, value in obj.items():
            if isinstance(value, str | int | float):
                object_candidates.append(f"{key}:{value}")
            elif isinstance(value, list):
                compact = "ã€".join(str(item) for item in value[:3])
                object_candidates.append(f"{key}:{compact}")
        object_part = "ï¼Œ".join(object_candidates) if object_candidates else str(obj)
    else:
        object_part = str(obj).strip()

    predicate_clean = predicate.strip()
    if not predicate_clean:
        return f"{subject_part} {object_part}".strip()

    if object_part:
        return f"{subject_part}{predicate_clean}{object_part}".strip()
    return f"{subject_part}{predicate_clean}".strip()


def create_memory_chunk(
    user_id: str,
    subject: str | list[str],
    predicate: str,
    obj: str | dict,
    memory_type: MemoryType,
    chat_id: str | None = None,
    source_context: str | None = None,
    importance: ImportanceLevel = ImportanceLevel.NORMAL,
    confidence: ConfidenceLevel = ConfidenceLevel.MEDIUM,
    display: str | None = None,
    **kwargs,
) -> MemoryChunk:
    """ä¾¿æ·çš„å†…å­˜å—åˆ›å»ºå‡½æ•°"""
    metadata = MemoryMetadata(
        memory_id="",
        user_id=user_id,
        chat_id=chat_id,
        created_at=time.time(),
        last_accessed=0,
        last_modified=0,
        confidence=confidence,
        importance=importance,
        source_context=source_context,
    )

    subjects: list[str]
    if isinstance(subject, list):
        subjects = [s for s in subject if isinstance(s, str) and s.strip()]
        subject_payload: str | list[str] = subjects
    else:
        cleaned = subject.strip() if isinstance(subject, str) else ""
        subjects = [cleaned] if cleaned else []
        subject_payload = cleaned

    display_text = display or _build_display_text(subjects, predicate, obj)

    content = ContentStructure(subject=subject_payload, predicate=predicate, object=obj, display=display_text)

    chunk = MemoryChunk(metadata=metadata, content=content, memory_type=memory_type, **kwargs)

    return chunk


@dataclass
class MessageCollection:
    """æ¶ˆæ¯é›†åˆæ•°æ®ç»“æž„"""

    collection_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    chat_id: str | None = None  # èŠå¤©IDï¼ˆç¾¤èŠæˆ–ç§èŠï¼‰
    messages: list[str] = field(default_factory=list)
    combined_text: str = ""
    created_at: float = field(default_factory=time.time)
    embedding: list[float] | None = None

    def to_dict(self) -> dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "collection_id": self.collection_id,
            "chat_id": self.chat_id,
            "messages": self.messages,
            "combined_text": self.combined_text,
            "created_at": self.created_at,
            "embedding": self.embedding,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "MessageCollection":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        return cls(
            collection_id=data.get("collection_id", str(uuid.uuid4())),
            chat_id=data.get("chat_id"),
            messages=data.get("messages", []),
            combined_text=data.get("combined_text", ""),
            created_at=data.get("created_at", time.time()),
            embedding=data.get("embedding"),
        )
