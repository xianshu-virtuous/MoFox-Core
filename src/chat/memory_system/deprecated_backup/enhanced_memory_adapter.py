# -*- coding: utf-8 -*-
"""
å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨
å°†å¢å¼ºè®°å¿†ç³»ç»Ÿé›†æˆåˆ°ç°æœ‰MoFox Botæ¶æ„ä¸­
"""

import asyncio
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from src.common.logger import get_logger
from src.chat.memory_system.integration_layer import MemoryIntegrationLayer, IntegrationConfig, IntegrationMode
from src.chat.memory_system.memory_chunk import MemoryChunk, MemoryType
from src.chat.memory_system.memory_formatter import MemoryFormatter, FormatterConfig, format_memories_for_llm
from src.llm_models.utils_model import LLMRequest

logger = get_logger(__name__)


MEMORY_TYPE_LABELS = {
    MemoryType.PERSONAL_FACT: "ä¸ªäººäº‹å®",
    MemoryType.EVENT: "äº‹ä»¶",
    MemoryType.PREFERENCE: "åå¥½",
    MemoryType.OPINION: "è§‚ç‚¹",
    MemoryType.RELATIONSHIP: "å…³ç³»",
    MemoryType.EMOTION: "æƒ…æ„Ÿ",
    MemoryType.KNOWLEDGE: "çŸ¥è¯†",
    MemoryType.SKILL: "æŠ€èƒ½",
    MemoryType.GOAL: "ç›®æ ‡",
    MemoryType.EXPERIENCE: "ç»éªŒ",
    MemoryType.CONTEXTUAL: "ä¸Šä¸‹æ–‡",
}


@dataclass
class AdapterConfig:
    """é€‚é…å™¨é…ç½®"""
    enable_enhanced_memory: bool = True
    integration_mode: str = "enhanced_only"  # replace, enhanced_only
    auto_migration: bool = True
    memory_value_threshold: float = 0.6
    fusion_threshold: float = 0.85
    max_retrieval_results: int = 10


class EnhancedMemoryAdapter:
    """å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨"""

    def __init__(self, llm_model: LLMRequest, config: Optional[AdapterConfig] = None):
        self.llm_model = llm_model
        self.config = config or AdapterConfig()
        self.integration_layer: Optional[MemoryIntegrationLayer] = None
        self._initialized = False

        # ç»Ÿè®¡ä¿¡æ¯
        self.adapter_stats = {
            "total_processed": 0,
            "enhanced_used": 0,
            "legacy_used": 0,
            "hybrid_used": 0,
            "memories_created": 0,
            "memories_retrieved": 0,
            "average_processing_time": 0.0
        }

    async def initialize(self):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        if self._initialized:
            return

        try:
            logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨...")

            # è½¬æ¢é…ç½®æ ¼å¼
            integration_config = IntegrationConfig(
                mode=IntegrationMode(self.config.integration_mode),
                enable_enhanced_memory=self.config.enable_enhanced_memory,
                memory_value_threshold=self.config.memory_value_threshold,
                fusion_threshold=self.config.fusion_threshold,
                max_retrieval_results=self.config.max_retrieval_results,
                enable_learning=True  # å¯ç”¨å­¦ä¹ åŠŸèƒ½
            )

            # åˆ›å»ºé›†æˆå±‚
            self.integration_layer = MemoryIntegrationLayer(
                llm_model=self.llm_model,
                config=integration_config
            )

            # åˆå§‹åŒ–é›†æˆå±‚
            await self.integration_layer.initialize()

            self._initialized = True
            logger.info("âœ… å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œç¦ç”¨å¢å¼ºè®°å¿†åŠŸèƒ½
            self.config.enable_enhanced_memory = False

    async def process_conversation_memory(
        self,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """å¤„ç†å¯¹è¯è®°å¿†ï¼Œä»¥ä¸Šä¸‹æ–‡ä¸ºå”¯ä¸€è¾“å…¥"""
        if not self._initialized or not self.config.enable_enhanced_memory:
            return {"success": False, "error": "Enhanced memory not available"}

        start_time = time.time()
        self.adapter_stats["total_processed"] += 1

        try:
            payload_context: Dict[str, Any] = dict(context or {})

            conversation_text = payload_context.get("conversation_text")
            if not conversation_text:
                conversation_candidate = (
                    payload_context.get("message_content")
                    or payload_context.get("latest_message")
                    or payload_context.get("raw_text")
                )
                if conversation_candidate is not None:
                    conversation_text = str(conversation_candidate)
                    payload_context["conversation_text"] = conversation_text
                else:
                    conversation_text = ""
            else:
                conversation_text = str(conversation_text)

            if "timestamp" not in payload_context:
                payload_context["timestamp"] = time.time()

            logger.debug("é€‚é…å™¨æ”¶åˆ°è®°å¿†æ„å»ºè¯·æ±‚ï¼Œæ–‡æœ¬é•¿åº¦=%d", len(conversation_text))

            # ä½¿ç”¨é›†æˆå±‚å¤„ç†å¯¹è¯
            result = await self.integration_layer.process_conversation(payload_context)

            # æ›´æ–°ç»Ÿè®¡
            processing_time = time.time() - start_time
            self._update_processing_stats(processing_time)

            if result["success"]:
                created_count = len(result.get("created_memories", []))
                self.adapter_stats["memories_created"] += created_count
                logger.debug(f"å¯¹è¯è®°å¿†å¤„ç†å®Œæˆï¼Œåˆ›å»º {created_count} æ¡è®°å¿†")

            return result

        except Exception as e:
            logger.error(f"å¤„ç†å¯¹è¯è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    async def retrieve_relevant_memories(
        self,
        query: str,
        user_id: str,
        context: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None
    ) -> List[MemoryChunk]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        if not self._initialized or not self.config.enable_enhanced_memory:
            return []

        try:
            limit = limit or self.config.max_retrieval_results
            memories = await self.integration_layer.retrieve_relevant_memories(
                query, None, context, limit
            )

            self.adapter_stats["memories_retrieved"] += len(memories)
            logger.debug(f"æ£€ç´¢åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")

            return memories

        except Exception as e:
            logger.error(f"æ£€ç´¢ç›¸å…³è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    async def get_memory_context_for_prompt(
        self,
        query: str,
        user_id: str,
        context: Optional[Dict[str, Any]] = None,
        max_memories: int = 5
    ) -> str:
        """è·å–ç”¨äºæç¤ºè¯çš„è®°å¿†ä¸Šä¸‹æ–‡"""
        memories = await self.retrieve_relevant_memories(query, user_id, context, max_memories)

        if not memories:
            return ""

        # ä½¿ç”¨æ–°çš„è®°å¿†æ ¼å¼åŒ–å™¨
        formatter_config = FormatterConfig(
            include_timestamps=True,
            include_memory_types=True,
            include_confidence=False,
            use_emoji_icons=True,
            group_by_type=False,
            max_display_length=150
        )
        
        return format_memories_for_llm(
            memories=memories,
            query_context=query,
            config=formatter_config
        )

    async def get_enhanced_memory_summary(self, user_id: str) -> Dict[str, Any]:
        """è·å–å¢å¼ºè®°å¿†ç³»ç»Ÿæ‘˜è¦"""
        if not self._initialized or not self.config.enable_enhanced_memory:
            return {"available": False, "reason": "Not initialized or disabled"}

        try:
            # è·å–ç³»ç»ŸçŠ¶æ€
            status = await self.integration_layer.get_system_status()

            # è·å–é€‚é…å™¨ç»Ÿè®¡
            adapter_stats = self.adapter_stats.copy()

            # è·å–é›†æˆç»Ÿè®¡
            integration_stats = self.integration_layer.get_integration_stats()

            return {
                "available": True,
                "system_status": status,
                "adapter_stats": adapter_stats,
                "integration_stats": integration_stats,
                "total_memories_created": adapter_stats["memories_created"],
                "total_memories_retrieved": adapter_stats["memories_retrieved"]
            }

        except Exception as e:
            logger.error(f"è·å–å¢å¼ºè®°å¿†æ‘˜è¦å¤±è´¥: {e}", exc_info=True)
            return {"available": False, "error": str(e)}

    def _update_processing_stats(self, processing_time: float):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡"""
        total_processed = self.adapter_stats["total_processed"]
        if total_processed > 0:
            current_avg = self.adapter_stats["average_processing_time"]
            new_avg = (current_avg * (total_processed - 1) + processing_time) / total_processed
            self.adapter_stats["average_processing_time"] = new_avg

    def get_adapter_stats(self) -> Dict[str, Any]:
        """è·å–é€‚é…å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return self.adapter_stats.copy()

    async def maintenance(self):
        """ç»´æŠ¤æ“ä½œ"""
        if not self._initialized:
            return

        try:
            logger.info("ğŸ”§ å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨ç»´æŠ¤...")
            await self.integration_layer.maintenance()
            logger.info("âœ… å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨ç»´æŠ¤å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨ç»´æŠ¤å¤±è´¥: {e}", exc_info=True)

    async def shutdown(self):
        """å…³é—­é€‚é…å™¨"""
        if not self._initialized:
            return

        try:
            logger.info("ğŸ”„ å…³é—­å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨...")
            await self.integration_layer.shutdown()
            self._initialized = False
            logger.info("âœ… å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­å¢å¼ºè®°å¿†ç³»ç»Ÿé€‚é…å™¨å¤±è´¥: {e}", exc_info=True)


# å…¨å±€é€‚é…å™¨å®ä¾‹
_enhanced_memory_adapter: Optional[EnhancedMemoryAdapter] = None


async def get_enhanced_memory_adapter(llm_model: LLMRequest) -> EnhancedMemoryAdapter:
    """è·å–å…¨å±€å¢å¼ºè®°å¿†é€‚é…å™¨å®ä¾‹"""
    global _enhanced_memory_adapter

    if _enhanced_memory_adapter is None:
        # ä»é…ç½®ä¸­è·å–é€‚é…å™¨é…ç½®
        from src.config.config import global_config

        adapter_config = AdapterConfig(
            enable_enhanced_memory=getattr(global_config.memory, 'enable_enhanced_memory', True),
            integration_mode=getattr(global_config.memory, 'enhanced_memory_mode', 'enhanced_only'),
            auto_migration=getattr(global_config.memory, 'enable_memory_migration', True),
            memory_value_threshold=getattr(global_config.memory, 'memory_value_threshold', 0.6),
            fusion_threshold=getattr(global_config.memory, 'fusion_threshold', 0.85),
            max_retrieval_results=getattr(global_config.memory, 'max_retrieval_results', 10)
        )

        _enhanced_memory_adapter = EnhancedMemoryAdapter(llm_model, adapter_config)
        await _enhanced_memory_adapter.initialize()

    return _enhanced_memory_adapter


async def initialize_enhanced_memory_system(llm_model: LLMRequest):
    """åˆå§‹åŒ–å¢å¼ºè®°å¿†ç³»ç»Ÿ"""
    try:
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºè®°å¿†ç³»ç»Ÿ...")
        adapter = await get_enhanced_memory_adapter(llm_model)
        logger.info("âœ… å¢å¼ºè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return adapter
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        return None


async def process_conversation_with_enhanced_memory(
    context: Dict[str, Any],
    llm_model: Optional[LLMRequest] = None
) -> Dict[str, Any]:
    """ä½¿ç”¨å¢å¼ºè®°å¿†ç³»ç»Ÿå¤„ç†å¯¹è¯ï¼Œä¸Šä¸‹æ–‡éœ€åŒ…å« conversation_text ç­‰ä¿¡æ¯"""
    if not llm_model:
        # è·å–é»˜è®¤çš„LLMæ¨¡å‹
        from src.llm_models.utils_model import get_global_llm_model
        llm_model = get_global_llm_model()

    try:
        adapter = await get_enhanced_memory_adapter(llm_model)
        payload_context = dict(context or {})

        if "conversation_text" not in payload_context:
            conversation_candidate = (
                payload_context.get("message_content")
                or payload_context.get("latest_message")
                or payload_context.get("raw_text")
            )
            if conversation_candidate is not None:
                payload_context["conversation_text"] = str(conversation_candidate)

        return await adapter.process_conversation_memory(payload_context)
    except Exception as e:
        logger.error(f"ä½¿ç”¨å¢å¼ºè®°å¿†ç³»ç»Ÿå¤„ç†å¯¹è¯å¤±è´¥: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


async def retrieve_memories_with_enhanced_system(
    query: str,
    user_id: str,
    context: Optional[Dict[str, Any]] = None,
    limit: int = 10,
    llm_model: Optional[LLMRequest] = None
) -> List[MemoryChunk]:
    """ä½¿ç”¨å¢å¼ºè®°å¿†ç³»ç»Ÿæ£€ç´¢è®°å¿†"""
    if not llm_model:
        # è·å–é»˜è®¤çš„LLMæ¨¡å‹
        from src.llm_models.utils_model import get_global_llm_model
        llm_model = get_global_llm_model()

    try:
        adapter = await get_enhanced_memory_adapter(llm_model)
        return await adapter.retrieve_relevant_memories(query, user_id, context, limit)
    except Exception as e:
        logger.error(f"ä½¿ç”¨å¢å¼ºè®°å¿†ç³»ç»Ÿæ£€ç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
        return []


async def get_memory_context_for_prompt(
    query: str,
    user_id: str,
    context: Optional[Dict[str, Any]] = None,
    max_memories: int = 5,
    llm_model: Optional[LLMRequest] = None
) -> str:
    """è·å–ç”¨äºæç¤ºè¯çš„è®°å¿†ä¸Šä¸‹æ–‡"""
    if not llm_model:
        # è·å–é»˜è®¤çš„LLMæ¨¡å‹
        from src.llm_models.utils_model import get_global_llm_model
        llm_model = get_global_llm_model()

    try:
        adapter = await get_enhanced_memory_adapter(llm_model)
        return await adapter.get_memory_context_for_prompt(query, user_id, context, max_memories)
    except Exception as e:
        logger.error(f"è·å–è®°å¿†ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
        return ""