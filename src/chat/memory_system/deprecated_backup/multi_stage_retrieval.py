# -*- coding: utf-8 -*-
"""
å¤šé˜¶æ®µå¬å›æœºåˆ¶
å®ç°ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„è®°å¿†æ£€ç´¢ä¼˜åŒ–
"""

import time
import asyncio
from typing import Dict, List, Optional, Tuple, Set, Any
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import orjson

from src.common.logger import get_logger
from src.chat.memory_system.memory_chunk import MemoryChunk, MemoryType, ConfidenceLevel, ImportanceLevel
from src.chat.memory_system.enhanced_reranker import EnhancedReRanker, ReRankingConfig

logger = get_logger(__name__)


class RetrievalStage(Enum):
    """æ£€ç´¢é˜¶æ®µ"""
    METADATA_FILTERING = "metadata_filtering"      # å…ƒæ•°æ®è¿‡æ»¤é˜¶æ®µ
    VECTOR_SEARCH = "vector_search"                 # å‘é‡æœç´¢é˜¶æ®µ
    SEMANTIC_RERANKING = "semantic_reranking"       # è¯­ä¹‰é‡æ’åºé˜¶æ®µ
    CONTEXTUAL_FILTERING = "contextual_filtering"    # ä¸Šä¸‹æ–‡è¿‡æ»¤é˜¶æ®µ


@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    # å„é˜¶æ®µé…ç½® - ä¼˜åŒ–å¬å›ç‡
    metadata_filter_limit: int = 150        # å…ƒæ•°æ®è¿‡æ»¤é˜¶æ®µè¿”å›æ•°é‡ï¼ˆå¢åŠ ï¼‰
    vector_search_limit: int = 80           # å‘é‡æœç´¢é˜¶æ®µè¿”å›æ•°é‡ï¼ˆå¢åŠ ï¼‰
    semantic_rerank_limit: int = 30         # è¯­ä¹‰é‡æ’åºé˜¶æ®µè¿”å›æ•°é‡ï¼ˆå¢åŠ ï¼‰
    final_result_limit: int = 10            # æœ€ç»ˆç»“æœæ•°é‡

    # ç›¸ä¼¼åº¦é˜ˆå€¼ - ä¼˜åŒ–å¬å›ç‡
    vector_similarity_threshold: float = 0.5    # å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé™ä½ä»¥æå‡å¬å›ç‡ï¼‰
    semantic_similarity_threshold: float = 0.05  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä¿æŒè¾ƒä½ä»¥è·å¾—æ›´å¤šç›¸å…³è®°å¿†ï¼‰

    # æƒé‡é…ç½®
    vector_weight: float = 0.4                 # å‘é‡ç›¸ä¼¼åº¦æƒé‡
    semantic_weight: float = 0.3               # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
    context_weight: float = 0.2                 # ä¸Šä¸‹æ–‡æƒé‡
    recency_weight: float = 0.1                 # æ—¶æ•ˆæ€§æƒé‡

    @classmethod
    def from_global_config(cls):
        """ä»å…¨å±€é…ç½®åˆ›å»ºé…ç½®å®ä¾‹"""
        from src.config.config import global_config

        return cls(
            # å„é˜¶æ®µé…ç½® - ä¼˜åŒ–å¬å›ç‡
            metadata_filter_limit=max(150, global_config.memory.metadata_filter_limit),    # å¢åŠ å€™é€‰æ± 
            vector_search_limit=max(80, global_config.memory.vector_search_limit),          # å¢åŠ å‘é‡æœç´¢ç»“æœ
            semantic_rerank_limit=max(30, global_config.memory.semantic_rerank_limit),      # å¢åŠ é‡æ’åºå€™é€‰
            final_result_limit=global_config.memory.final_result_limit,

            # ç›¸ä¼¼åº¦é˜ˆå€¼ - ä¼˜åŒ–å¬å›ç‡
            vector_similarity_threshold=max(0.5, global_config.memory.vector_similarity_threshold),  # ç¡®ä¿ä¸ä½äº0.5
            semantic_similarity_threshold=0.05,  # è¿›ä¸€æ­¥é™ä½ä»¥æå‡å¬å›ç‡

            # æƒé‡é…ç½®
            vector_weight=global_config.memory.vector_weight,
            semantic_weight=global_config.memory.semantic_weight,
            context_weight=global_config.memory.context_weight,
            recency_weight=global_config.memory.recency_weight
        )


@dataclass
class StageResult:
    """é˜¶æ®µç»“æœ"""
    stage: RetrievalStage
    memory_ids: List[str]
    processing_time: float
    filtered_count: int
    score_threshold: float
    details: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    user_id: str
    final_memories: List[MemoryChunk]
    stage_results: List[StageResult]
    total_processing_time: float
    total_filtered: int
    retrieval_stats: Dict[str, Any]


class MultiStageRetrieval:
    """å¤šé˜¶æ®µå¬å›ç³»ç»Ÿ"""

    def __init__(self, config: Optional[RetrievalConfig] = None):
        self.config = config or RetrievalConfig.from_global_config()
        
        # åˆå§‹åŒ–å¢å¼ºé‡æ’åºå™¨
        reranker_config = ReRankingConfig(
            semantic_weight=self.config.vector_weight,
            recency_weight=self.config.recency_weight,
            usage_freq_weight=0.2,  # æ–°å¢çš„ä½¿ç”¨é¢‘ç‡æƒé‡
            type_match_weight=0.1   # æ–°å¢çš„ç±»å‹åŒ¹é…æƒé‡
        )
        self.reranker = EnhancedReRanker(reranker_config)
        
        self.retrieval_stats = {
            "total_queries": 0,
            "average_retrieval_time": 0.0,
            "stage_stats": {
                "metadata_filtering": {"calls": 0, "avg_time": 0.0},
                "vector_search": {"calls": 0, "avg_time": 0.0},
                "semantic_reranking": {"calls": 0, "avg_time": 0.0},
                "contextual_filtering": {"calls": 0, "avg_time": 0.0},
                "enhanced_reranking": {"calls": 0, "avg_time": 0.0}  # æ–°å¢ç»Ÿè®¡
            }
        }

    async def retrieve_memories(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        metadata_index,
        vector_storage,
        all_memories_cache: Dict[str, MemoryChunk],
        limit: Optional[int] = None
    ) -> RetrievalResult:
        """å¤šé˜¶æ®µè®°å¿†æ£€ç´¢"""
        start_time = time.time()
        limit = limit or self.config.final_result_limit

        stage_results = []
        current_memory_ids = set()
        memory_debug_info: Dict[str, Dict[str, Any]] = {}

        try:
            logger.debug(f"å¼€å§‹å¤šé˜¶æ®µæ£€ç´¢ï¼šquery='{query}', user_id='{user_id}'")

            # é˜¶æ®µ1ï¼šå…ƒæ•°æ®è¿‡æ»¤
            stage1_result = await self._metadata_filtering_stage(
                query, user_id, context, metadata_index, all_memories_cache,
                debug_log=memory_debug_info
            )
            stage_results.append(stage1_result)
            current_memory_ids.update(stage1_result.memory_ids)

            # é˜¶æ®µ2ï¼šå‘é‡æœç´¢
            stage2_result = await self._vector_search_stage(
                query, user_id, context, vector_storage, current_memory_ids, all_memories_cache,
                debug_log=memory_debug_info
            )
            stage_results.append(stage2_result)
            current_memory_ids.update(stage2_result.memory_ids)

            # é˜¶æ®µ3ï¼šè¯­ä¹‰é‡æ’åº
            stage3_result = await self._semantic_reranking_stage(
                query, user_id, context, current_memory_ids, all_memories_cache,
                debug_log=memory_debug_info
            )
            stage_results.append(stage3_result)

            # é˜¶æ®µ4ï¼šä¸Šä¸‹æ–‡è¿‡æ»¤
            stage4_result = await self._contextual_filtering_stage(
                query, user_id, context, stage3_result.memory_ids, all_memories_cache, limit,
                debug_log=memory_debug_info
            )
            stage_results.append(stage4_result)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å›é€€æœºåˆ¶
            if len(stage4_result.memory_ids) < min(3, limit):
                logger.debug(f"ä¸Šä¸‹æ–‡è¿‡æ»¤ç»“æœè¿‡å°‘({len(stage4_result.memory_ids)})ï¼Œå¯ç”¨å›é€€æœºåˆ¶")
                # å›é€€åˆ°æ›´å®½æ¾çš„æ£€ç´¢ç­–ç•¥
                fallback_result = await self._fallback_retrieval_stage(
                    query, user_id, context, all_memories_cache, limit,
                    excluded_ids=set(stage4_result.memory_ids),
                    debug_log=memory_debug_info
                )
                if fallback_result.memory_ids:
                    stage4_result.memory_ids.extend(fallback_result.memory_ids[:limit - len(stage4_result.memory_ids)])
                    logger.debug(f"å›é€€æœºåˆ¶è¡¥å……äº† {len(fallback_result.memory_ids)} æ¡è®°å¿†")

            # é˜¶æ®µ5ï¼šå¢å¼ºé‡æ’åº (æ–°å¢)
            stage5_result = await self._enhanced_reranking_stage(
                query, user_id, context, stage4_result.memory_ids, all_memories_cache, limit,
                debug_log=memory_debug_info
            )
            stage_results.append(stage5_result)

            # è·å–æœ€ç»ˆè®°å¿†å¯¹è±¡
            final_memories = []
            for memory_id in stage5_result.memory_ids:  # ä½¿ç”¨é‡æ’åºåçš„ç»“æœ
                if memory_id in all_memories_cache:
                    memory = all_memories_cache[memory_id]
                    memory.update_access()  # æ›´æ–°è®¿é—®ç»Ÿè®¡
                    final_memories.append(memory)

            # æ›´æ–°ç»Ÿè®¡
            total_time = time.time() - start_time
            self._update_retrieval_stats(total_time, stage_results)

            total_filtered = sum(result.filtered_count for result in stage_results)

            logger.debug(f"å¤šé˜¶æ®µæ£€ç´¢å®Œæˆï¼šè¿”å› {len(final_memories)} æ¡è®°å¿†ï¼Œè€—æ—¶ {total_time:.3f}s")

            if memory_debug_info:
                final_ids_set = set(stage5_result.memory_ids)  # ä½¿ç”¨é‡æ’åºåçš„ç»“æœ
                debug_entries = []
                for memory_id, trace in memory_debug_info.items():
                    memory_obj = all_memories_cache.get(memory_id)
                    display_text = ""
                    if memory_obj:
                        display_text = (memory_obj.display or memory_obj.text_content or "").strip()
                        if len(display_text) > 80:
                            display_text = display_text[:77] + "..."

                    entry = {
                        "memory_id": memory_id,
                        "display": display_text,
                        "memory_type": memory_obj.memory_type.value if memory_obj else None,
                        "vector_similarity": trace.get("vector_stage", {}).get("similarity"),
                        "semantic_score": trace.get("semantic_stage", {}).get("score"),
                        "context_score": trace.get("context_stage", {}).get("context_score"),
                        "final_score": trace.get("context_stage", {}).get("final_score"),
                        "status": trace.get("context_stage", {}).get("status") or trace.get("vector_stage", {}).get("status") or trace.get("semantic_stage", {}).get("status"),
                        "is_final": memory_id in final_ids_set,
                    }
                    debug_entries.append(entry)

                # é™åˆ¶æ—¥å¿—è¾“å‡ºæ•°é‡
                debug_entries.sort(key=lambda item: (item.get("is_final", False), item.get("final_score") or item.get("vector_similarity") or 0.0), reverse=True)
                debug_payload = {
                    "query": query,
                    "semantic_query": context.get("resolved_query_text", query),
                    "user_id": user_id,
                    "stage_summaries": [
                        {
                            "stage": result.stage.value,
                            "returned": len(result.memory_ids),
                            "filtered": result.filtered_count,
                            "duration": round(result.processing_time, 4),
                            "details": result.details,
                        }
                        for result in stage_results
                    ],
                    "candidates": debug_entries[:20],
                }
                try:
                    logger.info(
                        f"ğŸ§­ è®°å¿†æ£€ç´¢è°ƒè¯• | query='{query}' | final={len(stage5_result.memory_ids)}",
                        extra={"memory_debug": debug_payload},
                    )
                except Exception:
                    logger.info(
                        f"ğŸ§­ è®°å¿†æ£€ç´¢è°ƒè¯•è¯¦æƒ…: {orjson.dumps(debug_payload, ensure_ascii=False).decode('utf-8')}",
                    )

            return RetrievalResult(
                query=query,
                user_id=user_id,
                final_memories=final_memories,
                stage_results=stage_results,
                total_processing_time=total_time,
                total_filtered=total_filtered,
                retrieval_stats=self.retrieval_stats.copy()
            )

        except Exception as e:
            logger.error(f"å¤šé˜¶æ®µæ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            # è¿”å›ç©ºç»“æœ
            return RetrievalResult(
                query=query,
                user_id=user_id,
                final_memories=[],
                stage_results=stage_results,
                total_processing_time=time.time() - start_time,
                total_filtered=0,
                retrieval_stats=self.retrieval_stats.copy()
            )

    async def _metadata_filtering_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        metadata_index,
        all_memories_cache: Dict[str, MemoryChunk],
        *,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """é˜¶æ®µ1ï¼šå…ƒæ•°æ®è¿‡æ»¤"""
        start_time = time.time()

        try:
            from .metadata_index import IndexQuery

            query_plan = context.get("query_plan")

            memory_types = self._extract_memory_types_from_context(context)
            keywords = self._extract_keywords_from_query(query, query_plan)
            subjects = query_plan.subject_includes if query_plan and getattr(query_plan, "subject_includes", None) else None

            index_query = IndexQuery(
                user_ids=None,
                memory_types=memory_types,
                subjects=subjects,
                keywords=keywords,
                limit=self.config.metadata_filter_limit,
                sort_by="last_accessed",
                sort_order="desc"
            )

            # æ‰§è¡ŒæŸ¥è¯¢
            result = await metadata_index.query_memories(index_query)
            result_ids = list(result.memory_ids)
            filtered_count = max(0, len(all_memories_cache) - len(result_ids))
            details: List[Dict[str, Any]] = []

            # å¦‚æœæœªå‘½ä¸­ä»»ä½•ç´¢å¼•ä¸”æœªæŒ‡å®šæ‰€æœ‰è€…è¿‡æ»¤ï¼Œåˆ™å›é€€åˆ°æœ€è¿‘è®¿é—®çš„è®°å¿†
            if not result_ids:
                sorted_ids = sorted(
                    (memory.memory_id for memory in all_memories_cache.values()),
                    key=lambda mid: all_memories_cache[mid].metadata.last_accessed if mid in all_memories_cache else 0,
                    reverse=True,
                )
                if memory_types:
                    type_filtered = [
                        mid for mid in sorted_ids
                        if all_memories_cache[mid].memory_type in memory_types
                    ]
                    sorted_ids = type_filtered or sorted_ids
                if subjects:
                    subject_candidates = [s.lower() for s in subjects if isinstance(s, str) and s.strip()]
                    if subject_candidates:
                        subject_filtered = [
                            mid for mid in sorted_ids
                            if any(
                                subj.strip().lower() in subject_candidates
                                for subj in all_memories_cache[mid].subjects
                            )
                        ]
                        sorted_ids = subject_filtered or sorted_ids

                if keywords:
                    keyword_pool = {kw.lower() for kw in keywords if isinstance(kw, str) and kw.strip()}
                    if keyword_pool:
                        keyword_filtered = []
                        for mid in sorted_ids:
                            memory_text = (
                                (all_memories_cache[mid].display or "")
                                + "\n"
                                + (all_memories_cache[mid].text_content or "")
                            ).lower()
                            if any(kw in memory_text for kw in keyword_pool):
                                keyword_filtered.append(mid)
                        sorted_ids = keyword_filtered or sorted_ids

                result_ids = sorted_ids[: self.config.metadata_filter_limit]
                filtered_count = max(0, len(all_memories_cache) - len(result_ids))
                logger.debug(
                    "å…ƒæ•°æ®è¿‡æ»¤æœªå‘½ä¸­ç´¢å¼•ï¼Œä½¿ç”¨è¿‘ä¼¼å›é€€: types=%s, subjects=%s, keywords=%s",
                    bool(memory_types),
                    bool(subjects),
                    bool(keywords),
                )
                details.append({
                    "note": "fallback_recent",
                    "requested_types": [mt.value for mt in memory_types] if memory_types else [],
                    "subjects": subjects or [],
                    "keywords": keywords or [],
                })

            logger.debug(
                "å…ƒæ•°æ®è¿‡æ»¤ï¼šå€™é€‰=%d, è¿”å›=%d",
                len(all_memories_cache),
                len(result_ids),
            )

            for memory_id in result_ids[:20]:
                detail_entry = {
                    "memory_id": memory_id,
                    "status": "candidate",
                }
                details.append(detail_entry)
                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("metadata_stage", {})
                    stage_entry["status"] = "candidate"

            return StageResult(
                stage=RetrievalStage.METADATA_FILTERING,
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=0.0,
                details=details,
            )

        except Exception as e:
            logger.error(f"å…ƒæ•°æ®è¿‡æ»¤é˜¶æ®µå¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.METADATA_FILTERING,
                memory_ids=[],
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=0.0,
                details=[{"error": str(e)}],
            )

    async def _vector_search_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        vector_storage,
        candidate_ids: Set[str],
        all_memories_cache: Dict[str, MemoryChunk],
        *,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """é˜¶æ®µ2ï¼šå‘é‡æœç´¢"""
        start_time = time.time()

        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self._generate_query_embedding(query, context, vector_storage)

            if not query_embedding:
                logger.warning("å‘é‡æœç´¢é˜¶æ®µï¼šæŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥")
                return StageResult(
                    stage=RetrievalStage.VECTOR_SEARCH,
                    memory_ids=[],
                    processing_time=time.time() - start_time,
                    filtered_count=0,
                    score_threshold=self.config.vector_similarity_threshold,
                    details=[{"note": "query_embedding_unavailable"}],
                )

            # æ‰§è¡Œå‘é‡æœç´¢
            search_result = await vector_storage.search_similar_memories(
                query_vector=query_embedding,
                limit=self.config.vector_search_limit
            )

            if not search_result:
                logger.warning("å‘é‡æœç´¢é˜¶æ®µï¼šæœç´¢è¿”å›ç©ºç»“æœï¼Œå°è¯•å›é€€åˆ°æ–‡æœ¬åŒ¹é…")
                # å‘é‡æœç´¢å¤±è´¥æ—¶çš„å›é€€ç­–ç•¥
                return self._create_text_search_fallback(candidate_ids, all_memories_cache, query, start_time)

            candidate_pool = candidate_ids or set(all_memories_cache.keys())

            # è¿‡æ»¤å€™é€‰è®°å¿†
            filtered_memories = []
            details: List[Dict[str, Any]] = []
            raw_details: List[Dict[str, Any]] = []
            threshold = self.config.vector_similarity_threshold

            for memory_id, similarity in search_result:
                in_metadata_candidates = memory_id in candidate_pool
                above_threshold = similarity >= threshold
                if in_metadata_candidates and above_threshold:
                    filtered_memories.append((memory_id, similarity))

                raw_details.append({
                    "memory_id": memory_id,
                    "similarity": similarity,
                    "in_metadata": in_metadata_candidates,
                    "above_threshold": above_threshold,
                })

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            filtered_memories.sort(key=lambda x: x[1], reverse=True)
            result_ids = [memory_id for memory_id, _ in filtered_memories[:self.config.vector_search_limit]]
            kept_ids = set(result_ids)

            for entry in raw_details:
                memory_id = entry["memory_id"]
                similarity = entry["similarity"]
                in_metadata = entry["in_metadata"]
                above_threshold = entry["above_threshold"]

                status = "kept"
                reason = None
                if not in_metadata:
                    status = "excluded"
                    reason = "not_in_metadata_candidates"
                elif not above_threshold:
                    status = "excluded"
                    reason = "below_threshold"
                elif memory_id not in kept_ids:
                    status = "excluded"
                    reason = "limit_pruned"

                detail_entry = {
                    "memory_id": memory_id,
                    "similarity": round(similarity, 4),
                    "status": status,
                    "reason": reason,
                }
                details.append(detail_entry)

                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("vector_stage", {})
                    stage_entry["similarity"] = round(similarity, 4)
                    stage_entry["status"] = status
                    if reason:
                        stage_entry["reason"] = reason

            filtered_count = max(0, len(candidate_pool) - len(result_ids))

            logger.debug(f"å‘é‡æœç´¢ï¼š{len(candidate_ids)} -> {len(result_ids)} æ¡è®°å¿†")

            return StageResult(
                stage=RetrievalStage.VECTOR_SEARCH,
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=self.config.vector_similarity_threshold,
                details=details,
            )

        except Exception as e:
            logger.error(f"å‘é‡æœç´¢é˜¶æ®µå¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.VECTOR_SEARCH,
                memory_ids=[],
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=self.config.vector_similarity_threshold,
                details=[{"error": str(e)}],
            )

    def _create_text_search_fallback(
        self,
        candidate_ids: Set[str],
        all_memories_cache: Dict[str, MemoryChunk],
        query_text: str,
        start_time: float
    ) -> StageResult:
        """å½“å‘é‡æœç´¢å¤±è´¥æ—¶ï¼Œä½¿ç”¨æ–‡æœ¬æœç´¢ä½œä¸ºå›é€€ç­–ç•¥"""
        try:
            query_lower = query_text.lower()
            query_words = set(query_lower.split())

            text_matches = []
            for memory_id in candidate_ids:
                if memory_id not in all_memories_cache:
                    continue

                memory = all_memories_cache[memory_id]
                memory_text = (memory.display or memory.text_content or "").lower()

                # ç®€å•çš„æ–‡æœ¬åŒ¹é…è¯„åˆ†
                word_matches = sum(1 for word in query_words if word in memory_text)
                if word_matches > 0:
                    score = word_matches / len(query_words)
                    text_matches.append((memory_id, score))

            # æŒ‰åŒ¹é…åº¦æ’åº
            text_matches.sort(key=lambda x: x[1], reverse=True)
            result_ids = [memory_id for memory_id, _ in text_matches[:self.config.vector_search_limit]]

            details = []
            for memory_id, score in text_matches[:self.config.vector_search_limit]:
                details.append({
                    "memory_id": memory_id,
                    "text_match_score": round(score, 4),
                    "status": "text_match_fallback"
                })

            logger.debug(f"å‘é‡æœç´¢å›é€€åˆ°æ–‡æœ¬åŒ¹é…ï¼šæ‰¾åˆ° {len(result_ids)} æ¡åŒ¹é…è®°å¿†")

            return StageResult(
                stage=RetrievalStage.VECTOR_SEARCH,
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=len(candidate_ids) - len(result_ids),
                score_threshold=0.0,  # æ–‡æœ¬åŒ¹é…æ— ä¸¥æ ¼é˜ˆå€¼
                details=details
            )

        except Exception as e:
            logger.error(f"æ–‡æœ¬æœç´¢å›é€€å¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.VECTOR_SEARCH,
                memory_ids=list(candidate_ids)[:self.config.vector_search_limit],
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=0.0,
                details=[{"error": str(e), "note": "text_fallback_failed"}]
            )

    async def _semantic_reranking_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        candidate_ids: Set[str],
        all_memories_cache: Dict[str, MemoryChunk],
        *,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """é˜¶æ®µ3ï¼šè¯­ä¹‰é‡æ’åº"""
        start_time = time.time()

        try:
            reranked_memories = []
            details: List[Dict[str, Any]] = []
            threshold = self.config.semantic_similarity_threshold

            for memory_id in candidate_ids:
                if memory_id not in all_memories_cache:
                    continue

                memory = all_memories_cache[memory_id]

                # è®¡ç®—ç»¼åˆè¯­ä¹‰ç›¸ä¼¼åº¦
                semantic_score = await self._calculate_semantic_similarity(query, memory, context)

                if semantic_score >= threshold:
                    reranked_memories.append((memory_id, semantic_score))

                status = "kept" if semantic_score >= threshold else "excluded"
                reason = None if status == "kept" else "below_threshold"

                detail_entry = {
                    "memory_id": memory_id,
                    "score": round(semantic_score, 4),
                    "status": status,
                    "reason": reason,
                }
                details.append(detail_entry)

                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("semantic_stage", {})
                    stage_entry["score"] = round(semantic_score, 4)
                    stage_entry["status"] = status
                    if reason:
                        stage_entry["reason"] = reason

            # æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº
            reranked_memories.sort(key=lambda x: x[1], reverse=True)
            result_ids = [memory_id for memory_id, _ in reranked_memories[:self.config.semantic_rerank_limit]]
            kept_ids = set(result_ids)

            filtered_count = len(candidate_ids) - len(result_ids)

            for detail in details:
                if detail["status"] == "kept" and detail["memory_id"] not in kept_ids:
                    detail["status"] = "excluded"
                    detail["reason"] = "limit_pruned"
                    if debug_log is not None:
                        stage_entry = debug_log.setdefault(detail["memory_id"], {}).setdefault("semantic_stage", {})
                        stage_entry["status"] = "excluded"
                        stage_entry["reason"] = "limit_pruned"

            logger.debug(f"è¯­ä¹‰é‡æ’åºï¼š{len(candidate_ids)} -> {len(result_ids)} æ¡è®°å¿†")

            return StageResult(
                stage=RetrievalStage.SEMANTIC_RERANKING,
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=self.config.semantic_similarity_threshold,
                details=details,
            )

        except Exception as e:
            logger.error(f"è¯­ä¹‰é‡æ’åºé˜¶æ®µå¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.SEMANTIC_RERANKING,
                memory_ids=list(candidate_ids),  # å¤±è´¥æ—¶è¿”å›åŸå€™é€‰é›†
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=self.config.semantic_similarity_threshold,
                details=[{"error": str(e)}],
            )

    async def _contextual_filtering_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        candidate_ids: List[str],
        all_memories_cache: Dict[str, MemoryChunk],
        limit: int,
        *,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """é˜¶æ®µ4ï¼šä¸Šä¸‹æ–‡è¿‡æ»¤"""
        start_time = time.time()

        try:
            final_memories = []
            details: List[Dict[str, Any]] = []

            for memory_id in candidate_ids:
                if memory_id not in all_memories_cache:
                    continue

                memory = all_memories_cache[memory_id]

                # è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³åº¦è¯„åˆ†
                context_score = await self._calculate_context_relevance(query, memory, context)

                # ç»“åˆå¤šå› å­è¯„åˆ†
                final_score = await self._calculate_final_score(query, memory, context, context_score)

                final_memories.append((memory_id, final_score))

                detail_entry = {
                    "memory_id": memory_id,
                    "context_score": round(context_score, 4),
                    "final_score": round(final_score, 4),
                    "status": "candidate",
                }
                details.append(detail_entry)

                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("context_stage", {})
                    stage_entry["context_score"] = round(context_score, 4)
                    stage_entry["final_score"] = round(final_score, 4)

            # æŒ‰æœ€ç»ˆè¯„åˆ†æ’åº
            final_memories.sort(key=lambda x: x[1], reverse=True)
            result_ids = [memory_id for memory_id, _ in final_memories[:limit]]
            kept_ids = set(result_ids)

            for detail in details:
                memory_id = detail["memory_id"]
                if memory_id in kept_ids:
                    detail["status"] = "final"
                    if debug_log is not None:
                        stage_entry = debug_log.setdefault(memory_id, {}).setdefault("context_stage", {})
                        stage_entry["status"] = "final"
                else:
                    detail["status"] = "excluded"
                    detail["reason"] = "ranked_out"
                    if debug_log is not None:
                        stage_entry = debug_log.setdefault(memory_id, {}).setdefault("context_stage", {})
                        stage_entry["status"] = "excluded"
                        stage_entry["reason"] = "ranked_out"

            filtered_count = len(candidate_ids) - len(result_ids)

            logger.debug(f"ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼š{len(candidate_ids)} -> {len(result_ids)} æ¡è®°å¿†")

            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=0.0,  # åŠ¨æ€é˜ˆå€¼
                details=details,
            )

        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡è¿‡æ»¤é˜¶æ®µå¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,
                memory_ids=candidate_ids[:limit],  # å¤±è´¥æ—¶è¿”å›å‰limitä¸ª
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=0.0,
                details=[{"error": str(e)}],
            )

    async def _fallback_retrieval_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        all_memories_cache: Dict[str, MemoryChunk],
        limit: int,
        *,
        excluded_ids: Optional[Set[str]] = None,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """å›é€€æ£€ç´¢é˜¶æ®µ - å½“ä¸»æ£€ç´¢å¤±è´¥æ—¶ä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥"""
        start_time = time.time()

        try:
            excluded_ids = excluded_ids or set()
            fallback_candidates = []

            # ç­–ç•¥1ï¼šåŸºäºå…³é”®è¯çš„ç®€å•åŒ¹é…
            query_lower = query.lower()
            query_words = set(query_lower.split())

            for memory_id, memory in all_memories_cache.items():
                if memory_id in excluded_ids:
                    continue

                memory_text = (memory.display or memory.text_content or "").lower()

                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                word_matches = sum(1 for word in query_words if word in memory_text)
                if word_matches > 0:
                    score = word_matches / len(query_words)
                    fallback_candidates.append((memory_id, score))

            # ç­–ç•¥2ï¼šå¦‚æœæ²¡æœ‰å…³é”®è¯åŒ¹é…ï¼Œä½¿ç”¨æ—¶åºæœ€è¿‘çš„åŸåˆ™
            if not fallback_candidates:
                logger.debug("å…³é”®è¯åŒ¹é…æ— ç»“æœï¼Œä½¿ç”¨æ—¶åºæœ€è¿‘ç­–ç•¥")
                recent_memories = sorted(
                    [(mid, mem.metadata.last_accessed or mem.metadata.created_at)
                     for mid, mem in all_memories_cache.items()
                     if mid not in excluded_ids],
                    key=lambda x: x[1],
                    reverse=True
                )
                fallback_candidates = [(mid, 0.5) for mid, _ in recent_memories[:limit*2]]

            # æŒ‰åˆ†æ•°æ’åº
            fallback_candidates.sort(key=lambda x: x[1], reverse=True)
            result_ids = [memory_id for memory_id, _ in fallback_candidates[:limit]]

            # è®°å½•è°ƒè¯•ä¿¡æ¯
            details = []
            for memory_id, score in fallback_candidates[:limit]:
                detail_entry = {
                    "memory_id": memory_id,
                    "fallback_score": round(score, 4),
                    "status": "fallback_candidate",
                }
                details.append(detail_entry)

                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("fallback_stage", {})
                    stage_entry["score"] = round(score, 4)
                    stage_entry["status"] = "fallback_candidate"

            filtered_count = len(all_memories_cache) - len(result_ids)

            logger.debug(f"å›é€€æ£€ç´¢å®Œæˆï¼šè¿”å› {len(result_ids)} æ¡è®°å¿†")

            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,  # å¤ç”¨ç°æœ‰æšä¸¾
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=0.0,  # å›é€€æœºåˆ¶æ— é˜ˆå€¼
                details=details,
            )

        except Exception as e:
            logger.error(f"å›é€€æ£€ç´¢é˜¶æ®µå¤±è´¥: {e}")
            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,
                memory_ids=[],
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=0.0,
                details=[{"error": str(e)}],
            )

    async def _generate_query_embedding(self, query: str, context: Dict[str, Any], vector_storage) -> Optional[List[float]]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡"""
        try:
            query_plan = context.get("query_plan")
            query_text = query
            if query_plan and getattr(query_plan, "semantic_query", None):
                query_text = query_plan.semantic_query

            if not query_text:
                logger.debug("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡")
                return None

            if not hasattr(vector_storage, "generate_query_embedding"):
                logger.warning("å‘é‡å­˜å‚¨å¯¹è±¡ç¼ºå°‘ generate_query_embedding æ–¹æ³•")
                return None

            logger.debug(f"æ­£åœ¨ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œæ–‡æœ¬: '{query_text[:100]}'")
            embedding = await vector_storage.generate_query_embedding(query_text)
            
            if embedding is None:
                logger.warning("å‘é‡å­˜å‚¨è¿”å›ç©ºçš„æŸ¥è¯¢å‘é‡")
                return None
                
            if len(embedding) == 0:
                logger.warning("å‘é‡å­˜å‚¨è¿”å›ç©ºåˆ—è¡¨ä½œä¸ºæŸ¥è¯¢å‘é‡")
                return None
                
            logger.debug(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
            return embedding

        except Exception as e:
            logger.error(f"ç”ŸæˆæŸ¥è¯¢å‘é‡æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return None

    async def _calculate_semantic_similarity(self, query: str, memory: MemoryChunk, context: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - ç®€åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæå‡å¬å›ç‡"""
        try:
            query_plan = context.get("query_plan")
            query_text = query
            if query_plan and getattr(query_plan, "semantic_query", None):
                query_text = query_plan.semantic_query

            # é¢„å¤„ç†ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
            memory_text = (memory.display or memory.text_content or "").strip()
            query_text = query_text.strip()

            if not query_text or not memory_text:
                return 0.0

            # åˆ›å»ºå°å†™ç‰ˆæœ¬ç”¨äºåŒ¹é…
            query_lower = query_text.lower()
            memory_lower = memory_text.lower()

            # æ ¸å¿ƒåŒ¹é…ç­–ç•¥1ï¼šç²¾ç¡®å­ä¸²åŒ¹é…ï¼ˆæœ€é‡è¦ï¼‰
            exact_score = 0.0
            if query_text in memory_text:
                exact_score = 1.0
            elif query_lower in memory_lower:
                exact_score = 0.9
            elif any(word in memory_lower for word in query_lower.split() if len(word) > 1):
                exact_score = 0.4

            # æ ¸å¿ƒåŒ¹é…ç­–ç•¥2ï¼šè¯æ±‡åŒ¹é…
            word_score = 0.0
            try:
                import jieba
                import re

                # åˆ†è¯å¤„ç†
                query_words = list(jieba.cut(query_text)) + re.findall(r'[a-zA-Z]+', query_text)
                memory_words = list(jieba.cut(memory_text)) + re.findall(r'[a-zA-Z]+', memory_text)

                # æ¸…ç†å’Œæ ‡å‡†åŒ–
                query_words = [w.strip().lower() for w in query_words if w.strip() and len(w.strip()) > 1]
                memory_words = [w.strip().lower() for w in memory_words if w.strip() and len(w.strip()) > 1]

                if query_words and memory_words:
                    query_set = set(query_words)
                    memory_set = set(memory_words)

                    # ç²¾ç¡®åŒ¹é…
                    exact_matches = query_set & memory_set
                    exact_ratio = len(exact_matches) / len(query_set) if query_set else 0

                    # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
                    partial_matches = 0
                    for q_word in query_set:
                        if any(q_word in m_word or m_word in q_word for m_word in memory_set if len(q_word) >= 2):
                            partial_matches += 1

                    partial_ratio = partial_matches / len(query_set) if query_set else 0
                    word_score = exact_ratio * 0.8 + partial_ratio * 0.3

            except ImportError:
                # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
                import re
                query_words = re.findall(r'[\w\u4e00-\u9fa5]+', query_lower)
                memory_words = re.findall(r'[\w\u4e00-\u9fa5]+', memory_lower)

                if query_words and memory_words:
                    query_set = set(w for w in query_words if len(w) > 1)
                    memory_set = set(w for w in memory_words if len(w) > 1)

                    if query_set:
                        intersection = query_set & memory_set
                        word_score = len(intersection) / len(query_set)

            # æ ¸å¿ƒåŒ¹é…ç­–ç•¥3ï¼šè¯­ä¹‰æ¦‚å¿µåŒ¹é…
            concept_score = 0.0
            concept_groups = {
                "é¥®é£Ÿ": ["åƒ", "é¥­", "èœ", "é¤", "é¥¿", "é¥±", "é£Ÿ", "dinner", "eat", "food", "meal"],
                "å¤©æ°”": ["å¤©æ°”", "é˜³å…‰", "é›¨", "æ™´", "é˜´", "æ¸©åº¦", "weather", "sunny", "rain"],
                "ç¼–ç¨‹": ["ç¼–ç¨‹", "ä»£ç ", "ç¨‹åº", "å¼€å‘", "è¯­è¨€", "programming", "code", "develop", "python"],
                "æ—¶é—´": ["ä»Šå¤©", "æ˜¨å¤©", "æ˜å¤©", "ç°åœ¨", "æ—¶é—´", "today", "yesterday", "tomorrow", "time"],
                "æƒ…æ„Ÿ": ["å¥½", "å", "å¼€å¿ƒ", "éš¾è¿‡", "æœ‰è¶£", "good", "bad", "happy", "sad", "fun"]
            }

            query_concepts = {concept for concept, keywords in concept_groups.items()
                            if any(keyword in query_lower for keyword in keywords)}
            memory_concepts = {concept for concept, keywords in concept_groups.items()
                             if any(keyword in memory_lower for keyword in keywords)}

            if query_concepts and memory_concepts:
                concept_overlap = query_concepts & memory_concepts
                concept_score = len(concept_overlap) / len(query_concepts) * 0.5

            # æ ¸å¿ƒåŒ¹é…ç­–ç•¥4ï¼šæŸ¥è¯¢è®¡åˆ’å¢å¼º
            plan_bonus = 0.0
            if query_plan:
                # ä¸»ä½“åŒ¹é…
                if hasattr(query_plan, 'subjects') and query_plan.subjects:
                    for subject in query_plan.subjects:
                        if subject.lower() in memory_lower:
                            plan_bonus += 0.15

                # å¯¹è±¡åŒ¹é…
                if hasattr(query_plan, 'objects') and query_plan.objects:
                    for obj in query_plan.objects:
                        if obj.lower() in memory_lower:
                            plan_bonus += 0.1

                # è®°å¿†ç±»å‹åŒ¹é…
                if hasattr(query_plan, 'memory_types') and query_plan.memory_types:
                    if memory.memory_type in query_plan.memory_types:
                        plan_bonus += 0.1

            # ç»¼åˆè¯„åˆ†è®¡ç®— - ç®€åŒ–æƒé‡åˆ†é…
            if exact_score >= 0.9:
                # ç²¾ç¡®åŒ¹é…ä¸ºä¸»
                final_score = exact_score * 0.6 + word_score * 0.2 + concept_score + plan_bonus
            else:
                # ç»¼åˆè¯„åˆ†
                final_score = exact_score * 0.3 + word_score * 0.3 + concept_score + plan_bonus

            # åŸºç¡€åˆ†æ•°ä¿éšœï¼šé¿å…è¿‡ä½åˆ†æ•°
            if final_score > 0:
                if exact_score > 0 or word_score > 0.1:
                    final_score = max(final_score, 0.1)  # æœ‰å®é™…åŒ¹é…çš„æœ€å°åˆ†æ•°
                else:
                    final_score = max(final_score, 0.05)  # ä»…æ¦‚å¿µåŒ¹é…çš„æœ€å°åˆ†æ•°

            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
            final_score = min(1.0, max(0.0, final_score))

            return final_score

        except Exception as e:
            logger.warning(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    async def _calculate_context_relevance(self, query: str, memory: MemoryChunk, context: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³åº¦"""
        try:
            score = 0.0

            query_plan = context.get("query_plan")

            # æ£€æŸ¥è®°å¿†ç±»å‹æ˜¯å¦åŒ¹é…ä¸Šä¸‹æ–‡
            if context.get("expected_memory_types"):
                if memory.memory_type in context["expected_memory_types"]:
                    score += 0.3
            elif query_plan and getattr(query_plan, "memory_types", None):
                if memory.memory_type in query_plan.memory_types:
                    score += 0.3

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            if context.get("keywords"):
                memory_keywords = set(memory.keywords)
                context_keywords = set(context["keywords"])
                overlap = memory_keywords & context_keywords
                if overlap:
                    score += len(overlap) / max(len(context_keywords), 1) * 0.4

            if query_plan:
                # ä¸»ä½“åŒ¹é…
                subject_score = self._calculate_subject_overlap(memory, getattr(query_plan, "subject_includes", []))
                score += subject_score * 0.3

                # å¯¹è±¡/æè¿°åŒ¹é…
                object_keywords = getattr(query_plan, "object_includes", []) or []
                if object_keywords:
                    display_text = (memory.display or memory.text_content or "").lower()
                    hits = sum(1 for kw in object_keywords if isinstance(kw, str) and kw.strip() and kw.strip().lower() in display_text)
                    if hits:
                        score += min(0.3, hits * 0.1)

                optional_keywords = getattr(query_plan, "optional_keywords", []) or []
                if optional_keywords:
                    display_text = (memory.display or memory.text_content or "").lower()
                    hits = sum(1 for kw in optional_keywords if isinstance(kw, str) and kw.strip() and kw.strip().lower() in display_text)
                    if hits:
                        score += min(0.2, hits * 0.05)

                # æ—¶é—´åå¥½
                recency_pref = getattr(query_plan, "recency_preference", "")
                if recency_pref:
                    memory_age = time.time() - memory.metadata.created_at
                    if recency_pref == "recent" and memory_age < 7 * 24 * 3600:
                        score += 0.2
                    elif recency_pref == "historical" and memory_age > 30 * 24 * 3600:
                        score += 0.1

            # æ£€æŸ¥æ—¶æ•ˆæ€§
            if context.get("recent_only", False):
                memory_age = time.time() - memory.metadata.created_at
                if memory_age < 7 * 24 * 3600:  # 7å¤©å†…
                    score += 0.3

            return min(score, 1.0)

        except Exception as e:
            logger.warning(f"è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³åº¦å¤±è´¥: {e}")
            return 0.0

    async def _calculate_final_score(self, query: str, memory: MemoryChunk, context: Dict[str, Any], context_score: float) -> float:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†"""
        try:
            query_plan = context.get("query_plan")

            # è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_score = await self._calculate_semantic_similarity(query, memory, context)

            # å‘é‡ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
            vector_score = 0.0
            if memory.embedding:
                # è¿™é‡Œåº”è¯¥æœ‰å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼Œç®€åŒ–å¤„ç†
                vector_score = 0.5

            # æ—¶æ•ˆæ€§è¯„åˆ†
            recency_score = self._calculate_recency_score(memory.metadata.created_at)
            if query_plan:
                recency_pref = getattr(query_plan, "recency_preference", "")
                if recency_pref == "recent":
                    recency_score = max(recency_score, 0.8)
                elif recency_pref == "historical":
                    recency_score = min(recency_score, 0.5)

            # æƒé‡ç»„åˆ
            vector_weight = self.config.vector_weight
            semantic_weight = self.config.semantic_weight
            context_weight = self.config.context_weight
            recency_weight = self.config.recency_weight

            if query_plan and getattr(query_plan, "emphasis", None) == "precision":
                semantic_weight += 0.05
            elif query_plan and getattr(query_plan, "emphasis", None) == "recall":
                context_weight += 0.05

            final_score = (
                semantic_score * semantic_weight +
                vector_score * vector_weight +
                context_score * context_weight +
                recency_score * recency_weight
            )

            # åŠ å…¥è®°å¿†é‡è¦æ€§æƒé‡
            importance_weight = memory.metadata.importance.value / 4.0  # æ ‡å‡†åŒ–åˆ°0-1
            final_score = final_score * (0.7 + importance_weight * 0.3)  # é‡è¦æ€§å½±å“30%

            return final_score

        except Exception as e:
            logger.warning(f"è®¡ç®—æœ€ç»ˆè¯„åˆ†å¤±è´¥: {e}")
            return 0.0

    def _calculate_subject_overlap(self, memory: MemoryChunk, required_subjects: Optional[List[str]]) -> float:
        if not required_subjects:
            return 0.0

        memory_subjects = {subject.lower() for subject in memory.subjects if isinstance(subject, str)}
        if not memory_subjects:
            return 0.0

        hit = 0
        total = 0
        for subject in required_subjects:
            if not isinstance(subject, str):
                continue
            total += 1
            normalized = subject.strip().lower()
            if not normalized:
                continue
            if any(normalized in mem_subject for mem_subject in memory_subjects):
                hit += 1

        if total == 0:
            return 0.0

        return hit / total

    def _calculate_recency_score(self, timestamp: float) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§è¯„åˆ†"""
        try:
            age = time.time() - timestamp
            age_days = age / (24 * 3600)

            if age_days < 1:
                return 1.0
            elif age_days < 7:
                return 0.8
            elif age_days < 30:
                return 0.6
            elif age_days < 90:
                return 0.4
            else:
                return 0.2

        except Exception:
            return 0.5

    def _extract_memory_types_from_context(self, context: Dict[str, Any]) -> List[MemoryType]:
        """ä»ä¸Šä¸‹æ–‡ä¸­æå–è®°å¿†ç±»å‹"""
        try:
            query_plan = context.get("query_plan")
            if query_plan and getattr(query_plan, "memory_types", None):
                return query_plan.memory_types

            if "expected_memory_types" in context:
                return context["expected_memory_types"]

            # æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­è®°å¿†ç±»å‹
            if "message_type" in context:
                message_type = context["message_type"]
                if message_type in ["personal_info", "fact"]:
                    return [MemoryType.PERSONAL_FACT]
                elif message_type in ["event", "activity"]:
                    return [MemoryType.EVENT]
                elif message_type in ["preference", "like"]:
                    return [MemoryType.PREFERENCE]
                elif message_type in ["opinion", "view"]:
                    return [MemoryType.OPINION]

            return []

        except Exception:
            return []

    def _extract_keywords_from_query(self, query: str, query_plan: Optional[Any] = None) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        try:
            extracted: List[str] = []

            if query_plan and getattr(query_plan, "required_keywords", None):
                extracted.extend([kw.lower() for kw in query_plan.required_keywords if isinstance(kw, str)])

            # ç®€å•çš„å…³é”®è¯æå–
            words = query.lower().split()
            # è¿‡æ»¤åœç”¨è¯
            stopwords = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "è¿™", "é‚£", "äº†", "å—", "å‘¢"}
            extracted.extend(word for word in words if len(word) > 1 and word not in stopwords)

            # å»é‡å¹¶ä¿ç•™é¡ºåº
            seen = set()
            deduplicated = []
            for word in extracted:
                if word in seen or not word:
                    continue
                seen.add(word)
                deduplicated.append(word)

            return deduplicated[:10]
        except Exception:
            return []

    def _update_retrieval_stats(self, total_time: float, stage_results: List[StageResult]):
        """æ›´æ–°æ£€ç´¢ç»Ÿè®¡"""
        self.retrieval_stats["total_queries"] += 1

        # æ›´æ–°å¹³å‡æ£€ç´¢æ—¶é—´
        current_avg = self.retrieval_stats["average_retrieval_time"]
        total_queries = self.retrieval_stats["total_queries"]
        new_avg = (current_avg * (total_queries - 1) + total_time) / total_queries
        self.retrieval_stats["average_retrieval_time"] = new_avg

        # æ›´æ–°å„é˜¶æ®µç»Ÿè®¡
        for result in stage_results:
            stage_name = result.stage.value
            if stage_name in self.retrieval_stats["stage_stats"]:
                stage_stat = self.retrieval_stats["stage_stats"][stage_name]
                stage_stat["calls"] += 1

                current_stage_avg = stage_stat["avg_time"]
                new_stage_avg = (current_stage_avg * (stage_stat["calls"] - 1) + result.processing_time) / stage_stat["calls"]
                stage_stat["avg_time"] = new_stage_avg

    def get_retrieval_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return self.retrieval_stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.retrieval_stats = {
            "total_queries": 0,
            "average_retrieval_time": 0.0,
            "stage_stats": {
                "metadata_filtering": {"calls": 0, "avg_time": 0.0},
                "vector_search": {"calls": 0, "avg_time": 0.0},
                "semantic_reranking": {"calls": 0, "avg_time": 0.0},
                "contextual_filtering": {"calls": 0, "avg_time": 0.0},
                "enhanced_reranking": {"calls": 0, "avg_time": 0.0}
            }
        }

    async def _enhanced_reranking_stage(
        self,
        query: str,
        user_id: str,
        context: Dict[str, Any],
        candidate_ids: List[str],
        all_memories_cache: Dict[str, MemoryChunk],
        limit: int,
        *,
        debug_log: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> StageResult:
        """é˜¶æ®µ5ï¼šå¢å¼ºé‡æ’åº - ä½¿ç”¨å¤šç»´åº¦è¯„åˆ†æ¨¡å‹"""
        start_time = time.time()

        try:
            if not candidate_ids:
                return StageResult(
                    stage=RetrievalStage.CONTEXTUAL_FILTERING,  # ä¿æŒä¸åŸæœ‰æšä¸¾å…¼å®¹
                    memory_ids=[],
                    processing_time=time.time() - start_time,
                    filtered_count=0,
                    score_threshold=0.0,
                    details=[{"note": "no_candidates"}],
                )

            # å‡†å¤‡å€™é€‰è®°å¿†æ•°æ®
            candidate_memories = []
            for memory_id in candidate_ids:
                memory = all_memories_cache.get(memory_id)
                if memory:
                    # ä½¿ç”¨åŸå§‹å‘é‡ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€åˆ†æ•°
                    vector_similarity = 0.8  # é»˜è®¤åˆ†æ•°ï¼Œå®é™…åº”è¯¥ä»å‰é¢é˜¶æ®µä¼ é€’
                    candidate_memories.append((memory_id, memory, vector_similarity))

            if not candidate_memories:
                return StageResult(
                    stage=RetrievalStage.CONTEXTUAL_FILTERING,
                    memory_ids=[],
                    processing_time=time.time() - start_time,
                    filtered_count=len(candidate_ids),
                    score_threshold=0.0,
                    details=[{"note": "candidates_not_found_in_cache"}],
                )

            # ä½¿ç”¨å¢å¼ºé‡æ’åºå™¨
            reranked_memories = self.reranker.rerank_memories(
                query=query,
                candidate_memories=candidate_memories,
                context=context,
                limit=limit
            )

            # æå–é‡æ’åºåçš„è®°å¿†ID
            result_ids = [memory_id for memory_id, _, _ in reranked_memories]
            
            # ç”Ÿæˆè°ƒè¯•è¯¦æƒ…
            details = []
            for memory_id, memory, final_score in reranked_memories:
                detail_entry = {
                    "memory_id": memory_id,
                    "final_score": round(final_score, 4),
                    "status": "reranked",
                    "memory_type": memory.memory_type.value,
                    "access_count": memory.metadata.access_count,
                }
                details.append(detail_entry)
                
                if debug_log is not None:
                    stage_entry = debug_log.setdefault(memory_id, {}).setdefault("enhanced_rerank_stage", {})
                    stage_entry["final_score"] = round(final_score, 4)
                    stage_entry["status"] = "reranked"
                    stage_entry["rank"] = len(details)

            # è®°å½•è¢«è¿‡æ»¤çš„è®°å¿†
            kept_ids = set(result_ids)
            for memory_id in candidate_ids:
                if memory_id not in kept_ids:
                    detail_entry = {
                        "memory_id": memory_id,
                        "status": "filtered_out",
                        "reason": "ranked_below_limit"
                    }
                    details.append(detail_entry)
                    
                    if debug_log is not None:
                        stage_entry = debug_log.setdefault(memory_id, {}).setdefault("enhanced_rerank_stage", {})
                        stage_entry["status"] = "filtered_out"
                        stage_entry["reason"] = "ranked_below_limit"

            filtered_count = len(candidate_ids) - len(result_ids)

            logger.debug(
                f"å¢å¼ºé‡æ’åºå®Œæˆï¼šå€™é€‰={len(candidate_ids)}, è¿”å›={len(result_ids)}, "
                f"è¿‡æ»¤={filtered_count}"
            )

            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,  # ä¿æŒä¸åŸæœ‰æšä¸¾å…¼å®¹
                memory_ids=result_ids,
                processing_time=time.time() - start_time,
                filtered_count=filtered_count,
                score_threshold=0.0,  # åŠ¨æ€é˜ˆå€¼ï¼Œç”±é‡æ’åºå™¨å†³å®š
                details=details,
            )

        except Exception as e:
            logger.error(f"å¢å¼ºé‡æ’åºé˜¶æ®µå¤±è´¥: {e}", exc_info=True)
            return StageResult(
                stage=RetrievalStage.CONTEXTUAL_FILTERING,
                memory_ids=candidate_ids[:limit],  # å¤±è´¥æ—¶è¿”å›å‰limitä¸ª
                processing_time=time.time() - start_time,
                filtered_count=0,
                score_threshold=0.0,
                details=[{"error": str(e)}],
            )