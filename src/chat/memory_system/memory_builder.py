"""
è®°å¿†æ„å»ºæ¨¡å—
ä»å¯¹è¯æµä¸­æå–é«˜è´¨é‡ã€ç»“æ„åŒ–è®°å¿†å•å…ƒ
è¾“å‡ºæ ¼å¼è¦æ±‚:
{
    "memories": [
        {
            "type": "è®°å¿†ç±»å‹",
            "display": "ä¸€å¥ä¼˜é›…è‡ªç„¶çš„ä¸­æ–‡æè¿°ï¼Œç”¨äºç›´æ¥å±•ç¤ºåŠæç¤ºè¯æ‹¼æ¥",
            "subject": ["ä¸»ä½“1", "ä¸»ä½“2"],
            "predicate": "è°“è¯­(åŠ¨ä½œ/çŠ¶æ€)",
            "object": "å®¾è¯­(å¯¹è±¡/å±æ€§æˆ–ç»“æ„ä½“)",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "importance": "é‡è¦æ€§ç­‰çº§(1-4)",
            "confidence": "ç½®ä¿¡åº¦(1-4)",
            "reasoning": "æå–ç†ç”±"
        }
    ]
}

æ³¨æ„ï¼š
1. `subject` å¯åŒ…å«å¤šä¸ªä¸»ä½“ï¼Œè¯·ç”¨æ•°ç»„è¡¨ç¤ºï¼›è‹¥ä¸»ä½“ä¸æ˜ç¡®ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡ç»™å‡ºæœ€åˆç†çš„ç§°å‘¼
2. `display` å­—æ®µå¿…å¡«ï¼Œå¿…é¡»æ˜¯å®Œæ•´é¡ºç•…çš„è‡ªç„¶è¯­è¨€ï¼Œç¦æ­¢ä¾èµ–å­—ç¬¦ä¸²æ‹¼æ¥
3. ä¸»è°“å®¾ç”¨äºç´¢å¼•å’Œæ£€ç´¢ç»“æ„åŒ–ä¿¡æ¯ï¼Œæç¤ºè¯æ„å»ºä»…ä½¿ç”¨ `display`
4. åªæå–ç¡®å®å€¼å¾—è®°å¿†çš„ä¿¡æ¯ï¼Œä¸è¦æå–çç¢å†…å®¹
5. ç¡®ä¿ä¿¡æ¯å‡†ç¡®ã€å…·ä½“ã€æœ‰ä»·å€¼
6. é‡è¦æ€§: 1=ä½, 2=ä¸€èˆ¬, 3=é«˜, 4=å…³é”®ï¼›ç½®ä¿¡åº¦: 1=ä½, 2=ä¸­ç­‰, 3=é«˜, 4=å·²éªŒè¯
"""

import re
import time
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, TypeVar

E = TypeVar("E", bound=Enum)


import orjson

from src.chat.memory_system.memory_chunk import (
    ConfidenceLevel,
    ImportanceLevel,
    MemoryChunk,
    MemoryType,
    create_memory_chunk,
)
from src.common.logger import get_logger
from src.llm_models.utils_model import LLMRequest

logger = get_logger(__name__)


CHINESE_TO_MEMORY_TYPE: dict[str, MemoryType] = {
    "ä¸ªäººäº‹å®": MemoryType.PERSONAL_FACT,
    "äº‹ä»¶": MemoryType.EVENT,
    "åå¥½": MemoryType.PREFERENCE,
    "è§‚ç‚¹": MemoryType.OPINION,
    "å…³ç³»": MemoryType.RELATIONSHIP,
    "æƒ…æ„Ÿ": MemoryType.EMOTION,
    "çŸ¥è¯†": MemoryType.KNOWLEDGE,
    "æŠ€èƒ½": MemoryType.SKILL,
    "ç›®æ ‡": MemoryType.GOAL,
    "ç»éªŒ": MemoryType.EXPERIENCE,
    "ä¸Šä¸‹æ–‡": MemoryType.CONTEXTUAL,
}


class ExtractionStrategy(Enum):
    """æå–ç­–ç•¥"""

    LLM_BASED = "llm_based"  # åŸºäºLLMçš„æ™ºèƒ½æå–
    RULE_BASED = "rule_based"  # åŸºäºè§„åˆ™çš„æå–
    HYBRID = "hybrid"  # æ··åˆç­–ç•¥


@dataclass
class ExtractionResult:
    """æå–ç»“æœ"""

    memories: list[MemoryChunk]
    confidence_scores: list[float]
    extraction_time: float
    strategy_used: ExtractionStrategy


class MemoryExtractionError(Exception):
    """è®°å¿†æå–è¿‡ç¨‹ä¸­å‘ç”Ÿçš„ä¸å¯æ¢å¤é”™è¯¯"""


class MemoryBuilder:
    """è®°å¿†æ„å»ºå™¨"""

    def __init__(self, llm_model: LLMRequest):
        self.llm_model = llm_model
        self.extraction_stats = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "failed_extractions": 0,
            "average_confidence": 0.0,
        }

    async def build_memories(
        self, conversation_text: str, context: dict[str, Any], user_id: str, timestamp: float
    ) -> list[MemoryChunk]:
        """ä»å¯¹è¯ä¸­æ„å»ºè®°å¿†"""
        start_time = time.time()

        try:
            logger.debug(f"å¼€å§‹ä»å¯¹è¯æ„å»ºè®°å¿†ï¼Œæ–‡æœ¬é•¿åº¦: {len(conversation_text)}")

            # ä½¿ç”¨LLMæå–è®°å¿†
            memories = await self._extract_with_llm(conversation_text, context, user_id, timestamp)

            # åå¤„ç†å’ŒéªŒè¯
            validated_memories = self._validate_and_enhance_memories(memories, context)

            # æ›´æ–°ç»Ÿè®¡
            extraction_time = time.time() - start_time
            self._update_extraction_stats(len(validated_memories), extraction_time)

            logger.info(f"âœ… æˆåŠŸæ„å»º {len(validated_memories)} æ¡è®°å¿†ï¼Œè€—æ—¶ {extraction_time:.2f}ç§’")
            return validated_memories

        except MemoryExtractionError as e:
            logger.error(f"âŒ è®°å¿†æ„å»ºå¤±è´¥ï¼ˆå“åº”è§£æé”™è¯¯ï¼‰: {e}")
            self.extraction_stats["failed_extractions"] += 1
            raise
        except Exception as e:
            logger.error(f"âŒ è®°å¿†æ„å»ºå¤±è´¥: {e}", exc_info=True)
            self.extraction_stats["failed_extractions"] += 1
            raise

    async def _extract_with_llm(
        self, text: str, context: dict[str, Any], user_id: str, timestamp: float
    ) -> list[MemoryChunk]:
        """ä½¿ç”¨LLMæå–è®°å¿†"""
        try:
            prompt = self._build_llm_extraction_prompt(text, context)

            response, _ = await self.llm_model.generate_response_async(prompt, temperature=0.3)

            # è§£æLLMå“åº”
            memories = self._parse_llm_response(response, user_id, timestamp, context)

            return memories

        except MemoryExtractionError:
            raise
        except Exception as e:
            logger.error(f"LLMæå–å¤±è´¥: {e}")
            raise MemoryExtractionError(str(e)) from e

    def _build_llm_extraction_prompt(self, text: str, context: dict[str, Any]) -> str:
        """æ„å»ºLLMæå–æç¤º"""
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        message_type = context.get("message_type", "normal")

        bot_name = context.get("bot_name")
        bot_identity = context.get("bot_identity")
        bot_personality = context.get("bot_personality")
        bot_personality_side = context.get("bot_personality_side")
        bot_aliases = context.get("bot_aliases") or []
        if isinstance(bot_aliases, str):
            bot_aliases = [bot_aliases]

        bot_name_display = bot_name or "æœºå™¨äºº"
        alias_display = "ã€".join(a for a in bot_aliases if a) or "æ— "
        persona_details = []
        if bot_identity:
            persona_details.append(f"èº«ä»½: {bot_identity}")
        if bot_personality:
            persona_details.append(f"æ ¸å¿ƒäººè®¾: {bot_personality}")
        if bot_personality_side:
            persona_details.append(f"ä¾§å†™: {bot_personality_side}")
        persona_display = "ï¼›".join(persona_details) if persona_details else "æ— "

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®°å¿†æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­ä¸»åŠ¨è¯†åˆ«å¹¶æå–æ‰€æœ‰å¯èƒ½é‡è¦çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ä¸ªäººäº‹å®ã€äº‹ä»¶ã€åå¥½ã€è§‚ç‚¹ç­‰è¦ç´ çš„å†…å®¹ã€‚

å½“å‰æ—¶é—´: {current_date}
æ¶ˆæ¯ç±»å‹: {message_type}

## ğŸ¤– æœºå™¨äººèº«ä»½ï¼ˆä»…ä¾›å‚è€ƒï¼Œç¦æ­¢å†™å…¥è®°å¿†ï¼‰
- æœºå™¨äººåç§°: {bot_name_display}
- åˆ«å: {alias_display}
- æœºå™¨äººäººè®¾æ¦‚è¿°: {persona_display}

è¿™äº›ä¿¡æ¯æ˜¯æœºå™¨äººçš„å›ºå®šè®¾å®šï¼Œå¯ç”¨äºå¸®åŠ©ä½ ç†è§£å¯¹è¯ã€‚ä½ å¯ä»¥åœ¨éœ€è¦æ—¶è®°å½•æœºå™¨äººè‡ªèº«çš„çŠ¶æ€ã€è¡Œä¸ºæˆ–è®¾å®šï¼Œä½†è¦ä¸ç”¨æˆ·ä¿¡æ¯æ¸…æ™°åŒºåˆ†ï¼Œé¿å…è¯¯å°†ç³»ç»ŸIDå†™å…¥è®°å¿†ã€‚

è¯·åŠ¡å¿…éµå®ˆä»¥ä¸‹å‘½åè§„èŒƒï¼š
- å½“è¯´è¯è€…æ˜¯æœºå™¨äººæ—¶ï¼Œè¯·ä½¿ç”¨â€œ{bot_name_display}â€æˆ–å…¶ä»–æ˜ç¡®ç§°å‘¼ä½œä¸ºä¸»è¯­ï¼›
- è®°å½•å…³é”®äº‹å®æ—¶ï¼Œè¯·å‡†ç¡®æ ‡è®°ä¸»ä½“æ˜¯æœºå™¨äººè¿˜æ˜¯ç”¨æˆ·ï¼Œé¿å…æ··æ·†ã€‚

å¯¹è¯å†…å®¹:
{text}

## ğŸ¯ é‡ç‚¹è®°å¿†ç±»å‹è¯†åˆ«æŒ‡å—

### 1. **ä¸ªäººäº‹å®** (personal_fact) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- åŸºæœ¬ä¿¡æ¯ï¼šå§“åã€å¹´é¾„ã€èŒä¸šã€å­¦æ ¡ã€ä¸“ä¸šã€å·¥ä½œåœ°ç‚¹
- ç”Ÿæ´»çŠ¶å†µï¼šä½å€ã€ç”µè¯ã€é‚®ç®±ã€ç¤¾äº¤è´¦å·
- èº«ä»½ç‰¹å¾ï¼šç”Ÿæ—¥ã€æ˜Ÿåº§ã€è¡€å‹ã€å›½ç±ã€è¯­è¨€èƒ½åŠ›
- å¥åº·ä¿¡æ¯ï¼šèº«ä½“çŠ¶å†µã€ç–¾ç—…å²ã€è¯ç‰©è¿‡æ•ã€è¿åŠ¨ä¹ æƒ¯
- å®¶åº­æƒ…å†µï¼šå®¶åº­æˆå‘˜ã€å©šå§»çŠ¶å†µã€å­å¥³ä¿¡æ¯ã€å® ç‰©ä¿¡æ¯

**åˆ¤æ–­æ ‡å‡†ï¼š** æ¶‰åŠä¸ªäººèº«ä»½å’Œç”Ÿæ´»çš„é‡è¦ä¿¡æ¯ï¼Œéƒ½åº”è¯¥è®°å¿†

### 2. **äº‹ä»¶** (event) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- é‡è¦æ—¶åˆ»ï¼šç”Ÿæ—¥èšä¼šã€æ¯•ä¸šå…¸ç¤¼ã€å©šç¤¼ã€æ—…è¡Œ
- æ—¥å¸¸æ´»åŠ¨ï¼šä¸Šç­ã€ä¸Šå­¦ã€çº¦ä¼šã€çœ‹ç”µå½±ã€åƒé¥­
- ç‰¹æ®Šç»å†ï¼šè€ƒè¯•ã€é¢è¯•ã€ä¼šè®®ã€æ¬å®¶ã€è´­ç‰©
- è®¡åˆ’å®‰æ’ï¼šçº¦ä¼šã€ä¼šè®®ã€æ—…è¡Œã€æ´»åŠ¨


**åˆ¤æ–­æ ‡å‡†ï¼š** æ¶‰åŠæ—¶é—´åœ°ç‚¹çš„å…·ä½“æ´»åŠ¨å’Œç»å†ï¼Œéƒ½åº”è¯¥è®°å¿†

### 3. **åå¥½** (preference) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- é¥®é£Ÿåå¥½ï¼šå–œæ¬¢çš„é£Ÿç‰©ã€é¤å…ã€å£å‘³ã€ç¦å¿Œ
- å¨±ä¹å–œå¥½ï¼šå–œæ¬¢çš„ç”µå½±ã€éŸ³ä¹ã€æ¸¸æˆã€ä¹¦ç±
- ç”Ÿæ´»ä¹ æƒ¯ï¼šä½œæ¯æ—¶é—´ã€è¿åŠ¨æ–¹å¼ã€è´­ç‰©ä¹ æƒ¯
- æ¶ˆè´¹åå¥½ï¼šå“ç‰Œå–œå¥½ã€ä»·æ ¼æ•æ„Ÿåº¦ã€è´­ç‰©åœºæ‰€
- é£æ ¼åå¥½ï¼šæœè£…é£æ ¼ã€è£…ä¿®é£æ ¼ã€é¢œè‰²å–œå¥½

**åˆ¤æ–­æ ‡å‡†ï¼š** ä»»ä½•è¡¨è¾¾"å–œæ¬¢"ã€"ä¸å–œæ¬¢"ã€"ä¹ æƒ¯"ã€"ç»å¸¸"ç­‰åå¥½çš„å†…å®¹ï¼Œéƒ½åº”è¯¥è®°å¿†

### 4. **è§‚ç‚¹** (opinion) - é«˜ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- è¯„ä»·çœ‹æ³•ï¼šå¯¹äº‹ç‰©çš„è¯„ä»·ã€æ„è§ã€å»ºè®®
- ä»·å€¼åˆ¤æ–­ï¼šè®¤ä¸ºä»€ä¹ˆé‡è¦ã€ä»€ä¹ˆä¸é‡è¦
- æ€åº¦ç«‹åœºï¼šæ”¯æŒã€åå¯¹ã€ä¸­ç«‹çš„æ€åº¦
- æ„Ÿå—åé¦ˆï¼šå¯¹ç»å†çš„æ„Ÿå—ã€åé¦ˆ

**åˆ¤æ–­æ ‡å‡†ï¼š** ä»»ä½•è¡¨è¾¾ä¸»è§‚çœ‹æ³•å’Œæ€åº¦çš„å†…å®¹ï¼Œéƒ½åº”è¯¥è®°å¿†

### 5. **å…³ç³»** (relationship) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- äººé™…å…³ç³»ï¼šæœ‹å‹ã€åŒäº‹ã€å®¶äººã€æ‹äººçš„å…³ç³»çŠ¶æ€
- ç¤¾äº¤äº’åŠ¨ï¼šä¸ä»–äººçš„äº’åŠ¨ã€äº¤æµã€åˆä½œ
- ç¾¤ä½“å½’å±ï¼šæ‰€å±å›¢é˜Ÿã€ç»„ç»‡ã€ç¤¾ç¾¤

### 6. **æƒ…æ„Ÿ** (emotion) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- æƒ…ç»ªçŠ¶æ€ï¼šå¼€å¿ƒã€éš¾è¿‡ã€ç”Ÿæ°”ã€ç„¦è™‘ã€å…´å¥‹
- æƒ…æ„Ÿå˜åŒ–ï¼šæƒ…ç»ªçš„è½¬å˜ã€åŸå› å’Œç»“æœ

### 7. **ç›®æ ‡** (goal) - ä¸­ç­‰ä¼˜å…ˆçº§è®°å¿†
**åŒ…æ‹¬ä½†ä¸é™äºï¼š**
- è®¡åˆ’å®‰æ’ï¼šçŸ­æœŸè®¡åˆ’ã€é•¿æœŸç›®æ ‡
- æ„¿æœ›æœŸå¾…ï¼šæƒ³è¦å®ç°çš„äº‹æƒ…ã€æœŸæœ›çš„ç»“æœ

## ğŸ“ è®°å¿†æå–åŸåˆ™

### âœ… ç§¯ææå–åŸåˆ™ï¼š
1. **å®å¯é”™è®°ï¼Œä¸å¯é—æ¼** - å¯¹äºå¯èƒ½çš„ä¸ªäººä¿¡æ¯ä¼˜å…ˆè®°å¿†
2. **æŒç»­è¿½è¸ª** - ç›¸åŒä¿¡æ¯çš„å¤šæ¬¡æåŠè¦å¼ºåŒ–è®°å¿†
3. **ä¸Šä¸‹æ–‡å…³è”** - ç»“åˆå¯¹è¯èƒŒæ™¯ç†è§£ä¿¡æ¯é‡è¦æ€§
4. **ç»†èŠ‚ä¸°å¯Œ** - è®°å½•å…·ä½“çš„ç»†èŠ‚å’Œæè¿°

### ğŸš« ç¦æ­¢ä½¿ç”¨æ¨¡ç³Šä»£ç§°åŸåˆ™ï¼š
1. **ç»å¯¹ç¦æ­¢ä½¿ç”¨"ç”¨æˆ·"ä½œä¸ºä»£ç§°** - å¿…é¡»ä½¿ç”¨æ˜ç¡®çš„åå­—æˆ–ç§°å‘¼
2. **ä¼˜å…ˆä½¿ç”¨çœŸå®å§“å** - å¦‚æœçŸ¥é“å¯¹æ–¹çš„åå­—ï¼Œå¿…é¡»ä½¿ç”¨çœŸå®å§“å
3. **ä½¿ç”¨æ˜µç§°æˆ–ç‰¹å®šç§°å‘¼** - å¦‚æœæ²¡æœ‰çœŸå®å§“åï¼Œä½¿ç”¨å¯¹è¯ä¸­å‡ºç°çš„æ˜µç§°
4. **æ— æ³•è·å–å…·ä½“åå­—æ—¶æ‹’ç»æ„å»º** - å¦‚æœä¸çŸ¥é“å¯¹æ–¹çš„å…·ä½“åå­—ï¼Œå®å¯ä¸æ„å»ºè¿™æ¡è®°å¿†ï¼Œä¹Ÿä¸è¦ä½¿ç”¨"ç”¨æˆ·"ã€"è¯¥å¯¹è¯è€…"ç­‰æ¨¡ç³Šä»£ç§°

### ğŸ•’ æ—¶é—´å¤„ç†åŸåˆ™ï¼ˆé‡è¦ï¼‰ï¼š
1. **ç»å¯¹æ—¶é—´è¦æ±‚** - æ¶‰åŠæ—¶é—´çš„è®°å¿†å¿…é¡»ä½¿ç”¨ç»å¯¹æ—¶é—´ï¼ˆå¹´æœˆæ—¥ï¼‰
2. **ç›¸å¯¹æ—¶é—´è½¬æ¢** - å°†"æ˜å¤©"ã€"åå¤©"ã€"ä¸‹å‘¨"ç­‰ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºå…·ä½“æ—¥æœŸ
3. **æ—¶é—´æ ¼å¼è§„èŒƒ** - ä½¿ç”¨"YYYY-MM-DD"æ ¼å¼è®°å½•æ—¥æœŸ
4. **å½“å‰æ—¶é—´å‚è€ƒ** - å½“å‰æ—¶é—´ï¼š{current_date}ï¼ŒåŸºäºæ­¤è®¡ç®—ç›¸å¯¹æ—¶é—´

**ç›¸å¯¹æ—¶é—´è½¬æ¢ç¤ºä¾‹ï¼š**
- "æ˜å¤©" â†’ "2024-09-30"
- "åå¤©" â†’ "2024-10-01"
- "ä¸‹å‘¨" â†’ "2024-10-07"
- "ä¸‹ä¸ªæœˆ" â†’ "2024-10-01"
- "æ˜å¹´" â†’ "2025-01-01"

### ğŸ¯ é‡è¦æ€§ç­‰çº§æ ‡å‡†ï¼š
- **4åˆ† (å…³é”®)**ï¼šä¸ªäººæ ¸å¿ƒä¿¡æ¯ï¼ˆå§“åã€è”ç³»æ–¹å¼ã€é‡è¦æ—¥æœŸï¼‰
- **3åˆ† (é«˜)**ï¼šé‡è¦åå¥½ã€è§‚ç‚¹ã€ç»å†äº‹ä»¶
- **2åˆ† (ä¸€èˆ¬)**ï¼šä¸€èˆ¬æ€§ä¿¡æ¯ã€æ—¥å¸¸æ´»åŠ¨ã€æ„Ÿå—è¡¨è¾¾
- **1åˆ† (ä½)**ï¼šçç¢ç»†èŠ‚ã€é‡å¤ä¿¡æ¯ã€ä¸´æ—¶çŠ¶æ€

### ğŸ” ç½®ä¿¡åº¦æ ‡å‡†ï¼š
- **4åˆ† (å·²éªŒè¯)**ï¼šç”¨æˆ·æ˜ç¡®ç¡®è®¤çš„ä¿¡æ¯
- **3åˆ† (é«˜)**ï¼šç”¨æˆ·ç›´æ¥è¡¨è¾¾çš„æ¸…æ™°ä¿¡æ¯
- **2åˆ† (ä¸­ç­‰)**ï¼šéœ€è¦æ¨ç†æˆ–ä¸Šä¸‹æ–‡åˆ¤æ–­çš„ä¿¡æ¯
- **1åˆ† (ä½)**ï¼šæ¨¡ç³Šæˆ–ä¸å®Œæ•´çš„ä¿¡æ¯

è¾“å‡ºæ ¼å¼è¦æ±‚:
{{
    "memories": [
        {{
            "type": "è®°å¿†ç±»å‹",
            "display": "ä¸€å¥è‡ªç„¶æµç•…çš„ä¸­æ–‡æè¿°ï¼Œç”¨äºç›´æ¥å±•ç¤ºå’Œæç¤ºè¯æ„å»º",
            "subject": "ä¸»è¯­(é€šå¸¸æ˜¯ç”¨æˆ·)",
            "predicate": "è°“è¯­(åŠ¨ä½œ/çŠ¶æ€)",
            "object": "å®¾è¯­(å¯¹è±¡/å±æ€§)",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "importance": "é‡è¦æ€§ç­‰çº§(1-4)",
            "confidence": "ç½®ä¿¡åº¦(1-4)",
            "reasoning": "æå–ç†ç”±"
        }}
    ]
}}

æ³¨æ„ï¼š
1. `display` å­—æ®µå¿…å¡«ï¼Œå¿…é¡»æ˜¯å®Œæ•´é¡ºç•…çš„è‡ªç„¶è¯­è¨€ï¼Œç¦æ­¢ä¾èµ–å­—ç¬¦ä¸²æ‹¼æ¥
2. **display å­—æ®µæ ¼å¼è¦æ±‚**: ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡æè¿°ï¼Œ**ç»å¯¹ç¦æ­¢ä½¿ç”¨"ç”¨æˆ·"ä½œä¸ºä»£ç§°**ï¼Œæ ¼å¼ç¤ºä¾‹ï¼š
   - æ°ç‘å–µå…»äº†ä¸€åªåå«Whiskersçš„çŒ«ã€‚
   - why ocean QAQç‰¹åˆ«å–œæ¬¢æ‹¿é“å’–å•¡ã€‚
   - åœ¨2024å¹´5æœˆ15æ—¥ï¼Œvelida QAQæåˆ°å¯¹æ–°é¡¹ç›®æ„Ÿåˆ°å¾ˆæœ‰å‹åŠ›ã€‚
   - æ°ç‘å–µè®¤ä¸ºè¿™ä¸ªç”µå½±å¾ˆæœ‰è¶£ã€‚
3. **å¿…é¡»ä½¿ç”¨æ˜ç¡®çš„åå­—**ï¼šå¦‚æœçŸ¥é“å¯¹è¯è€…çš„åå­—ï¼ˆå¦‚æ°ç‘å–µã€why ocean QAQç­‰ï¼‰ï¼Œå¿…é¡»ç›´æ¥ä½¿ç”¨å…¶åå­—
4. **ä¸çŸ¥é“åå­—æ—¶ä¸è¦æ„å»º**ï¼šå¦‚æœæ— æ³•ä»å¯¹è¯ä¸­ç¡®å®šå¯¹æ–¹çš„å…·ä½“åå­—ï¼Œå®å¯ä¸æ„å»ºè¿™æ¡è®°å¿†
5. ä¸»è°“å®¾ç”¨äºç´¢å¼•å’Œæ£€ç´¢ï¼Œæç¤ºè¯æ„å»ºä»…ä½¿ç”¨ `display` çš„è‡ªç„¶è¯­è¨€æè¿°
6. åªæå–ç¡®å®å€¼å¾—è®°å¿†çš„ä¿¡æ¯ï¼Œä¸è¦æå–çç¢å†…å®¹
7. ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®ã€å…·ä½“ã€æœ‰ä»·å€¼
8. é‡è¦æ€§ç­‰çº§: 1=ä½, 2=ä¸€èˆ¬, 3=é«˜, 4=å…³é”®ï¼›ç½®ä¿¡åº¦: 1=ä½, 2=ä¸­ç­‰, 3=é«˜, 4=å·²éªŒè¯

## ğŸš¨ æ—¶é—´å¤„ç†è¦æ±‚ï¼ˆå¼ºåˆ¶ï¼‰ï¼š
- **ç»å¯¹æ—¶é—´ä¼˜å…ˆ**ï¼šä»»ä½•æ¶‰åŠæ—¶é—´çš„è®°å¿†éƒ½å¿…é¡»ä½¿ç”¨ç»å¯¹æ—¥æœŸæ ¼å¼
- **ç›¸å¯¹æ—¶é—´è½¬æ¢**ï¼šé‡åˆ°"æ˜å¤©"ã€"åå¤©"ã€"ä¸‹å‘¨"ç­‰ç›¸å¯¹æ—¶é—´å¿…é¡»è½¬æ¢ä¸ºå…·ä½“æ—¥æœŸ
- **æ—¶é—´æ ¼å¼**ï¼šç»Ÿä¸€ä½¿ç”¨ "YYYY-MM-DD" æ ¼å¼
- **è®¡ç®—ä¾æ®**ï¼šåŸºäºå½“å‰æ—¶é—´ {current_date} è¿›è¡Œè½¬æ¢è®¡ç®—
"""

        return prompt

    def _extract_json_payload(self, response: str) -> str | None:
        """ä»æ¨¡å‹å“åº”ä¸­æå–JSONéƒ¨åˆ†ï¼Œå…¼å®¹Markdownä»£ç å—ç­‰æ ¼å¼"""
        if not response:
            return None

        stripped = response.strip()

        # ä¼˜å…ˆå¤„ç†Markdownä»£ç å—æ ¼å¼ ```json ... ```
        code_block_match = re.search(r"```(?:json)?\s*(.*?)```", stripped, re.IGNORECASE | re.DOTALL)
        if code_block_match:
            candidate = code_block_match.group(1).strip()
            if candidate:
                return candidate

        # å›é€€åˆ°æŸ¥æ‰¾ç¬¬ä¸€ä¸ª JSON å¯¹è±¡çš„å¤§æ‹¬å·èŒƒå›´
        start = stripped.find("{")
        end = stripped.rfind("}")
        if start != -1 and end != -1 and end > start:
            return stripped[start : end + 1].strip()

        return stripped if stripped.startswith("{") and stripped.endswith("}") else None

    def _parse_llm_response(
        self, response: str, user_id: str, timestamp: float, context: dict[str, Any]
    ) -> list[MemoryChunk]:
        """è§£æLLMå“åº”"""
        if not response:
            raise MemoryExtractionError("LLMæœªè¿”å›ä»»ä½•å“åº”")

        json_payload = self._extract_json_payload(response)
        if not json_payload:
            preview = response[:200] if response else "ç©ºå“åº”"
            raise MemoryExtractionError(f"æœªåœ¨LLMå“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONè´Ÿè½½ï¼Œå“åº”ç‰‡æ®µ: {preview}")

        try:
            data = orjson.loads(json_payload)
        except Exception as e:
            preview = json_payload[:200]
            raise MemoryExtractionError(f"LLMå“åº”JSONè§£æå¤±è´¥: {e}, ç‰‡æ®µ: {preview}") from e

        memory_list = data.get("memories", [])

        bot_identifiers = self._collect_bot_identifiers(context)
        system_identifiers = self._collect_system_identifiers(context)
        default_subjects = self._resolve_conversation_participants(context, user_id)

        bot_display = None
        if context:
            primary_bot_name = context.get("bot_name")
            if isinstance(primary_bot_name, str) and primary_bot_name.strip():
                bot_display = primary_bot_name.strip()
            if bot_display is None:
                aliases = context.get("bot_aliases")
                if isinstance(aliases, list | tuple | set):
                    for alias in aliases:
                        if isinstance(alias, str) and alias.strip():
                            bot_display = alias.strip()
                            break
                elif isinstance(aliases, str) and aliases.strip():
                    bot_display = aliases.strip()
            if bot_display is None:
                identity = context.get("bot_identity")
                if isinstance(identity, str) and identity.strip():
                    bot_display = identity.strip()

        if not bot_display:
            bot_display = "æœºå™¨äºº"

        bot_display = self._clean_subject_text(bot_display)

        memories: list[MemoryChunk] = []

        for mem_data in memory_list:
            try:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡ç³Šä»£ç§°
                display_text = mem_data.get("display", "")
                if any(
                    ambiguous_term in display_text for ambiguous_term in ["ç”¨æˆ·", "user", "the user", "å¯¹æ–¹", "å¯¹æ‰‹"]
                ):
                    logger.debug(f"æ‹’ç»æ„å»ºåŒ…å«æ¨¡ç³Šä»£ç§°çš„è®°å¿†ï¼Œdisplayå­—æ®µ: {display_text}")
                    continue

                subject_value = mem_data.get("subject")
                normalized_subject = self._normalize_subjects(
                    subject_value, bot_identifiers, system_identifiers, default_subjects, bot_display
                )

                if not normalized_subject:
                    logger.debug("è·³è¿‡ç–‘ä¼¼æœºå™¨äººè‡ªèº«ä¿¡æ¯çš„è®°å¿†: %s", mem_data)
                    continue

                # åˆ›å»ºè®°å¿†å—
                importance_level = self._parse_enum_value(
                    ImportanceLevel, mem_data.get("importance"), ImportanceLevel.NORMAL, "importance"
                )

                confidence_level = self._parse_enum_value(
                    ConfidenceLevel, mem_data.get("confidence"), ConfidenceLevel.MEDIUM, "confidence"
                )

                predicate_value = mem_data.get("predicate", "")
                object_value = mem_data.get("object", "")

                display_text = self._sanitize_display_text(mem_data.get("display"))
                used_fallback_display = False
                if not display_text:
                    display_text = self._compose_display_text(normalized_subject, predicate_value, object_value)
                    used_fallback_display = True

                memory = create_memory_chunk(
                    user_id=user_id,
                    subject=normalized_subject,
                    predicate=predicate_value,
                    obj=object_value,
                    memory_type=self._resolve_memory_type(mem_data.get("type")),
                    chat_id=context.get("chat_id"),
                    source_context=mem_data.get("reasoning", ""),
                    importance=importance_level,
                    confidence=confidence_level,
                    display=display_text,
                )

                if used_fallback_display:
                    logger.warning(
                        "LLM è®°å¿†ç¼ºå°‘è‡ªç„¶è¯­è¨€ display å­—æ®µï¼Œå·²åŸºäºä¸»è°“å®¾ä¸´æ—¶ç”Ÿæˆæè¿°",
                        fallback_generated=True,
                        memory_type=memory.memory_type.value,
                        subjects=memory.content.to_subject_list(),
                        predicate=predicate_value,
                        object_payload=object_value,
                    )

                # æ·»åŠ å…³é”®è¯
                keywords = mem_data.get("keywords", [])
                for keyword in keywords:
                    memory.add_keyword(keyword)

                memories.append(memory)

            except Exception as e:
                logger.warning(f"è§£æå•ä¸ªè®°å¿†å¤±è´¥: {e}, æ•°æ®: {mem_data}")
                continue

        return memories

    def _resolve_memory_type(self, type_str: Any) -> MemoryType:
        """å¥å£®åœ°è§£æè®°å¿†ç±»å‹ï¼Œå…¼å®¹ä¸­æ–‡å’Œè‹±æ–‡"""
        if not isinstance(type_str, str) or not type_str.strip():
            return MemoryType.CONTEXTUAL

        cleaned_type = type_str.strip()

        # å°è¯•ä¸­æ–‡æ˜ å°„
        if cleaned_type in CHINESE_TO_MEMORY_TYPE:
            return CHINESE_TO_MEMORY_TYPE[cleaned_type]

        # å°è¯•ç›´æ¥ä½œä¸ºæšä¸¾å€¼è§£æ
        try:
            return MemoryType(cleaned_type.lower().replace(" ", "_"))
        except ValueError:
            pass

        # å°è¯•ä½œä¸ºæšä¸¾åè§£æ
        try:
            return MemoryType[cleaned_type.upper()]
        except KeyError:
            pass

        logger.warning(f"æ— æ³•è§£ææœªçŸ¥çš„è®°å¿†ç±»å‹ '{type_str}'ï¼Œå›é€€åˆ°ä¸Šä¸‹æ–‡ç±»å‹")
        return MemoryType.CONTEXTUAL

    def _parse_enum_value(self, enum_cls: type[E], raw_value: Any, default: E, field_name: str) -> E:
        """è§£ææšä¸¾å€¼ï¼Œå…¼å®¹æ•°å­—/å­—ç¬¦ä¸²è¡¨ç¤º"""
        if isinstance(raw_value, enum_cls):
            return raw_value

        if raw_value is None:
            return default

        # ç›´æ¥å°è¯•æ•´æ•°è½¬æ¢
        if isinstance(raw_value, int | float):
            int_value = int(raw_value)
            try:
                return enum_cls(int_value)
            except ValueError:
                logger.debug("%s=%s æ— æ³•è§£æä¸º %s", field_name, raw_value, enum_cls.__name__)
                return default

        if isinstance(raw_value, str):
            value_str = raw_value.strip()
            if not value_str:
                return default

            if value_str.isdigit():
                try:
                    return enum_cls(int(value_str))
                except ValueError:
                    logger.debug("%s='%s' æ— æ³•è§£æä¸º %s", field_name, value_str, enum_cls.__name__)
            else:
                normalized = value_str.replace("-", "_").replace(" ", "_").upper()
                for member in enum_cls:
                    if member.name == normalized:
                        return member
                for member in enum_cls:
                    if str(member.value).lower() == value_str.lower():
                        return member

                try:
                    return enum_cls(value_str)
                except ValueError:
                    logger.debug("%s='%s' æ— æ³•è§£æä¸º %s", field_name, value_str, enum_cls.__name__)

        try:
            return enum_cls(raw_value)
        except Exception:
            logger.debug(
                "%s=%s ç±»å‹ %s æ— æ³•è§£æä¸º %sï¼Œä½¿ç”¨é»˜è®¤å€¼ %s",
                field_name,
                raw_value,
                type(raw_value).__name__,
                enum_cls.__name__,
                default.name,
            )
            return default

    def _collect_bot_identifiers(self, context: dict[str, Any] | None) -> set[str]:
        identifiers: set[str] = {"bot", "æœºå™¨äºº", "aiåŠ©æ‰‹"}
        if not context:
            return identifiers

        for key in [
            "bot_name",
            "bot_identity",
            "bot_personality",
            "bot_personality_side",
            "bot_account",
        ]:
            value = context.get(key)
            if isinstance(value, str) and value.strip():
                identifiers.add(value.strip().lower())

        aliases = context.get("bot_aliases")
        if isinstance(aliases, list | tuple | set):
            for alias in aliases:
                if isinstance(alias, str) and alias.strip():
                    identifiers.add(alias.strip().lower())
        elif isinstance(aliases, str) and aliases.strip():
            identifiers.add(aliases.strip().lower())

        return identifiers

    def _collect_system_identifiers(self, context: dict[str, Any] | None) -> set[str]:
        identifiers: set[str] = set()
        if not context:
            return identifiers

        keys = [
            "chat_id",
            "stream_id",
            "stram_id",
            "session_id",
            "conversation_id",
            "message_id",
            "topic_id",
            "thread_id",
        ]

        for key in keys:
            value = context.get(key)
            if isinstance(value, str) and value.strip():
                identifiers.add(value.strip().lower())

        user_id_value = context.get("user_id")
        if isinstance(user_id_value, str) and user_id_value.strip():
            if self._looks_like_system_identifier(user_id_value):
                identifiers.add(user_id_value.strip().lower())

        return identifiers

    def _resolve_conversation_participants(self, context: dict[str, Any] | None, user_id: str) -> list[str]:
        participants: list[str] = []

        if context:
            candidate_keys = [
                "participants",
                "participant_names",
                "speaker_names",
                "members",
                "member_names",
                "mention_users",
                "audiences",
            ]

            for key in candidate_keys:
                value = context.get(key)
                if isinstance(value, list | tuple | set):
                    for item in value:
                        if isinstance(item, str):
                            cleaned = self._clean_subject_text(item)
                            if cleaned:
                                participants.append(cleaned)
                elif isinstance(value, str):
                    for part in self._split_subject_string(value):
                        if part:
                            participants.append(part)

        fallback = self._resolve_user_display(context, user_id)
        if fallback:
            participants.append(fallback)

        if context:
            bot_name = context.get("bot_name") or context.get("bot_identity")
            if isinstance(bot_name, str):
                cleaned = self._clean_subject_text(bot_name)
                if cleaned:
                    participants.append(cleaned)

        if not participants:
            participants = ["å¯¹è¯å‚ä¸è€…"]

        deduplicated: list[str] = []
        seen = set()
        for name in participants:
            key = name.lower()
            if key in seen:
                continue
            seen.add(key)
            deduplicated.append(name)

        return deduplicated

    def _resolve_user_display(self, context: dict[str, Any] | None, user_id: str) -> str:
        candidate_keys = [
            "user_display_name",
            "user_name",
            "nickname",
            "sender_name",
            "member_name",
            "display_name",
            "from_user_name",
            "author_name",
            "speaker_name",
        ]

        if context:
            for key in candidate_keys:
                value = context.get(key)
                if isinstance(value, str):
                    candidate = value.strip()
                    if candidate:
                        return self._clean_subject_text(candidate)

        if user_id and not self._looks_like_system_identifier(user_id):
            return self._clean_subject_text(user_id)

        return "è¯¥ç”¨æˆ·"

    def _clean_subject_text(self, text: str) -> str:
        if not text:
            return ""
        cleaned = re.sub(r"[\s\u3000]+", " ", text).strip()
        cleaned = re.sub(r"[ã€ï¼Œ,ï¼›;]+$", "", cleaned)
        return cleaned

    def _sanitize_display_text(self, value: Any) -> str:
        if value is None:
            return ""

        if isinstance(value, list | dict):
            try:
                value = orjson.dumps(value).decode("utf-8")
            except Exception:
                value = str(value)

        text = str(value).strip()
        if not text or text.lower() in {"null", "none", "undefined"}:
            return ""

        text = re.sub(r"[\s\u3000]+", " ", text)
        return text.strip("\n ")

    def _looks_like_system_identifier(self, value: str) -> bool:
        if not value:
            return False

        condensed = value.replace("-", "").replace("_", "").strip()
        if len(condensed) >= 16 and re.fullmatch(r"[0-9a-fA-F]+", condensed):
            return True

        if len(value) >= 12 and re.fullmatch(r"[0-9A-Z_:-]+", value) and any(ch.isdigit() for ch in value):
            return True

        return False

    def _split_subject_string(self, value: str) -> list[str]:
        if not value:
            return []

        replaced = re.sub(r"\band\b", "ã€", value, flags=re.IGNORECASE)
        replaced = replaced.replace("å’Œ", "ã€").replace("ä¸", "ã€").replace("åŠ", "ã€")
        replaced = replaced.replace("&", "ã€").replace("/", "ã€").replace("+", "ã€")

        tokens = [self._clean_subject_text(token) for token in re.split(r"[ã€,ï¼Œ;ï¼›]+", replaced)]
        return [token for token in tokens if token]

    def _normalize_subjects(
        self,
        subject: Any,
        bot_identifiers: set[str],
        system_identifiers: set[str],
        default_subjects: list[str],
        bot_display: str | None = None,
    ) -> list[str]:
        defaults = default_subjects or ["å¯¹è¯å‚ä¸è€…"]

        raw_candidates: list[str] = []
        if isinstance(subject, list):
            for item in subject:
                if isinstance(item, str):
                    raw_candidates.extend(self._split_subject_string(item))
                elif item is not None:
                    raw_candidates.extend(self._split_subject_string(str(item)))
        elif isinstance(subject, str):
            raw_candidates.extend(self._split_subject_string(subject))
        elif subject is not None:
            raw_candidates.extend(self._split_subject_string(str(subject)))

        normalized: list[str] = []
        bot_primary = self._clean_subject_text(bot_display or "")

        for candidate in raw_candidates:
            if not candidate:
                continue

            lowered = candidate.lower()
            if lowered in bot_identifiers:
                normalized.append(bot_primary or candidate)
                continue

            if lowered in {"ç”¨æˆ·", "user", "the user", "å¯¹æ–¹", "å¯¹æ‰‹"}:
                # ç›´æ¥æ‹’ç»æ„å»ºåŒ…å«æ¨¡ç³Šä»£ç§°çš„è®°å¿†
                logger.debug(f"æ‹’ç»æ„å»ºåŒ…å«æ¨¡ç³Šä»£ç§°çš„è®°å¿†: {candidate}")
                return []  # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºæ‹’ç»æ„å»º

            if lowered in system_identifiers or self._looks_like_system_identifier(candidate):
                continue

            normalized.append(candidate)

        if not normalized:
            normalized = list(defaults)

        deduplicated: list[str] = []
        seen = set()
        for name in normalized:
            key = name.lower()
            if key in seen:
                continue
            seen.add(key)
            deduplicated.append(name)

        return deduplicated

    def _extract_value_from_object(self, obj: str | dict[str, Any] | list[Any], keys: list[str]) -> str | None:
        if isinstance(obj, dict):
            for key in keys:
                value = obj.get(key)
                if value is None:
                    continue
                if isinstance(value, list):
                    compact = "ã€".join(str(item) for item in value[:3])
                    if compact:
                        return compact
                else:
                    value_str = str(value).strip()
                    if value_str:
                        return value_str
        elif isinstance(obj, list):
            compact = "ã€".join(str(item) for item in obj[:3])
            return compact or None
        elif isinstance(obj, str):
            return obj.strip() or None
        return None

    def _compose_display_text(self, subjects: list[str], predicate: str, obj: str | dict[str, Any] | list[Any]) -> str:
        subject_phrase = "ã€".join(subjects) if subjects else "å¯¹è¯å‚ä¸è€…"
        predicate = (predicate or "").strip()

        if predicate == "is_named":
            name = self._extract_value_from_object(obj, ["name", "nickname"]) or ""
            name = self._clean_subject_text(name)
            if name:
                quoted = name if (name.startswith("ã€Œ") and name.endswith("ã€")) else f"ã€Œ{name}ã€"
                return f"{subject_phrase}çš„æ˜µç§°æ˜¯{quoted}"
        elif predicate == "is_age":
            age = self._extract_value_from_object(obj, ["age"]) or ""
            age = self._clean_subject_text(age)
            if age:
                return f"{subject_phrase}ä»Šå¹´{age}å²"
        elif predicate == "is_profession":
            profession = self._extract_value_from_object(obj, ["profession", "job"]) or ""
            profession = self._clean_subject_text(profession)
            if profession:
                return f"{subject_phrase}çš„èŒä¸šæ˜¯{profession}"
        elif predicate == "lives_in":
            location = self._extract_value_from_object(obj, ["location", "city", "place"]) or ""
            location = self._clean_subject_text(location)
            if location:
                return f"{subject_phrase}å±…ä½åœ¨{location}"
        elif predicate == "has_phone":
            phone = self._extract_value_from_object(obj, ["phone", "number"]) or ""
            phone = self._clean_subject_text(phone)
            if phone:
                return f"{subject_phrase}çš„ç”µè¯å·ç æ˜¯{phone}"
        elif predicate == "has_email":
            email = self._extract_value_from_object(obj, ["email"]) or ""
            email = self._clean_subject_text(email)
            if email:
                return f"{subject_phrase}çš„é‚®ç®±æ˜¯{email}"
        elif predicate in {"likes", "likes_food", "favorite_is"}:
            liked = self._extract_value_from_object(obj, ["item", "value", "name"]) or ""
            liked = self._clean_subject_text(liked)
            if liked:
                verb = "å–œæ¬¢" if predicate != "likes_food" else "çˆ±åƒ"
                if predicate == "favorite_is":
                    verb = "æœ€å–œæ¬¢"
                return f"{subject_phrase}{verb}{liked}"
        elif predicate in {"dislikes", "hates"}:
            disliked = self._extract_value_from_object(obj, ["item", "value", "name"]) or ""
            disliked = self._clean_subject_text(disliked)
            if disliked:
                verb = "ä¸å–œæ¬¢" if predicate == "dislikes" else "è®¨åŒ"
                return f"{subject_phrase}{verb}{disliked}"
        elif predicate == "mentioned_event":
            description = self._extract_value_from_object(obj, ["event_text", "description"]) or ""
            description = self._clean_subject_text(description)
            if description:
                return f"{subject_phrase}æåˆ°äº†ï¼š{description}"

        obj_text = self._extract_value_from_object(obj, ["value", "detail", "content"]) or ""
        obj_text = self._clean_subject_text(obj_text)

        if predicate and obj_text:
            return f"{subject_phrase}{predicate}{obj_text}".strip()
        if obj_text:
            return f"{subject_phrase}{obj_text}".strip()
        if predicate:
            return f"{subject_phrase}{predicate}".strip()
        return subject_phrase

    def _validate_and_enhance_memories(self, memories: list[MemoryChunk], context: dict[str, Any]) -> list[MemoryChunk]:
        """éªŒè¯å’Œå¢å¼ºè®°å¿†"""
        validated_memories = []

        for memory in memories:
            # åŸºæœ¬éªŒè¯
            if not self._validate_memory(memory):
                continue

            # å¢å¼ºè®°å¿†
            enhanced_memory = self._enhance_memory(memory, context)
            validated_memories.append(enhanced_memory)

        return validated_memories

    def _validate_memory(self, memory: MemoryChunk) -> bool:
        """éªŒè¯è®°å¿†å—"""
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if not memory.content.subject or not memory.content.predicate:
            logger.debug(f"è®°å¿†å—ç¼ºå°‘ä¸»è¯­æˆ–è°“è¯­: {memory.memory_id}")
            return False

        # æ£€æŸ¥å†…å®¹é•¿åº¦
        content_length = len(memory.text_content)
        if content_length < 5 or content_length > 500:
            logger.debug(f"è®°å¿†å—å†…å®¹é•¿åº¦å¼‚å¸¸: {content_length}")
            return False

        # æ£€æŸ¥ç½®ä¿¡åº¦
        if memory.metadata.confidence == ConfidenceLevel.LOW:
            logger.debug(f"è®°å¿†å—ç½®ä¿¡åº¦è¿‡ä½: {memory.memory_id}")
            return False

        return True

    def _enhance_memory(self, memory: MemoryChunk, context: dict[str, Any]) -> MemoryChunk:
        """å¢å¼ºè®°å¿†å—"""
        # æ—¶é—´è§„èŒƒåŒ–å¤„ç†
        self._normalize_time_in_memory(memory)

        # æ·»åŠ æ—¶é—´ä¸Šä¸‹æ–‡
        if not memory.temporal_context:
            memory.temporal_context = {
                "timestamp": memory.metadata.created_at,
                "timezone": context.get("timezone", "UTC"),
                "day_of_week": datetime.fromtimestamp(memory.metadata.created_at).strftime("%A"),
            }

        # æ·»åŠ æƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if context.get("sentiment"):
            memory.metadata.emotional_context = context["sentiment"]

        # è‡ªåŠ¨æ·»åŠ æ ‡ç­¾
        self._auto_tag_memory(memory)

        return memory

    def _normalize_time_in_memory(self, memory: MemoryChunk):
        """è§„èŒƒåŒ–è®°å¿†ä¸­çš„æ—¶é—´è¡¨è¾¾"""
        import re
        from datetime import datetime, timedelta

        # è·å–å½“å‰æ—¶é—´ä½œä¸ºå‚è€ƒ
        current_time = datetime.fromtimestamp(memory.metadata.created_at)

        # å®šä¹‰ç›¸å¯¹æ—¶é—´æ˜ å°„
        relative_time_patterns = {
            r"ä»Šå¤©|ä»Šæ—¥": current_time.strftime("%Y-%m-%d"),
            r"æ˜¨å¤©|æ˜¨æ—¥": (current_time - timedelta(days=1)).strftime("%Y-%m-%d"),
            r"æ˜å¤©|æ˜æ—¥": (current_time + timedelta(days=1)).strftime("%Y-%m-%d"),
            r"åå¤©": (current_time + timedelta(days=2)).strftime("%Y-%m-%d"),
            r"å¤§åå¤©": (current_time + timedelta(days=3)).strftime("%Y-%m-%d"),
            r"å‰å¤©": (current_time - timedelta(days=2)).strftime("%Y-%m-%d"),
            r"å¤§å‰å¤©": (current_time - timedelta(days=3)).strftime("%Y-%m-%d"),
            r"æœ¬å‘¨|è¿™å‘¨|è¿™æ˜ŸæœŸ": current_time.strftime("%Y-%m-%d"),
            r"ä¸Šå‘¨|ä¸Šæ˜ŸæœŸ": (current_time - timedelta(weeks=1)).strftime("%Y-%m-%d"),
            r"ä¸‹å‘¨|ä¸‹æ˜ŸæœŸ": (current_time + timedelta(weeks=1)).strftime("%Y-%m-%d"),
            r"æœ¬æœˆ|è¿™ä¸ªæœˆ": current_time.strftime("%Y-%m-01"),
            r"ä¸Šæœˆ|ä¸Šä¸ªæœˆ": (current_time.replace(day=1) - timedelta(days=1)).strftime("%Y-%m-01"),
            r"ä¸‹æœˆ|ä¸‹ä¸ªæœˆ": (current_time.replace(day=1) + timedelta(days=32)).replace(day=1).strftime("%Y-%m-01"),
            r"ä»Šå¹´|ä»Šå¹´": current_time.strftime("%Y"),
            r"å»å¹´|ä¸Šä¸€å¹´": str(current_time.year - 1),
            r"æ˜å¹´|ä¸‹ä¸€å¹´": str(current_time.year + 1),
        }

        def _normalize_value(value):
            if isinstance(value, str):
                normalized = value
                for pattern, replacement in relative_time_patterns.items():
                    normalized = re.sub(pattern, replacement, normalized)
                return normalized
            if isinstance(value, dict):
                return {k: _normalize_value(v) for k, v in value.items()}
            if isinstance(value, list):
                return [_normalize_value(item) for item in value]
            return value

        # è§„èŒƒåŒ–ä¸»è¯­å’Œè°“è¯­ï¼ˆé€šå¸¸æ˜¯å­—ç¬¦ä¸²ï¼‰
        memory.content.subject = _normalize_value(memory.content.subject)
        memory.content.predicate = _normalize_value(memory.content.predicate)

        # è§„èŒƒåŒ–å®¾è¯­ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å­—å…¸ï¼‰
        memory.content.object = _normalize_value(memory.content.object)

        # è®°å½•æ—¶é—´è§„èŒƒåŒ–æ“ä½œ
        logger.debug(f"è®°å¿† {memory.memory_id} å·²è¿›è¡Œæ—¶é—´è§„èŒƒåŒ–")

    def _auto_tag_memory(self, memory: MemoryChunk):
        """è‡ªåŠ¨ä¸ºè®°å¿†æ·»åŠ æ ‡ç­¾"""
        # åŸºäºè®°å¿†ç±»å‹çš„è‡ªåŠ¨æ ‡ç­¾
        type_tags = {
            MemoryType.PERSONAL_FACT: ["ä¸ªäººä¿¡æ¯", "åŸºæœ¬èµ„æ–™"],
            MemoryType.EVENT: ["äº‹ä»¶", "æ—¥ç¨‹"],
            MemoryType.PREFERENCE: ["åå¥½", "å–œå¥½"],
            MemoryType.OPINION: ["è§‚ç‚¹", "æ€åº¦"],
            MemoryType.RELATIONSHIP: ["å…³ç³»", "ç¤¾äº¤"],
            MemoryType.EMOTION: ["æƒ…æ„Ÿ", "æƒ…ç»ª"],
            MemoryType.KNOWLEDGE: ["çŸ¥è¯†", "ä¿¡æ¯"],
            MemoryType.SKILL: ["æŠ€èƒ½", "èƒ½åŠ›"],
            MemoryType.GOAL: ["ç›®æ ‡", "è®¡åˆ’"],
            MemoryType.EXPERIENCE: ["ç»éªŒ", "ç»å†"],
        }

        tags = type_tags.get(memory.memory_type, [])
        for tag in tags:
            memory.add_tag(tag)

    def _update_extraction_stats(self, success_count: int, extraction_time: float):
        """æ›´æ–°æå–ç»Ÿè®¡"""
        self.extraction_stats["total_extractions"] += 1
        self.extraction_stats["successful_extractions"] += success_count
        self.extraction_stats["failed_extractions"] += max(0, 1 - success_count)

        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        if self.extraction_stats["successful_extractions"] > 0:
            total_confidence = self.extraction_stats["average_confidence"] * (
                self.extraction_stats["successful_extractions"] - success_count
            )
            # å‡è®¾æ–°è®°å¿†çš„å¹³å‡ç½®ä¿¡åº¦ä¸º0.8
            total_confidence += 0.8 * success_count
            self.extraction_stats["average_confidence"] = (
                total_confidence / self.extraction_stats["successful_extractions"]
            )

    def get_extraction_stats(self) -> dict[str, Any]:
        """è·å–æå–ç»Ÿè®¡ä¿¡æ¯"""
        return self.extraction_stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.extraction_stats = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "failed_extractions": 0,
            "average_confidence": 0.0,
        }
