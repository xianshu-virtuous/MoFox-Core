"""
ç²¾å‡†è®°å¿†ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
1. åŸºäºæ–‡æ¡£è®¾è®¡çš„é«˜æ•ˆè®°å¿†æ„å»ºã€å­˜å‚¨ä¸å¬å›ä¼˜åŒ–ç³»ç»Ÿï¼Œè¦†ç›–æ„å»ºã€å‘é‡åŒ–ä¸å¤šé˜¶æ®µæ£€ç´¢å…¨æµç¨‹ã€‚
2. å†…ç½® LLM æŸ¥è¯¢è§„åˆ’å™¨ä¸åµŒå…¥ç»´åº¦è‡ªåŠ¨è§£ææœºåˆ¶ï¼Œç›´æ¥ä»æ¨¡å‹é…ç½®æ¨æ–­å‘é‡å­˜å‚¨å‚æ•°ã€‚
"""

import asyncio
import hashlib
import re
import time
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import TYPE_CHECKING, Any

import orjson

from src.chat.memory_system.memory_builder import MemoryBuilder, MemoryExtractionError
from src.chat.memory_system.memory_chunk import MemoryChunk
from src.chat.memory_system.memory_fusion import MemoryFusionEngine
from src.chat.memory_system.memory_query_planner import MemoryQueryPlanner


# è®°å¿†é‡‡æ ·æ¨¡å¼æšä¸¾
class MemorySamplingMode(Enum):
    """è®°å¿†é‡‡æ ·æ¨¡å¼"""
    HIPPOCAMPUS = "hippocampus"  # æµ·é©¬ä½“æ¨¡å¼ï¼šå®šæ—¶ä»»åŠ¡é‡‡æ ·
    IMMEDIATE = "immediate"       # å³æ—¶æ¨¡å¼ï¼šå›å¤åç«‹å³é‡‡æ ·
    ALL = "all"                   # æ‰€æœ‰æ¨¡å¼ï¼šåŒæ—¶ä½¿ç”¨æµ·é©¬ä½“å’Œå³æ—¶é‡‡æ ·
from src.common.logger import get_logger
from src.config.config import global_config, model_config
from src.llm_models.utils_model import LLMRequest

if TYPE_CHECKING:
    from src.chat.memory_system.memory_forgetting_engine import MemoryForgettingEngine
    from src.common.data_models.database_data_model import DatabaseMessages

logger = get_logger(__name__)

# å…¨å±€è®°å¿†ä½œç”¨åŸŸï¼ˆå…±äº«è®°å¿†åº“ï¼‰
GLOBAL_MEMORY_SCOPE = "global"


class MemorySystemStatus(Enum):
    """è®°å¿†ç³»ç»ŸçŠ¶æ€"""

    INITIALIZING = "initializing"
    READY = "ready"
    BUILDING = "building"
    RETRIEVING = "retrieving"
    ERROR = "error"


@dataclass
class MemorySystemConfig:
    """è®°å¿†ç³»ç»Ÿé…ç½®"""

    # è®°å¿†æ„å»ºé…ç½®
    min_memory_length: int = 10
    max_memory_length: int = 500
    memory_value_threshold: float = 0.7
    min_build_interval_seconds: float = 300.0

    # å‘é‡å­˜å‚¨é…ç½®ï¼ˆåµŒå…¥ç»´åº¦è‡ªåŠ¨æ¥è‡ªæ¨¡å‹é…ç½®ï¼‰
    vector_dimension: int = 1024
    similarity_threshold: float = 0.8

    # å¬å›é…ç½®
    coarse_recall_limit: int = 50
    fine_recall_limit: int = 10
    semantic_rerank_limit: int = 20
    final_recall_limit: int = 5
    semantic_similarity_threshold: float = 0.6
    vector_weight: float = 0.4
    semantic_weight: float = 0.3
    context_weight: float = 0.2
    recency_weight: float = 0.1

    # èåˆé…ç½®
    fusion_similarity_threshold: float = 0.85
    deduplication_window: timedelta = timedelta(hours=24)

    @classmethod
    def from_global_config(cls):
        """ä»å…¨å±€é…ç½®åˆ›å»ºé…ç½®å®ä¾‹"""

        embedding_dimension = None
        try:
            embedding_task = getattr(model_config.model_task_config, "embedding", None)
            if embedding_task is not None:
                embedding_dimension = getattr(embedding_task, "embedding_dimension", None)
        except Exception:
            embedding_dimension = None

        if not embedding_dimension:
            try:
                embedding_dimension = getattr(global_config.lpmm_knowledge, "embedding_dimension", None)
            except Exception:
                embedding_dimension = None

        if not embedding_dimension:
            embedding_dimension = 1024

        return cls(
            # è®°å¿†æ„å»ºé…ç½®
            min_memory_length=global_config.memory.min_memory_length,
            max_memory_length=global_config.memory.max_memory_length,
            memory_value_threshold=global_config.memory.memory_value_threshold,
            min_build_interval_seconds=getattr(global_config.memory, "memory_build_interval", 300.0),
            # å‘é‡å­˜å‚¨é…ç½®
            vector_dimension=int(embedding_dimension),
            similarity_threshold=global_config.memory.vector_similarity_threshold,
            # å¬å›é…ç½®
            coarse_recall_limit=global_config.memory.metadata_filter_limit,
            fine_recall_limit=global_config.memory.vector_search_limit,
            semantic_rerank_limit=global_config.memory.semantic_rerank_limit,
            final_recall_limit=global_config.memory.final_result_limit,
            semantic_similarity_threshold=getattr(global_config.memory, "semantic_similarity_threshold", 0.6),
            vector_weight=global_config.memory.vector_weight,
            semantic_weight=global_config.memory.semantic_weight,
            context_weight=global_config.memory.context_weight,
            recency_weight=global_config.memory.recency_weight,
            # èåˆé…ç½®
            fusion_similarity_threshold=global_config.memory.fusion_similarity_threshold,
            deduplication_window=timedelta(hours=global_config.memory.deduplication_window_hours),
        )


class MemorySystem:
    """ç²¾å‡†è®°å¿†ç³»ç»Ÿæ ¸å¿ƒç±»"""

    def __init__(self, llm_model: LLMRequest | None = None, config: MemorySystemConfig | None = None):
        self.config = config or MemorySystemConfig.from_global_config()
        self.llm_model = llm_model
        self.status = MemorySystemStatus.INITIALIZING

        # æ ¸å¿ƒç»„ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.memory_builder: MemoryBuilder = None
        self.fusion_engine: MemoryFusionEngine = None
        self.unified_storage = None  # ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿ
        self.query_planner: MemoryQueryPlanner = None
        self.forgetting_engine: MemoryForgettingEngine | None = None

        # LLMæ¨¡å‹
        self.value_assessment_model: LLMRequest = None
        self.memory_extraction_model: LLMRequest = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_memories = 0
        self.last_build_time = None
        self.last_retrieval_time = None

        # æ„å»ºèŠ‚æµè®°å½•
        self._last_memory_build_times: dict[str, float] = {}

        # è®°å¿†æŒ‡çº¹ç¼“å­˜ï¼Œç”¨äºå¿«é€Ÿæ£€æµ‹é‡å¤è®°å¿†
        self._memory_fingerprints: dict[str, str] = {}

        # æµ·é©¬ä½“é‡‡æ ·å™¨
        self.hippocampus_sampler = None

        logger.info("MemorySystem åˆå§‹åŒ–å¼€å§‹")

    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        try:

            # åˆå§‹åŒ–LLMæ¨¡å‹
            fallback_task = getattr(self.llm_model, "model_for_task", None) if self.llm_model else None

            value_task_config = getattr(model_config.model_task_config, "utils_small", None)
            extraction_task_config = getattr(model_config.model_task_config, "utils", None)

            if value_task_config is None:
                logger.warning("æœªæ‰¾åˆ° utils_small æ¨¡å‹é…ç½®ï¼Œå›é€€åˆ° utils æˆ–å¤–éƒ¨æä¾›çš„æ¨¡å‹é…ç½®ã€‚")
                value_task_config = extraction_task_config or fallback_task

            if extraction_task_config is None:
                logger.warning("æœªæ‰¾åˆ° utils æ¨¡å‹é…ç½®ï¼Œå›é€€åˆ° utils_small æˆ–å¤–éƒ¨æä¾›çš„æ¨¡å‹é…ç½®ã€‚")
                extraction_task_config = value_task_config or fallback_task

            if value_task_config is None or extraction_task_config is None:
                raise RuntimeError(
                    "æ— æ³•åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿæ‰€éœ€çš„æ¨¡å‹é…ç½®ï¼Œè¯·æ£€æŸ¥ model_task_config ä¸­çš„ utils / utils_small è®¾ç½®ã€‚"
                )

            self.value_assessment_model = LLMRequest(
                model_set=value_task_config, request_type="memory.value_assessment"
            )

            self.memory_extraction_model = LLMRequest(
                model_set=extraction_task_config, request_type="memory.extraction"
            )

            # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
            self.memory_builder = MemoryBuilder(self.memory_extraction_model)
            self.fusion_engine = MemoryFusionEngine(self.config.fusion_similarity_threshold)

            # åˆå§‹åŒ–Vector DBå­˜å‚¨ç³»ç»Ÿï¼ˆæ›¿ä»£æ—§çš„unified_memory_storageï¼‰
            from src.chat.memory_system.vector_memory_storage_v2 import VectorMemoryStorage, VectorStorageConfig

            storage_config = VectorStorageConfig(
                memory_collection="unified_memory_v2",
                metadata_collection="memory_metadata_v2",
                similarity_threshold=self.config.similarity_threshold,
                search_limit=getattr(global_config.memory, "unified_storage_search_limit", 20),
                batch_size=getattr(global_config.memory, "unified_storage_batch_size", 100),
                enable_caching=getattr(global_config.memory, "unified_storage_enable_caching", True),
                cache_size_limit=getattr(global_config.memory, "unified_storage_cache_limit", 1000),
                auto_cleanup_interval=getattr(global_config.memory, "unified_storage_auto_cleanup_interval", 3600),
                enable_forgetting=getattr(global_config.memory, "enable_memory_forgetting", True),
                retention_hours=getattr(global_config.memory, "memory_retention_hours", 720),  # 30å¤©
            )

            try:
                self.unified_storage = VectorMemoryStorage(storage_config)
                logger.info("âœ… Vector DBå­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as storage_error:
                logger.error(f"âŒ Vector DBå­˜å‚¨ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {storage_error}", exc_info=True)
                raise

            # åˆå§‹åŒ–é—å¿˜å¼•æ“
            from src.chat.memory_system.memory_forgetting_engine import ForgettingConfig, MemoryForgettingEngine

            # ä»å…¨å±€é…ç½®åˆ›å»ºé—å¿˜å¼•æ“é…ç½®
            forgetting_config = ForgettingConfig(
                # æ£€æŸ¥é¢‘ç‡é…ç½®
                check_interval_hours=getattr(global_config.memory, "forgetting_check_interval_hours", 24),
                batch_size=100,  # å›ºå®šå€¼ï¼Œæš‚ä¸é…ç½®
                # é—å¿˜é˜ˆå€¼é…ç½®
                base_forgetting_days=getattr(global_config.memory, "base_forgetting_days", 30.0),
                min_forgetting_days=getattr(global_config.memory, "min_forgetting_days", 7.0),
                max_forgetting_days=getattr(global_config.memory, "max_forgetting_days", 365.0),
                # é‡è¦ç¨‹åº¦æƒé‡
                critical_importance_bonus=getattr(global_config.memory, "critical_importance_bonus", 45.0),
                high_importance_bonus=getattr(global_config.memory, "high_importance_bonus", 30.0),
                normal_importance_bonus=getattr(global_config.memory, "normal_importance_bonus", 15.0),
                low_importance_bonus=getattr(global_config.memory, "low_importance_bonus", 0.0),
                # ç½®ä¿¡åº¦æƒé‡
                verified_confidence_bonus=getattr(global_config.memory, "verified_confidence_bonus", 30.0),
                high_confidence_bonus=getattr(global_config.memory, "high_confidence_bonus", 20.0),
                medium_confidence_bonus=getattr(global_config.memory, "medium_confidence_bonus", 10.0),
                low_confidence_bonus=getattr(global_config.memory, "low_confidence_bonus", 0.0),
                # æ¿€æ´»é¢‘ç‡æƒé‡
                activation_frequency_weight=getattr(global_config.memory, "activation_frequency_weight", 0.5),
                max_frequency_bonus=getattr(global_config.memory, "max_frequency_bonus", 10.0),
                # ä¼‘çœ é…ç½®
                dormant_threshold_days=getattr(global_config.memory, "dormant_threshold_days", 90),
            )

            self.forgetting_engine = MemoryForgettingEngine(forgetting_config)

            planner_task_config = model_config.model_task_config.utils_small
            planner_model: LLMRequest | None = None
            try:
                planner_model = LLMRequest(model_set=planner_task_config, request_type="memory.query_planner")
            except Exception as planner_exc:
                logger.warning("æŸ¥è¯¢è§„åˆ’æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤è§„åˆ’ç­–ç•¥: %s", planner_exc, exc_info=True)

            self.query_planner = MemoryQueryPlanner(planner_model, default_limit=self.config.final_recall_limit)

            # åˆå§‹åŒ–æµ·é©¬ä½“é‡‡æ ·å™¨
            if global_config.memory.enable_hippocampus_sampling:
                try:
                    from .hippocampus_sampler import initialize_hippocampus_sampler
                    self.hippocampus_sampler = await initialize_hippocampus_sampler(self)
                    logger.info("âœ… æµ·é©¬ä½“é‡‡æ ·å™¨åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"æµ·é©¬ä½“é‡‡æ ·å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.hippocampus_sampler = None

            # ç»Ÿä¸€å­˜å‚¨å·²ç»è‡ªåŠ¨åŠ è½½æ•°æ®ï¼Œæ— éœ€é¢å¤–åŠ è½½

            self.status = MemorySystemStatus.READY

        except Exception as e:
            self.status = MemorySystemStatus.ERROR
            logger.error(f"âŒ è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def retrieve_memories_for_building(
        self, query_text: str, user_id: str | None = None, context: dict[str, Any] | None = None, limit: int = 5
    ) -> list[MemoryChunk]:
        """åœ¨æ„å»ºè®°å¿†æ—¶æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œä½¿ç”¨ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿ

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        if self.status not in [MemorySystemStatus.READY, MemorySystemStatus.BUILDING]:
            logger.warning(f"è®°å¿†ç³»ç»ŸçŠ¶æ€ä¸å…è®¸æ£€ç´¢: {self.status.value}")
            return []

        if not self.unified_storage:
            logger.warning("ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿæœªåˆå§‹åŒ–")
            return []

        try:
            # ä½¿ç”¨ç»Ÿä¸€å­˜å‚¨æ£€ç´¢ç›¸ä¼¼è®°å¿†
            filters = {"user_id": user_id} if user_id else None
            search_results = await self.unified_storage.search_similar_memories(
                query_text=query_text, limit=limit, filters=filters
            )

            # è½¬æ¢ä¸ºè®°å¿†å¯¹è±¡
            memories = []
            for memory, similarity_score in search_results:
                if memory:
                    memory.update_access()  # æ›´æ–°è®¿é—®ä¿¡æ¯
                    memories.append(memory)

            return memories

        except Exception as e:
            logger.error(f"æ„å»ºè¿‡ç¨‹ä¸­æ£€ç´¢è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return []

    async def build_memory_from_conversation(
        self, conversation_text: str, context: dict[str, Any], timestamp: float | None = None, bypass_interval: bool = False
    ) -> list[MemoryChunk]:
        """ä»å¯¹è¯ä¸­æ„å»ºè®°å¿†

        Args:
            conversation_text: å¯¹è¯æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            timestamp: æ—¶é—´æˆ³ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´
            bypass_interval: æ˜¯å¦ç»•è¿‡æ„å»ºé—´éš”æ£€æŸ¥ï¼ˆæµ·é©¬ä½“é‡‡æ ·å™¨ä¸“ç”¨ï¼‰

        Returns:
            æ„å»ºçš„è®°å¿†å—åˆ—è¡¨
        """
        original_status = self.status
        self.status = MemorySystemStatus.BUILDING
        start_time = time.time()

        build_scope_key: str | None = None
        build_marker_time: float | None = None

        try:
            normalized_context = self._normalize_context(context, GLOBAL_MEMORY_SCOPE, timestamp)

            build_scope_key = self._get_build_scope_key(normalized_context, GLOBAL_MEMORY_SCOPE)
            min_interval = max(0.0, getattr(self.config, "min_build_interval_seconds", 0.0))
            current_time = time.time()

            # æ„å»ºé—´éš”æ£€æŸ¥ï¼ˆæµ·é©¬ä½“é‡‡æ ·å™¨å¯ä»¥ç»•è¿‡ï¼‰
            if build_scope_key and min_interval > 0 and not bypass_interval:
                last_time = self._last_memory_build_times.get(build_scope_key)
                if last_time and (current_time - last_time) < min_interval:
                    remaining = min_interval - (current_time - last_time)
                    logger.info(
                        f"è·ç¦»ä¸Šæ¬¡è®°å¿†æ„å»ºé—´éš”ä¸è¶³ï¼Œè·³è¿‡æ­¤æ¬¡æ„å»º | key={build_scope_key} | å‰©ä½™{remaining:.2f}ç§’",
                    )
                    self.status = MemorySystemStatus.READY
                    return []

                build_marker_time = current_time
                self._last_memory_build_times[build_scope_key] = current_time
            elif bypass_interval:
                # æµ·é©¬ä½“é‡‡æ ·æ¨¡å¼ï¼šä¸æ›´æ–°æ„å»ºæ—¶é—´è®°å½•ï¼Œé¿å…å½±å“å³æ—¶æ¨¡å¼
                logger.debug("æµ·é©¬ä½“é‡‡æ ·æ¨¡å¼ï¼šç»•è¿‡æ„å»ºé—´éš”æ£€æŸ¥")

            conversation_text = await self._resolve_conversation_context(conversation_text, normalized_context)

            logger.debug("å¼€å§‹æ„å»ºè®°å¿†ï¼Œæ–‡æœ¬é•¿åº¦: %d", len(conversation_text))

            # 1. ä¿¡æ¯ä»·å€¼è¯„ä¼°ï¼ˆæµ·é©¬ä½“é‡‡æ ·å™¨å¯ä»¥ç»•è¿‡ï¼‰
            if not bypass_interval and not context.get("bypass_value_threshold", False):
                value_score = await self._assess_information_value(conversation_text, normalized_context)

                if value_score < self.config.memory_value_threshold:
                    logger.info(f"ä¿¡æ¯ä»·å€¼è¯„åˆ† {value_score:.2f} ä½äºé˜ˆå€¼ï¼Œè·³è¿‡è®°å¿†æ„å»º")
                    self.status = original_status
                    return []
            else:
                # æµ·é©¬ä½“é‡‡æ ·å™¨ï¼šä½¿ç”¨é»˜è®¤ä»·å€¼åˆ†æ•°æˆ–ç®€å•è¯„ä¼°
                value_score = 0.6  # é»˜è®¤ä¸­ç­‰ä»·å€¼
                if context.get("is_hippocampus_sample", False):
                    # å¯¹æµ·é©¬ä½“æ ·æœ¬è¿›è¡Œç®€å•ä»·å€¼è¯„ä¼°
                    if len(conversation_text) > 100:  # é•¿æ–‡æœ¬å¯èƒ½æœ‰æ›´å¤šä¿¡æ¯
                        value_score = 0.7
                    elif len(conversation_text) > 50:
                        value_score = 0.6
                    else:
                        value_score = 0.5

                logger.debug(f"æµ·é©¬ä½“é‡‡æ ·æ¨¡å¼ï¼šä½¿ç”¨ä»·å€¼è¯„åˆ† {value_score:.2f}")

            # 2. æ„å»ºè®°å¿†å—ï¼ˆæ‰€æœ‰è®°å¿†ç»Ÿä¸€ä½¿ç”¨ global ä½œç”¨åŸŸï¼Œå®ç°å®Œå…¨å…±äº«ï¼‰
            memory_chunks = await self.memory_builder.build_memories(
                conversation_text,
                normalized_context,
                GLOBAL_MEMORY_SCOPE,  # å¼ºåˆ¶ä½¿ç”¨ globalï¼Œä¸åŒºåˆ†ç”¨æˆ·
                timestamp or time.time(),
            )

            if not memory_chunks:
                logger.debug("æœªæå–åˆ°æœ‰æ•ˆè®°å¿†å—")
                self.status = original_status
                return []

            # 3. è®°å¿†èåˆä¸å»é‡ï¼ˆåŒ…å«ä¸å†å²è®°å¿†çš„èåˆï¼‰
            existing_candidates = await self._collect_fusion_candidates(memory_chunks)
            fused_chunks = await self.fusion_engine.fuse_memories(memory_chunks, existing_candidates)

            # 4. å­˜å‚¨è®°å¿†åˆ°ç»Ÿä¸€å­˜å‚¨
            stored_count = await self._store_memories_unified(fused_chunks)

            # 4.1 æ§åˆ¶å°é¢„è§ˆ
            self._log_memory_preview(fused_chunks)

            # 5. æ›´æ–°ç»Ÿè®¡
            self.total_memories += stored_count
            self.last_build_time = time.time()
            if build_scope_key:
                self._last_memory_build_times[build_scope_key] = self.last_build_time

            build_time = time.time() - start_time
            logger.info(
                f"âœ… ç”Ÿæˆ {len(fused_chunks)} æ¡è®°å¿†ï¼ŒæˆåŠŸå…¥åº“ {stored_count} æ¡ï¼Œè€—æ—¶ {build_time:.2f}ç§’",
            )

            self.status = original_status
            return fused_chunks

        except MemoryExtractionError as e:
            if build_scope_key and build_marker_time is not None:
                recorded_time = self._last_memory_build_times.get(build_scope_key)
                if recorded_time == build_marker_time:
                    self._last_memory_build_times.pop(build_scope_key, None)
            self.status = original_status
            logger.warning("è®°å¿†æ„å»ºå› LLMå“åº”é—®é¢˜ä¸­æ–­: %s", e)
            return []

        except Exception as e:
            if build_scope_key and build_marker_time is not None:
                recorded_time = self._last_memory_build_times.get(build_scope_key)
                if recorded_time == build_marker_time:
                    self._last_memory_build_times.pop(build_scope_key, None)
            self.status = MemorySystemStatus.ERROR
            logger.error(f"âŒ è®°å¿†æ„å»ºå¤±è´¥: {e}", exc_info=True)
            raise

    def _log_memory_preview(self, memories: list[MemoryChunk]) -> None:
        """åœ¨æ§åˆ¶å°è¾“å‡ºè®°å¿†é¢„è§ˆï¼Œä¾¿äºäººå·¥æ£€æŸ¥"""
        if not memories:
            logger.info("ğŸ“ æœ¬æ¬¡æœªç”Ÿæˆæ–°çš„è®°å¿†")
            return

        logger.info(f"ğŸ“ æœ¬æ¬¡ç”Ÿæˆçš„è®°å¿†é¢„è§ˆ ({len(memories)} æ¡):")
        for idx, memory in enumerate(memories, start=1):
            text = memory.text_content or ""
            if len(text) > 120:
                text = text[:117] + "..."

            logger.info(
                f"  {idx}) ç±»å‹={memory.memory_type.value} é‡è¦æ€§={memory.metadata.importance.name} "
                f"ç½®ä¿¡åº¦={memory.metadata.confidence.name} | å†…å®¹={text}"
            )

    async def _collect_fusion_candidates(self, new_memories: list[MemoryChunk]) -> list[MemoryChunk]:
        """æ”¶é›†ä¸æ–°è®°å¿†ç›¸ä¼¼çš„ç°æœ‰è®°å¿†ï¼Œä¾¿äºèåˆå»é‡"""
        if not new_memories:
            return []

        candidate_ids: set[str] = set()
        new_memory_ids = {memory.memory_id for memory in new_memories if memory and getattr(memory, "memory_id", None)}

        # åŸºäºæŒ‡çº¹çš„ç›´æ¥åŒ¹é…
        for memory in new_memories:
            try:
                fingerprint = self._build_memory_fingerprint(memory)
                fingerprint_key = self._fingerprint_key(memory.user_id, fingerprint)
                existing_id = self._memory_fingerprints.get(fingerprint_key)
                if existing_id and existing_id not in new_memory_ids:
                    candidate_ids.add(existing_id)
            except Exception as exc:
                logger.debug("æ„å»ºè®°å¿†æŒ‡çº¹å¤±è´¥ï¼Œè·³è¿‡å€™é€‰æ”¶é›†: %s", exc)

        # åŸºäºä¸»ä½“ç´¢å¼•çš„å€™é€‰ï¼ˆä½¿ç”¨ç»Ÿä¸€å­˜å‚¨ï¼‰
        if self.unified_storage and self.unified_storage.keyword_index:
            for memory in new_memories:
                for subject in memory.subjects:
                    normalized = subject.strip().lower() if isinstance(subject, str) else ""
                    if not normalized:
                        continue
                    subject_candidates = self.unified_storage.keyword_index.get(normalized)
                    if subject_candidates:
                        candidate_ids.update(subject_candidates)

        # åŸºäºå‘é‡æœç´¢çš„å€™é€‰ï¼ˆä½¿ç”¨ç»Ÿä¸€å­˜å‚¨ï¼‰
        total_vectors = 0
        if self.unified_storage:
            storage_stats = self.unified_storage.get_storage_stats()
            total_vectors = storage_stats.get("total_vectors", 0) or 0

        if self.unified_storage and total_vectors > 0:
            search_tasks = []
            for memory in new_memories:
                display_text = (memory.display or "").strip()
                if not display_text:
                    continue
                search_tasks.append(
                    self.unified_storage.search_similar_memories(
                        query_text=display_text, limit=8, filters={"user_id": GLOBAL_MEMORY_SCOPE}
                    )
                )

            if search_tasks:
                search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
                similarity_threshold = getattr(
                    self.fusion_engine,
                    "similarity_threshold",
                    self.config.similarity_threshold,
                )
                min_threshold = max(0.0, min(1.0, similarity_threshold * 0.8))

                for result in search_results:
                    if isinstance(result, Exception):
                        logger.warning("èåˆå€™é€‰å‘é‡æœç´¢å¤±è´¥: %s", result)
                        continue
                    for memory_id, similarity in result:
                        if memory_id in new_memory_ids:
                            continue
                        if similarity is None or similarity < min_threshold:
                            continue
                        candidate_ids.add(memory_id)

        existing_candidates: list[MemoryChunk] = []
        cache = self.unified_storage.memory_cache if self.unified_storage else {}
        for candidate_id in candidate_ids:
            if candidate_id in new_memory_ids:
                continue
            candidate_memory = cache.get(candidate_id)
            if candidate_memory:
                existing_candidates.append(candidate_memory)

        if existing_candidates:
            logger.debug(
                "èåˆå€™é€‰æ”¶é›†å®Œæˆï¼Œæ–°è®°å¿†=%dï¼Œå€™é€‰=%d",
                len(new_memories),
                len(existing_candidates),
            )

        return existing_candidates

    async def process_conversation_memory(self, context: dict[str, Any]) -> dict[str, Any]:
        """å¯¹å¤–æš´éœ²çš„å¯¹è¯è®°å¿†å¤„ç†æ¥å£ï¼Œæ”¯æŒæµ·é©¬ä½“ã€ç²¾å‡†è®°å¿†ã€è‡ªé€‚åº”ä¸‰ç§é‡‡æ ·æ¨¡å¼"""
        start_time = time.time()

        try:
            context = dict(context or {})

            # è·å–é…ç½®çš„é‡‡æ ·æ¨¡å¼
            sampling_mode = getattr(global_config.memory, "memory_sampling_mode", "precision")
            current_mode = MemorySamplingMode(sampling_mode)


            context["__sampling_mode"] = current_mode.value
            logger.debug(f"ä½¿ç”¨è®°å¿†é‡‡æ ·æ¨¡å¼: {current_mode.value}")

            # æ ¹æ®é‡‡æ ·æ¨¡å¼å¤„ç†è®°å¿†
            if current_mode == MemorySamplingMode.HIPPOCAMPUS:
                # æµ·é©¬ä½“æ¨¡å¼ï¼šä»…åå°å®šæ—¶é‡‡æ ·ï¼Œä¸ç«‹å³å¤„ç†
                return {
                    "success": True,
                    "created_memories": [],
                    "memory_count": 0,
                    "processing_time": time.time() - start_time,
                    "status": self.status.value,
                    "processing_mode": "hippocampus",
                    "message": "æµ·é©¬ä½“æ¨¡å¼ï¼šè®°å¿†å°†ç”±åå°å®šæ—¶ä»»åŠ¡é‡‡æ ·å¤„ç†",
                }

            elif current_mode == MemorySamplingMode.IMMEDIATE:
                # å³æ—¶æ¨¡å¼ï¼šç«‹å³å¤„ç†è®°å¿†æ„å»º
                return await self._process_immediate_memory(context, start_time)

            elif current_mode == MemorySamplingMode.ALL:
                # æ‰€æœ‰æ¨¡å¼ï¼šåŒæ—¶è¿›è¡Œå³æ—¶å¤„ç†å’Œæµ·é©¬ä½“é‡‡æ ·
                immediate_result = await self._process_immediate_memory(context, start_time)

                # æµ·é©¬ä½“é‡‡æ ·å™¨ä¼šåœ¨åå°ç»§ç»­å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯è®°å½•
                if self.hippocampus_sampler:
                    immediate_result["processing_mode"] = "all_modes"
                    immediate_result["hippocampus_status"] = "background_sampling_enabled"
                    immediate_result["message"] = "æ‰€æœ‰æ¨¡å¼ï¼šå³æ—¶å¤„ç†å·²å®Œæˆï¼Œæµ·é©¬ä½“é‡‡æ ·å°†åœ¨åå°ç»§ç»­"
                else:
                    immediate_result["processing_mode"] = "immediate_fallback"
                    immediate_result["hippocampus_status"] = "not_available"
                    immediate_result["message"] = "æµ·é©¬ä½“é‡‡æ ·å™¨ä¸å¯ç”¨ï¼Œå›é€€åˆ°å³æ—¶æ¨¡å¼"

                return immediate_result

            else:
                # é»˜è®¤å›é€€åˆ°å³æ—¶æ¨¡å¼
                logger.warning(f"æœªçŸ¥çš„é‡‡æ ·æ¨¡å¼ {sampling_mode}ï¼Œå›é€€åˆ°å³æ—¶æ¨¡å¼")
                return await self._process_immediate_memory(context, start_time)

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å¯¹è¯è®°å¿†å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "status": self.status.value,
                "processing_mode": "error",
            }

    async def _process_immediate_memory(self, context: dict[str, Any], start_time: float) -> dict[str, Any]:
        """å³æ—¶è®°å¿†å¤„ç†çš„è¾…åŠ©æ–¹æ³•"""
        try:
            conversation_candidate = (
                context.get("conversation_text")
                or context.get("message_content")
                or context.get("latest_message")
                or context.get("raw_text")
                or ""
            )

            conversation_text = (
                conversation_candidate if isinstance(conversation_candidate, str) else str(conversation_candidate)
            )

            timestamp = context.get("timestamp")
            if timestamp is None:
                timestamp = time.time()

            normalized_context = self._normalize_context(context, GLOBAL_MEMORY_SCOPE, timestamp)
            normalized_context.setdefault("conversation_text", conversation_text)

            # æ£€æŸ¥ä¿¡æ¯ä»·å€¼é˜ˆå€¼
            value_score = await self._assess_information_value(conversation_text, normalized_context)
            threshold = getattr(global_config.memory, "precision_memory_reply_threshold", 0.5)

            if value_score < threshold:
                logger.debug(f"ä¿¡æ¯ä»·å€¼è¯„åˆ† {value_score:.2f} ä½äºé˜ˆå€¼ {threshold}ï¼Œè·³è¿‡è®°å¿†æ„å»º")
                return {
                    "success": True,
                    "created_memories": [],
                    "memory_count": 0,
                    "processing_time": time.time() - start_time,
                    "status": self.status.value,
                    "processing_mode": "immediate",
                    "skip_reason": f"value_score_{value_score:.2f}_below_threshold_{threshold}",
                    "value_score": value_score,
                }

            memories = await self.build_memory_from_conversation(
                conversation_text=conversation_text, context=normalized_context, timestamp=timestamp
            )

            processing_time = time.time() - start_time
            memory_count = len(memories)

            return {
                "success": True,
                "created_memories": memories,
                "memory_count": memory_count,
                "processing_time": processing_time,
                "status": self.status.value,
                "processing_mode": "immediate",
                "value_score": value_score,
            }

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å³æ—¶è®°å¿†å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "status": self.status.value,
                "processing_mode": "immediate_error",
            }

    async def retrieve_relevant_memories(
        self,
        query_text: str | None = None,
        user_id: str | None = None,
        context: dict[str, Any] | None = None,
        limit: int = 5,
        **kwargs,
    ) -> list[MemoryChunk]:
        """æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä¸‰é˜¶æ®µå¬å›ï¼šå…ƒæ•°æ®ç²—ç­› â†’ å‘é‡ç²¾ç­› â†’ ç»¼åˆé‡æ’ï¼‰"""
        raw_query = query_text or kwargs.get("query")
        if not raw_query:
            raise ValueError("query_text æˆ– query å‚æ•°ä¸èƒ½ä¸ºç©º")

        if not self.unified_storage:
            logger.warning("ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿæœªåˆå§‹åŒ–")
            return []

        context = context or {}

        # æ‰€æœ‰è®°å¿†å®Œå…¨å…±äº«ï¼Œç»Ÿä¸€ä½¿ç”¨ global ä½œç”¨åŸŸï¼Œä¸åŒºåˆ†ç”¨æˆ·
        resolved_user_id = GLOBAL_MEMORY_SCOPE

        self.status = MemorySystemStatus.RETRIEVING
        start_time = time.time()

        try:
            normalized_context = self._normalize_context(context, GLOBAL_MEMORY_SCOPE, None)
            effective_limit = self.config.final_recall_limit

            # === é˜¶æ®µä¸€ï¼šå…ƒæ•°æ®ç²—ç­›ï¼ˆè½¯æ€§è¿‡æ»¤ï¼‰ ===
            coarse_filters = {
                "user_id": GLOBAL_MEMORY_SCOPE,  # å¿…é€‰ï¼šç¡®ä¿ä½œç”¨åŸŸæ­£ç¡®
            }

            # åº”ç”¨æŸ¥è¯¢è§„åˆ’ï¼ˆä¼˜åŒ–æŸ¥è¯¢è¯­å¥å¹¶æ„å»ºå…ƒæ•°æ®è¿‡æ»¤ï¼‰
            optimized_query = raw_query
            metadata_filters = {}

            if self.query_planner:
                try:
                    # æ„å»ºåŒ…å«æœªè¯»æ¶ˆæ¯çš„å¢å¼ºä¸Šä¸‹æ–‡
                    enhanced_context = await self._build_enhanced_query_context(raw_query, normalized_context)
                    query_plan = await self.query_planner.plan_query(raw_query, enhanced_context)

                    # ä½¿ç”¨LLMä¼˜åŒ–åçš„æŸ¥è¯¢è¯­å¥ï¼ˆæ›´ç²¾ç¡®çš„è¯­ä¹‰è¡¨è¾¾ï¼‰
                    if getattr(query_plan, "semantic_query", None):
                        optimized_query = query_plan.semantic_query

                    # æ„å»ºJSONå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼ˆç”¨äºé˜¶æ®µä¸€ç²—ç­›ï¼‰
                    # å°†æŸ¥è¯¢è§„åˆ’çš„ç»“æœè½¬æ¢ä¸ºå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
                    if getattr(query_plan, "memory_types", None):
                        metadata_filters["memory_types"] = [mt.value for mt in query_plan.memory_types]

                    if getattr(query_plan, "subject_includes", None):
                        metadata_filters["subjects"] = query_plan.subject_includes

                    if getattr(query_plan, "required_keywords", None):
                        metadata_filters["keywords"] = query_plan.required_keywords

                    # æ—¶é—´èŒƒå›´è¿‡æ»¤
                    recency = getattr(query_plan, "recency_preference", "any")
                    current_time = time.time()
                    if recency == "recent":
                        # æœ€è¿‘7å¤©
                        metadata_filters["created_after"] = current_time - (7 * 24 * 3600)
                    elif recency == "historical":
                        # 30å¤©ä»¥å‰
                        metadata_filters["created_before"] = current_time - (30 * 24 * 3600)

                    # æ·»åŠ ç”¨æˆ·IDåˆ°å…ƒæ•°æ®è¿‡æ»¤
                    metadata_filters["user_id"] = GLOBAL_MEMORY_SCOPE

                    logger.debug(f"[é˜¶æ®µä¸€] æŸ¥è¯¢ä¼˜åŒ–: '{raw_query}' â†’ '{optimized_query}'")
                    logger.debug(f"[é˜¶æ®µä¸€] å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶: {metadata_filters}")

                except Exception as plan_exc:
                    logger.warning("æŸ¥è¯¢è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: %s", plan_exc, exc_info=True)
                    # å³ä½¿æŸ¥è¯¢è§„åˆ’å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸºæœ¬çš„user_idè¿‡æ»¤
                    metadata_filters = {"user_id": GLOBAL_MEMORY_SCOPE}

            # === é˜¶æ®µäºŒï¼šå‘é‡ç²¾ç­› ===
            coarse_limit = self.config.coarse_recall_limit  # ç²—ç­›é˜¶æ®µè¿”å›æ›´å¤šå€™é€‰

            logger.debug(f"[é˜¶æ®µäºŒ] å¼€å§‹å‘é‡æœç´¢: query='{optimized_query[:60]}...', limit={coarse_limit}")

            search_results = await self.unified_storage.search_similar_memories(
                query_text=optimized_query,
                limit=coarse_limit,
                filters=coarse_filters,  # ChromaDB whereæ¡ä»¶ï¼ˆä¿ç•™å…¼å®¹ï¼‰
                metadata_filters=metadata_filters,  # JSONå…ƒæ•°æ®ç´¢å¼•è¿‡æ»¤
            )

            logger.info(f"[é˜¶æ®µäºŒ] å‘é‡æœç´¢å®Œæˆ: è¿”å› {len(search_results)} æ¡å€™é€‰")

            # === é˜¶æ®µä¸‰ï¼šç»¼åˆé‡æ’ ===
            scored_memories = []
            current_time = time.time()

            for memory, vector_similarity in search_results:
                # 1. å‘é‡ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆå·²å½’ä¸€åŒ–åˆ° 0-1ï¼‰
                vector_score = vector_similarity

                # 2. æ—¶æ•ˆæ€§å¾—åˆ†ï¼ˆæŒ‡æ•°è¡°å‡ï¼Œ30å¤©åŠè¡°æœŸï¼‰
                age_seconds = current_time - memory.metadata.created_at
                age_days = age_seconds / (24 * 3600)
                # ä½¿ç”¨ math.exp è€Œé np.expï¼ˆé¿å…ä¾èµ–numpyï¼‰
                import math

                recency_score = math.exp(-age_days / 30)

                # 3. é‡è¦æ€§å¾—åˆ†ï¼ˆæšä¸¾å€¼è½¬æ¢ä¸ºå½’ä¸€åŒ–å¾—åˆ† 0-1ï¼‰
                # ImportanceLevel: LOW=1, NORMAL=2, HIGH=3, CRITICAL=4
                importance_enum = memory.metadata.importance
                if hasattr(importance_enum, "value"):
                    # æšä¸¾ç±»å‹ï¼Œè½¬æ¢ä¸º0-1èŒƒå›´ï¼š(value - 1) / 3
                    importance_score = (importance_enum.value - 1) / 3.0
                else:
                    # å¦‚æœå·²ç»æ˜¯æ•°å€¼ï¼Œç›´æ¥ä½¿ç”¨
                    importance_score = float(importance_enum) if importance_enum else 0.5

                # 4. è®¿é—®é¢‘ç‡å¾—åˆ†ï¼ˆå½’ä¸€åŒ–ï¼Œè®¿é—®10æ¬¡ä»¥ä¸Šå¾—æ»¡åˆ†ï¼‰
                access_count = memory.metadata.access_count
                frequency_score = min(access_count / 10.0, 1.0)

                # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
                final_score = (
                    self.config.vector_weight * vector_score
                    + self.config.recency_weight * recency_score
                    + self.config.context_weight * importance_score
                    + 0.1 * frequency_score  # è®¿é—®é¢‘ç‡æƒé‡ï¼ˆå›ºå®š10%ï¼‰
                )

                scored_memories.append(
                    (
                        memory,
                        final_score,
                        {
                            "vector": vector_score,
                            "recency": recency_score,
                            "importance": importance_score,
                            "frequency": frequency_score,
                            "final": final_score,
                        },
                    )
                )

                # æ›´æ–°è®¿é—®è®°å½•
                memory.update_access()

            # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
            scored_memories.sort(key=lambda x: x[1], reverse=True)

            # è¿”å› Top-K
            final_memories = [mem for mem, score, details in scored_memories[:effective_limit]]

            retrieval_time = time.time() - start_time

            # è¯¦ç»†æ—¥å¿—
            if scored_memories:
                logger.info("[é˜¶æ®µä¸‰] ç»¼åˆé‡æ’å®Œæˆ: Top 3 å¾—åˆ†è¯¦æƒ…")
                for i, (mem, score, details) in enumerate(scored_memories[:3], 1):
                    try:
                        summary = mem.content[:60] if hasattr(mem, "content") and mem.content else ""
                    except:
                        summary = ""
                    logger.info(
                        f"  #{i} | final={details['final']:.3f} "
                        f"(vec={details['vector']:.3f}, rec={details['recency']:.3f}, "
                        f"imp={details['importance']:.3f}, freq={details['frequency']:.3f}) "
                        f"| {summary}"
                    )

            logger.info(
                "âœ… ä¸‰é˜¶æ®µè®°å¿†æ£€ç´¢å®Œæˆ"
                f" | user={resolved_user_id}"
                f" | ç²—ç­›={len(search_results)}"
                f" | ç²¾ç­›={len(scored_memories)}"
                f" | è¿”å›={len(final_memories)}"
                f" | duration={retrieval_time:.3f}s"
                f" | query='{optimized_query[:60]}...'"
            )

            self.last_retrieval_time = time.time()
            self.status = MemorySystemStatus.READY

            return final_memories

        except Exception as e:
            self.status = MemorySystemStatus.ERROR
            logger.error(f"âŒ è®°å¿†æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    @staticmethod
    def _extract_json_payload(response: str) -> str | None:
        """ä»æ¨¡å‹å“åº”ä¸­æå–JSONéƒ¨åˆ†ï¼Œå…¼å®¹Markdownä»£ç å—ç­‰æ ¼å¼"""
        if not response:
            return None

        stripped = response.strip()

        # ä¼˜å…ˆå¤„ç†Markdownä»£ç å—æ ¼å¼ ```json ... ```
        code_block_match = re.search(r"```(?:json)?\s*(.*?)```", stripped, re.IGNORECASE | re.DOTALL)
        if code_block_match:
            candidate = code_block_match.group(1).strip()
            if candidate:
                return candidate

        # å›é€€åˆ°æŸ¥æ‰¾ç¬¬ä¸€ä¸ª JSON å¯¹è±¡çš„å¤§æ‹¬å·èŒƒå›´
        start = stripped.find("{")
        end = stripped.rfind("}")
        if start != -1 and end != -1 and end > start:
            return stripped[start : end + 1].strip()

        return stripped if stripped.startswith("{") and stripped.endswith("}") else None

    def _normalize_context(
        self, raw_context: dict[str, Any] | None, user_id: str | None, timestamp: float | None
    ) -> dict[str, Any]:
        """æ ‡å‡†åŒ–ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿å¿…å¤‡å­—æ®µå­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®"""
        context: dict[str, Any] = {}
        if raw_context:
            try:
                context = dict(raw_context)
            except Exception:
                context = dict(raw_context or {})

        # åŸºç¡€å­—æ®µï¼šå¼ºåˆ¶ä½¿ç”¨ä¼ å…¥çš„ user_id å‚æ•°ï¼ˆå·²ç»Ÿä¸€ä¸º GLOBAL_MEMORY_SCOPEï¼‰
        context["user_id"] = user_id or GLOBAL_MEMORY_SCOPE
        context["timestamp"] = context.get("timestamp") or timestamp or time.time()
        context["message_type"] = context.get("message_type") or "normal"
        context["platform"] = context.get("platform") or context.get("source_platform") or "unknown"

        # æ ‡å‡†åŒ–å…³é”®è¯ç±»å‹
        keywords = context.get("keywords")
        if keywords is None:
            context["keywords"] = []
        elif isinstance(keywords, tuple):
            context["keywords"] = list(keywords)
        elif not isinstance(keywords, list):
            context["keywords"] = [str(keywords)] if keywords else []

        # ç»Ÿä¸€ stream_id
        stream_id = context.get("stream_id") or context.get("stram_id")
        if not stream_id:
            potential = context.get("chat_id") or context.get("session_id")
            if isinstance(potential, str) and potential:
                stream_id = potential
        if stream_id:
            context["stream_id"] = stream_id

        # å…¨å±€è®°å¿†æ— éœ€èŠå¤©éš”ç¦»
        context["chat_id"] = context.get("chat_id") or "global_chat"

        # å†å²çª—å£é…ç½®
        window_candidate = (
            context.get("history_limit") or context.get("history_window") or context.get("memory_history_limit")
        )
        if window_candidate is not None:
            try:
                context["history_limit"] = int(window_candidate)
            except (TypeError, ValueError):
                context.pop("history_limit", None)

        return context

    async def _build_enhanced_query_context(self, raw_query: str, normalized_context: dict[str, Any]) -> dict[str, Any]:
        """æ„å»ºåŒ…å«æœªè¯»æ¶ˆæ¯ç»¼åˆä¸Šä¸‹æ–‡çš„å¢å¼ºæŸ¥è¯¢ä¸Šä¸‹æ–‡

        Args:
            raw_query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            normalized_context: æ ‡å‡†åŒ–åçš„åŸºç¡€ä¸Šä¸‹æ–‡

        Returns:
            Dict[str, Any]: åŒ…å«æœªè¯»æ¶ˆæ¯ç»¼åˆä¿¡æ¯çš„å¢å¼ºä¸Šä¸‹æ–‡
        """
        enhanced_context = dict(normalized_context)  # å¤åˆ¶åŸºç¡€ä¸Šä¸‹æ–‡

        try:
            # è·å–stream_idä»¥æŸ¥æ‰¾æœªè¯»æ¶ˆæ¯
            stream_id = normalized_context.get("stream_id")
            if not stream_id:
                logger.debug("æœªæ‰¾åˆ°stream_idï¼Œä½¿ç”¨åŸºç¡€ä¸Šä¸‹æ–‡è¿›è¡ŒæŸ¥è¯¢è§„åˆ’")
                return enhanced_context

            # è·å–æœªè¯»æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
            unread_messages_summary = await self._collect_unread_messages_context(stream_id)

            if unread_messages_summary:
                enhanced_context["unread_messages_context"] = unread_messages_summary
                enhanced_context["has_unread_context"] = True

                logger.debug(
                    f"ä¸ºæŸ¥è¯¢è§„åˆ’æ„å»ºäº†å¢å¼ºä¸Šä¸‹æ–‡ï¼ŒåŒ…å« {len(unread_messages_summary.get('messages', []))} æ¡æœªè¯»æ¶ˆæ¯"
                )
            else:
                enhanced_context["has_unread_context"] = False
                logger.debug("æœªæ‰¾åˆ°æœªè¯»æ¶ˆæ¯ï¼Œä½¿ç”¨åŸºç¡€ä¸Šä¸‹æ–‡è¿›è¡ŒæŸ¥è¯¢è§„åˆ’")

        except Exception as e:
            logger.warning(f"æ„å»ºå¢å¼ºæŸ¥è¯¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            enhanced_context["has_unread_context"] = False

        return enhanced_context

    async def _collect_unread_messages_context(self, stream_id: str) -> dict[str, Any] | None:
        """æ”¶é›†æœªè¯»æ¶ˆæ¯çš„ç»¼åˆä¸Šä¸‹æ–‡ä¿¡æ¯

        Args:
            stream_id: æµID

        Returns:
            Optional[Dict[str, Any]]: æœªè¯»æ¶ˆæ¯çš„ç»¼åˆä¿¡æ¯ï¼ŒåŒ…å«æ¶ˆæ¯åˆ—è¡¨ã€å…³é”®è¯ã€ä¸»é¢˜ç­‰
        """
        try:
            from src.chat.message_receive.chat_stream import get_chat_manager

            chat_manager = get_chat_manager()
            chat_stream = chat_manager.get_stream(stream_id)

            if not chat_stream or not hasattr(chat_stream, "context_manager"):
                logger.debug(f"æœªæ‰¾åˆ°stream_id={stream_id}çš„èŠå¤©æµæˆ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
                return None

            # è·å–æœªè¯»æ¶ˆæ¯
            context_manager = chat_stream.context_manager
            unread_messages = context_manager.get_unread_messages()

            if not unread_messages:
                logger.debug(f"stream_id={stream_id}æ²¡æœ‰æœªè¯»æ¶ˆæ¯")
                return None

            # æ„å»ºæœªè¯»æ¶ˆæ¯æ‘˜è¦
            messages_summary = []
            all_keywords = set()
            participant_names = set()

            for msg in unread_messages[:10]:  # é™åˆ¶å¤„ç†æœ€è¿‘10æ¡æœªè¯»æ¶ˆæ¯
                try:
                    # æå–æ¶ˆæ¯å†…å®¹
                    content = getattr(msg, "processed_plain_text", None) or getattr(msg, "display_message", None) or ""
                    if not content:
                        continue

                    # æå–å‘é€è€…ä¿¡æ¯
                    sender_name = "æœªçŸ¥ç”¨æˆ·"
                    if hasattr(msg, "user_info") and msg.user_info:
                        sender_name = (
                            getattr(msg.user_info, "user_nickname", None)
                            or getattr(msg.user_info, "user_cardname", None)
                            or getattr(msg.user_info, "user_id", None)
                            or "æœªçŸ¥ç”¨æˆ·"
                        )

                    participant_names.add(sender_name)

                    # æ·»åŠ åˆ°æ¶ˆæ¯æ‘˜è¦
                    messages_summary.append(
                        {
                            "sender": sender_name,
                            "content": content[:200],  # é™åˆ¶é•¿åº¦é¿å…è¿‡é•¿
                            "timestamp": getattr(msg, "time", None),
                        }
                    )

                    # æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
                    content_lower = content.lower()
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å…³é”®è¯æå–é€»è¾‘
                    words = [w.strip() for w in content_lower.split() if len(w.strip()) > 1]
                    all_keywords.update(words[:5])  # æ¯æ¡æ¶ˆæ¯æœ€å¤šå–5ä¸ªè¯

                except Exception as msg_e:
                    logger.debug(f"å¤„ç†æœªè¯»æ¶ˆæ¯æ—¶å‡ºé”™: {msg_e}")
                    continue

            if not messages_summary:
                return None

            # æ„å»ºç»¼åˆä¸Šä¸‹æ–‡ä¿¡æ¯
            unread_context = {
                "messages": messages_summary,
                "total_count": len(unread_messages),
                "processed_count": len(messages_summary),
                "keywords": list(all_keywords)[:20],  # æœ€å¤š20ä¸ªå…³é”®è¯
                "participants": list(participant_names),
                "context_summary": self._build_unread_context_summary(messages_summary),
            }

            logger.debug(
                f"æ”¶é›†åˆ°æœªè¯»æ¶ˆæ¯ä¸Šä¸‹æ–‡: {len(messages_summary)}æ¡æ¶ˆæ¯ï¼Œ{len(all_keywords)}ä¸ªå…³é”®è¯ï¼Œ{len(participant_names)}ä¸ªå‚ä¸è€…"
            )
            return unread_context

        except Exception as e:
            logger.warning(f"æ”¶é›†æœªè¯»æ¶ˆæ¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            return None

    def _build_unread_context_summary(self, messages_summary: list[dict[str, Any]]) -> str:
        """æ„å»ºæœªè¯»æ¶ˆæ¯çš„æ–‡æœ¬æ‘˜è¦

        Args:
            messages_summary: æœªè¯»æ¶ˆæ¯æ‘˜è¦åˆ—è¡¨

        Returns:
            str: æœªè¯»æ¶ˆæ¯çš„æ–‡æœ¬æ‘˜è¦
        """
        if not messages_summary:
            return ""

        summary_parts = []
        for msg_info in messages_summary:
            sender = msg_info.get("sender", "æœªçŸ¥")
            content = msg_info.get("content", "")
            if content:
                summary_parts.append(f"{sender}: {content}")

        return " | ".join(summary_parts)

    async def _resolve_conversation_context(self, fallback_text: str, context: dict[str, Any] | None) -> str:
        """ä½¿ç”¨ stream_id å†å²æ¶ˆæ¯å’Œç›¸å…³è®°å¿†å……å®å¯¹è¯æ–‡æœ¬ï¼Œé»˜è®¤å›é€€åˆ°ä¼ å…¥æ–‡æœ¬"""
        if not context:
            return fallback_text

        user_id = context.get("user_id")
        stream_id = context.get("stream_id") or context.get("stram_id")

        # ä¼˜å…ˆä½¿ç”¨ stream_id è·å–å†å²æ¶ˆæ¯
        if stream_id:
            try:
                from src.chat.message_receive.chat_stream import get_chat_manager

                chat_manager = get_chat_manager()
                chat_stream = chat_manager.get_stream(stream_id)
                if chat_stream and hasattr(chat_stream, "context_manager"):
                    history_limit = self._determine_history_limit(context)
                    messages = chat_stream.context_manager.get_messages(limit=history_limit, include_unread=True)
                    if messages:
                        transcript = self._format_history_messages(messages)
                        if transcript:
                            cleaned_fallback = (fallback_text or "").strip()
                            if cleaned_fallback and cleaned_fallback not in transcript:
                                transcript = f"{transcript}\n[å½“å‰æ¶ˆæ¯] {cleaned_fallback}"

                            logger.debug(
                                "ä½¿ç”¨ stream_id=%s çš„å†å²æ¶ˆæ¯æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡ï¼Œæ¶ˆæ¯æ•°=%dï¼Œé™åˆ¶=%d",
                                stream_id,
                                len(messages),
                                history_limit,
                            )
                            return transcript
                        else:
                            logger.debug(f"stream_id={stream_id} å†å²æ¶ˆæ¯æ ¼å¼åŒ–å¤±è´¥")
                    else:
                        logger.debug(f"stream_id={stream_id} æœªè·å–åˆ°å†å²æ¶ˆæ¯")
                else:
                    logger.debug(f"æœªæ‰¾åˆ° stream_id={stream_id} å¯¹åº”çš„èŠå¤©æµæˆ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
            except Exception as exc:
                logger.warning(f"è·å– stream_id={stream_id} çš„å†å²æ¶ˆæ¯å¤±è´¥: {exc}", exc_info=True)

        # å¦‚æœæ— æ³•è·å–å†å²æ¶ˆæ¯ï¼Œå°è¯•æ£€ç´¢ç›¸å…³è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡
        if user_id and fallback_text:
            try:
                relevant_memories = await self.retrieve_memories_for_building(
                    query_text=fallback_text, user_id=user_id, context=context, limit=3
                )

                if relevant_memories:
                    memory_contexts = []
                    for memory in relevant_memories:
                        memory_contexts.append(f"[å†å²è®°å¿†] {memory.text_content}")

                    memory_transcript = "\n".join(memory_contexts)
                    cleaned_fallback = (fallback_text or "").strip()
                    if cleaned_fallback and cleaned_fallback not in memory_transcript:
                        memory_transcript = f"{memory_transcript}\n[å½“å‰æ¶ˆæ¯] {cleaned_fallback}"

                    logger.debug(
                        "ä½¿ç”¨æ£€ç´¢åˆ°çš„å†å²è®°å¿†æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡ï¼Œè®°å¿†æ•°=%dï¼Œç”¨æˆ·=%s", len(relevant_memories), user_id
                    )
                    return memory_transcript

            except Exception as exc:
                logger.warning(f"æ£€ç´¢å†å²è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)

        # å›é€€åˆ°ä¼ å…¥æ–‡æœ¬
        return fallback_text

    def _get_build_scope_key(self, context: dict[str, Any], user_id: str | None) -> str | None:
        """ç¡®å®šç”¨äºèŠ‚æµæ§åˆ¶çš„è®°å¿†æ„å»ºä½œç”¨åŸŸ"""
        return "global_scope"

    def _determine_history_limit(self, context: dict[str, Any]) -> int:
        """ç¡®å®šå†å²æ¶ˆæ¯è·å–æ•°é‡ï¼Œé™åˆ¶åœ¨30-50ä¹‹é—´"""
        default_limit = 40
        candidate = context.get("history_limit") or context.get("history_window") or context.get("memory_history_limit")

        if isinstance(candidate, str):
            try:
                candidate = int(candidate)
            except ValueError:
                candidate = None

        if isinstance(candidate, int):
            history_limit = max(30, min(50, candidate))
        else:
            history_limit = default_limit

        return history_limit

    def _format_history_messages(self, messages: list["DatabaseMessages"]) -> str | None:
        """å°†å†å²æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºå¯ä¾›LLMå¤„ç†çš„å¤šè½®å¯¹è¯æ–‡æœ¬"""
        if not messages:
            return None

        lines: list[str] = []
        for msg in messages:
            try:
                content = getattr(msg, "processed_plain_text", None) or getattr(msg, "display_message", None)
                if not content:
                    continue

                content = re.sub(r"\s+", " ", str(content).strip())
                if not content:
                    continue

                speaker = None
                if hasattr(msg, "user_info") and msg.user_info:
                    speaker = (
                        getattr(msg.user_info, "user_nickname", None)
                        or getattr(msg.user_info, "user_cardname", None)
                        or getattr(msg.user_info, "user_id", None)
                    )
                speaker = speaker or getattr(msg, "user_nickname", None) or getattr(msg, "user_id", None) or "ç”¨æˆ·"

                timestamp_value = getattr(msg, "time", None) or 0.0
                try:
                    timestamp_dt = datetime.fromtimestamp(float(timestamp_value)) if timestamp_value else datetime.now()
                except (TypeError, ValueError, OSError):
                    timestamp_dt = datetime.now()

                timestamp_str = timestamp_dt.strftime("%Y-%m-%d %H:%M:%S")
                lines.append(f"[{timestamp_str}] {speaker}: {content}")

            except Exception as message_exc:
                logger.debug(f"æ ¼å¼åŒ–å†å²æ¶ˆæ¯å¤±è´¥: {message_exc}")
                continue

        return "\n".join(lines) if lines else None

    async def _assess_information_value(self, text: str, context: dict[str, Any]) -> float:
        """è¯„ä¼°ä¿¡æ¯ä»·å€¼

        Args:
            text: æ–‡æœ¬å†…å®¹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            ä»·å€¼è¯„åˆ† (0.0-1.0)
        """
        try:
            # æ„å»ºè¯„ä¼°æç¤º
            prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹å¯¹è¯å†…å®¹çš„ä¿¡æ¯ä»·å€¼ï¼Œé‡ç‚¹è¯†åˆ«åŒ…å«ä¸ªäººäº‹å®ã€äº‹ä»¶ã€åå¥½ã€è§‚ç‚¹ç­‰é‡è¦ä¿¡æ¯çš„å†…å®¹ã€‚

## ğŸ¯ ä»·å€¼è¯„ä¼°é‡ç‚¹æ ‡å‡†ï¼š

### é«˜ä»·å€¼ä¿¡æ¯ (0.7-1.0åˆ†)ï¼š
1. **ä¸ªäººäº‹å®** (personal_fact)ï¼šåŒ…å«å§“åã€å¹´é¾„ã€èŒä¸šã€è”ç³»æ–¹å¼ã€ä½å€ã€å¥åº·çŠ¶å†µã€å®¶åº­æƒ…å†µç­‰ä¸ªäººä¿¡æ¯
2. **é‡è¦äº‹ä»¶** (event)ï¼šçº¦ä¼šã€ä¼šè®®ã€æ—…è¡Œã€è€ƒè¯•ã€é¢è¯•ã€æ¬å®¶ç­‰é‡è¦æ´»åŠ¨æˆ–ç»å†
3. **æ˜ç¡®åå¥½** (preference)ï¼šè¡¨è¾¾å–œæ¬¢/ä¸å–œæ¬¢çš„é£Ÿç‰©ã€ç”µå½±ã€éŸ³ä¹ã€å“ç‰Œã€ç”Ÿæ´»ä¹ æƒ¯ç­‰åå¥½ä¿¡æ¯
4. **è§‚ç‚¹æ€åº¦** (opinion)ï¼šå¯¹äº‹ç‰©çš„è¯„ä»·ã€çœ‹æ³•ã€å»ºè®®ã€æ€åº¦ç­‰ä¸»è§‚è§‚ç‚¹
5. **æ ¸å¿ƒå…³ç³»** (relationship)ï¼šé‡è¦çš„æœ‹å‹ã€å®¶äººã€åŒäº‹ç­‰äººé™…å…³ç³»ä¿¡æ¯

### ä¸­ç­‰ä»·å€¼ä¿¡æ¯ (0.4-0.7åˆ†)ï¼š
1. **æƒ…æ„Ÿè¡¨è¾¾**ï¼šå½“å‰æƒ…ç»ªçŠ¶æ€ã€å¿ƒæƒ…å˜åŒ–
2. **æ—¥å¸¸æ´»åŠ¨**ï¼šå¸¸è§„çš„å·¥ä½œã€å­¦ä¹ ã€ç”Ÿæ´»å®‰æ’
3. **ä¸€èˆ¬å…´è¶£**ï¼šå…´è¶£çˆ±å¥½ã€ä¼‘é—²æ´»åŠ¨
4. **çŸ­æœŸè®¡åˆ’**ï¼šå³å°†è¿›è¡Œçš„å®‰æ’å’Œè®¡åˆ’

### ä½ä»·å€¼ä¿¡æ¯ (0.0-0.4åˆ†)ï¼š
1. **å¯’æš„é—®å€™**ï¼šç®€å•çš„æ‰“æ‹›å‘¼ã€ç¤¼è²Œç”¨è¯­
2. **é‡å¤ä¿¡æ¯**ï¼šå·²ç»å¤šæ¬¡æåˆ°çš„ç›¸åŒå†…å®¹
3. **ä¸´æ—¶çŠ¶æ€**ï¼šçŸ­æš‚çš„æƒ…ç»ªæ³¢åŠ¨ã€ä¸´æ—¶æƒ³æ³•
4. **æ— å…³å†…å®¹**ï¼šä¸ç”¨æˆ·ç”»åƒå»ºç«‹æ— å…³çš„ä¿¡æ¯

å¯¹è¯å†…å®¹ï¼š
{text}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
- ç”¨æˆ·ID: {context.get("user_id", "unknown")}
- æ¶ˆæ¯ç±»å‹: {context.get("message_type", "unknown")}
- æ—¶é—´: {datetime.fromtimestamp(context.get("timestamp", time.time()))}

## ğŸ“‹ è¯„ä¼°è¦æ±‚ï¼š

### ç§¯æè¯†åˆ«åŸåˆ™ï¼š
- **å®å¯é«˜ä¼°ï¼Œä¸å¯ä½ä¼°** - å¯¹äºå¯èƒ½çš„ä¸ªäººä¿¡æ¯ç»™äºˆè¾ƒé«˜è¯„ä¼°
- **é‡ç‚¹å…³æ³¨** - ç‰¹åˆ«æ³¨æ„åŒ…å« personal_factã€eventã€preferenceã€opinion çš„å†…å®¹
- **ç»†èŠ‚ä¸°å¯Œ** - å…·ä½“çš„ç»†èŠ‚ä¿¡æ¯æ¯”ç¬¼ç»Ÿçš„æè¿°æ›´æœ‰ä»·å€¼
- **å»ºç«‹ç”»åƒ** - æœ‰åŠ©äºå»ºç«‹å®Œæ•´ç”¨æˆ·ç”»åƒçš„ä¿¡æ¯æ›´æœ‰ä»·å€¼

### è¯„åˆ†æŒ‡å¯¼ï¼š
- **0.9-1.0**ï¼šæ ¸å¿ƒä¸ªäººä¿¡æ¯ï¼ˆå§“åã€è”ç³»æ–¹å¼ã€é‡è¦åå¥½ï¼‰
- **0.7-0.8**ï¼šé‡è¦çš„ä¸ªäººäº‹å®ã€è§‚ç‚¹ã€äº‹ä»¶ç»å†
- **0.5-0.6**ï¼šä¸€èˆ¬æ€§åå¥½ã€æ—¥å¸¸æ´»åŠ¨ã€æƒ…æ„Ÿè¡¨è¾¾
- **0.3-0.4**ï¼šç®€å•çš„å…´è¶£è¡¨è¾¾ã€ä¸´æ—¶çŠ¶æ€
- **0.0-0.2**ï¼šå¯’æš„é—®å€™ã€é‡å¤å†…å®¹ã€æ— å…³ä¿¡æ¯

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
    "value_score": 0.0åˆ°1.0ä¹‹é—´çš„æ•°å€¼,
    "reasoning": "è¯„ä¼°ç†ç”±ï¼ŒåŒ…å«å…·ä½“è¯†åˆ«åˆ°çš„ä¿¡æ¯ç±»å‹",
    "key_factors": ["å…³é”®å› ç´ 1", "å…³é”®å› ç´ 2"],
    "detected_types": ["personal_fact", "preference", "opinion", "event", "relationship", "emotion", "goal"]
}}
"""

            response, _ = await self.value_assessment_model.generate_response_async(prompt, temperature=0.3)

            # è§£æå“åº”
            try:
                payload = self._extract_json_payload(response)
                if not payload:
                    raise ValueError("æœªåœ¨å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONè´Ÿè½½")

                result = orjson.loads(payload)
                value_score = float(result.get("value_score", 0.0))
                reasoning = result.get("reasoning", "")
                key_factors = result.get("key_factors", [])

                logger.info(f"ä¿¡æ¯ä»·å€¼è¯„ä¼°: {value_score:.2f}, ç†ç”±: {reasoning}")
                if key_factors:
                    logger.info(f"å…³é”®å› ç´ : {', '.join(key_factors)}")

                return max(0.0, min(1.0, value_score))

            except (orjson.JSONDecodeError, ValueError) as e:
                preview = response[:200].replace("\n", " ")
                logger.warning(f"è§£æä»·å€¼è¯„ä¼°å“åº”å¤±è´¥: {e}, å“åº”ç‰‡æ®µ: {preview}")
                return 0.5  # é»˜è®¤ä¸­ç­‰ä»·å€¼

        except Exception as e:
            logger.error(f"ä¿¡æ¯ä»·å€¼è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
            return 0.5  # é»˜è®¤ä¸­ç­‰ä»·å€¼

    async def _store_memories_unified(self, memory_chunks: list[MemoryChunk]) -> int:
        """ä½¿ç”¨ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿå­˜å‚¨è®°å¿†å—"""
        if not memory_chunks or not self.unified_storage:
            return 0

        try:
            # ç›´æ¥å­˜å‚¨åˆ°ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿ
            stored_count = await self.unified_storage.store_memories(memory_chunks)

            logger.debug(
                "ç»Ÿä¸€å­˜å‚¨æˆåŠŸå­˜å‚¨ %d æ¡è®°å¿†",
                stored_count,
            )

            return stored_count

        except Exception as e:
            logger.error(f"ç»Ÿä¸€å­˜å‚¨è®°å¿†å¤±è´¥: {e}", exc_info=True)
            return 0

    # ä¿ç•™åŸæœ‰æ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç 
    async def _store_memories(self, memory_chunks: list[MemoryChunk]) -> int:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šé‡å®šå‘åˆ°ç»Ÿä¸€å­˜å‚¨"""
        return await self._store_memories_unified(memory_chunks)

    def _merge_existing_memory(self, existing: MemoryChunk, incoming: MemoryChunk) -> None:
        """å°†æ–°è®°å¿†çš„ä¿¡æ¯åˆå¹¶åˆ°å·²å­˜åœ¨çš„è®°å¿†ä¸­"""
        updated = False

        for keyword in incoming.keywords:
            if keyword not in existing.keywords:
                existing.add_keyword(keyword)
                updated = True

        for tag in incoming.tags:
            if tag not in existing.tags:
                existing.add_tag(tag)
                updated = True

        for category in incoming.categories:
            if category not in existing.categories:
                existing.add_category(category)
                updated = True

        if incoming.metadata.source_context:
            existing.metadata.source_context = incoming.metadata.source_context

        if incoming.metadata.importance.value > existing.metadata.importance.value:
            existing.metadata.importance = incoming.metadata.importance
            updated = True

        if incoming.metadata.confidence.value > existing.metadata.confidence.value:
            existing.metadata.confidence = incoming.metadata.confidence
            updated = True

        if incoming.metadata.relevance_score > existing.metadata.relevance_score:
            existing.metadata.relevance_score = incoming.metadata.relevance_score
            updated = True

        if updated:
            existing.metadata.last_modified = time.time()

    def _populate_memory_fingerprints(self) -> None:
        """åŸºäºå½“å‰ç¼“å­˜æ„å»ºè®°å¿†æŒ‡çº¹æ˜ å°„"""
        self._memory_fingerprints.clear()
        for memory in self.unified_storage.memory_cache.values():
            fingerprint = self._build_memory_fingerprint(memory)
            key = self._fingerprint_key(memory.user_id, fingerprint)
            self._memory_fingerprints[key] = memory.memory_id

    def _register_memory_fingerprints(self, memories: list[MemoryChunk]) -> None:
        for memory in memories:
            fingerprint = self._build_memory_fingerprint(memory)
            key = self._fingerprint_key(memory.user_id, fingerprint)
            self._memory_fingerprints[key] = memory.memory_id

    def _build_memory_fingerprint(self, memory: MemoryChunk) -> str:
        subjects = memory.subjects or []
        subject_part = "|".join(sorted(s.strip() for s in subjects if s))
        predicate_part = (memory.content.predicate or "").strip()

        obj = memory.content.object
        if isinstance(obj, (dict, list)):
            obj_part = orjson.dumps(obj, option=orjson.OPT_SORT_KEYS).decode("utf-8")
        else:
            obj_part = str(obj).strip()

        base = "|".join(
            [
                str(memory.user_id or "unknown"),
                memory.memory_type.value,
                subject_part,
                predicate_part,
                obj_part,
            ]
        )

        return hashlib.sha256(base.encode("utf-8")).hexdigest()

    @staticmethod
    def _fingerprint_key(user_id: str, fingerprint: str) -> str:
        return f"{user_id!s}:{fingerprint}"

    def _compute_memory_score(self, query_text: str, memory: MemoryChunk, context: dict[str, Any]) -> float:
        """æ ¹æ®æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¸ºè®°å¿†è®¡ç®—åŒ¹é…åˆ†æ•°"""
        tokens_query = self._tokenize_text(query_text)
        tokens_memory = self._tokenize_text(memory.text_content)

        if tokens_query and tokens_memory:
            base_score = len(tokens_query & tokens_memory) / len(tokens_query | tokens_memory)
        else:
            base_score = 0.0

        context_keywords = context.get("keywords") or []
        keyword_overlap = 0.0
        if context_keywords:
            memory_keywords = set(k.lower() for k in memory.keywords)
            keyword_overlap = len(memory_keywords & set(k.lower() for k in context_keywords)) / max(
                len(context_keywords), 1
            )

        importance_boost = (memory.metadata.importance.value - 1) / 3 * 0.1
        confidence_boost = (memory.metadata.confidence.value - 1) / 3 * 0.05

        final_score = base_score * 0.7 + keyword_overlap * 0.15 + importance_boost + confidence_boost
        return max(0.0, min(1.0, final_score))

    def _tokenize_text(self, text: str) -> set[str]:
        """ç®€å•åˆ†è¯ï¼Œå…¼å®¹ä¸­è‹±æ–‡"""
        if not text:
            return set()

        tokens = re.findall(r"[\w\u4e00-\u9fa5]+", text.lower())
        return {token for token in tokens if len(token) > 1}

    async def maintenance(self):
        """ç³»ç»Ÿç»´æŠ¤æ“ä½œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            logger.info("å¼€å§‹ç®€åŒ–è®°å¿†ç³»ç»Ÿç»´æŠ¤...")

            # æ‰§è¡Œé—å¿˜æ£€æŸ¥
            if self.unified_storage and self.forgetting_engine:
                forgetting_result = await self.unified_storage.perform_forgetting_check()
                if "error" not in forgetting_result:
                    logger.info(f"é—å¿˜æ£€æŸ¥å®Œæˆ: {forgetting_result.get('stats', {})}")
                else:
                    logger.warning(f"é—å¿˜æ£€æŸ¥å¤±è´¥: {forgetting_result['error']}")

            # ä¿å­˜å­˜å‚¨æ•°æ®
            if self.unified_storage:
                await self.unified_storage.save_storage()

            # è®°å¿†èåˆå¼•æ“ç»´æŠ¤
            if self.fusion_engine:
                await self.fusion_engine.maintenance()

            logger.info("âœ… ç®€åŒ–è®°å¿†ç³»ç»Ÿç»´æŠ¤å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ è®°å¿†ç³»ç»Ÿç»´æŠ¤å¤±è´¥: {e}", exc_info=True)

    def start_hippocampus_sampling(self):
        """å¯åŠ¨æµ·é©¬ä½“é‡‡æ ·"""
        if self.hippocampus_sampler:
            asyncio.create_task(self.hippocampus_sampler.start_background_sampling())
            logger.info("æµ·é©¬ä½“åå°é‡‡æ ·å·²å¯åŠ¨")
        else:
            logger.warning("æµ·é©¬ä½“é‡‡æ ·å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¯åŠ¨é‡‡æ ·")

    def stop_hippocampus_sampling(self):
        """åœæ­¢æµ·é©¬ä½“é‡‡æ ·"""
        if self.hippocampus_sampler:
            self.hippocampus_sampler.stop_background_sampling()
            logger.info("æµ·é©¬ä½“åå°é‡‡æ ·å·²åœæ­¢")

    def get_system_stats(self) -> dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        base_stats = {
            "status": self.status.value,
            "total_memories": self.total_memories,
            "last_build_time": self.last_build_time,
            "last_retrieval_time": self.last_retrieval_time,
            "config": asdict(self.config),
        }

        # æ·»åŠ æµ·é©¬ä½“é‡‡æ ·å™¨ç»Ÿè®¡
        if self.hippocampus_sampler:
            base_stats["hippocampus_sampler"] = self.hippocampus_sampler.get_sampling_stats()

        # æ·»åŠ å­˜å‚¨ç»Ÿè®¡
        if self.unified_storage:
            try:
                storage_stats = self.unified_storage.get_storage_stats()
                base_stats["storage_stats"] = storage_stats
            except Exception as e:
                logger.debug(f"è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}")

        return base_stats

    async def shutdown(self):
        """å…³é—­ç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            logger.info("æ­£åœ¨å…³é—­ç®€åŒ–è®°å¿†ç³»ç»Ÿ...")

            # åœæ­¢æµ·é©¬ä½“é‡‡æ ·
            if self.hippocampus_sampler:
                self.hippocampus_sampler.stop_background_sampling()

            # ä¿å­˜ç»Ÿä¸€å­˜å‚¨æ•°æ®
            if self.unified_storage:
                self.unified_storage.cleanup()

            logger.info("ç®€åŒ–è®°å¿†ç³»ç»Ÿå·²å…³é—­")

        except Exception as e:
            logger.error(f"è®°å¿†ç³»ç»Ÿå…³é—­å¤±è´¥: {e}", exc_info=True)

    async def _rebuild_vector_storage_if_needed(self):
        """é‡å»ºå‘é‡å­˜å‚¨ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è®°å¿†ç¼“å­˜æ•°æ®
            if not hasattr(self.unified_storage, "memory_cache") or not self.unified_storage.memory_cache:
                logger.info("æ— è®°å¿†ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡å‘é‡å­˜å‚¨é‡å»º")
                return

            logger.info(f"å¼€å§‹é‡å»ºå‘é‡å­˜å‚¨ï¼Œè®°å¿†æ•°é‡: {len(self.unified_storage.memory_cache)}")

            # æ”¶é›†éœ€è¦é‡å»ºå‘é‡çš„è®°å¿†
            memories_to_rebuild = []
            for memory_id, memory in self.unified_storage.memory_cache.items():
                # æ£€æŸ¥è®°å¿†æ˜¯å¦æœ‰æœ‰æ•ˆçš„ display æ–‡æœ¬
                if memory.display and memory.display.strip():
                    memories_to_rebuild.append(memory)
                elif memory.text_content and memory.text_content.strip():
                    memories_to_rebuild.append(memory)

            if not memories_to_rebuild:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯é‡å»ºå‘é‡çš„è®°å¿†")
                return

            logger.info(f"å‡†å¤‡ä¸º {len(memories_to_rebuild)} æ¡è®°å¿†é‡å»ºå‘é‡")

            # æ‰¹é‡é‡å»ºå‘é‡
            batch_size = 10
            rebuild_count = 0

            for i in range(0, len(memories_to_rebuild), batch_size):
                batch = memories_to_rebuild[i : i + batch_size]
                try:
                    await self.unified_storage.store_memories(batch)
                    rebuild_count += len(batch)

                    if rebuild_count % 50 == 0:
                        logger.info(f"å·²é‡å»ºå‘é‡: {rebuild_count}/{len(memories_to_rebuild)}")

                except Exception as e:
                    logger.error(f"æ‰¹é‡é‡å»ºå‘é‡å¤±è´¥: {e}")
                    continue

            # å‘é‡æ•°æ®åœ¨ store_memories ä¸­å·²ä¿å­˜ï¼Œæ­¤å¤„æ— éœ€é¢å¤–æ“ä½œ
            if self.unified_storage:
                storage_stats = self.unified_storage.get_storage_stats()
                final_count = storage_stats.get("total_vectors", 0)
                logger.info(f"âœ… å‘é‡å­˜å‚¨é‡å»ºå®Œæˆï¼Œæœ€ç»ˆå‘é‡æ•°é‡: {final_count}")
            else:
                logger.warning("å‘é‡å­˜å‚¨é‡å»ºå®Œæˆï¼Œä½†æ— æ³•è·å–æœ€ç»ˆå‘é‡æ•°é‡ï¼Œå› ä¸ºå­˜å‚¨ç³»ç»Ÿæœªåˆå§‹åŒ–")

        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨é‡å»ºå¤±è´¥: {e}", exc_info=True)


# å…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹
memory_system: MemorySystem = None


def get_memory_system() -> MemorySystem:
    """è·å–å…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹"""
    global memory_system
    if memory_system is None:
        memory_system = MemorySystem()
    return memory_system


async def initialize_memory_system(llm_model: LLMRequest | None = None):
    """åˆå§‹åŒ–å…¨å±€è®°å¿†ç³»ç»Ÿ"""
    global memory_system
    if memory_system is None:
        memory_system = MemorySystem(llm_model=llm_model)
    await memory_system.initialize()

    # æ ¹æ®é…ç½®å¯åŠ¨æµ·é©¬ä½“é‡‡æ ·
    sampling_mode = getattr(global_config.memory, "memory_sampling_mode", "immediate")
    if sampling_mode in ["hippocampus", "all"]:
        memory_system.start_hippocampus_sampling()

    return memory_system
