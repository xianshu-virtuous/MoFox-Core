"""
æµ·é©¬ä½“åŒå³°åˆ†å¸ƒé‡‡æ ·å™¨
åŸºäºæ—§ç‰ˆæµ·é©¬ä½“çš„é‡‡æ ·ç­–ç•¥ï¼Œé€‚é…æ–°ç‰ˆè®°å¿†ç³»ç»Ÿ
å®ç°ä½æ¶ˆè€—ã€é«˜æ•ˆç‡çš„è®°å¿†é‡‡æ ·æ¨¡å¼
"""

import asyncio
import random
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any

import numpy as np

from src.chat.utils.chat_message_builder import (
    build_readable_messages,
    get_raw_msg_by_timestamp,
    get_raw_msg_by_timestamp_with_chat,
)
from src.chat.utils.utils import translate_timestamp_to_human_readable
from src.common.logger import get_logger
from src.config.config import global_config
from src.llm_models.utils_model import LLMRequest

logger = get_logger(__name__)


@dataclass
class HippocampusSampleConfig:
    """æµ·é©¬ä½“é‡‡æ ·é…ç½®"""

    # åŒå³°åˆ†å¸ƒå‚æ•°
    recent_mean_hours: float = 12.0  # è¿‘æœŸåˆ†å¸ƒå‡å€¼ï¼ˆå°æ—¶ï¼‰
    recent_std_hours: float = 8.0  # è¿‘æœŸåˆ†å¸ƒæ ‡å‡†å·®ï¼ˆå°æ—¶ï¼‰
    recent_weight: float = 0.7  # è¿‘æœŸåˆ†å¸ƒæƒé‡

    distant_mean_hours: float = 48.0  # è¿œæœŸåˆ†å¸ƒå‡å€¼ï¼ˆå°æ—¶ï¼‰
    distant_std_hours: float = 24.0  # è¿œæœŸåˆ†å¸ƒæ ‡å‡†å·®ï¼ˆå°æ—¶ï¼‰
    distant_weight: float = 0.3  # è¿œæœŸåˆ†å¸ƒæƒé‡

    # é‡‡æ ·å‚æ•°
    total_samples: int = 50  # æ€»é‡‡æ ·æ•°
    sample_interval: int = 1800  # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
    max_sample_length: int = 30  # æ¯æ¬¡é‡‡æ ·çš„æœ€å¤§æ¶ˆæ¯æ•°é‡
    batch_size: int = 5  # æ‰¹å¤„ç†å¤§å°

    @classmethod
    def from_global_config(cls) -> "HippocampusSampleConfig":
        """ä»å…¨å±€é…ç½®åˆ›å»ºæµ·é©¬ä½“é‡‡æ ·é…ç½®"""
        config = global_config.memory.hippocampus_distribution_config
        return cls(
            recent_mean_hours=config[0],
            recent_std_hours=config[1],
            recent_weight=config[2],
            distant_mean_hours=config[3],
            distant_std_hours=config[4],
            distant_weight=config[5],
            total_samples=global_config.memory.hippocampus_sample_size,
            sample_interval=global_config.memory.hippocampus_sample_interval,
            max_sample_length=global_config.memory.hippocampus_batch_size,
            batch_size=global_config.memory.hippocampus_batch_size,
        )


class HippocampusSampler:
    """æµ·é©¬ä½“åŒå³°åˆ†å¸ƒé‡‡æ ·å™¨"""

    def __init__(self, memory_system=None):
        self.memory_system = memory_system
        self.config = HippocampusSampleConfig.from_global_config()
        self.last_sample_time = 0
        self.is_running = False

        # è®°å¿†æ„å»ºæ¨¡å‹
        self.memory_builder_model: LLMRequest | None = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.sample_count = 0
        self.success_count = 0
        self.last_sample_results: list[dict[str, Any]] = []

    async def initialize(self):
        """åˆå§‹åŒ–é‡‡æ ·å™¨"""
        try:
            # åˆå§‹åŒ–LLMæ¨¡å‹
            from src.config.config import model_config

            task_config = getattr(model_config.model_task_config, "utils", None)
            if task_config:
                self.memory_builder_model = LLMRequest(model_set=task_config, request_type="memory.hippocampus_build")
                asyncio.create_task(self.start_background_sampling())
                logger.info("âœ… æµ·é©¬ä½“é‡‡æ ·å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                raise RuntimeError("æœªæ‰¾åˆ°è®°å¿†æ„å»ºæ¨¡å‹é…ç½®")

        except Exception as e:
            logger.error(f"âŒ æµ·é©¬ä½“é‡‡æ ·å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def generate_time_samples(self) -> list[datetime]:
        """ç”ŸæˆåŒå³°åˆ†å¸ƒçš„æ—¶é—´é‡‡æ ·ç‚¹"""
        # è®¡ç®—æ¯ä¸ªåˆ†å¸ƒçš„æ ·æœ¬æ•°
        recent_samples = max(1, int(self.config.total_samples * self.config.recent_weight))
        distant_samples = max(1, self.config.total_samples - recent_samples)

        # ç”Ÿæˆä¸¤ä¸ªæ­£æ€åˆ†å¸ƒçš„å°æ—¶åç§»
        recent_offsets = np.random.normal(
            loc=self.config.recent_mean_hours, scale=self.config.recent_std_hours, size=recent_samples
        )
        distant_offsets = np.random.normal(
            loc=self.config.distant_mean_hours, scale=self.config.distant_std_hours, size=distant_samples
        )

        # åˆå¹¶ä¸¤ä¸ªåˆ†å¸ƒçš„åç§»
        all_offsets = np.concatenate([recent_offsets, distant_offsets])

        # è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆä½¿ç”¨ç»å¯¹å€¼ç¡®ä¿æ—¶é—´ç‚¹åœ¨è¿‡å»ï¼‰
        base_time = datetime.now()
        timestamps = [base_time - timedelta(hours=abs(offset)) for offset in all_offsets]

        # æŒ‰æ—¶é—´æ’åºï¼ˆä»æœ€æ—©åˆ°æœ€è¿‘ï¼‰
        return sorted(timestamps)

    async def collect_message_samples(self, target_timestamp: float) -> list[dict[str, Any]] | None:
        """æ”¶é›†æŒ‡å®šæ—¶é—´æˆ³é™„è¿‘çš„æ¶ˆæ¯æ ·æœ¬"""
        try:
            # éšæœºæ—¶é—´çª—å£ï¼š5-30åˆ†é’Ÿ
            time_window_seconds = random.randint(300, 1800)

            # å°è¯•3æ¬¡è·å–æ¶ˆæ¯
            for attempt in range(3):
                timestamp_start = target_timestamp
                timestamp_end = target_timestamp + time_window_seconds

                # è·å–å•æ¡æ¶ˆæ¯ä½œä¸ºé”šç‚¹
                anchor_messages = await get_raw_msg_by_timestamp(
                    timestamp_start=timestamp_start,
                    timestamp_end=timestamp_end,
                    limit=1,
                    limit_mode="earliest",
                )

                if not anchor_messages:
                    target_timestamp -= 120  # å‘å‰è°ƒæ•´2åˆ†é’Ÿ
                    continue

                anchor_message = anchor_messages[0]
                chat_id = anchor_message.get("chat_id")

                if not chat_id:
                    continue

                # è·å–åŒèŠå¤©çš„å¤šæ¡æ¶ˆæ¯
                messages = await get_raw_msg_by_timestamp_with_chat(
                    timestamp_start=timestamp_start,
                    timestamp_end=timestamp_end,
                    limit=self.config.max_sample_length,
                    limit_mode="earliest",
                    chat_id=chat_id,
                )

                if messages and len(messages) >= 2:  # è‡³å°‘éœ€è¦2æ¡æ¶ˆæ¯
                    # è¿‡æ»¤æ‰å·²ç»è®°å¿†è¿‡çš„æ¶ˆæ¯
                    filtered_messages = [
                        msg
                        for msg in messages
                        if msg.get("memorized_times", 0) < 2  # æœ€å¤šè®°å¿†2æ¬¡
                    ]

                    if filtered_messages:
                        logger.debug(f"æˆåŠŸæ”¶é›† {len(filtered_messages)} æ¡æ¶ˆæ¯æ ·æœ¬")
                        return filtered_messages

                target_timestamp -= 120  # å‘å‰è°ƒæ•´å†è¯•

            logger.debug(f"æ—¶é—´æˆ³ {target_timestamp} é™„è¿‘æœªæ‰¾åˆ°æœ‰æ•ˆæ¶ˆæ¯æ ·æœ¬")
            return None

        except Exception as e:
            logger.error(f"æ”¶é›†æ¶ˆæ¯æ ·æœ¬å¤±è´¥: {e}")
            return None

    async def build_memory_from_samples(self, messages: list[dict[str, Any]], target_timestamp: float) -> str | None:
        """ä»æ¶ˆæ¯æ ·æœ¬æ„å»ºè®°å¿†"""
        if not messages or not self.memory_system or not self.memory_builder_model:
            return None

        try:
            # æ„å»ºå¯è¯»æ¶ˆæ¯æ–‡æœ¬
            readable_text = await build_readable_messages(
                messages,
                merge_messages=True,
                timestamp_mode="normal_no_YMD",
                replace_bot_name=False,
            )

            if not readable_text:
                logger.warning("æ— æ³•ä»æ¶ˆæ¯æ ·æœ¬ç”Ÿæˆå¯è¯»æ–‡æœ¬")
                return None

            # æ·»åŠ å½“å‰æ—¥æœŸä¿¡æ¯
            current_date = f"å½“å‰æ—¥æœŸ: {datetime.now().isoformat()}"
            input_text = f"{current_date}\n{readable_text}"

            logger.debug(f"å¼€å§‹æ„å»ºè®°å¿†ï¼Œæ–‡æœ¬é•¿åº¦: {len(input_text)}")

            # æ„å»ºä¸Šä¸‹æ–‡
            context = {
                "user_id": "hippocampus_sampler",
                "timestamp": time.time(),
                "source": "hippocampus_sampling",
                "message_count": len(messages),
                "sample_mode": "bimodal_distribution",
                "is_hippocampus_sample": True,  # æ ‡è¯†ä¸ºæµ·é©¬ä½“æ ·æœ¬
                "bypass_value_threshold": True,  # ç»•è¿‡ä»·å€¼é˜ˆå€¼æ£€æŸ¥
                "hippocampus_sample_time": target_timestamp,  # è®°å½•æ ·æœ¬æ—¶é—´
            }

            # ä½¿ç”¨è®°å¿†ç³»ç»Ÿæ„å»ºè®°å¿†ï¼ˆç»•è¿‡æ„å»ºé—´éš”æ£€æŸ¥ï¼‰
            memories = await self.memory_system.build_memory_from_conversation(
                conversation_text=input_text,
                context=context,
                timestamp=time.time(),
                bypass_interval=True,  # æµ·é©¬ä½“é‡‡æ ·å™¨ç»•è¿‡æ„å»ºé—´éš”é™åˆ¶
            )

            if memories:
                memory_count = len(memories)
                self.success_count += 1

                # è®°å½•é‡‡æ ·ç»“æœ
                result = {
                    "timestamp": time.time(),
                    "memory_count": memory_count,
                    "message_count": len(messages),
                    "text_preview": readable_text[:100] + "..." if len(readable_text) > 100 else readable_text,
                    "memory_types": [m.memory_type.value for m in memories],
                }
                self.last_sample_results.append(result)

                # é™åˆ¶ç»“æœå†å²é•¿åº¦
                if len(self.last_sample_results) > 10:
                    self.last_sample_results.pop(0)

                logger.info(f"âœ… æµ·é©¬ä½“é‡‡æ ·æˆåŠŸæ„å»º {memory_count} æ¡è®°å¿†")
                return f"æ„å»º{memory_count}æ¡è®°å¿†"
            else:
                logger.debug("æµ·é©¬ä½“é‡‡æ ·æœªç”Ÿæˆæœ‰æ•ˆè®°å¿†")
                return None

        except Exception as e:
            logger.error(f"æµ·é©¬ä½“é‡‡æ ·æ„å»ºè®°å¿†å¤±è´¥: {e}")
            return None

    async def perform_sampling_cycle(self) -> dict[str, Any]:
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„é‡‡æ ·å‘¨æœŸï¼ˆä¼˜åŒ–ç‰ˆï¼šæ‰¹é‡èåˆæ„å»ºï¼‰"""
        if not self.should_sample():
            return {"status": "skipped", "reason": "interval_not_met"}

        start_time = time.time()
        self.sample_count += 1

        try:
            # ç”Ÿæˆæ—¶é—´é‡‡æ ·ç‚¹
            time_samples = self.generate_time_samples()
            logger.debug(f"ç”Ÿæˆ {len(time_samples)} ä¸ªæ—¶é—´é‡‡æ ·ç‚¹")

            # è®°å½•æ—¶é—´é‡‡æ ·ç‚¹ï¼ˆè°ƒè¯•ç”¨ï¼‰
            readable_timestamps = [
                translate_timestamp_to_human_readable(int(ts.timestamp()), mode="normal")
                for ts in time_samples[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            ]
            logger.debug(f"æ—¶é—´é‡‡æ ·ç‚¹ç¤ºä¾‹: {readable_timestamps}")

            # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æ”¶é›†æ‰€æœ‰æ¶ˆæ¯æ ·æœ¬
            logger.debug("å¼€å§‹æ‰¹é‡æ”¶é›†æ¶ˆæ¯æ ·æœ¬...")
            collected_messages = await self._collect_all_message_samples(time_samples)

            if not collected_messages:
                logger.info("æœªæ”¶é›†åˆ°æœ‰æ•ˆæ¶ˆæ¯æ ·æœ¬ï¼Œè·³è¿‡æœ¬æ¬¡é‡‡æ ·")
                self.last_sample_time = time.time()
                return {
                    "status": "success",
                    "sample_count": self.sample_count,
                    "success_count": self.success_count,
                    "processed_samples": len(time_samples),
                    "successful_builds": 0,
                    "duration": time.time() - start_time,
                    "samples_generated": len(time_samples),
                    "message": "æœªæ”¶é›†åˆ°æœ‰æ•ˆæ¶ˆæ¯æ ·æœ¬",
                }

            logger.info(f"æ”¶é›†åˆ° {len(collected_messages)} ç»„æ¶ˆæ¯æ ·æœ¬")

            # ç¬¬äºŒæ­¥ï¼šèåˆå’Œå»é‡æ¶ˆæ¯
            logger.debug("å¼€å§‹èåˆå’Œå»é‡æ¶ˆæ¯...")
            fused_messages = await self._fuse_and_deduplicate_messages(collected_messages)

            if not fused_messages:
                logger.info("æ¶ˆæ¯èåˆåä¸ºç©ºï¼Œè·³è¿‡è®°å¿†æ„å»º")
                self.last_sample_time = time.time()
                return {
                    "status": "success",
                    "sample_count": self.sample_count,
                    "success_count": self.success_count,
                    "processed_samples": len(time_samples),
                    "successful_builds": 0,
                    "duration": time.time() - start_time,
                    "samples_generated": len(time_samples),
                    "message": "æ¶ˆæ¯èåˆåä¸ºç©º",
                }

            logger.info(f"èåˆåå¾—åˆ° {len(fused_messages)} ç»„æœ‰æ•ˆæ¶ˆæ¯")

            # ç¬¬ä¸‰æ­¥ï¼šä¸€æ¬¡æ€§æ„å»ºè®°å¿†
            logger.debug("å¼€å§‹æ‰¹é‡æ„å»ºè®°å¿†...")
            build_result = await self._build_batch_memory(fused_messages, time_samples)

            # æ›´æ–°æœ€åé‡‡æ ·æ—¶é—´
            self.last_sample_time = time.time()

            duration = time.time() - start_time
            result = {
                "status": "success",
                "sample_count": self.sample_count,
                "success_count": self.success_count,
                "processed_samples": len(time_samples),
                "successful_builds": build_result.get("memory_count", 0),
                "duration": duration,
                "samples_generated": len(time_samples),
                "messages_collected": len(collected_messages),
                "messages_fused": len(fused_messages),
                "optimization_mode": "batch_fusion",
            }

            logger.info(
                f"âœ… æµ·é©¬ä½“é‡‡æ ·å‘¨æœŸå®Œæˆï¼ˆæ‰¹é‡èåˆæ¨¡å¼ï¼‰ | "
                f"é‡‡æ ·ç‚¹: {len(time_samples)} | "
                f"æ”¶é›†æ¶ˆæ¯: {len(collected_messages)} | "
                f"èåˆæ¶ˆæ¯: {len(fused_messages)} | "
                f"æ„å»ºè®°å¿†: {build_result.get('memory_count', 0)} | "
                f"è€—æ—¶: {duration:.2f}s"
            )

            return result

        except Exception as e:
            logger.error(f"âŒ æµ·é©¬ä½“é‡‡æ ·å‘¨æœŸå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "sample_count": self.sample_count,
                "duration": time.time() - start_time,
            }

    async def _collect_all_message_samples(self, time_samples: list[datetime]) -> list[list[dict[str, Any]]]:
        """æ‰¹é‡æ”¶é›†æ‰€æœ‰æ—¶é—´ç‚¹çš„æ¶ˆæ¯æ ·æœ¬"""
        collected_messages = []
        max_concurrent = min(5, len(time_samples))  # æé«˜å¹¶å‘æ•°åˆ°5

        for i in range(0, len(time_samples), max_concurrent):
            batch = time_samples[i : i + max_concurrent]
            tasks = []

            # åˆ›å»ºå¹¶å‘æ”¶é›†ä»»åŠ¡
            for timestamp in batch:
                target_ts = timestamp.timestamp()
                task = self.collect_message_samples(target_ts)
                tasks.append(task)

            # æ‰§è¡Œå¹¶å‘æ”¶é›†
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†æ”¶é›†ç»“æœ
            for result in results:
                if isinstance(result, list) and result:
                    collected_messages.append(result)
                elif isinstance(result, Exception):
                    logger.debug(f"æ¶ˆæ¯æ”¶é›†å¼‚å¸¸: {result}")

            # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ
            if i + max_concurrent < len(time_samples):
                await asyncio.sleep(0.5)

        return collected_messages

    async def _fuse_and_deduplicate_messages(
        self, collected_messages: list[list[dict[str, Any]]]
    ) -> list[list[dict[str, Any]]]:
        """èåˆå’Œå»é‡æ¶ˆæ¯æ ·æœ¬"""
        if not collected_messages:
            return []

        try:
            # å±•å¹³æ‰€æœ‰æ¶ˆæ¯
            all_messages = []
            for message_group in collected_messages:
                all_messages.extend(message_group)

            logger.debug(f"å±•å¼€åæ€»æ¶ˆæ¯æ•°: {len(all_messages)}")

            # å»é‡é€»è¾‘ï¼šåŸºäºæ¶ˆæ¯å†…å®¹å’Œæ—¶é—´æˆ³
            unique_messages = []
            seen_hashes = set()

            for message in all_messages:
                # åˆ›å»ºæ¶ˆæ¯å“ˆå¸Œç”¨äºå»é‡
                content = message.get("processed_plain_text", "") or message.get("display_message", "")
                timestamp = message.get("time", 0)
                chat_id = message.get("chat_id", "")

                # ç®€å•å“ˆå¸Œï¼šå†…å®¹å‰50å­—ç¬¦ + æ—¶é—´æˆ³(ç²¾ç¡®åˆ°åˆ†é’Ÿ) + èŠå¤©ID
                hash_key = f"{content[:50]}_{int(timestamp // 60)}_{chat_id}"

                if hash_key not in seen_hashes and len(content.strip()) > 10:
                    seen_hashes.add(hash_key)
                    unique_messages.append(message)

            logger.debug(f"å»é‡åæ¶ˆæ¯æ•°: {len(unique_messages)}")

            # æŒ‰æ—¶é—´æ’åº
            unique_messages.sort(key=lambda x: x.get("time", 0))

            # æŒ‰èŠå¤©IDåˆ†ç»„é‡æ–°ç»„ç»‡
            chat_groups = {}
            for message in unique_messages:
                chat_id = message.get("chat_id", "unknown")
                if chat_id not in chat_groups:
                    chat_groups[chat_id] = []
                chat_groups[chat_id].append(message)

            # åˆå¹¶ç›¸é‚»æ—¶é—´èŒƒå›´å†…çš„æ¶ˆæ¯
            fused_groups = []
            for chat_id, messages in chat_groups.items():
                fused_groups.extend(self._merge_adjacent_messages(messages))

            logger.debug(f"èåˆåæ¶ˆæ¯ç»„æ•°: {len(fused_groups)}")
            return fused_groups

        except Exception as e:
            logger.error(f"æ¶ˆæ¯èåˆå¤±è´¥: {e}")
            # è¿”å›åŸå§‹æ¶ˆæ¯ç»„ä½œä¸ºå¤‡é€‰
            return collected_messages[:5]  # é™åˆ¶è¿”å›æ•°é‡

    def _merge_adjacent_messages(
        self, messages: list[dict[str, Any]], time_gap: int = 1800
    ) -> list[list[dict[str, Any]]]:
        """åˆå¹¶æ—¶é—´é—´éš”å†…çš„æ¶ˆæ¯"""
        if not messages:
            return []

        merged_groups = []
        current_group = [messages[0]]

        for i in range(1, len(messages)):
            current_time = messages[i].get("time", 0)
            prev_time = current_group[-1].get("time", 0)

            # å¦‚æœæ—¶é—´é—´éš”å°äºé˜ˆå€¼ï¼Œåˆå¹¶åˆ°å½“å‰ç»„
            if current_time - prev_time <= time_gap:
                current_group.append(messages[i])
            else:
                # å¦åˆ™å¼€å§‹æ–°ç»„
                merged_groups.append(current_group)
                current_group = [messages[i]]

        # æ·»åŠ æœ€åä¸€ç»„
        merged_groups.append(current_group)

        # è¿‡æ»¤æ‰åªæœ‰ä¸€æ¡æ¶ˆæ¯çš„ç»„ï¼ˆé™¤éå†…å®¹è¾ƒé•¿ï¼‰
        result_groups = []
        for group in merged_groups:
            if len(group) > 1 or any(len(msg.get("processed_plain_text", "")) > 100 for msg in group):
                result_groups.append(group)

        return result_groups

    async def _build_batch_memory(
        self, fused_messages: list[list[dict[str, Any]]], time_samples: list[datetime]
    ) -> dict[str, Any]:
        """æ‰¹é‡æ„å»ºè®°å¿†"""
        if not fused_messages:
            return {"memory_count": 0, "memories": []}

        try:
            total_memories = []
            total_memory_count = 0

            # æ„å»ºèåˆåçš„æ–‡æœ¬
            batch_input_text = await self._build_fused_conversation_text(fused_messages)

            if not batch_input_text:
                logger.warning("æ— æ³•æ„å»ºèåˆæ–‡æœ¬ï¼Œå°è¯•å•ç‹¬æ„å»º")
                # å¤‡é€‰æ–¹æ¡ˆï¼šåˆ†åˆ«æ„å»º
                return await self._fallback_individual_build(fused_messages)

            # åˆ›å»ºæ‰¹é‡ä¸Šä¸‹æ–‡
            batch_context = {
                "user_id": "hippocampus_batch_sampler",
                "timestamp": time.time(),
                "source": "hippocampus_batch_sampling",
                "message_groups_count": len(fused_messages),
                "total_messages": sum(len(group) for group in fused_messages),
                "sample_count": len(time_samples),
                "is_hippocampus_sample": True,
                "bypass_value_threshold": True,
                "optimization_mode": "batch_fusion",
            }

            logger.debug(f"æ‰¹é‡æ„å»ºè®°å¿†ï¼Œæ–‡æœ¬é•¿åº¦: {len(batch_input_text)}")

            # ä¸€æ¬¡æ€§æ„å»ºè®°å¿†
            memories = await self.memory_system.build_memory_from_conversation(
                conversation_text=batch_input_text, context=batch_context, timestamp=time.time(), bypass_interval=True
            )

            if memories:
                memory_count = len(memories)
                self.success_count += 1
                total_memory_count += memory_count
                total_memories.extend(memories)

                logger.info(f"âœ… æ‰¹é‡æµ·é©¬ä½“é‡‡æ ·æˆåŠŸæ„å»º {memory_count} æ¡è®°å¿†")
            else:
                logger.debug("æ‰¹é‡æµ·é©¬ä½“é‡‡æ ·æœªç”Ÿæˆæœ‰æ•ˆè®°å¿†")

            # è®°å½•é‡‡æ ·ç»“æœ
            result = {
                "timestamp": time.time(),
                "memory_count": total_memory_count,
                "message_groups_count": len(fused_messages),
                "total_messages": sum(len(group) for group in fused_messages),
                "text_preview": batch_input_text[:200] + "..." if len(batch_input_text) > 200 else batch_input_text,
                "memory_types": [m.memory_type.value for m in total_memories],
            }

            self.last_sample_results.append(result)

            # é™åˆ¶ç»“æœå†å²é•¿åº¦
            if len(self.last_sample_results) > 10:
                self.last_sample_results.pop(0)

            return {"memory_count": total_memory_count, "memories": total_memories, "result": result}

        except Exception as e:
            logger.error(f"æ‰¹é‡æ„å»ºè®°å¿†å¤±è´¥: {e}")
            return {"memory_count": 0, "error": str(e)}

    async def _build_fused_conversation_text(self, fused_messages: list[list[dict[str, Any]]]) -> str:
        """æ„å»ºèåˆåçš„å¯¹è¯æ–‡æœ¬"""
        try:
            # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
            current_date = f"æµ·é©¬ä½“æ‰¹é‡é‡‡æ · - {datetime.now().isoformat()}\n"
            conversation_parts = [current_date]

            for group_idx, message_group in enumerate(fused_messages):
                if not message_group:
                    continue

                # ä¸ºæ¯ä¸ªæ¶ˆæ¯ç»„æ·»åŠ åˆ†éš”ç¬¦
                group_header = f"\n=== å¯¹è¯ç‰‡æ®µ {group_idx + 1} ==="
                conversation_parts.append(group_header)

                # æ„å»ºå¯è¯»æ¶ˆæ¯
                group_text = await build_readable_messages(
                    message_group,
                    merge_messages=True,
                    timestamp_mode="normal_no_YMD",
                    replace_bot_name=False,
                )

                if group_text and len(group_text.strip()) > 10:
                    conversation_parts.append(group_text.strip())

            return "\n".join(conversation_parts)

        except Exception as e:
            logger.error(f"æ„å»ºèåˆæ–‡æœ¬å¤±è´¥: {e}")
            return ""

    async def _fallback_individual_build(self, fused_messages: list[list[dict[str, Any]]]) -> dict[str, Any]:
        """å¤‡é€‰æ–¹æ¡ˆï¼šå•ç‹¬æ„å»ºæ¯ä¸ªæ¶ˆæ¯ç»„"""
        total_memories = []
        total_count = 0

        for group in fused_messages[:5]:  # é™åˆ¶æœ€å¤š5ç»„
            try:
                memories = await self.build_memory_from_samples(group, time.time())
                if memories:
                    total_memories.extend(memories)
                    total_count += len(memories)
            except Exception as e:
                logger.debug(f"å•ç‹¬æ„å»ºå¤±è´¥: {e}")

        return {"memory_count": total_count, "memories": total_memories, "fallback_mode": True}

    async def process_sample_timestamp(self, target_timestamp: float) -> str | None:
        """å¤„ç†å•ä¸ªæ—¶é—´æˆ³é‡‡æ ·ï¼ˆä¿ç•™ä½œä¸ºå¤‡é€‰æ–¹æ³•ï¼‰"""
        try:
            # æ”¶é›†æ¶ˆæ¯æ ·æœ¬
            messages = await self.collect_message_samples(target_timestamp)
            if not messages:
                return None

            # æ„å»ºè®°å¿†
            result = await self.build_memory_from_samples(messages, target_timestamp)
            return result

        except Exception as e:
            logger.debug(f"å¤„ç†æ—¶é—´æˆ³é‡‡æ ·å¤±è´¥ {target_timestamp}: {e}")
            return None

    def should_sample(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œé‡‡æ ·"""
        current_time = time.time()

        # æ£€æŸ¥æ—¶é—´é—´éš”
        if current_time - self.last_sample_time < self.config.sample_interval:
            return False

        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
        if not self.memory_builder_model:
            logger.warning("æµ·é©¬ä½“é‡‡æ ·å™¨æœªåˆå§‹åŒ–")
            return False

        return True

    async def start_background_sampling(self):
        """å¯åŠ¨åå°é‡‡æ ·"""
        if self.is_running:
            logger.warning("æµ·é©¬ä½“åå°é‡‡æ ·å·²åœ¨è¿è¡Œ")
            return

        self.is_running = True
        logger.info("ğŸš€ å¯åŠ¨æµ·é©¬ä½“åå°é‡‡æ ·ä»»åŠ¡")

        try:
            while self.is_running:
                try:
                    # æ‰§è¡Œé‡‡æ ·å‘¨æœŸ
                    result = await self.perform_sampling_cycle()

                    # å¦‚æœæ˜¯è·³è¿‡çŠ¶æ€ï¼ŒçŸ­æš‚ç¡çœ 
                    if result.get("status") == "skipped":
                        await asyncio.sleep(60)  # 1åˆ†é’Ÿåé‡è¯•
                    else:
                        # æ­£å¸¸ç­‰å¾…ä¸‹ä¸€ä¸ªé‡‡æ ·é—´éš”
                        await asyncio.sleep(self.config.sample_interval)

                except Exception as e:
                    logger.error(f"æµ·é©¬ä½“åå°é‡‡æ ·å¼‚å¸¸: {e}")
                    await asyncio.sleep(300)  # å¼‚å¸¸æ—¶ç­‰å¾…5åˆ†é’Ÿ

        except asyncio.CancelledError:
            logger.info("æµ·é©¬ä½“åå°é‡‡æ ·ä»»åŠ¡è¢«å–æ¶ˆ")
        finally:
            self.is_running = False

    def stop_background_sampling(self):
        """åœæ­¢åå°é‡‡æ ·"""
        self.is_running = False
        logger.info("ğŸ›‘ åœæ­¢æµ·é©¬ä½“åå°é‡‡æ ·ä»»åŠ¡")

    def get_sampling_stats(self) -> dict[str, Any]:
        """è·å–é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (self.success_count / self.sample_count * 100) if self.sample_count > 0 else 0

        # è®¡ç®—æœ€è¿‘çš„å¹³å‡æ•°æ®
        recent_avg_messages = 0
        recent_avg_memory_count = 0
        if self.last_sample_results:
            recent_results = self.last_sample_results[-5:]  # æœ€è¿‘5æ¬¡
            recent_avg_messages = sum(r.get("total_messages", 0) for r in recent_results) / len(recent_results)
            recent_avg_memory_count = sum(r.get("memory_count", 0) for r in recent_results) / len(recent_results)

        return {
            "is_running": self.is_running,
            "sample_count": self.sample_count,
            "success_count": self.success_count,
            "success_rate": f"{success_rate:.1f}%",
            "last_sample_time": self.last_sample_time,
            "optimization_mode": "batch_fusion",  # æ˜¾ç¤ºä¼˜åŒ–æ¨¡å¼
            "performance_metrics": {
                "avg_messages_per_sample": f"{recent_avg_messages:.1f}",
                "avg_memories_per_sample": f"{recent_avg_memory_count:.1f}",
                "fusion_efficiency": f"{(recent_avg_messages / max(recent_avg_memory_count, 1)):.1f}x"
                if recent_avg_messages > 0
                else "N/A",
            },
            "config": {
                "sample_interval": self.config.sample_interval,
                "total_samples": self.config.total_samples,
                "recent_weight": f"{self.config.recent_weight:.1%}",
                "distant_weight": f"{self.config.distant_weight:.1%}",
                "max_concurrent": 5,  # æ‰¹é‡æ¨¡å¼å¹¶å‘æ•°
                "fusion_time_gap": "30åˆ†é’Ÿ",  # æ¶ˆæ¯èåˆæ—¶é—´é—´éš”
            },
            "recent_results": self.last_sample_results[-5:],  # æœ€è¿‘5æ¬¡ç»“æœ
        }


# å…¨å±€æµ·é©¬ä½“é‡‡æ ·å™¨å®ä¾‹
_hippocampus_sampler: HippocampusSampler | None = None


def get_hippocampus_sampler(memory_system=None) -> HippocampusSampler:
    """è·å–å…¨å±€æµ·é©¬ä½“é‡‡æ ·å™¨å®ä¾‹"""
    global _hippocampus_sampler
    if _hippocampus_sampler is None:
        _hippocampus_sampler = HippocampusSampler(memory_system)
    return _hippocampus_sampler


async def initialize_hippocampus_sampler(memory_system=None) -> HippocampusSampler:
    """åˆå§‹åŒ–å…¨å±€æµ·é©¬ä½“é‡‡æ ·å™¨"""
    sampler = get_hippocampus_sampler(memory_system)
    await sampler.initialize()
    return sampler
