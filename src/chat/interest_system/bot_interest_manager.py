"""
æœºå™¨äººå…´è¶£æ ‡ç­¾ç®¡ç†ç³»ç»Ÿ
åŸºäºäººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨embeddingè®¡ç®—åŒ¹é…åº¦
"""

import orjson
import traceback
from typing import List, Dict, Optional, Any
from datetime import datetime
import numpy as np

from src.common.logger import get_logger
from src.config.config import global_config
from src.common.data_models.bot_interest_data_model import BotPersonalityInterests, BotInterestTag, InterestMatchResult

logger = get_logger("bot_interest_manager")


class BotInterestManager:
    """æœºå™¨äººå…´è¶£æ ‡ç­¾ç®¡ç†å™¨"""

    def __init__(self):
        self.current_interests: Optional[BotPersonalityInterests] = None
        self.embedding_cache: Dict[str, List[float]] = {}  # embeddingç¼“å­˜
        self._initialized = False

        # Embeddingå®¢æˆ·ç«¯é…ç½®
        self.embedding_request = None
        self.embedding_config = None
        self.embedding_dimension = 1024  # é»˜è®¤BGE-M3 embeddingç»´åº¦

    @property
    def is_initialized(self) -> bool:
        """æ£€æŸ¥å…´è¶£ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–"""
        return self._initialized

    async def initialize(self, personality_description: str, personality_id: str = "default"):
        """åˆå§‹åŒ–å…´è¶£æ ‡ç­¾ç³»ç»Ÿ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æœºå™¨äººå…´è¶£æ ‡ç­¾ç³»ç»Ÿ")
            logger.info(f"ğŸ“‹ äººè®¾ID: {personality_id}")
            logger.info(f"ğŸ“ äººè®¾æè¿°é•¿åº¦: {len(personality_description)} å­—ç¬¦")
            logger.info("=" * 60)

            # åˆå§‹åŒ–embeddingæ¨¡å‹
            logger.info("ğŸ§  æ­£åœ¨åˆå§‹åŒ–embeddingæ¨¡å‹...")
            await self._initialize_embedding_model()

            # æ£€æŸ¥embeddingå®¢æˆ·ç«¯æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
            if not self.embedding_request:
                raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")

            # ç”Ÿæˆæˆ–åŠ è½½å…´è¶£æ ‡ç­¾
            logger.info("ğŸ¯ æ­£åœ¨ç”Ÿæˆæˆ–åŠ è½½å…´è¶£æ ‡ç­¾...")
            await self._load_or_generate_interests(personality_description, personality_id)

            self._initialized = True

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å…´è¶£æ ‡ç­¾
            if self.current_interests and len(self.current_interests.get_active_tags()) > 0:
                active_tags_count = len(self.current_interests.get_active_tags())
                logger.info("=" * 60)
                logger.info("âœ… æœºå™¨äººå…´è¶£æ ‡ç­¾ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
                logger.info(f"ğŸ“Š æ´»è·ƒå…´è¶£æ ‡ç­¾æ•°é‡: {active_tags_count}")
                logger.info(f"ğŸ’¾ Embeddingç¼“å­˜å¤§å°: {len(self.embedding_cache)}")
                logger.info("=" * 60)
            else:
                raise RuntimeError("âŒ æœªèƒ½æˆåŠŸç”Ÿæˆæˆ–åŠ è½½å…´è¶£æ ‡ç­¾")

        except Exception as e:
            logger.error("=" * 60)
            logger.error(f"âŒ åˆå§‹åŒ–æœºå™¨äººå…´è¶£æ ‡ç­¾ç³»ç»Ÿå¤±è´¥: {e}")
            logger.error("=" * 60)
            traceback.print_exc()
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å…è®¸é™çº§åˆå§‹åŒ–

    async def _initialize_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        logger.info("ğŸ”§ æ­£åœ¨é…ç½®embeddingå®¢æˆ·ç«¯...")

        # ä½¿ç”¨é¡¹ç›®é…ç½®çš„embeddingæ¨¡å‹
        from src.config.config import model_config
        from src.llm_models.utils_model import LLMRequest

        logger.debug("âœ… æˆåŠŸå¯¼å…¥embeddingç›¸å…³æ¨¡å—")

        # æ£€æŸ¥embeddingé…ç½®æ˜¯å¦å­˜åœ¨
        if not hasattr(model_config.model_task_config, "embedding"):
            raise RuntimeError("âŒ æœªæ‰¾åˆ°embeddingæ¨¡å‹é…ç½®")

        logger.info("ğŸ“‹ æ‰¾åˆ°embeddingæ¨¡å‹é…ç½®")
        self.embedding_config = model_config.model_task_config.embedding
        self.embedding_dimension = 1024  # BGE-M3çš„ç»´åº¦
        logger.info(f"ğŸ“ ä½¿ç”¨æ¨¡å‹ç»´åº¦: {self.embedding_dimension}")

        # åˆ›å»ºLLMRequestå®ä¾‹ç”¨äºembedding
        self.embedding_request = LLMRequest(model_set=self.embedding_config, request_type="interest_embedding")
        logger.info("âœ… Embeddingè¯·æ±‚å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"ğŸ”— å®¢æˆ·ç«¯ç±»å‹: {type(self.embedding_request).__name__}")

        # è·å–ç¬¬ä¸€ä¸ªembeddingæ¨¡å‹çš„ModelInfo
        if hasattr(self.embedding_config, "model_list") and self.embedding_config.model_list:
            first_model_name = self.embedding_config.model_list[0]
            logger.info(f"ğŸ¯ ä½¿ç”¨embeddingæ¨¡å‹: {first_model_name}")
        else:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°embeddingæ¨¡å‹åˆ—è¡¨")

        logger.info("âœ… Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    async def _load_or_generate_interests(self, personality_description: str, personality_id: str):
        """åŠ è½½æˆ–ç”Ÿæˆå…´è¶£æ ‡ç­¾"""
        logger.info(f"ğŸ“š æ­£åœ¨ä¸º '{personality_id}' åŠ è½½æˆ–ç”Ÿæˆå…´è¶£æ ‡ç­¾...")

        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“åŠ è½½
        logger.info("ğŸ’¾ å°è¯•ä»æ•°æ®åº“åŠ è½½ç°æœ‰å…´è¶£æ ‡ç­¾...")
        loaded_interests = await self._load_interests_from_database(personality_id)

        if loaded_interests:
            self.current_interests = loaded_interests
            active_count = len(loaded_interests.get_active_tags())
            logger.info(f"âœ… æˆåŠŸä»æ•°æ®åº“åŠ è½½ {active_count} ä¸ªå…´è¶£æ ‡ç­¾")
            logger.info(f"ğŸ“… æœ€åæ›´æ–°æ—¶é—´: {loaded_interests.last_updated}")
            logger.info(f"ğŸ”„ ç‰ˆæœ¬å·: {loaded_interests.version}")
        else:
            # ç”Ÿæˆæ–°çš„å…´è¶£æ ‡ç­¾
            logger.info("ğŸ†• æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å…´è¶£æ ‡ç­¾ï¼Œå¼€å§‹ç”Ÿæˆæ–°çš„...")
            logger.info("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–å…´è¶£æ ‡ç­¾...")
            generated_interests = await self._generate_interests_from_personality(
                personality_description, personality_id
            )

            if generated_interests:
                self.current_interests = generated_interests
                active_count = len(generated_interests.get_active_tags())
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {active_count} ä¸ªå…´è¶£æ ‡ç­¾")

                # ä¿å­˜åˆ°æ•°æ®åº“
                logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“...")
                await self._save_interests_to_database(generated_interests)
            else:
                raise RuntimeError("âŒ å…´è¶£æ ‡ç­¾ç”Ÿæˆå¤±è´¥")

    async def _generate_interests_from_personality(
        self, personality_description: str, personality_id: str
    ) -> Optional[BotPersonalityInterests]:
        """æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾"""
        try:
            logger.info("ğŸ¨ å¼€å§‹æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾...")
            logger.info(f"ğŸ“ äººè®¾é•¿åº¦: {len(personality_description)} å­—ç¬¦")

            # æ£€æŸ¥embeddingå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not hasattr(self, "embedding_request"):
                raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå…´è¶£æ ‡ç­¾")

            # æ„å»ºæç¤ºè¯
            logger.info("ğŸ“ æ„å»ºLLMæç¤ºè¯...")
            prompt = f"""
åŸºäºä»¥ä¸‹æœºå™¨äººäººè®¾æè¿°ï¼Œç”Ÿæˆä¸€å¥—åˆé€‚çš„å…´è¶£æ ‡ç­¾ï¼š

äººè®¾æè¿°ï¼š
{personality_description}

è¯·ç”Ÿæˆä¸€ç³»åˆ—å…´è¶£å…³é”®è¯æ ‡ç­¾ï¼Œè¦æ±‚ï¼š
1. æ ‡ç­¾åº”è¯¥ç¬¦åˆäººè®¾ç‰¹ç‚¹å’Œæ€§æ ¼
2. æ¯ä¸ªæ ‡ç­¾éƒ½æœ‰æƒé‡ï¼ˆ0.1-1.0ï¼‰ï¼Œè¡¨ç¤ºå¯¹è¯¥å…´è¶£çš„å–œå¥½ç¨‹åº¦
3. ç”Ÿæˆ15-25ä¸ªä¸ç­‰çš„æ ‡ç­¾
4. æ ‡ç­¾åº”è¯¥æ˜¯å…·ä½“çš„å…³é”®è¯ï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¦‚å¿µ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "interests": [
        {{"name": "æ ‡ç­¾å", "weight": 0.8}},
        {{"name": "æ ‡ç­¾å", "weight": 0.6}},
        {{"name": "æ ‡ç­¾å", "weight": 0.9}}
    ]
}}

æ³¨æ„ï¼š
- æƒé‡èŒƒå›´0.1-1.0ï¼Œæƒé‡è¶Šé«˜è¡¨ç¤ºè¶Šæ„Ÿå…´è¶£
- æ ‡ç­¾è¦å…·ä½“ï¼Œå¦‚"ç¼–ç¨‹"ã€"æ¸¸æˆ"ã€"æ—…è¡Œ"ç­‰
- æ ¹æ®äººè®¾ç”Ÿæˆä¸ªæ€§åŒ–çš„æ ‡ç­¾
"""

            # è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾
            logger.info("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾...")
            response = await self._call_llm_for_interest_generation(prompt)

            if not response:
                raise RuntimeError("âŒ LLMæœªè¿”å›æœ‰æ•ˆå“åº”")

            logger.info("âœ… LLMå“åº”æˆåŠŸï¼Œå¼€å§‹è§£æå…´è¶£æ ‡ç­¾...")
            interests_data = orjson.loads(response)

            bot_interests = BotPersonalityInterests(
                personality_id=personality_id, personality_description=personality_description
            )

            # è§£æç”Ÿæˆçš„å…´è¶£æ ‡ç­¾
            interests_list = interests_data.get("interests", [])
            logger.info(f"ğŸ“‹ è§£æåˆ° {len(interests_list)} ä¸ªå…´è¶£æ ‡ç­¾")

            for i, tag_data in enumerate(interests_list):
                tag_name = tag_data.get("name", f"æ ‡ç­¾_{i}")
                weight = tag_data.get("weight", 0.5)

                tag = BotInterestTag(tag_name=tag_name, weight=weight)
                bot_interests.interest_tags.append(tag)

                logger.debug(f"   ğŸ·ï¸  {tag_name} (æƒé‡: {weight:.2f})")

            # ä¸ºæ‰€æœ‰æ ‡ç­¾ç”Ÿæˆembedding
            logger.info("ğŸ§  å¼€å§‹ä¸ºå…´è¶£æ ‡ç­¾ç”Ÿæˆembeddingå‘é‡...")
            await self._generate_embeddings_for_tags(bot_interests)

            logger.info("âœ… å…´è¶£æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
            return bot_interests

        except orjson.JSONDecodeError as e:
            logger.error(f"âŒ è§£æLLMå“åº”JSONå¤±è´¥: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            traceback.print_exc()
            raise

    async def _call_llm_for_interest_generation(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾"""
        try:
            logger.info("ğŸ”§ é…ç½®LLMå®¢æˆ·ç«¯...")

            # ä½¿ç”¨llm_apiæ¥å¤„ç†è¯·æ±‚
            from src.plugin_system.apis import llm_api
            from src.config.config import model_config

            # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚åªè¿”å›çº¯JSON
            full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨äººäººè®¾åˆ†æå¸ˆï¼Œæ“…é•¿æ ¹æ®äººè®¾æè¿°ç”Ÿæˆåˆé€‚çš„å…´è¶£æ ‡ç­¾ã€‚

{prompt}

è¯·ç¡®ä¿è¿”å›æ ¼å¼ä¸ºæœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚åªè¿”å›JSONå¯¹è±¡æœ¬èº«ã€‚"""

            # ä½¿ç”¨replyeræ¨¡å‹é…ç½®
            replyer_config = model_config.model_task_config.replyer

            # è°ƒç”¨LLM API
            logger.info("ğŸš€ æ­£åœ¨é€šè¿‡LLM APIå‘é€è¯·æ±‚...")
            success, response, reasoning_content, model_name = await llm_api.generate_with_model(
                prompt=full_prompt,
                model_config=replyer_config,
                request_type="interest_generation",
                temperature=0.7,
                max_tokens=2000,
            )

            if success and response:
                logger.info(f"âœ… LLMå“åº”æˆåŠŸï¼Œæ¨¡å‹: {model_name}, å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                logger.debug(
                    f"ğŸ“„ LLMå“åº”å†…å®¹: {response[:200]}..." if len(response) > 200 else f"ğŸ“„ LLMå“åº”å†…å®¹: {response}"
                )
                if reasoning_content:
                    logger.debug(f"ğŸ§  æ¨ç†å†…å®¹: {reasoning_content[:100]}...")

                # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
                cleaned_response = self._clean_llm_response(response)
                return cleaned_response
            else:
                logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”æˆ–è°ƒç”¨å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return None

    def _clean_llm_response(self, response: str) -> str:
        """æ¸…ç†LLMå“åº”ï¼Œç§»é™¤ä»£ç å—æ ‡è®°å’Œå…¶ä»–éJSONå†…å®¹"""
        import re

        # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
        cleaned = re.sub(r"```json\s*", "", response)
        cleaned = re.sub(r"\s*```", "", cleaned)

        # ç§»é™¤å¯èƒ½çš„å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        cleaned = cleaned.strip()

        # å°è¯•æå–JSONå¯¹è±¡ï¼ˆå¦‚æœå“åº”ä¸­æœ‰å…¶ä»–æ–‡æœ¬ï¼‰
        json_match = re.search(r"\{.*\}", cleaned, re.DOTALL)
        if json_match:
            cleaned = json_match.group(0)

        logger.debug(f"ğŸ§¹ æ¸…ç†åçš„å“åº”: {cleaned[:200]}..." if len(cleaned) > 200 else f"ğŸ§¹ æ¸…ç†åçš„å“åº”: {cleaned}")
        return cleaned

    async def _generate_embeddings_for_tags(self, interests: BotPersonalityInterests):
        """ä¸ºæ‰€æœ‰å…´è¶£æ ‡ç­¾ç”Ÿæˆembedding"""
        if not hasattr(self, "embedding_request"):
            raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆembedding")

        total_tags = len(interests.interest_tags)
        logger.info(f"ğŸ§  å¼€å§‹ä¸º {total_tags} ä¸ªå…´è¶£æ ‡ç­¾ç”Ÿæˆembeddingå‘é‡...")

        cached_count = 0
        generated_count = 0
        failed_count = 0

        for i, tag in enumerate(interests.interest_tags, 1):
            if tag.tag_name in self.embedding_cache:
                # ä½¿ç”¨ç¼“å­˜çš„embedding
                tag.embedding = self.embedding_cache[tag.tag_name]
                cached_count += 1
                logger.debug(f"   [{i}/{total_tags}] ğŸ·ï¸  '{tag.tag_name}' - ä½¿ç”¨ç¼“å­˜")
            else:
                # ç”Ÿæˆæ–°çš„embedding
                embedding_text = tag.tag_name

                logger.debug(f"   [{i}/{total_tags}] ğŸ”„ æ­£åœ¨ä¸º '{tag.tag_name}' ç”Ÿæˆembedding...")
                embedding = await self._get_embedding(embedding_text)

                if embedding:
                    tag.embedding = embedding
                    self.embedding_cache[tag.tag_name] = embedding
                    generated_count += 1
                    logger.debug(f"   âœ… '{tag.tag_name}' embeddingç”ŸæˆæˆåŠŸ")
                else:
                    failed_count += 1
                    logger.warning(f"   âŒ '{tag.tag_name}' embeddingç”Ÿæˆå¤±è´¥")

        if failed_count > 0:
            raise RuntimeError(f"âŒ æœ‰ {failed_count} ä¸ªå…´è¶£æ ‡ç­¾embeddingç”Ÿæˆå¤±è´¥")

        interests.last_updated = datetime.now()
        logger.info("=" * 50)
        logger.info("âœ… Embeddingç”Ÿæˆå®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»æ ‡ç­¾æ•°: {total_tags}")
        logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cached_count}")
        logger.info(f"ğŸ†• æ–°ç”Ÿæˆ: {generated_count}")
        logger.info(f"âŒ å¤±è´¥: {failed_count}")
        logger.info(f"ğŸ—ƒï¸  æ€»ç¼“å­˜å¤§å°: {len(self.embedding_cache)}")
        logger.info("=" * 50)

    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        if not hasattr(self, "embedding_request"):
            raise RuntimeError("âŒ Embeddingè¯·æ±‚å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        # æ£€æŸ¥ç¼“å­˜
        if text in self.embedding_cache:
            logger.debug(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜çš„embedding: '{text[:30]}...'")
            return self.embedding_cache[text]

        # ä½¿ç”¨LLMRequestè·å–embedding
        logger.debug(f"ğŸ”„ æ­£åœ¨è·å–embedding: '{text[:30]}...'")
        embedding, model_name = await self.embedding_request.get_embedding(text)

        if embedding and len(embedding) > 0:
            self.embedding_cache[text] = embedding
            logger.debug(f"âœ… Embeddingè·å–æˆåŠŸï¼Œç»´åº¦: {len(embedding)}, æ¨¡å‹: {model_name}")
            return embedding
        else:
            raise RuntimeError(f"âŒ è¿”å›çš„embeddingä¸ºç©º: {embedding}")

    async def _generate_message_embedding(self, message_text: str, keywords: List[str]) -> List[float]:
        """ä¸ºæ¶ˆæ¯ç”Ÿæˆembeddingå‘é‡"""
        # ç»„åˆæ¶ˆæ¯æ–‡æœ¬å’Œå…³é”®è¯ä½œä¸ºembeddingè¾“å…¥
        if keywords:
            combined_text = f"{message_text} {' '.join(keywords)}"
        else:
            combined_text = message_text

        logger.debug(f"ğŸ”„ æ­£åœ¨ä¸ºæ¶ˆæ¯ç”Ÿæˆembeddingï¼Œè¾“å…¥é•¿åº¦: {len(combined_text)}")

        # ç”Ÿæˆembedding
        embedding = await self._get_embedding(combined_text)
        logger.debug(f"âœ… æ¶ˆæ¯embeddingç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        return embedding

    async def _calculate_similarity_scores(
        self, result: InterestMatchResult, message_embedding: List[float], keywords: List[str]
    ):
        """è®¡ç®—æ¶ˆæ¯ä¸å…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not self.current_interests:
                return

            active_tags = self.current_interests.get_active_tags()
            if not active_tags:
                return

            logger.debug(f"ğŸ” å¼€å§‹è®¡ç®—ä¸ {len(active_tags)} ä¸ªå…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦")

            for tag in active_tags:
                if tag.embedding:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = self._calculate_cosine_similarity(message_embedding, tag.embedding)
                    weighted_score = similarity * tag.weight

                    # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º0.3
                    if similarity > 0.3:
                        result.add_match(tag.tag_name, weighted_score, keywords)
                        logger.debug(
                            f"   ğŸ·ï¸  '{tag.tag_name}': ç›¸ä¼¼åº¦={similarity:.3f}, æƒé‡={tag.weight:.2f}, åŠ æƒåˆ†æ•°={weighted_score:.3f}"
                        )

        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°å¤±è´¥: {e}")

    async def calculate_interest_match(self, message_text: str, keywords: List[str] = None) -> InterestMatchResult:
        """è®¡ç®—æ¶ˆæ¯ä¸æœºå™¨äººå…´è¶£çš„åŒ¹é…åº¦"""
        if not self.current_interests or not self._initialized:
            raise RuntimeError("âŒ å…´è¶£æ ‡ç­¾ç³»ç»Ÿæœªåˆå§‹åŒ–")

        logger.info("ğŸ¯ å¼€å§‹è®¡ç®—å…´è¶£åŒ¹é…åº¦...")
        logger.debug(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(message_text)} å­—ç¬¦")
        if keywords:
            logger.debug(f"ğŸ·ï¸  å…³é”®è¯æ•°é‡: {len(keywords)}")

        message_id = f"msg_{datetime.now().timestamp()}"
        result = InterestMatchResult(message_id=message_id)

        # è·å–æ´»è·ƒçš„å…´è¶£æ ‡ç­¾
        active_tags = self.current_interests.get_active_tags()
        if not active_tags:
            raise RuntimeError("âŒ æ²¡æœ‰æ´»è·ƒçš„å…´è¶£æ ‡ç­¾")

        logger.info(f"ğŸ“Š æœ‰ {len(active_tags)} ä¸ªæ´»è·ƒå…´è¶£æ ‡ç­¾å‚ä¸åŒ¹é…")

        # ç”Ÿæˆæ¶ˆæ¯çš„embedding
        logger.debug("ğŸ”„ æ­£åœ¨ç”Ÿæˆæ¶ˆæ¯embedding...")
        message_embedding = await self._get_embedding(message_text)
        logger.debug(f"âœ… æ¶ˆæ¯embeddingç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(message_embedding)}")

        # è®¡ç®—ä¸æ¯ä¸ªå…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦
        match_count = 0
        high_similarity_count = 0
        medium_similarity_count = 0
        low_similarity_count = 0

        # åˆ†çº§ç›¸ä¼¼åº¦é˜ˆå€¼
        affinity_config = global_config.affinity_flow
        high_threshold = affinity_config.high_match_interest_threshold
        medium_threshold = affinity_config.medium_match_interest_threshold
        low_threshold = affinity_config.low_match_interest_threshold

        logger.debug(f"ğŸ” ä½¿ç”¨åˆ†çº§ç›¸ä¼¼åº¦é˜ˆå€¼: é«˜={high_threshold}, ä¸­={medium_threshold}, ä½={low_threshold}")

        for tag in active_tags:
            if tag.embedding:
                similarity = self._calculate_cosine_similarity(message_embedding, tag.embedding)

                # åŸºç¡€åŠ æƒåˆ†æ•°
                weighted_score = similarity * tag.weight

                # æ ¹æ®ç›¸ä¼¼åº¦ç­‰çº§åº”ç”¨ä¸åŒçš„åŠ æˆ
                if similarity > high_threshold:
                    # é«˜ç›¸ä¼¼åº¦ï¼šå¼ºåŠ æˆ
                    enhanced_score = weighted_score * affinity_config.high_match_keyword_multiplier
                    match_count += 1
                    high_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])
                    logger.debug(
                        f"   ğŸ·ï¸  '{tag.tag_name}': ç›¸ä¼¼åº¦={similarity:.3f}, æƒé‡={tag.weight:.2f}, åŸºç¡€åˆ†æ•°={weighted_score:.3f}, å¢å¼ºåˆ†æ•°={enhanced_score:.3f} [é«˜åŒ¹é…]"
                    )

                elif similarity > medium_threshold:
                    # ä¸­ç›¸ä¼¼åº¦ï¼šä¸­ç­‰åŠ æˆ
                    enhanced_score = weighted_score * affinity_config.medium_match_keyword_multiplier
                    match_count += 1
                    medium_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])
                    logger.debug(
                        f"   ğŸ·ï¸  '{tag.tag_name}': ç›¸ä¼¼åº¦={similarity:.3f}, æƒé‡={tag.weight:.2f}, åŸºç¡€åˆ†æ•°={weighted_score:.3f}, å¢å¼ºåˆ†æ•°={enhanced_score:.3f} [ä¸­åŒ¹é…]"
                    )

                elif similarity > low_threshold:
                    # ä½ç›¸ä¼¼åº¦ï¼šè½»å¾®åŠ æˆ
                    enhanced_score = weighted_score * affinity_config.low_match_keyword_multiplier
                    match_count += 1
                    low_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])
                    logger.debug(
                        f"   ğŸ·ï¸  '{tag.tag_name}': ç›¸ä¼¼åº¦={similarity:.3f}, æƒé‡={tag.weight:.2f}, åŸºç¡€åˆ†æ•°={weighted_score:.3f}, å¢å¼ºåˆ†æ•°={enhanced_score:.3f} [ä½åŒ¹é…]"
                    )

        logger.info(f"ğŸ“ˆ åŒ¹é…ç»Ÿè®¡: {match_count}/{len(active_tags)} ä¸ªæ ‡ç­¾è¶…è¿‡é˜ˆå€¼")
        logger.info(f"ğŸ”¥ é«˜ç›¸ä¼¼åº¦åŒ¹é…(>{high_threshold}): {high_similarity_count} ä¸ª")
        logger.info(f"âš¡ ä¸­ç›¸ä¼¼åº¦åŒ¹é…(>{medium_threshold}): {medium_similarity_count} ä¸ª")
        logger.info(f"ğŸŒŠ ä½ç›¸ä¼¼åº¦åŒ¹é…(>{low_threshold}): {low_similarity_count} ä¸ª")

        # æ·»åŠ ç›´æ¥å…³é”®è¯åŒ¹é…å¥–åŠ±
        keyword_bonus = self._calculate_keyword_match_bonus(keywords, result.matched_tags)
        logger.debug(f"ğŸ¯ å…³é”®è¯ç›´æ¥åŒ¹é…å¥–åŠ±: {keyword_bonus}")

        # åº”ç”¨å…³é”®è¯å¥–åŠ±åˆ°åŒ¹é…åˆ†æ•°
        for tag_name in result.matched_tags:
            if tag_name in keyword_bonus:
                original_score = result.match_scores[tag_name]
                bonus = keyword_bonus[tag_name]
                result.match_scores[tag_name] = original_score + bonus
                logger.debug(
                    f"   ğŸ·ï¸  '{tag_name}': åŸå§‹åˆ†æ•°={original_score:.3f}, å¥–åŠ±={bonus:.3f}, æœ€ç»ˆåˆ†æ•°={result.match_scores[tag_name]:.3f}"
                )

        # è®¡ç®—æ€»ä½“åˆ†æ•°
        result.calculate_overall_score()

        # ç¡®å®šæœ€ä½³åŒ¹é…æ ‡ç­¾
        if result.matched_tags:
            top_tag_name = max(result.match_scores.items(), key=lambda x: x[1])[0]
            result.top_tag = top_tag_name
            logger.info(f"ğŸ† æœ€ä½³åŒ¹é…æ ‡ç­¾: '{top_tag_name}' (åˆ†æ•°: {result.match_scores[top_tag_name]:.3f})")

        logger.info(
            f"ğŸ“Š æœ€ç»ˆç»“æœ: æ€»åˆ†={result.overall_score:.3f}, ç½®ä¿¡åº¦={result.confidence:.3f}, åŒ¹é…æ ‡ç­¾æ•°={len(result.matched_tags)}"
        )
        return result

    def _calculate_keyword_match_bonus(self, keywords: List[str], matched_tags: List[str]) -> Dict[str, float]:
        """è®¡ç®—å…³é”®è¯ç›´æ¥åŒ¹é…å¥–åŠ±"""
        if not keywords or not matched_tags:
            return {}

        affinity_config = global_config.affinity_flow
        bonus_dict = {}

        for tag_name in matched_tags:
            bonus = 0.0

            # æ£€æŸ¥å…³é”®è¯ä¸æ ‡ç­¾çš„ç›´æ¥åŒ¹é…
            for keyword in keywords:
                keyword_lower = keyword.lower().strip()
                tag_name_lower = tag_name.lower()

                # å®Œå…¨åŒ¹é…
                if keyword_lower == tag_name_lower:
                    bonus += affinity_config.high_match_interest_threshold * 0.6  # ä½¿ç”¨é«˜åŒ¹é…é˜ˆå€¼çš„60%ä½œä¸ºå®Œå…¨åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯å®Œå…¨åŒ¹é…: '{keyword}' == '{tag_name}' (+{affinity_config.high_match_interest_threshold * 0.6:.3f})"
                    )

                # åŒ…å«åŒ¹é…
                elif keyword_lower in tag_name_lower or tag_name_lower in keyword_lower:
                    bonus += (
                        affinity_config.medium_match_interest_threshold * 0.3
                    )  # ä½¿ç”¨ä¸­åŒ¹é…é˜ˆå€¼çš„30%ä½œä¸ºåŒ…å«åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯åŒ…å«åŒ¹é…: '{keyword}' âŠƒ '{tag_name}' (+{affinity_config.medium_match_interest_threshold * 0.3:.3f})"
                    )

                # éƒ¨åˆ†åŒ¹é…ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
                elif self._calculate_partial_match(keyword_lower, tag_name_lower):
                    bonus += affinity_config.low_match_interest_threshold * 0.4  # ä½¿ç”¨ä½åŒ¹é…é˜ˆå€¼çš„40%ä½œä¸ºéƒ¨åˆ†åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯éƒ¨åˆ†åŒ¹é…: '{keyword}' â‰ˆ '{tag_name}' (+{affinity_config.low_match_interest_threshold * 0.4:.3f})"
                    )

            if bonus > 0:
                bonus_dict[tag_name] = min(bonus, affinity_config.max_match_bonus)  # ä½¿ç”¨é…ç½®çš„æœ€å¤§å¥–åŠ±é™åˆ¶

        return bonus_dict

    def _calculate_partial_match(self, text1: str, text2: str) -> bool:
        """è®¡ç®—éƒ¨åˆ†åŒ¹é…ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰"""
        try:
            # ç®€å•çš„ç¼–è¾‘è·ç¦»è®¡ç®—
            max_len = max(len(text1), len(text2))
            if max_len == 0:
                return False

            # è®¡ç®—ç¼–è¾‘è·ç¦»
            distance = self._levenshtein_distance(text1, text2)

            # å¦‚æœç¼–è¾‘è·ç¦»å°äºè¾ƒçŸ­å­—ç¬¦ä¸²é•¿åº¦çš„ä¸€åŠï¼Œè®¤ä¸ºæ˜¯éƒ¨åˆ†åŒ¹é…
            min_len = min(len(text1), len(text2))
            return distance <= min_len // 2

        except Exception:
            return False

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—è±æ–‡æ–¯å¦è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)

            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return dot_product / (norm1 * norm2)

        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    async def _load_interests_from_database(self, personality_id: str) -> Optional[BotPersonalityInterests]:
        """ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾"""
        try:
            logger.info(f"ğŸ’¾ æ­£åœ¨ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾ï¼Œpersonality_id: {personality_id}")

            # å¯¼å…¥SQLAlchemyç›¸å…³æ¨¡å—
            from src.common.database.sqlalchemy_models import BotPersonalityInterests as DBBotPersonalityInterests
            from src.common.database.sqlalchemy_database_api import get_db_session
            import orjson

            with get_db_session() as session:
                # æŸ¥è¯¢æœ€æ–°çš„å…´è¶£æ ‡ç­¾é…ç½®
                db_interests = (
                    session.query(DBBotPersonalityInterests)
                    .filter(DBBotPersonalityInterests.personality_id == personality_id)
                    .order_by(DBBotPersonalityInterests.version.desc(), DBBotPersonalityInterests.last_updated.desc())
                    .first()
                )

                if db_interests:
                    logger.info(f"âœ… æ‰¾åˆ°æ•°æ®åº“ä¸­çš„å…´è¶£æ ‡ç­¾é…ç½®ï¼Œç‰ˆæœ¬: {db_interests.version}")
                    logger.debug(f"ğŸ“… æœ€åæ›´æ–°æ—¶é—´: {db_interests.last_updated}")
                    logger.debug(f"ğŸ§  ä½¿ç”¨çš„embeddingæ¨¡å‹: {db_interests.embedding_model}")

                    # è§£æJSONæ ¼å¼çš„å…´è¶£æ ‡ç­¾
                    try:
                        tags_data = orjson.loads(db_interests.interest_tags)
                        logger.debug(f"ğŸ·ï¸  è§£æåˆ° {len(tags_data)} ä¸ªå…´è¶£æ ‡ç­¾")

                        # åˆ›å»ºBotPersonalityInterestså¯¹è±¡
                        interests = BotPersonalityInterests(
                            personality_id=db_interests.personality_id,
                            personality_description=db_interests.personality_description,
                            embedding_model=db_interests.embedding_model,
                            version=db_interests.version,
                            last_updated=db_interests.last_updated,
                        )

                        # è§£æå…´è¶£æ ‡ç­¾
                        for tag_data in tags_data:
                            tag = BotInterestTag(
                                tag_name=tag_data.get("tag_name", ""),
                                weight=tag_data.get("weight", 0.5),
                                created_at=datetime.fromisoformat(
                                    tag_data.get("created_at", datetime.now().isoformat())
                                ),
                                updated_at=datetime.fromisoformat(
                                    tag_data.get("updated_at", datetime.now().isoformat())
                                ),
                                is_active=tag_data.get("is_active", True),
                                embedding=tag_data.get("embedding"),
                            )
                            interests.interest_tags.append(tag)

                        logger.info(f"âœ… æˆåŠŸä»æ•°æ®åº“åŠ è½½ {len(interests.interest_tags)} ä¸ªå…´è¶£æ ‡ç­¾")
                        return interests

                    except (orjson.JSONDecodeError, Exception) as e:
                        logger.error(f"âŒ è§£æå…´è¶£æ ‡ç­¾JSONå¤±è´¥: {e}")
                        logger.debug(f"ğŸ” åŸå§‹JSONæ•°æ®: {db_interests.interest_tags[:200]}...")
                        return None
                else:
                    logger.info(f"â„¹ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°personality_idä¸º '{personality_id}' çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    return None

        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return None

    async def _save_interests_to_database(self, interests: BotPersonalityInterests):
        """ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“"""
        try:
            logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“...")
            logger.info(f"ğŸ“‹ personality_id: {interests.personality_id}")
            logger.info(f"ğŸ·ï¸  å…´è¶£æ ‡ç­¾æ•°é‡: {len(interests.interest_tags)}")
            logger.info(f"ğŸ”„ ç‰ˆæœ¬: {interests.version}")

            # å¯¼å…¥SQLAlchemyç›¸å…³æ¨¡å—
            from src.common.database.sqlalchemy_models import BotPersonalityInterests as DBBotPersonalityInterests
            from src.common.database.sqlalchemy_database_api import get_db_session
            import orjson

            # å°†å…´è¶£æ ‡ç­¾è½¬æ¢ä¸ºJSONæ ¼å¼
            tags_data = []
            for tag in interests.interest_tags:
                tag_dict = {
                    "tag_name": tag.tag_name,
                    "weight": tag.weight,
                    "created_at": tag.created_at.isoformat(),
                    "updated_at": tag.updated_at.isoformat(),
                    "is_active": tag.is_active,
                    "embedding": tag.embedding,
                }
                tags_data.append(tag_dict)

            # åºåˆ—åŒ–ä¸ºJSON
            json_data = orjson.dumps(tags_data)

            with get_db_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒpersonality_idçš„è®°å½•
                existing_record = (
                    session.query(DBBotPersonalityInterests)
                    .filter(DBBotPersonalityInterests.personality_id == interests.personality_id)
                    .first()
                )

                if existing_record:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    logger.info("ğŸ”„ æ›´æ–°ç°æœ‰çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    existing_record.interest_tags = json_data
                    existing_record.personality_description = interests.personality_description
                    existing_record.embedding_model = interests.embedding_model
                    existing_record.version = interests.version
                    existing_record.last_updated = interests.last_updated

                    logger.info(f"âœ… æˆåŠŸæ›´æ–°å…´è¶£æ ‡ç­¾é…ç½®ï¼Œç‰ˆæœ¬: {interests.version}")

                else:
                    # åˆ›å»ºæ–°è®°å½•
                    logger.info("ğŸ†• åˆ›å»ºæ–°çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    new_record = DBBotPersonalityInterests(
                        personality_id=interests.personality_id,
                        personality_description=interests.personality_description,
                        interest_tags=json_data,
                        embedding_model=interests.embedding_model,
                        version=interests.version,
                        last_updated=interests.last_updated,
                    )
                    session.add(new_record)
                    session.commit()
                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºå…´è¶£æ ‡ç­¾é…ç½®ï¼Œç‰ˆæœ¬: {interests.version}")

            logger.info("âœ… å…´è¶£æ ‡ç­¾å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")

            # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
            with get_db_session() as session:
                saved_record = (
                    session.query(DBBotPersonalityInterests)
                    .filter(DBBotPersonalityInterests.personality_id == interests.personality_id)
                    .first()
                )
                session.commit()
                if saved_record:
                    logger.info(f"âœ… éªŒè¯æˆåŠŸï¼šæ•°æ®åº“ä¸­å­˜åœ¨personality_idä¸º {interests.personality_id} çš„è®°å½•")
                    logger.info(f"   ç‰ˆæœ¬: {saved_record.version}")
                    logger.info(f"   æœ€åæ›´æ–°: {saved_record.last_updated}")
                else:
                    logger.error(f"âŒ éªŒè¯å¤±è´¥ï¼šæ•°æ®åº“ä¸­æœªæ‰¾åˆ°personality_idä¸º {interests.personality_id} çš„è®°å½•")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()

    def get_current_interests(self) -> Optional[BotPersonalityInterests]:
        """è·å–å½“å‰çš„å…´è¶£æ ‡ç­¾é…ç½®"""
        return self.current_interests

    def get_interest_stats(self) -> Dict[str, Any]:
        """è·å–å…´è¶£ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.current_interests:
            return {"initialized": False}

        active_tags = self.current_interests.get_active_tags()

        return {
            "initialized": self._initialized,
            "total_tags": len(active_tags),
            "embedding_model": self.current_interests.embedding_model,
            "last_updated": self.current_interests.last_updated.isoformat(),
            "cache_size": len(self.embedding_cache),
        }

    async def update_interest_tags(self, new_personality_description: str = None):
        """æ›´æ–°å…´è¶£æ ‡ç­¾"""
        try:
            if not self.current_interests:
                logger.warning("æ²¡æœ‰å½“å‰çš„å…´è¶£æ ‡ç­¾é…ç½®ï¼Œæ— æ³•æ›´æ–°")
                return

            if new_personality_description:
                self.current_interests.personality_description = new_personality_description

            # é‡æ–°ç”Ÿæˆå…´è¶£æ ‡ç­¾
            new_interests = await self._generate_interests_from_personality(
                self.current_interests.personality_description, self.current_interests.personality_id
            )

            if new_interests:
                new_interests.version = self.current_interests.version + 1
                self.current_interests = new_interests
                await self._save_interests_to_database(new_interests)
                logger.info(f"å…´è¶£æ ‡ç­¾å·²æ›´æ–°ï¼Œç‰ˆæœ¬: {new_interests.version}")

        except Exception as e:
            logger.error(f"æ›´æ–°å…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            traceback.print_exc()


# åˆ›å»ºå…¨å±€å®ä¾‹ï¼ˆé‡æ–°åˆ›å»ºä»¥åŒ…å«æ–°çš„å±æ€§ï¼‰
bot_interest_manager = BotInterestManager()
