"""
æœºå™¨äººå…´è¶£æ ‡ç­¾ç®¡ç†ç³»ç»Ÿ
åŸºäºäººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨embeddingè®¡ç®—åŒ¹é…åº¦
"""

import traceback
from datetime import datetime
from typing import Any, cast

import numpy as np
from sqlalchemy import select

from src.common.config_helpers import resolve_embedding_dimension
from src.common.data_models.bot_interest_data_model import BotInterestTag, BotPersonalityInterests, InterestMatchResult
from src.common.logger import get_logger
from src.config.config import global_config
from src.utils.json_parser import extract_and_parse_json

logger = get_logger("bot_interest_manager")


class BotInterestManager:
    """æœºå™¨äººå…´è¶£æ ‡ç­¾ç®¡ç†å™¨"""

    def __init__(self):
        self.current_interests: BotPersonalityInterests | None = None
        self.embedding_cache: dict[str, list[float]] = {}  # embeddingç¼“å­˜
        self.expanded_tag_cache: dict[str, str] = {}  # æ‰©å±•æ ‡ç­¾ç¼“å­˜
        self.expanded_embedding_cache: dict[str, list[float]] = {}  # æ‰©å±•æ ‡ç­¾çš„embeddingç¼“å­˜
        self._initialized = False

        # Embeddingå®¢æˆ·ç«¯é…ç½®
        self.embedding_request = None
        self.embedding_config = None
        configured_dim = resolve_embedding_dimension()
        self.embedding_dimension = int(configured_dim) if configured_dim else 0
        self._detected_embedding_dimension: int | None = None

    @property
    def is_initialized(self) -> bool:
        """æ£€æŸ¥å…´è¶£ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–"""
        return self._initialized

    async def initialize(self, personality_description: str, personality_id: str = "default"):
        """åˆå§‹åŒ–å…´è¶£æ ‡ç­¾ç³»ç»Ÿ"""
        try:
            logger.debug("æœºå™¨äººå…´è¶£ç³»ç»Ÿå¼€å§‹åˆå§‹åŒ–...")

            # åˆå§‹åŒ–embeddingæ¨¡å‹
            await self._initialize_embedding_model()

            # æ£€æŸ¥embeddingå®¢æˆ·ç«¯æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
            if not self.embedding_request:
                raise RuntimeError("Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")

            # ç”Ÿæˆæˆ–åŠ è½½å…´è¶£æ ‡ç­¾
            await self._load_or_generate_interests(personality_description, personality_id)

            self._initialized = True

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å…´è¶£æ ‡ç­¾
            if self.current_interests and len(self.current_interests.get_active_tags()) > 0:
                active_tags_count = len(self.current_interests.get_active_tags())
                logger.debug("æœºå™¨äººå…´è¶£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
                logger.debug(f"å½“å‰å·²æ¿€æ´» {active_tags_count} ä¸ªå…´è¶£æ ‡ç­¾, Embeddingç¼“å­˜ {len(self.embedding_cache)} ä¸ª")
            else:
                raise RuntimeError("æœªèƒ½æˆåŠŸåŠ è½½æˆ–ç”Ÿæˆå…´è¶£æ ‡ç­¾")

        except Exception as e:
            logger.error(f"æœºå™¨äººå…´è¶£ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            traceback.print_exc()
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å…è®¸é™çº§åˆå§‹åŒ–

    async def _initialize_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        # ä½¿ç”¨é¡¹ç›®é…ç½®çš„embeddingæ¨¡å‹
        from src.config.config import model_config
        from src.llm_models.utils_model import LLMRequest

        if model_config is None:
            raise RuntimeError("Model config is not initialized")

        # æ£€æŸ¥embeddingé…ç½®æ˜¯å¦å­˜åœ¨
        if not hasattr(model_config.model_task_config, "embedding"):
            raise RuntimeError("âŒ æœªæ‰¾åˆ°embeddingæ¨¡å‹é…ç½®")

        self.embedding_config = model_config.model_task_config.embedding

        if not self.embedding_dimension:
            logger.debug("æœªåœ¨é…ç½®ä¸­æ£€æµ‹åˆ°embeddingç»´åº¦ï¼Œå°†æ ¹æ®é¦–æ¬¡è¿”å›çš„å‘é‡è‡ªåŠ¨è¯†åˆ«")

        # åˆ›å»ºLLMRequestå®ä¾‹ç”¨äºembedding
        self.embedding_request = LLMRequest(model_set=self.embedding_config, request_type="interest_embedding")

    async def _load_or_generate_interests(self, personality_description: str, personality_id: str):
        """åŠ è½½æˆ–ç”Ÿæˆå…´è¶£æ ‡ç­¾"""

        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“åŠ è½½
        loaded_interests = await self._load_interests_from_database(personality_id)

        if loaded_interests:
            self.current_interests = loaded_interests
            active_count = len(loaded_interests.get_active_tags())        
            tags_info = [f"  - '{tag.tag_name}' (æƒé‡: {tag.weight:.2f})" for tag in loaded_interests.get_active_tags()]
            tags_str = "\n".join(tags_info)

            # ä¸ºåŠ è½½çš„æ ‡ç­¾ç”Ÿæˆembeddingï¼ˆæ•°æ®åº“ä¸å­˜å‚¨embeddingï¼Œå¯åŠ¨æ—¶åŠ¨æ€ç”Ÿæˆï¼‰
            await self._generate_embeddings_for_tags(loaded_interests)
        else:
            # ç”Ÿæˆæ–°çš„å…´è¶£æ ‡ç­¾
            logger.debug("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å…´è¶£æ ‡ç­¾ï¼Œå¼€å§‹ç”Ÿæˆ...")
            generated_interests = await self._generate_interests_from_personality(
                personality_description, personality_id
            )

            if generated_interests:
                self.current_interests = generated_interests
                active_count = len(generated_interests.get_active_tags())
                logger.debug(f"æˆåŠŸç”Ÿæˆ {active_count} ä¸ªæ–°å…´è¶£æ ‡ç­¾ã€‚")
                tags_info = [
                    f"  - '{tag.tag_name}' (æƒé‡: {tag.weight:.2f})" for tag in generated_interests.get_active_tags()
                ]
                tags_str = "\n".join(tags_info)
                logger.debug(f"å½“å‰å…´è¶£æ ‡ç­¾:\n{tags_str}")

                # ä¿å­˜åˆ°æ•°æ®åº“
                logger.debug("æ­£åœ¨ä¿å­˜è‡³æ•°æ®åº“...")
                await self._save_interests_to_database(generated_interests)
            else:
                raise RuntimeError("âŒ å…´è¶£æ ‡ç­¾ç”Ÿæˆå¤±è´¥")

    async def _generate_interests_from_personality(
        self, personality_description: str, personality_id: str
    ) -> BotPersonalityInterests | None:
        """æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾"""
        try:
            logger.debug("å¼€å§‹æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾...")

            # æ£€æŸ¥embeddingå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not hasattr(self, "embedding_request"):
                raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå…´è¶£æ ‡ç­¾")

            # æ„å»ºæç¤ºè¯
            prompt = f"""
åŸºäºä»¥ä¸‹æœºå™¨äººäººè®¾æè¿°ï¼Œç”Ÿæˆä¸€å¥—åˆé€‚çš„å…´è¶£æ ‡ç­¾ï¼š

äººè®¾æè¿°ï¼š
{personality_description}

è¯·ç”Ÿæˆä¸€ç³»åˆ—å…´è¶£å…³é”®è¯æ ‡ç­¾ï¼Œè¦æ±‚ï¼š
1. æ ‡ç­¾åº”è¯¥ç¬¦åˆäººè®¾ç‰¹ç‚¹å’Œæ€§æ ¼
2. æ¯ä¸ªæ ‡ç­¾éƒ½æœ‰æƒé‡ï¼ˆ0.1-1.0ï¼‰ï¼Œè¡¨ç¤ºå¯¹è¯¥å…´è¶£çš„å–œå¥½ç¨‹åº¦
3. ç”Ÿæˆ15-25ä¸ªä¸ç­‰çš„æ ‡ç­¾
4. æ¯ä¸ªæ ‡ç­¾åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
   - name: ç®€çŸ­çš„æ ‡ç­¾åï¼ˆ2-6ä¸ªå­—ç¬¦ï¼‰ï¼Œç”¨äºæ˜¾ç¤ºå’Œç®¡ç†ï¼Œå¦‚"Python"ã€"è¿½ç•ª"ã€"æ’¸çŒ«"
   - expanded: å®Œæ•´çš„æè¿°æ€§æ–‡æœ¬ï¼ˆ20-50ä¸ªå­—ç¬¦ï¼‰ï¼Œç”¨äºè¯­ä¹‰åŒ¹é…ï¼Œæè¿°è¿™ä¸ªå…´è¶£çš„å…·ä½“å†…å®¹å’Œåœºæ™¯
5. expanded æ‰©å±•æè¿°è¦æ±‚ï¼š
   - å¿…é¡»æ˜¯å®Œæ•´çš„å¥å­æˆ–çŸ­è¯­ï¼ŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯
   - æè¿°å…·ä½“çš„å¯¹è¯åœºæ™¯ã€æ´»åŠ¨å†…å®¹ã€ç›¸å…³è¯é¢˜
   - é¿å…è¿‡äºæŠ½è±¡ï¼Œè¦æœ‰æ˜ç¡®çš„è¯­å¢ƒ
   - ç¤ºä¾‹ï¼š
     * "Python" -> "è®¨è®ºPythonç¼–ç¨‹è¯­è¨€ã€å†™Pythonä»£ç ã€Pythonè„šæœ¬å¼€å‘ã€PythonæŠ€æœ¯é—®é¢˜"
     * "è¿½ç•ª" -> "è®¨è®ºæ­£åœ¨æ’­å‡ºçš„åŠ¨æ¼«ç•ªå‰§ã€è¿½ç•ªè¿›åº¦ã€åŠ¨æ¼«å‰§æƒ…ã€ç•ªå‰§æ¨èã€åŠ¨æ¼«è§’è‰²"
     * "æ’¸çŒ«" -> "è®¨è®ºçŒ«å’ªå® ç‰©ã€æ™’çŒ«åˆ†äº«ã€èŒå® æ—¥å¸¸ã€å¯çˆ±çŒ«çŒ«ã€å…»çŒ«å¿ƒå¾—"
     * "ç¤¾æ" -> "è¡¨è¾¾ç¤¾äº¤ç„¦è™‘ã€ä¸æƒ³è§äººã€æƒ³èº²èµ·æ¥ã€å®³æ€•ç¤¾äº¤çš„å¿ƒæƒ…"
     * "æ·±å¤œç ä»£ç " -> "æ·±å¤œå†™ä»£ç ã€ç†¬å¤œç¼–ç¨‹ã€å¤œçŒ«å­ç¨‹åºå‘˜ã€æ·±å¤œè°ƒè¯•bug"

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "interests": [
        {{
            "name": "Python",
            "expanded": "è®¨è®ºPythonç¼–ç¨‹è¯­è¨€ã€å†™Pythonä»£ç ã€Pythonè„šæœ¬å¼€å‘ã€PythonæŠ€æœ¯é—®é¢˜",
            "weight": 0.9
        }},
        {{
            "name": "è¿½ç•ª",
            "expanded": "è®¨è®ºæ­£åœ¨æ’­å‡ºçš„åŠ¨æ¼«ç•ªå‰§ã€è¿½ç•ªè¿›åº¦ã€åŠ¨æ¼«å‰§æƒ…ã€ç•ªå‰§æ¨èã€åŠ¨æ¼«è§’è‰²",
            "weight": 0.85
        }},
        {{
            "name": "æ’¸çŒ«",
            "expanded": "è®¨è®ºçŒ«å’ªå® ç‰©ã€æ™’çŒ«åˆ†äº«ã€èŒå® æ—¥å¸¸ã€å¯çˆ±çŒ«çŒ«ã€å…»çŒ«å¿ƒå¾—",
            "weight": 0.95
        }}
    ]
}}

æ³¨æ„ï¼š
- name: ç®€çŸ­æ ‡ç­¾åï¼Œ2-6ä¸ªå­—ç¬¦ï¼Œæ–¹ä¾¿æ˜¾ç¤º
- expanded: å®Œæ•´æè¿°ï¼Œ20-50ä¸ªå­—ç¬¦ï¼Œç”¨äºç²¾å‡†çš„è¯­ä¹‰åŒ¹é…
- weight: æƒé‡èŒƒå›´0.1-1.0ï¼Œæƒé‡è¶Šé«˜è¡¨ç¤ºè¶Šæ„Ÿå…´è¶£
- æ ¹æ®äººè®¾ç”Ÿæˆä¸ªæ€§åŒ–ã€å…·ä½“çš„æ ‡ç­¾å’Œæè¿°
- expanded æè¿°è¦æœ‰å…·ä½“åœºæ™¯ï¼Œé¿å…æ³›åŒ–
"""

            # è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾
            response = await self._call_llm_for_interest_generation(prompt)

            if not response:
                raise RuntimeError("âŒ LLMæœªè¿”å›æœ‰æ•ˆå“åº”")

            # ä½¿ç”¨ç»Ÿä¸€çš„ JSON è§£æå·¥å…·
            interests_data = extract_and_parse_json(response, strict=False)
            if not interests_data or not isinstance(interests_data, dict):
                raise RuntimeError("âŒ è§£æLLMå“åº”å¤±è´¥ï¼Œæœªè·å–åˆ°æœ‰æ•ˆçš„JSONæ•°æ®")

            bot_interests = BotPersonalityInterests(
                personality_id=personality_id, personality_description=personality_description
            )

            # è§£æç”Ÿæˆçš„å…´è¶£æ ‡ç­¾
            interests_list = interests_data.get("interests", [])
            logger.debug(f"ğŸ“‹ è§£æåˆ° {len(interests_list)} ä¸ªå…´è¶£æ ‡ç­¾")

            for i, tag_data in enumerate(interests_list):
                tag_name = tag_data.get("name", f"æ ‡ç­¾_{i}")
                weight = tag_data.get("weight", 0.5)
                expanded = tag_data.get("expanded")  # è·å–æ‰©å±•æè¿°

                # æ£€æŸ¥æ ‡ç­¾é•¿åº¦ï¼Œå¦‚æœè¿‡é•¿åˆ™æˆªæ–­
                if len(tag_name) > 10:
                    logger.warning(f"âš ï¸ æ ‡ç­¾ '{tag_name}' è¿‡é•¿ï¼Œå°†æˆªæ–­ä¸º10ä¸ªå­—ç¬¦")
                    tag_name = tag_name[:10]

                # éªŒè¯æ‰©å±•æè¿°
                if expanded:
                    logger.debug(f"   ğŸ·ï¸  {tag_name} (æƒé‡: {weight:.2f})")
                    logger.debug(f"      ğŸ“ æ‰©å±•: {expanded}")
                else:
                    logger.warning(f"   âš ï¸ æ ‡ç­¾ '{tag_name}' ç¼ºå°‘æ‰©å±•æè¿°ï¼Œå°†ä½¿ç”¨å›é€€æ–¹æ¡ˆ")

                tag = BotInterestTag(tag_name=tag_name, weight=weight, expanded=expanded)
                bot_interests.interest_tags.append(tag)

            # ä¸ºæ‰€æœ‰æ ‡ç­¾ç”Ÿæˆembedding
            logger.debug("å¼€å§‹ä¸ºå…´è¶£æ ‡ç­¾ç”Ÿæˆembeddingå‘é‡...")
            await self._generate_embeddings_for_tags(bot_interests)

            logger.debug("å…´è¶£æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
            return bot_interests

        except Exception as e:
            logger.error(f"âŒ æ ¹æ®äººè®¾ç”Ÿæˆå…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            traceback.print_exc()
            raise

    async def _call_llm_for_interest_generation(self, prompt: str) -> str | None:
        """è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¼šä¸´æ—¶å¢åŠ  API è¶…æ—¶æ—¶é—´ï¼Œä»¥ç¡®ä¿åˆå§‹åŒ–é˜¶æ®µçš„äººè®¾æ ‡ç­¾ç”Ÿæˆ
        ä¸ä¼šå› ç”¨æˆ·é…ç½®çš„è¾ƒçŸ­è¶…æ—¶è€Œå¤±è´¥ã€‚
        """
        try:
            logger.debug("é…ç½®LLMå®¢æˆ·ç«¯...")

            # ä½¿ç”¨llm_apiæ¥å¤„ç†è¯·æ±‚
            from src.config.config import model_config
            from src.plugin_system.apis import llm_api

            if model_config is None:
                raise RuntimeError("Model config is not initialized")

            # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚åªè¿”å›çº¯JSON
            full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨äººäººè®¾åˆ†æå¸ˆï¼Œæ“…é•¿æ ¹æ®äººè®¾æè¿°ç”Ÿæˆåˆé€‚çš„å…´è¶£æ ‡ç­¾ã€‚

{prompt}

è¯·ç¡®ä¿è¿”å›æ ¼å¼ä¸ºæœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚åªè¿”å›JSONå¯¹è±¡æœ¬èº«ã€‚"""

            # ä½¿ç”¨replyeræ¨¡å‹é…ç½®
            replyer_config = model_config.model_task_config.replyer

            # ğŸ”§ ä¸´æ—¶å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé¿å…åˆå§‹åŒ–é˜¶æ®µå› è¶…æ—¶å¤±è´¥
            # äººè®¾æ ‡ç­¾ç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆ15-25ä¸ªæ ‡ç­¾çš„JSONï¼‰ï¼Œä½¿ç”¨æ›´é•¿çš„è¶…æ—¶
            INIT_TIMEOUT = 180  # åˆå§‹åŒ–é˜¶æ®µä½¿ç”¨ 180 ç§’è¶…æ—¶
            original_timeouts: dict[str, int] = {}
            
            try:
                # ä¿å­˜å¹¶ä¿®æ”¹æ‰€æœ‰ç›¸å…³æ¨¡å‹çš„ API provider è¶…æ—¶è®¾ç½®
                for model_name in replyer_config.model_list:
                    try:
                        model_info = model_config.get_model_info(model_name)
                        provider = model_config.get_provider(model_info.api_provider)
                        original_timeouts[provider.name] = provider.timeout
                        if provider.timeout < INIT_TIMEOUT:
                            logger.debug(f"â±ï¸ ä¸´æ—¶å¢åŠ  API provider '{provider.name}' è¶…æ—¶: {provider.timeout}s â†’ {INIT_TIMEOUT}s")
                            provider.timeout = INIT_TIMEOUT
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ— æ³•ä¿®æ”¹æ¨¡å‹ '{model_name}' çš„è¶…æ—¶è®¾ç½®: {e}")
                
                # è°ƒç”¨LLM API
                success, response, reasoning_content, model_name = await llm_api.generate_with_model(
                    prompt=full_prompt,
                    model_config=replyer_config,
                    request_type="interest_generation",
                    temperature=0.7,
                    max_tokens=2000,
                )
            finally:
                # ğŸ”§ æ¢å¤åŸå§‹è¶…æ—¶è®¾ç½®
                for provider_name, original_timeout in original_timeouts.items():
                    try:
                        provider = model_config.get_provider(provider_name)
                        if provider.timeout != original_timeout:
                            logger.debug(f"â±ï¸ æ¢å¤ API provider '{provider_name}' è¶…æ—¶: {provider.timeout}s â†’ {original_timeout}s")
                            provider.timeout = original_timeout
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ— æ³•æ¢å¤ provider '{provider_name}' çš„è¶…æ—¶è®¾ç½®: {e}")

            if success and response:
                # ç›´æ¥è¿”å›åŸå§‹å“åº”ï¼Œåç»­ä½¿ç”¨ç»Ÿä¸€çš„ JSON è§£æå·¥å…·
                return response
            else:
                logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”æˆ–è°ƒç”¨å¤±è´¥")
                return None

        except Exception as e:
            logger.error(f"âŒ è°ƒç”¨LLMç”Ÿæˆå…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return None

    async def _generate_embeddings_for_tags(self, interests: BotPersonalityInterests):
        """ä¸ºæ‰€æœ‰å…´è¶£æ ‡ç­¾ç”Ÿæˆembeddingï¼ˆç¼“å­˜åœ¨å†…å­˜å’Œæ–‡ä»¶ä¸­ï¼‰"""
        if not hasattr(self, "embedding_request"):
            raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆembedding")

        total_tags = len(interests.interest_tags)

        # å°è¯•ä»æ–‡ä»¶åŠ è½½ç¼“å­˜
        file_cache = await self._load_embedding_cache_from_file(interests.personality_id)
        if file_cache:
            self.embedding_cache.update(file_cache)

        memory_cached_count = 0
        file_cached_count = 0
        generated_count = 0
        failed_count = 0

        for i, tag in enumerate(interests.interest_tags, 1):
            if tag.tag_name in self.embedding_cache:
                # ä½¿ç”¨ç¼“å­˜çš„embeddingï¼ˆå¯èƒ½æ¥è‡ªå†…å­˜æˆ–æ–‡ä»¶ï¼‰
                tag.embedding = self.embedding_cache[tag.tag_name]
                if file_cache and tag.tag_name in file_cache:
                    file_cached_count += 1
                    logger.debug(f"   [{i}/{total_tags}] ğŸ“‚ '{tag.tag_name}' - ä½¿ç”¨æ–‡ä»¶ç¼“å­˜")
                else:
                    memory_cached_count += 1
                    logger.debug(f"   [{i}/{total_tags}] ğŸ’¾ '{tag.tag_name}' - ä½¿ç”¨å†…å­˜ç¼“å­˜")
            else:
                # åŠ¨æ€ç”Ÿæˆæ–°çš„embedding
                embedding_text = tag.tag_name
                embedding = await self._get_embedding(embedding_text)

                if embedding:
                    tag.embedding = embedding  # è®¾ç½®åˆ° tag å¯¹è±¡ï¼ˆå†…å­˜ä¸­ï¼‰
                    self.embedding_cache[tag.tag_name] = embedding  # åŒæ—¶ç¼“å­˜åˆ°å†…å­˜
                    generated_count += 1
                    logger.debug(f"   âœ… '{tag.tag_name}' embeddingåŠ¨æ€ç”ŸæˆæˆåŠŸ")
                else:
                    failed_count += 1
                    logger.warning(f"   âŒ '{tag.tag_name}' embeddingç”Ÿæˆå¤±è´¥")

        if failed_count > 0:
            raise RuntimeError(f"âŒ æœ‰ {failed_count} ä¸ªå…´è¶£æ ‡ç­¾embeddingç”Ÿæˆå¤±è´¥")

        # å¦‚æœæœ‰æ–°ç”Ÿæˆçš„embeddingï¼Œä¿å­˜åˆ°æ–‡ä»¶
        if generated_count > 0:
            await self._save_embedding_cache_to_file(interests.personality_id)

        interests.last_updated = datetime.now()

    async def _get_embedding(self, text: str) -> list[float]:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        if not hasattr(self, "embedding_request"):
            raise RuntimeError("âŒ Embeddingè¯·æ±‚å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        # æ£€æŸ¥ç¼“å­˜
        if text in self.embedding_cache:
            return self.embedding_cache[text]

        # ä½¿ç”¨LLMRequestè·å–embedding
        if not self.embedding_request:
            raise RuntimeError("âŒ Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        embedding, model_name = await self.embedding_request.get_embedding(text)

        if embedding and len(embedding) > 0:
            if isinstance(embedding[0], list):
                # If it's a list of lists, take the first one (though get_embedding(str) should return list[float])
                embedding = embedding[0]
            
            # Now we can safely cast to list[float] as we've handled the nested list case
            embedding_float = cast(list[float], embedding)
            self.embedding_cache[text] = embedding_float

            current_dim = len(embedding_float)
            if self._detected_embedding_dimension is None:
                self._detected_embedding_dimension = current_dim
                if self.embedding_dimension and self.embedding_dimension != current_dim:
                    logger.warning(
                        "âš ï¸ å®é™…embeddingç»´åº¦(%d)ä¸é…ç½®å€¼(%d)ä¸ä¸€è‡´ï¼Œè¯·åœ¨ model_config.model_task_config.embedding.embedding_dimension ä¸­åŒæ­¥æ›´æ–°",
                        current_dim,
                        self.embedding_dimension,
                    )
                else:
                    self.embedding_dimension = current_dim
            elif current_dim != self.embedding_dimension:
                logger.warning(
                    "âš ï¸ æ”¶åˆ°çš„embeddingç»´åº¦å‘ç”Ÿå˜åŒ–: ä¹‹å‰=%d, å½“å‰=%dã€‚è¯·ç¡®è®¤æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚",
                    self.embedding_dimension,
                    current_dim,
                )
            return embedding_float
        else:
            raise RuntimeError(f"âŒ è¿”å›çš„embeddingä¸ºç©º: {embedding}")

    async def _generate_message_embedding(self, message_text: str, keywords: list[str]) -> list[float]:
        """ä¸ºæ¶ˆæ¯ç”Ÿæˆembeddingå‘é‡"""
        # ç»„åˆæ¶ˆæ¯æ–‡æœ¬å’Œå…³é”®è¯ä½œä¸ºembeddingè¾“å…¥
        if keywords:
            combined_text = f"{message_text} {' '.join(keywords)}"
        else:
            combined_text = message_text

        # ç”Ÿæˆembedding
        embedding = await self._get_embedding(combined_text)
        return embedding

    async def generate_embeddings_for_texts(
        self, text_map: dict[str, str], batch_size: int = 16
    ) -> dict[str, list[float]]:
        """æ‰¹é‡è·å–å¤šæ®µæ–‡æœ¬çš„embeddingï¼Œä¾›ä¸Šå±‚ç»Ÿä¸€å¤„ç†ã€‚"""
        if not text_map:
            return {}

        if not self.embedding_request:
            raise RuntimeError("Embeddingå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        batch_size = max(1, batch_size)
        keys = list(text_map.keys())
        results: dict[str, list[float]] = {}

        for start in range(0, len(keys), batch_size):
            chunk_keys = keys[start : start + batch_size]
            chunk_texts = [text_map[key] or "" for key in chunk_keys]

            try:
                chunk_embeddings, _ = await self.embedding_request.get_embedding(chunk_texts)
            except Exception as exc:  # noqa: BLE001
                logger.error(f"æ‰¹é‡è·å–embeddingå¤±è´¥ (chunk {start // batch_size + 1}): {exc}")
                continue

            if isinstance(chunk_embeddings, list) and chunk_embeddings and isinstance(chunk_embeddings[0], list):
                normalized = chunk_embeddings
            elif isinstance(chunk_embeddings, list):
                normalized = [chunk_embeddings]
            else:
                normalized = []

            for idx_offset, message_id in enumerate(chunk_keys):
                vector = normalized[idx_offset] if idx_offset < len(normalized) else []
                if isinstance(vector, list) and vector and isinstance(vector[0], float):
                     results[message_id] = cast(list[float], vector)
                else:
                     results[message_id] = []

        return results

    async def _calculate_similarity_scores(
        self, result: InterestMatchResult, message_embedding: list[float], keywords: list[str]
    ):
        """è®¡ç®—æ¶ˆæ¯ä¸å…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not self.current_interests:
                return

            active_tags = self.current_interests.get_active_tags()
            if not active_tags:
                return

            logger.debug(f"ğŸ” å¼€å§‹è®¡ç®—ä¸ {len(active_tags)} ä¸ªå…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦")

            for tag in active_tags:
                if tag.embedding:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = self._calculate_cosine_similarity(message_embedding, tag.embedding)
                    weighted_score = similarity * tag.weight

                    # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º0.3
                    if similarity > 0.3:
                        result.add_match(tag.tag_name, weighted_score, keywords)
                        logger.debug(
                            f"   ğŸ·ï¸  '{tag.tag_name}': ç›¸ä¼¼åº¦={similarity:.3f}, æƒé‡={tag.weight:.2f}, åŠ æƒåˆ†æ•°={weighted_score:.3f}"
                        )

        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°å¤±è´¥: {e}")

    async def calculate_interest_match(
        self, message_text: str, keywords: list[str] | None = None, message_embedding: list[float] | None = None
    ) -> InterestMatchResult:
        """è®¡ç®—æ¶ˆæ¯ä¸æœºå™¨äººå…´è¶£çš„åŒ¹é…åº¦ï¼ˆä¼˜åŒ–ç‰ˆ - æ ‡ç­¾æ‰©å±•ç­–ç•¥ï¼‰

        æ ¸å¿ƒä¼˜åŒ–ï¼šå°†çŸ­æ ‡ç­¾æ‰©å±•ä¸ºå®Œæ•´çš„æè¿°æ€§å¥å­ï¼Œè§£å†³è¯­ä¹‰ç²’åº¦ä¸åŒ¹é…é—®é¢˜

        åŸé—®é¢˜ï¼š
        - æ¶ˆæ¯: "ä»Šå¤©å¤©æ°”ä¸é”™" (å®Œæ•´å¥å­)
        - æ ‡ç­¾: "è¹­äººæ²»æ„ˆ" (2-4å­—çŸ­è¯­)
        - ç»“æœ: è¯¯åŒ¹é…ï¼Œå› ä¸ºçŸ­æ ‡ç­¾çš„ embedding è¿‡äºæŠ½è±¡

        è§£å†³æ–¹æ¡ˆï¼š
        - æ ‡ç­¾æ‰©å±•: "è¹­äººæ²»æ„ˆ" -> "è¡¨è¾¾äº²è¿‘ã€å¯»æ±‚å®‰æ…°ã€æ’’å¨‡çš„å†…å®¹"
        - ç°åœ¨æ˜¯: å¥å­ vs å¥å­ï¼ŒåŒ¹é…æ›´å‡†ç¡®
        """
        if not self.current_interests or not self._initialized:
            raise RuntimeError("âŒ å…´è¶£æ ‡ç­¾ç³»ç»Ÿæœªåˆå§‹åŒ–")

        logger.debug(f"å¼€å§‹è®¡ç®—å…´è¶£åŒ¹é…åº¦: æ¶ˆæ¯é•¿åº¦={len(message_text)}, å…³é”®è¯æ•°={len(keywords) if keywords else 0}")

        message_id = f"msg_{datetime.now().timestamp()}"
        result = InterestMatchResult(message_id=message_id)

        # è·å–æ´»è·ƒçš„å…´è¶£æ ‡ç­¾
        active_tags = self.current_interests.get_active_tags()
        if not active_tags:
            raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ°æ´»è·ƒçš„å…´è¶£æ ‡ç­¾")

        logger.debug(f"æ­£åœ¨ä¸ {len(active_tags)} ä¸ªå…´è¶£æ ‡ç­¾è¿›è¡ŒåŒ¹é…...")

        # ç”Ÿæˆæ¶ˆæ¯çš„embedding
        logger.debug("æ­£åœ¨ç”Ÿæˆæ¶ˆæ¯ embedding...")
        if not message_embedding:
            message_embedding = await self._get_embedding(message_text)
        logger.debug(f"æ¶ˆæ¯ embedding ç”ŸæˆæˆåŠŸ, ç»´åº¦: {len(message_embedding)}")

        # è®¡ç®—ä¸æ¯ä¸ªå…´è¶£æ ‡ç­¾çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨æ‰©å±•æ ‡ç­¾ï¼‰
        match_count = 0
        high_similarity_count = 0
        medium_similarity_count = 0
        low_similarity_count = 0

        if global_config is None:
            raise RuntimeError("Global config is not initialized")

        # åˆ†çº§ç›¸ä¼¼åº¦é˜ˆå€¼ - ä¼˜åŒ–åå¯ä»¥æé«˜é˜ˆå€¼ï¼Œå› ä¸ºåŒ¹é…æ›´å‡†ç¡®äº†
        affinity_config = global_config.affinity_flow
        high_threshold = affinity_config.high_match_interest_threshold
        medium_threshold = affinity_config.medium_match_interest_threshold
        low_threshold = affinity_config.low_match_interest_threshold

        logger.debug(f"ğŸ” ä½¿ç”¨åˆ†çº§ç›¸ä¼¼åº¦é˜ˆå€¼: é«˜={high_threshold}, ä¸­={medium_threshold}, ä½={low_threshold}")

        for tag in active_tags:
            if tag.embedding:
                # ğŸ”§ ä¼˜åŒ–ï¼šè·å–æ‰©å±•æ ‡ç­¾çš„ embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
                expanded_embedding = await self._get_expanded_tag_embedding(tag.tag_name)

                if expanded_embedding:
                    # ä½¿ç”¨æ‰©å±•æ ‡ç­¾çš„ embedding è¿›è¡ŒåŒ¹é…
                    similarity = self._calculate_cosine_similarity(message_embedding, expanded_embedding)

                    # åŒæ—¶è®¡ç®—åŸå§‹æ ‡ç­¾çš„ç›¸ä¼¼åº¦ä½œä¸ºå‚è€ƒ
                    original_similarity = self._calculate_cosine_similarity(message_embedding, tag.embedding)

                    # æ··åˆç­–ç•¥ï¼šæ‰©å±•æ ‡ç­¾æƒé‡æ›´é«˜ï¼ˆ70%ï¼‰ï¼ŒåŸå§‹æ ‡ç­¾ä½œä¸ºè¡¥å……ï¼ˆ30%ï¼‰
                    # è¿™æ ·å¯ä»¥å…¼é¡¾å‡†ç¡®æ€§ï¼ˆæ‰©å±•ï¼‰å’Œçµæ´»æ€§ï¼ˆåŸå§‹ï¼‰
                    final_similarity = similarity * 0.7 + original_similarity * 0.3

                    logger.debug(f"æ ‡ç­¾'{tag.tag_name}': åŸå§‹={original_similarity:.3f}, æ‰©å±•={similarity:.3f}, æœ€ç»ˆ={final_similarity:.3f}")
                else:
                    # å¦‚æœæ‰©å±• embedding è·å–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ embedding
                    final_similarity = self._calculate_cosine_similarity(message_embedding, tag.embedding)
                    logger.debug(f"æ ‡ç­¾'{tag.tag_name}': ä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦={final_similarity:.3f}")

                # åŸºç¡€åŠ æƒåˆ†æ•°
                weighted_score = final_similarity * tag.weight

                # æ ¹æ®ç›¸ä¼¼åº¦ç­‰çº§åº”ç”¨ä¸åŒçš„åŠ æˆ
                if final_similarity > high_threshold:
                    # é«˜ç›¸ä¼¼åº¦ï¼šå¼ºåŠ æˆ
                    enhanced_score = weighted_score * affinity_config.high_match_keyword_multiplier
                    match_count += 1
                    high_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])

                elif final_similarity > medium_threshold:
                    # ä¸­ç›¸ä¼¼åº¦ï¼šä¸­ç­‰åŠ æˆ
                    enhanced_score = weighted_score * affinity_config.medium_match_keyword_multiplier
                    match_count += 1
                    medium_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])

                elif final_similarity > low_threshold:
                    # ä½ç›¸ä¼¼åº¦ï¼šè½»å¾®åŠ æˆ
                    enhanced_score = weighted_score * affinity_config.low_match_keyword_multiplier
                    match_count += 1
                    low_similarity_count += 1
                    result.add_match(tag.tag_name, enhanced_score, [tag.tag_name])

        logger.debug(
            f"åŒ¹é…ç»Ÿè®¡: {match_count}/{len(active_tags)} ä¸ªæ ‡ç­¾å‘½ä¸­ | "
            f"é«˜(>{high_threshold}): {high_similarity_count}, "
            f"ä¸­(>{medium_threshold}): {medium_similarity_count}, "
            f"ä½(>{low_threshold}): {low_similarity_count}"
        )

        # æ·»åŠ ç›´æ¥å…³é”®è¯åŒ¹é…å¥–åŠ±
        keyword_bonus = self._calculate_keyword_match_bonus(keywords or [], result.matched_tags)
        logger.debug(f"ğŸ¯ å…³é”®è¯ç›´æ¥åŒ¹é…å¥–åŠ±: {keyword_bonus}")

        # åº”ç”¨å…³é”®è¯å¥–åŠ±åˆ°åŒ¹é…åˆ†æ•°
        for tag_name in result.matched_tags:
            if tag_name in keyword_bonus:
                original_score = result.match_scores[tag_name]
                bonus = keyword_bonus[tag_name]
                result.match_scores[tag_name] = original_score + bonus
                logger.debug(
                    f"   ğŸ·ï¸  '{tag_name}': åŸå§‹åˆ†æ•°={original_score:.3f}, å¥–åŠ±={bonus:.3f}, æœ€ç»ˆåˆ†æ•°={result.match_scores[tag_name]:.3f}"
                )

        # è®¡ç®—æ€»ä½“åˆ†æ•°
        result.calculate_overall_score()

        # ç¡®å®šæœ€ä½³åŒ¹é…æ ‡ç­¾
        if result.matched_tags:
            top_tag_name = max(result.match_scores.items(), key=lambda x: x[1])[0]
            result.top_tag = top_tag_name
            logger.debug(f"æœ€ä½³åŒ¹é…: '{top_tag_name}' (åˆ†æ•°: {result.match_scores[top_tag_name]:.3f})")

        logger.debug(
            f"æœ€ç»ˆç»“æœ: æ€»åˆ†={result.overall_score:.3f}, ç½®ä¿¡åº¦={result.confidence:.3f}, åŒ¹é…æ ‡ç­¾æ•°={len(result.matched_tags)}"
        )

        # å¦‚æœæœ‰æ–°ç”Ÿæˆçš„æ‰©å±•embeddingï¼Œä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
        if hasattr(self, "_new_expanded_embeddings_generated") and self._new_expanded_embeddings_generated:
            await self._save_embedding_cache_to_file(self.current_interests.personality_id)
            self._new_expanded_embeddings_generated = False
            logger.debug("ğŸ’¾ å·²ä¿å­˜æ–°ç”Ÿæˆçš„æ‰©å±•embeddingåˆ°ç¼“å­˜æ–‡ä»¶")

        return result

    async def _get_expanded_tag_embedding(self, tag_name: str) -> list[float] | None:
        """è·å–æ‰©å±•æ ‡ç­¾çš„ embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰

        ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆå¹¶ç¼“å­˜
        """
        # æ£€æŸ¥ç¼“å­˜
        if tag_name in self.expanded_embedding_cache:
            return self.expanded_embedding_cache[tag_name]

        # æ‰©å±•æ ‡ç­¾
        expanded_tag = self._expand_tag_for_matching(tag_name)

        # ç”Ÿæˆ embedding
        try:
            embedding = await self._get_embedding(expanded_tag)
            if embedding:
                # ç¼“å­˜ç»“æœ
                self.expanded_tag_cache[tag_name] = expanded_tag
                self.expanded_embedding_cache[tag_name] = embedding
                self._new_expanded_embeddings_generated = True  # æ ‡è®°æœ‰æ–°ç”Ÿæˆçš„embedding
                logger.debug(f"âœ… ä¸ºæ ‡ç­¾'{tag_name}'ç”Ÿæˆå¹¶ç¼“å­˜æ‰©å±•embedding: {expanded_tag[:50]}...")
                return embedding
        except Exception as e:
            logger.warning(f"ä¸ºæ ‡ç­¾'{tag_name}'ç”Ÿæˆæ‰©å±•embeddingå¤±è´¥: {e}")

        return None

    def _expand_tag_for_matching(self, tag_name: str) -> str:
        """å°†çŸ­æ ‡ç­¾æ‰©å±•ä¸ºå®Œæ•´çš„æè¿°æ€§å¥å­

        è¿™æ˜¯è§£å†³"æ ‡ç­¾å¤ªçŸ­å¯¼è‡´è¯¯åŒ¹é…"çš„æ ¸å¿ƒæ–¹æ³•

        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨ LLM ç”Ÿæˆçš„ expanded å­—æ®µï¼ˆæœ€å‡†ç¡®ï¼‰
        2. å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„å›é€€æ–¹æ¡ˆ
        3. æœ€åä½¿ç”¨é€šç”¨æ¨¡æ¿

        ç¤ºä¾‹ï¼š
        - "Python" + expanded -> "è®¨è®ºPythonç¼–ç¨‹è¯­è¨€ã€å†™Pythonä»£ç ã€Pythonè„šæœ¬å¼€å‘ã€PythonæŠ€æœ¯é—®é¢˜"
        - "è¹­äººæ²»æ„ˆ" + expanded -> "æƒ³è¦è·å¾—å®‰æ…°ã€å¯»æ±‚æ¸©æš–å…³æ€€ã€æ’’å¨‡å–èŒã€è¡¨è¾¾äº²æ˜µã€æ±‚æŠ±æŠ±æ±‚é™ªä¼´çš„å¯¹è¯"
        """
        # ä½¿ç”¨ç¼“å­˜
        if tag_name in self.expanded_tag_cache:
            return self.expanded_tag_cache[tag_name]

        # ğŸ¯ ä¼˜å…ˆç­–ç•¥ï¼šä½¿ç”¨ LLM ç”Ÿæˆçš„ expanded å­—æ®µ
        if self.current_interests:
            for tag in self.current_interests.interest_tags:
                if tag.tag_name == tag_name and tag.expanded:
                    logger.debug(f"âœ… ä½¿ç”¨LLMç”Ÿæˆçš„æ‰©å±•æè¿°: {tag_name} -> {tag.expanded[:50]}...")
                    self.expanded_tag_cache[tag_name] = tag.expanded
                    return tag.expanded

        # ğŸ”§ å›é€€ç­–ç•¥ï¼šåŸºäºè§„åˆ™çš„æ‰©å±•ï¼ˆç”¨äºå…¼å®¹æ—§æ•°æ®æˆ–LLMæœªç”Ÿæˆæ‰©å±•çš„æƒ…å†µï¼‰
        logger.debug(f"âš ï¸ æ ‡ç­¾'{tag_name}'æ²¡æœ‰LLMæ‰©å±•æè¿°ï¼Œä½¿ç”¨è§„åˆ™å›é€€æ–¹æ¡ˆ")
        tag_lower = tag_name.lower()

        # æŠ€æœ¯ç¼–ç¨‹ç±»æ ‡ç­¾ï¼ˆå…·ä½“åŒ–æè¿°ï¼‰
        if any(word in tag_lower for word in ["python", "java", "code", "ä»£ç ", "ç¼–ç¨‹", "è„šæœ¬", "ç®—æ³•", "å¼€å‘"]):
            if "python" in tag_lower:
                return "è®¨è®ºPythonç¼–ç¨‹è¯­è¨€ã€å†™Pythonä»£ç ã€Pythonè„šæœ¬å¼€å‘ã€PythonæŠ€æœ¯é—®é¢˜"
            elif "ç®—æ³•" in tag_lower:
                return "è®¨è®ºç®—æ³•é¢˜ç›®ã€æ•°æ®ç»“æ„ã€ç¼–ç¨‹ç«èµ›ã€åˆ·LeetCodeé¢˜ç›®ã€ä»£ç ä¼˜åŒ–"
            elif "ä»£ç " in tag_lower or "è¢«çª" in tag_lower:
                return "è®¨è®ºå†™ä»£ç ã€ç¼–ç¨‹å¼€å‘ã€ä»£ç å®ç°ã€æŠ€æœ¯æ–¹æ¡ˆã€ç¼–ç¨‹æŠ€å·§"
            else:
                return "è®¨è®ºç¼–ç¨‹å¼€å‘ã€è½¯ä»¶æŠ€æœ¯ã€ä»£ç ç¼–å†™ã€æŠ€æœ¯å®ç°"

        # æƒ…æ„Ÿè¡¨è¾¾ç±»æ ‡ç­¾ï¼ˆå…·ä½“åŒ–ä¸ºçœŸå®å¯¹è¯åœºæ™¯ï¼‰
        elif any(word in tag_lower for word in ["æ²»æ„ˆ", "æ’’å¨‡", "å®‰æ…°", "å‘¼å™œ", "è¹­", "å–èŒ"]):
            return "æƒ³è¦è·å¾—å®‰æ…°ã€å¯»æ±‚æ¸©æš–å…³æ€€ã€æ’’å¨‡å–èŒã€è¡¨è¾¾äº²æ˜µã€æ±‚æŠ±æŠ±æ±‚é™ªä¼´çš„å¯¹è¯"

        # æ¸¸æˆå¨±ä¹ç±»æ ‡ç­¾ï¼ˆå…·ä½“æ¸¸æˆåœºæ™¯ï¼‰
        elif any(word in tag_lower for word in ["æ¸¸æˆ", "ç½‘æ¸¸", "mmo", "æ¸¸", "ç©"]):
            return "è®¨è®ºç½‘ç»œæ¸¸æˆã€MMOæ¸¸æˆã€æ¸¸æˆç©æ³•ã€ç»„é˜Ÿæ‰“å‰¯æœ¬ã€æ¸¸æˆæ”»ç•¥å¿ƒå¾—"

        # åŠ¨æ¼«å½±è§†ç±»æ ‡ç­¾ï¼ˆå…·ä½“è§‚çœ‹è¡Œä¸ºï¼‰
        elif any(word in tag_lower for word in ["ç•ª", "åŠ¨æ¼«", "è§†é¢‘", "bç«™", "å¼¹å¹•", "è¿½ç•ª", "äº‘æ–°ç•ª"]):
            # ç‰¹åˆ«å¤„ç†"äº‘æ–°ç•ª" - å®ƒçš„æ„æ€æ˜¯åœ¨ç½‘ä¸Šçœ‹æ–°åŠ¨æ¼«ï¼Œä¸æ˜¯æ³›æ³›çš„"æ–°ä¸œè¥¿"
            if "äº‘" in tag_lower or "æ–°ç•ª" in tag_lower:
                return "è®¨è®ºæ­£åœ¨æ’­å‡ºçš„æ–°åŠ¨æ¼«ã€æ–°ç•ªå‰§é›†ã€åŠ¨æ¼«å‰§æƒ…ã€è¿½ç•ªå¿ƒå¾—ã€åŠ¨æ¼«è§’è‰²"
            else:
                return "è®¨è®ºåŠ¨æ¼«ç•ªå‰§å†…å®¹ã€Bç«™è§†é¢‘ã€å¼¹å¹•æ–‡åŒ–ã€è¿½ç•ªä½“éªŒ"

        # ç¤¾äº¤å¹³å°ç±»æ ‡ç­¾ï¼ˆå…·ä½“å¹³å°è¡Œä¸ºï¼‰
        elif any(word in tag_lower for word in ["å°çº¢ä¹¦", "è´´å§", "è®ºå›", "ç¤¾åŒº", "åƒç“œ", "å…«å¦"]):
            if "åƒç“œ" in tag_lower:
                return "èŠå…«å¦çˆ†æ–™ã€åƒç“œçœ‹çƒ­é—¹ã€ç½‘ç»œçƒ­ç‚¹äº‹ä»¶ã€ç¤¾äº¤å¹³å°çƒ­è®®è¯é¢˜"
            else:
                return "è®¨è®ºç¤¾äº¤å¹³å°å†…å®¹ã€ç½‘ç»œç¤¾åŒºè¯é¢˜ã€è®ºå›è®¨è®ºã€åˆ†äº«ç”Ÿæ´»"

        # ç”Ÿæ´»æ—¥å¸¸ç±»æ ‡ç­¾ï¼ˆå…·ä½“èŒå® åœºæ™¯ï¼‰
        elif any(word in tag_lower for word in ["çŒ«", "å® ç‰©", "å°¾å·´", "è€³æœµ", "æ¯›ç»’"]):
            return "è®¨è®ºçŒ«å’ªå® ç‰©ã€æ™’çŒ«åˆ†äº«ã€èŒå® æ—¥å¸¸ã€å¯çˆ±çŒ«çŒ«ã€å…»çŒ«å¿ƒå¾—"

        # çŠ¶æ€å¿ƒæƒ…ç±»æ ‡ç­¾ï¼ˆå…·ä½“æƒ…ç»ªçŠ¶æ€ï¼‰
        elif any(word in tag_lower for word in ["ç¤¾æ", "éšèº«", "æµæµª", "æ·±å¤œ", "è¢«çª"]):
            if "ç¤¾æ" in tag_lower:
                return "è¡¨è¾¾ç¤¾äº¤ç„¦è™‘ã€ä¸æƒ³è§äººã€æƒ³èº²èµ·æ¥ã€å®³æ€•ç¤¾äº¤çš„å¿ƒæƒ…"
            elif "æ·±å¤œ" in tag_lower:
                return "æ·±å¤œç¡ä¸ç€ã€ç†¬å¤œã€å¤œçŒ«å­ã€æ·±å¤œæ€è€ƒäººç”Ÿçš„å¯¹è¯"
            else:
                return "è¡¨è¾¾å½“å‰å¿ƒæƒ…çŠ¶æ€ã€ä¸ªäººæ„Ÿå—ã€ç”Ÿæ´»çŠ¶æ€"

        # ç‰©å“è£…å¤‡ç±»æ ‡ç­¾ï¼ˆå…·ä½“ä½¿ç”¨åœºæ™¯ï¼‰
        elif any(word in tag_lower for word in ["é”®ç›˜", "è€³æœº", "è£…å¤‡", "è®¾å¤‡"]):
            return "è®¨è®ºé”®ç›˜è€³æœºè£…å¤‡ã€æ•°ç äº§å“ã€ä½¿ç”¨ä½“éªŒã€è£…å¤‡æ¨èè¯„æµ‹"

        # äº’åŠ¨å…³ç³»ç±»æ ‡ç­¾
        elif any(word in tag_lower for word in ["æ‹¾é£", "äº’æ€¼", "äº’åŠ¨"]):
            return "èŠå¤©äº’åŠ¨ã€å¼€ç©ç¬‘ã€å‹å¥½äº’æ€¼ã€æ—¥å¸¸å¯¹è¯äº¤æµ"

        # é»˜è®¤ï¼šå°½é‡å…·ä½“åŒ–
        else:
            return f"æ˜ç¡®è®¨è®º{tag_name}è¿™ä¸ªç‰¹å®šä¸»é¢˜çš„å…·ä½“å†…å®¹å’Œç›¸å…³è¯é¢˜"

    def _calculate_keyword_match_bonus(self, keywords: list[str], matched_tags: list[str]) -> dict[str, float]:
        """è®¡ç®—å…³é”®è¯ç›´æ¥åŒ¹é…å¥–åŠ±"""
        if not keywords or not matched_tags:
            return {}

        if global_config is None:
            return {}

        affinity_config = global_config.affinity_flow
        bonus_dict = {}

        for tag_name in matched_tags:
            bonus = 0.0

            # æ£€æŸ¥å…³é”®è¯ä¸æ ‡ç­¾çš„ç›´æ¥åŒ¹é…
            for keyword in keywords:
                keyword_lower = keyword.lower().strip()
                tag_name_lower = tag_name.lower()

                # å®Œå…¨åŒ¹é…
                if keyword_lower == tag_name_lower:
                    bonus += affinity_config.high_match_interest_threshold * 0.6  # ä½¿ç”¨é«˜åŒ¹é…é˜ˆå€¼çš„60%ä½œä¸ºå®Œå…¨åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯å®Œå…¨åŒ¹é…: '{keyword}' == '{tag_name}' (+{affinity_config.high_match_interest_threshold * 0.6:.3f})"
                    )

                # åŒ…å«åŒ¹é…
                elif keyword_lower in tag_name_lower or tag_name_lower in keyword_lower:
                    bonus += (
                        affinity_config.medium_match_interest_threshold * 0.3
                    )  # ä½¿ç”¨ä¸­åŒ¹é…é˜ˆå€¼çš„30%ä½œä¸ºåŒ…å«åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯åŒ…å«åŒ¹é…: '{keyword}' âŠƒ '{tag_name}' (+{affinity_config.medium_match_interest_threshold * 0.3:.3f})"
                    )

                # éƒ¨åˆ†åŒ¹é…ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
                elif self._calculate_partial_match(keyword_lower, tag_name_lower):
                    bonus += affinity_config.low_match_interest_threshold * 0.4  # ä½¿ç”¨ä½åŒ¹é…é˜ˆå€¼çš„40%ä½œä¸ºéƒ¨åˆ†åŒ¹é…å¥–åŠ±
                    logger.debug(
                        f"   ğŸ¯ å…³é”®è¯éƒ¨åˆ†åŒ¹é…: '{keyword}' â‰ˆ '{tag_name}' (+{affinity_config.low_match_interest_threshold * 0.4:.3f})"
                    )

            if bonus > 0:
                bonus_dict[tag_name] = min(bonus, affinity_config.max_match_bonus)  # ä½¿ç”¨é…ç½®çš„æœ€å¤§å¥–åŠ±é™åˆ¶

        return bonus_dict

    def _calculate_partial_match(self, text1: str, text2: str) -> bool:
        """è®¡ç®—éƒ¨åˆ†åŒ¹é…ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰"""
        try:
            # ç®€å•çš„ç¼–è¾‘è·ç¦»è®¡ç®—
            max_len = max(len(text1), len(text2))
            if max_len == 0:
                return False

            # è®¡ç®—ç¼–è¾‘è·ç¦»
            distance = self._levenshtein_distance(text1, text2)

            # å¦‚æœç¼–è¾‘è·ç¦»å°äºè¾ƒçŸ­å­—ç¬¦ä¸²é•¿åº¦çš„ä¸€åŠï¼Œè®¤ä¸ºæ˜¯éƒ¨åˆ†åŒ¹é…
            min_len = min(len(text1), len(text2))
            return distance <= min_len // 2

        except Exception:
            return False

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—è±æ–‡æ–¯å¦è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    def _calculate_cosine_similarity(self, vec1: list[float], vec2: list[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            np_vec1 = np.array(vec1)
            np_vec2 = np.array(vec2)

            dot_product = np.dot(np_vec1, np_vec2)
            norm1 = np.linalg.norm(np_vec1)
            norm2 = np.linalg.norm(np_vec2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return float(similarity)

        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    async def _load_interests_from_database(self, personality_id: str) -> BotPersonalityInterests | None:
        """ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾"""
        try:
            logger.debug(f"ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾, personality_id: {personality_id}")

            # å¯¼å…¥SQLAlchemyç›¸å…³æ¨¡å—
            import orjson

            from src.common.database.compatibility import get_db_session
            from src.common.database.core.models import BotPersonalityInterests as DBBotPersonalityInterests

            async with get_db_session() as session:
                # æŸ¥è¯¢æœ€æ–°çš„å…´è¶£æ ‡ç­¾é…ç½®
                db_interests = (
                    (
                        await session.execute(
                            select(DBBotPersonalityInterests)
                            .where(DBBotPersonalityInterests.personality_id == personality_id)
                            .order_by(
                                DBBotPersonalityInterests.version.desc(), DBBotPersonalityInterests.last_updated.desc()
                            )
                        )
                    )
                    .scalars()
                    .first()
                )

                if db_interests:
                    logger.debug(f"åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°å…´è¶£æ ‡ç­¾é…ç½®, ç‰ˆæœ¬: {db_interests.version}")
                    logger.debug(f"ğŸ“… æœ€åæ›´æ–°æ—¶é—´: {db_interests.last_updated}")
                    logger.debug(f"ğŸ§  ä½¿ç”¨çš„embeddingæ¨¡å‹: {db_interests.embedding_model}")

                    # è§£æJSONæ ¼å¼çš„å…´è¶£æ ‡ç­¾
                    try:
                        tags_data = orjson.loads(db_interests.interest_tags)
                        logger.debug(f"ğŸ·ï¸  è§£æåˆ° {len(tags_data)} ä¸ªå…´è¶£æ ‡ç­¾")

                        # åˆ›å»ºBotPersonalityInterestså¯¹è±¡
                        interests = BotPersonalityInterests(
                            personality_id=db_interests.personality_id,
                            personality_description=db_interests.personality_description,
                            embedding_model=db_interests.embedding_model,
                            version=db_interests.version,
                            last_updated=db_interests.last_updated,
                        )

                        # è§£æå…´è¶£æ ‡ç­¾ï¼ˆembedding ä»æ•°æ®åº“åŠ è½½åä¼šè¢«å¿½ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†å­˜å‚¨å®ƒï¼‰
                        for tag_data in tags_data:
                            tag = BotInterestTag(
                                tag_name=tag_data.get("tag_name", ""),
                                weight=tag_data.get("weight", 0.5),
                                expanded=tag_data.get("expanded"),  # åŠ è½½æ‰©å±•æè¿°
                                created_at=datetime.fromisoformat(
                                    tag_data.get("created_at", datetime.now().isoformat())
                                ),
                                updated_at=datetime.fromisoformat(
                                    tag_data.get("updated_at", datetime.now().isoformat())
                                ),
                                is_active=tag_data.get("is_active", True),
                                embedding=None,  # ä¸å†ä»æ•°æ®åº“åŠ è½½ embeddingï¼Œæ”¹ä¸ºåŠ¨æ€ç”Ÿæˆ
                            )
                            interests.interest_tags.append(tag)

                        logger.debug(f"æˆåŠŸè§£æ {len(interests.interest_tags)} ä¸ªå…´è¶£æ ‡ç­¾ï¼ˆembedding å°†åœ¨åˆå§‹åŒ–æ—¶åŠ¨æ€ç”Ÿæˆï¼‰")
                        return interests

                    except (orjson.JSONDecodeError, Exception) as e:
                        logger.error(f"âŒ è§£æå…´è¶£æ ‡ç­¾JSONå¤±è´¥: {e}")
                        logger.debug(f"ğŸ” åŸå§‹JSONæ•°æ®: {db_interests.interest_tags[:200]}...")
                        return None
                else:
                    logger.info(f"â„¹ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°personality_idä¸º '{personality_id}' çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    return None

        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½å…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            return None

    async def _save_interests_to_database(self, interests: BotPersonalityInterests):
        """ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“"""
        try:
            logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“...")
            logger.info(f"ğŸ“‹ personality_id: {interests.personality_id}")
            logger.info(f"ğŸ·ï¸  å…´è¶£æ ‡ç­¾æ•°é‡: {len(interests.interest_tags)}")
            logger.info(f"ğŸ”„ ç‰ˆæœ¬: {interests.version}")

            # å¯¼å…¥SQLAlchemyç›¸å…³æ¨¡å—
            import orjson

            from src.common.database.compatibility import get_db_session
            from src.common.database.core.models import BotPersonalityInterests as DBBotPersonalityInterests

            # å°†å…´è¶£æ ‡ç­¾è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆä¸å†ä¿å­˜embeddingï¼Œå¯åŠ¨æ—¶åŠ¨æ€ç”Ÿæˆï¼‰
            tags_data = []
            for tag in interests.interest_tags:
                tag_dict = {
                    "tag_name": tag.tag_name,
                    "weight": tag.weight,
                    "expanded": tag.expanded,  # ä¿å­˜æ‰©å±•æè¿°
                    "created_at": tag.created_at.isoformat(),
                    "updated_at": tag.updated_at.isoformat(),
                    "is_active": tag.is_active,
                    # embedding ä¸å†å­˜å‚¨åˆ°æ•°æ®åº“ï¼Œæ”¹ä¸ºå†…å­˜ç¼“å­˜
                }
                tags_data.append(tag_dict)

            # åºåˆ—åŒ–ä¸ºJSON
            json_data = orjson.dumps(tags_data)

            async with get_db_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒpersonality_idçš„è®°å½•
                existing_record = (
                    (
                        await session.execute(
                            select(DBBotPersonalityInterests).where(
                                DBBotPersonalityInterests.personality_id == interests.personality_id
                            )
                        )
                    )
                    .scalars()
                    .first()
                )

                if existing_record:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    logger.info("ğŸ”„ æ›´æ–°ç°æœ‰çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    existing_record.interest_tags = json_data.decode("utf-8")
                    existing_record.personality_description = interests.personality_description
                    existing_record.embedding_model = interests.embedding_model
                    existing_record.version = interests.version
                    existing_record.last_updated = interests.last_updated

                    logger.info(f"âœ… æˆåŠŸæ›´æ–°å…´è¶£æ ‡ç­¾é…ç½®ï¼Œç‰ˆæœ¬: {interests.version}")

                else:
                    # åˆ›å»ºæ–°è®°å½•
                    logger.info("ğŸ†• åˆ›å»ºæ–°çš„å…´è¶£æ ‡ç­¾é…ç½®")
                    new_record = DBBotPersonalityInterests(
                        personality_id=interests.personality_id,
                        personality_description=interests.personality_description,
                        interest_tags=json_data.decode("utf-8"),
                        embedding_model=interests.embedding_model,
                        version=interests.version,
                        last_updated=interests.last_updated,
                    )
                    session.add(new_record)
                    await session.commit()
                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºå…´è¶£æ ‡ç­¾é…ç½®ï¼Œç‰ˆæœ¬: {interests.version}")

            logger.info("âœ… å…´è¶£æ ‡ç­¾å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")

            # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
            async with get_db_session() as session:
                saved_record = (
                    (
                        await session.execute(
                            select(DBBotPersonalityInterests).where(
                                DBBotPersonalityInterests.personality_id == interests.personality_id
                            )
                        )
                    )
                    .scalars()
                    .first()
                )
                if saved_record:
                    logger.info(f"âœ… éªŒè¯æˆåŠŸï¼šæ•°æ®åº“ä¸­å­˜åœ¨personality_idä¸º {interests.personality_id} çš„è®°å½•")
                    logger.info(f"   ç‰ˆæœ¬: {saved_record.version}")
                    logger.info(f"   æœ€åæ›´æ–°: {saved_record.last_updated}")
                else:
                    logger.error(f"âŒ éªŒè¯å¤±è´¥ï¼šæ•°æ®åº“ä¸­æœªæ‰¾åˆ°personality_idä¸º {interests.personality_id} çš„è®°å½•")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å…´è¶£æ ‡ç­¾åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            logger.error("ğŸ” é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()

    async def _load_embedding_cache_from_file(self, personality_id: str) -> dict[str, list[float]] | None:
        """ä»æ–‡ä»¶åŠ è½½embeddingç¼“å­˜"""
        try:
            from pathlib import Path

            import orjson

            cache_dir = Path("data/embedding")
            cache_dir.mkdir(parents=True, exist_ok=True)
            cache_file = cache_dir / f"{personality_id}_embeddings.json"

            if not cache_file.exists():
                logger.debug(f"ğŸ“‚ Embeddingç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
                return None

            # è¯»å–ç¼“å­˜æ–‡ä»¶
            import aiofiles
            async with aiofiles.open(cache_file, "rb") as f:
                content = await f.read()
                cache_data = orjson.loads(content)

            # éªŒè¯ç¼“å­˜ç‰ˆæœ¬å’Œembeddingæ¨¡å‹
            cache_version = cache_data.get("version", 1)
            cache_embedding_model = cache_data.get("embedding_model", "")
            
            current_embedding_model = ""
            if self.embedding_config and hasattr(self.embedding_config, "model_list") and self.embedding_config.model_list:
                 current_embedding_model = self.embedding_config.model_list[0]

            if cache_embedding_model != current_embedding_model:
                logger.warning(f"âš ï¸ Embeddingæ¨¡å‹å·²å˜æ›´ ({cache_embedding_model} â†’ {current_embedding_model})ï¼Œå¿½ç•¥æ—§ç¼“å­˜")
                return None

            embeddings = cache_data.get("embeddings", {})

            # åŒæ—¶åŠ è½½æ‰©å±•æ ‡ç­¾çš„embeddingç¼“å­˜
            expanded_embeddings = cache_data.get("expanded_embeddings", {})
            if expanded_embeddings:
                self.expanded_embedding_cache.update(expanded_embeddings)
                logger.info(f"ğŸ“‚ åŠ è½½ {len(expanded_embeddings)} ä¸ªæ‰©å±•æ ‡ç­¾embeddingç¼“å­˜")

            logger.info(f"âœ… æˆåŠŸä»æ–‡ä»¶åŠ è½½ {len(embeddings)} ä¸ªæ ‡ç­¾embeddingç¼“å­˜ (ç‰ˆæœ¬: {cache_version}, æ¨¡å‹: {cache_embedding_model})")
            return embeddings

        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½embeddingç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None

    async def _save_embedding_cache_to_file(self, personality_id: str):
        """ä¿å­˜embeddingç¼“å­˜åˆ°æ–‡ä»¶ï¼ˆåŒ…æ‹¬æ‰©å±•æ ‡ç­¾çš„embeddingï¼‰"""
        try:
            from datetime import datetime
            from pathlib import Path

            import orjson

            cache_dir = Path("data/embedding")
            cache_dir.mkdir(parents=True, exist_ok=True)
            cache_file = cache_dir / f"{personality_id}_embeddings.json"

            # å‡†å¤‡ç¼“å­˜æ•°æ®
            current_embedding_model = ""
            if self.embedding_config and hasattr(self.embedding_config, "model_list") and self.embedding_config.model_list:
                 current_embedding_model = self.embedding_config.model_list[0]

            cache_data = {
                "version": 1,
                "personality_id": personality_id,
                "embedding_model": current_embedding_model,
                "last_updated": datetime.now().isoformat(),
                "embeddings": self.embedding_cache,
                "expanded_embeddings": self.expanded_embedding_cache,  # åŒæ—¶ä¿å­˜æ‰©å±•æ ‡ç­¾çš„embedding
            }

            # å†™å…¥æ–‡ä»¶
            import aiofiles
            async with aiofiles.open(cache_file, "wb") as f:
                await f.write(orjson.dumps(cache_data, option=orjson.OPT_INDENT_2))

            logger.debug(f"ğŸ’¾ å·²ä¿å­˜ {len(self.embedding_cache)} ä¸ªæ ‡ç­¾embeddingå’Œ {len(self.expanded_embedding_cache)} ä¸ªæ‰©å±•embeddingåˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")

        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜embeddingç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def get_current_interests(self) -> BotPersonalityInterests | None:
        """è·å–å½“å‰çš„å…´è¶£æ ‡ç­¾é…ç½®"""
        return self.current_interests

    def get_interest_stats(self) -> dict[str, Any]:
        """è·å–å…´è¶£ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.current_interests:
            return {"initialized": False}

        active_tags = self.current_interests.get_active_tags()

        return {
            "initialized": self._initialized,
            "total_tags": len(active_tags),
            "embedding_model": self.current_interests.embedding_model,
            "last_updated": self.current_interests.last_updated.isoformat(),
            "cache_size": len(self.embedding_cache),
        }

    async def update_interest_tags(self, new_personality_description: str | None = None):
        """æ›´æ–°å…´è¶£æ ‡ç­¾"""
        try:
            if not self.current_interests:
                logger.warning("æ²¡æœ‰å½“å‰çš„å…´è¶£æ ‡ç­¾é…ç½®ï¼Œæ— æ³•æ›´æ–°")
                return

            if new_personality_description:
                self.current_interests.personality_description = new_personality_description

            # é‡æ–°ç”Ÿæˆå…´è¶£æ ‡ç­¾
            new_interests = await self._generate_interests_from_personality(
                self.current_interests.personality_description, self.current_interests.personality_id
            )

            if new_interests:
                new_interests.version = self.current_interests.version + 1
                self.current_interests = new_interests
                await self._save_interests_to_database(new_interests)
                logger.info(f"å…´è¶£æ ‡ç­¾å·²æ›´æ–°ï¼Œç‰ˆæœ¬: {new_interests.version}")

        except Exception as e:
            logger.error(f"æ›´æ–°å…´è¶£æ ‡ç­¾å¤±è´¥: {e}")
            traceback.print_exc()


# åˆ›å»ºå…¨å±€å®ä¾‹ï¼ˆé‡æ–°åˆ›å»ºä»¥åŒ…å«æ–°çš„å±æ€§ï¼‰
bot_interest_manager = BotInterestManager()
