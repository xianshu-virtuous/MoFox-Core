import asyncio
import math
import os
from dataclasses import dataclass
from typing import Any

# import tqdm
import aiofiles
import faiss
import numpy as np
import orjson
import pandas as pd
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich.traceback import install

from src.common.config_helpers import resolve_embedding_dimension
from src.config.config import global_config

from .global_logger import logger
from .utils.hash import get_sha256

install(extra_lines=3)

# å¤šçº¿ç¨‹embeddingé…ç½®å¸¸é‡
DEFAULT_MAX_WORKERS = 10  # é»˜è®¤æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°ï¼ˆæå‡å¹¶å‘èƒ½åŠ›ï¼‰
DEFAULT_CHUNK_SIZE = 20  # é»˜è®¤æ¯ä¸ªæ‰¹æ¬¡å¤„ç†çš„æ•°æ®å—å¤§å°ï¼ˆæ‰¹é‡è¯·æ±‚ï¼‰
MIN_CHUNK_SIZE = 1  # æœ€å°åˆ†å—å¤§å°
MAX_CHUNK_SIZE = 100  # æœ€å¤§åˆ†å—å¤§å°ï¼ˆæå‡æ‰¹é‡èƒ½åŠ›ï¼‰
MIN_WORKERS = 1  # æœ€å°çº¿ç¨‹æ•°
MAX_WORKERS = 50  # æœ€å¤§çº¿ç¨‹æ•°ï¼ˆæå‡å¹¶å‘ä¸Šé™ï¼‰

ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
EMBEDDING_DATA_DIR = os.path.join(ROOT_PATH, "data", "embedding")
EMBEDDING_DATA_DIR_STR = str(EMBEDDING_DATA_DIR).replace("\\", "/")
TOTAL_EMBEDDING_TIMES = 3  # ç»Ÿè®¡åµŒå…¥æ¬¡æ•°

# åµŒå…¥æ¨¡å‹æµ‹è¯•å­—ç¬¦ä¸²ï¼Œæµ‹è¯•æ¨¡å‹ä¸€è‡´æ€§ï¼Œæ¥è‡ªå¼€å‘ç¾¤çš„èŠå¤©è®°å½•
# è¿™äº›å­—ç¬¦ä¸²çš„åµŒå…¥ç»“æœåº”è¯¥æ˜¯å›ºå®šçš„ï¼Œä¸èƒ½éšæ—¶é—´å˜åŒ–
EMBEDDING_TEST_STRINGS = [
    "é˜¿å¡ä¼ŠçœŸçš„å¤ªå¥½ç©äº†ï¼Œç¥ç§˜æ€§æ„Ÿå¤§å¥³åŒç­‰ç€ä½ ",
    "ä½ æ€ä¹ˆçŸ¥é“æˆ‘arc12.64äº†",
    "æˆ‘æ˜¯è•¾ç¼ªä¹å°å§çš„ç‹—",
    "å…³æ³¨Octè°¢è°¢å–µ",
    "ä¸æ˜¯w6æˆ‘ä¸è‰",
    "å…³æ³¨åƒçŸ³å¯ä¹è°¢è°¢å–µ",
    "æ¥ç©CLANNADï¼ŒAIRï¼Œæ¨±ä¹‹è¯—ï¼Œæ¨±ä¹‹åˆ»è°¢è°¢å–µ",
    "å…³æ³¨å¢¨æ¢“æŸ’è°¢è°¢å–µ",
    "Ciallo~",
    "æ¥ç©å·§å…‹ç”œæ‹è°¢è°¢å–µ",
    "æ°´å°",
    "æˆ‘ä¹Ÿåœ¨çº ç»“æ™šé¥­ï¼Œé“é”…ç‚’é¸¡å¬ç€å°±é¦™ï¼",
    "testä½ å¦ˆå–µ",
]
EMBEDDING_TEST_FILE = os.path.join(ROOT_PATH, "data", "embedding_model_test.json")
EMBEDDING_SIM_THRESHOLD = 0.99


def cosine_similarity(a, b):
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot = sum(x * y for x, y in zip(a, b, strict=False))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)


@dataclass
class EmbeddingStoreItem:
    """åµŒå…¥åº“ä¸­çš„é¡¹"""

    def __init__(self, item_hash: str, embedding: list[float], content: str):
        self.hash = item_hash
        self.embedding = embedding
        self.str = content

    def to_dict(self) -> dict:
        """è½¬ä¸ºdict"""
        return {
            "hash": self.hash,
            "embedding": self.embedding,
            "str": self.str,
        }


class EmbeddingStore:
    def __init__(
        self,
        namespace: str,
        dir_path: str,
        max_workers: int = DEFAULT_MAX_WORKERS,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
    ):
        self.namespace = namespace
        self.dir = dir_path
        self.embedding_file_path = f"{dir_path}/{namespace}.parquet"
        self.index_file_path = f"{dir_path}/{namespace}.index"
        self.idx2hash_file_path = dir_path + "/" + namespace + "_i2h.json"

        # å¤šçº¿ç¨‹é…ç½®å‚æ•°éªŒè¯å’Œè®¾ç½®
        self.max_workers = max(MIN_WORKERS, min(MAX_WORKERS, max_workers))
        self.chunk_size = max(MIN_CHUNK_SIZE, min(MAX_CHUNK_SIZE, chunk_size))

        # å¦‚æœé…ç½®å€¼è¢«è°ƒæ•´ï¼Œè®°å½•æ—¥å¿—
        if self.max_workers != max_workers:
            logger.warning(
                f"max_workers å·²ä» {max_workers} è°ƒæ•´ä¸º {self.max_workers} (èŒƒå›´: {MIN_WORKERS}-{MAX_WORKERS})"
            )
        if self.chunk_size != chunk_size:
            logger.warning(
                f"chunk_size å·²ä» {chunk_size} è°ƒæ•´ä¸º {self.chunk_size} (èŒƒå›´: {MIN_CHUNK_SIZE}-{MAX_CHUNK_SIZE})"
            )

        self.store = {}

        self.faiss_index: Any = None
        self.idx2hash = None

    @staticmethod
    async def _get_embedding_async(llm, s: str) -> list[float]:
        """å¼‚æ­¥ã€å®‰å…¨åœ°è·å–å•ä¸ªå­—ç¬¦ä¸²çš„åµŒå…¥å‘é‡"""
        try:
            embedding, _ = await llm.get_embedding(s)
            if embedding and len(embedding) > 0:
                return embedding
            else:
                logger.error(f"è·å–åµŒå…¥å¤±è´¥: {s}")
                return []
        except Exception as e:
            logger.error(f"è·å–åµŒå…¥æ—¶å‘ç”Ÿå¼‚å¸¸: {s}, é”™è¯¯: {e}")
            return []

    @staticmethod
    @staticmethod
    async def _get_embeddings_batch_async(
        strs: list[str], chunk_size: int = 10, max_workers: int = 4, progress_callback=None
    ) -> list[tuple[str, list[float]]]:
        """
        å¼‚æ­¥ã€å¹¶å‘åœ°æ‰¹é‡è·å–åµŒå…¥å‘é‡ã€‚
        ä½¿ç”¨ chunk_size è¿›è¡Œæ‰¹é‡è¯·æ±‚ï¼Œmax_workers æ§åˆ¶å¹¶å‘æ‰¹æ¬¡æ•°ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å°†å­—ç¬¦ä¸²åˆ†æˆå¤šä¸ª chunkï¼Œæ¯ä¸ª chunk åŒ…å« chunk_size ä¸ªå­—ç¬¦ä¸²
        2. ä½¿ç”¨ asyncio.Semaphore æ§åˆ¶åŒæ—¶å¤„ç†çš„ chunk æ•°é‡
        3. æ¯ä¸ª chunk å†…çš„å­—ç¬¦ä¸²ä¸€æ¬¡æ€§å‘é€ç»™ LLMï¼ˆåˆ©ç”¨æ‰¹é‡ APIï¼‰
        """
        if not strs:
            return []

        from src.config.config import model_config
        from src.llm_models.utils_model import LLMRequest

        assert model_config is not None

        # é™åˆ¶ chunk_size å’Œ max_workers åœ¨åˆç†èŒƒå›´å†…
        chunk_size = max(MIN_CHUNK_SIZE, min(chunk_size, MAX_CHUNK_SIZE))
        max_workers = max(MIN_WORKERS, min(max_workers, MAX_WORKERS))

        semaphore = asyncio.Semaphore(max_workers)
        llm = LLMRequest(model_set=model_config.model_task_config.embedding, request_type="embedding")
        results = {}

        # å°†å­—ç¬¦ä¸²åˆ—è¡¨åˆ†æˆå¤šä¸ª chunk
        chunks = []
        for i in range(0, len(strs), chunk_size):
            chunks.append(strs[i : i + chunk_size])

        async def _process_chunk(chunk: list[str]):
            """å¤„ç†ä¸€ä¸ª chunk çš„å­—ç¬¦ä¸²ï¼ˆæ‰¹é‡è·å– embeddingï¼‰"""
            async with semaphore:
                # æ‰¹é‡è·å– embeddingï¼ˆä¸€æ¬¡è¯·æ±‚å¤„ç†æ•´ä¸ª chunkï¼‰
                embeddings = []
                for s in chunk:
                    embedding = await EmbeddingStore._get_embedding_async(llm, s)
                    embeddings.append(embedding)
                    results[s] = embedding

                if progress_callback:
                    progress_callback(len(chunk))

                return embeddings

        # å¹¶å‘å¤„ç†æ‰€æœ‰ chunks
        tasks = [_process_chunk(chunk) for chunk in chunks]
        await asyncio.gather(*tasks)

        # æŒ‰ç…§åŸå§‹é¡ºåºè¿”å›ç»“æœ
        return [(s, results.get(s, [])) for s in strs]

    @staticmethod
    def get_test_file_path():
        return EMBEDDING_TEST_FILE

    async def save_embedding_test_vectors(self):
        """ä¿å­˜æµ‹è¯•å­—ç¬¦ä¸²çš„åµŒå…¥åˆ°æœ¬åœ°ï¼ˆå¼‚æ­¥å•çº¿ç¨‹ï¼‰"""
        logger.info("å¼€å§‹ä¿å­˜æµ‹è¯•å­—ç¬¦ä¸²çš„åµŒå…¥å‘é‡...")

        embedding_results = await self._get_embeddings_batch_async(
            EMBEDDING_TEST_STRINGS,
            chunk_size=self.chunk_size,
            max_workers=self.max_workers,
        )

        # æ„å»ºæµ‹è¯•å‘é‡å­—å…¸
        test_vectors = {}
        for idx, (s, embedding) in enumerate(embedding_results):
            if embedding:
                test_vectors[str(idx)] = embedding
            else:
                logger.error(f"è·å–æµ‹è¯•å­—ç¬¦ä¸²åµŒå…¥å¤±è´¥: {s}")
                # Since _get_embedding is problematic, we just fail here
                test_vectors[str(idx)] = []


        async with aiofiles.open(self.get_test_file_path(), "w", encoding="utf-8") as f:
            await f.write(orjson.dumps(test_vectors, option=orjson.OPT_INDENT_2).decode("utf-8"))

        logger.info("æµ‹è¯•å­—ç¬¦ä¸²åµŒå…¥å‘é‡ä¿å­˜å®Œæˆ")

    def load_embedding_test_vectors(self):
        """åŠ è½½æœ¬åœ°ä¿å­˜çš„æµ‹è¯•å­—ç¬¦ä¸²åµŒå…¥"""
        path = self.get_test_file_path()
        if not os.path.exists(path):
            return None
        with open(path, encoding="utf-8") as f:
            return orjson.loads(f.read())

    async def check_embedding_model_consistency(self):
        """æ ¡éªŒå½“å‰æ¨¡å‹ä¸æœ¬åœ°åµŒå…¥æ¨¡å‹æ˜¯å¦ä¸€è‡´ï¼ˆå¼‚æ­¥å•çº¿ç¨‹ï¼‰"""
        local_vectors = self.load_embedding_test_vectors()
        if local_vectors is None:
            logger.warning("æœªæ£€æµ‹åˆ°æœ¬åœ°åµŒå…¥æ¨¡å‹æµ‹è¯•æ–‡ä»¶ï¼Œå°†ä¿å­˜å½“å‰æ¨¡å‹çš„æµ‹è¯•åµŒå…¥ã€‚")
            await self.save_embedding_test_vectors()
            return True

        # æ£€æŸ¥æœ¬åœ°å‘é‡å®Œæ•´æ€§
        for idx in range(len(EMBEDDING_TEST_STRINGS)):
            if local_vectors.get(str(idx)) is None:
                logger.warning("æœ¬åœ°åµŒå…¥æ¨¡å‹æµ‹è¯•æ–‡ä»¶ç¼ºå¤±éƒ¨åˆ†æµ‹è¯•å­—ç¬¦ä¸²ï¼Œå°†é‡æ–°ä¿å­˜ã€‚")
                await self.save_embedding_test_vectors()
                return True

        logger.info("å¼€å§‹æ£€éªŒåµŒå…¥æ¨¡å‹ä¸€è‡´æ€§...")

        embedding_results = await self._get_embeddings_batch_async(
            EMBEDDING_TEST_STRINGS,
            chunk_size=self.chunk_size,
            max_workers=self.max_workers,
        )

        # æ£€æŸ¥ä¸€è‡´æ€§
        for idx, (s, new_emb) in enumerate(embedding_results):
            local_emb = local_vectors.get(str(idx))
            if not new_emb:
                logger.error(f"è·å–æµ‹è¯•å­—ç¬¦ä¸²åµŒå…¥å¤±è´¥: {s}")
                return False

            sim = cosine_similarity(local_emb, new_emb)
            if sim < EMBEDDING_SIM_THRESHOLD:
                logger.error(f"åµŒå…¥æ¨¡å‹ä¸€è‡´æ€§æ ¡éªŒå¤±è´¥ï¼Œå­—ç¬¦ä¸²: {s}, ç›¸ä¼¼åº¦: {sim:.4f}")
                return False

        logger.info("åµŒå…¥æ¨¡å‹ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡ã€‚")
        return True

    async def batch_insert_strs(self, strs: list[str], times: int) -> None:
        """å‘åº“ä¸­å­˜å…¥å­—ç¬¦ä¸²ï¼ˆå¼‚æ­¥å•çº¿ç¨‹ï¼‰"""
        if not strs:
            return

        total = len(strs)

        # è¿‡æ»¤å·²å­˜åœ¨çš„å­—ç¬¦ä¸²
        new_strs = []
        for s in strs:
            item_hash = self.namespace + "-" + get_sha256(s)
            if item_hash not in self.store:
                new_strs.append(s)

        if not new_strs:
            logger.info(f"æ‰€æœ‰å­—ç¬¦ä¸²å·²å­˜åœ¨äº{self.namespace}åµŒå…¥åº“ä¸­ï¼Œè·³è¿‡å¤„ç†")
            return

        logger.info(f"éœ€è¦å¤„ç† {len(new_strs)}/{total} ä¸ªæ–°å­—ç¬¦ä¸²")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            MofNCompleteColumn(),
            "â€¢",
            TimeElapsedColumn(),
            "<",
            TimeRemainingColumn(),
            transient=False,
        ) as progress:
            task = progress.add_task(f"å­˜å…¥åµŒå…¥åº“ï¼š({times}/{TOTAL_EMBEDDING_TIMES})", total=total)

            # é¦–å…ˆæ›´æ–°å·²å­˜åœ¨é¡¹çš„è¿›åº¦
            already_processed = total - len(new_strs)
            if already_processed > 0:
                progress.update(task, advance=already_processed)

            if new_strs:
                # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
                def update_progress(count):
                    progress.update(task, advance=count)

                embedding_results = await self._get_embeddings_batch_async(
                    new_strs,
                    chunk_size=self.chunk_size,
                    max_workers=self.max_workers,
                    progress_callback=update_progress,
                )

                # å­˜å…¥ç»“æœ
                for s, embedding in embedding_results:
                    item_hash = self.namespace + "-" + get_sha256(s)
                    if embedding:  # åªæœ‰æˆåŠŸè·å–åˆ°åµŒå…¥æ‰å­˜å…¥
                        self.store[item_hash] = EmbeddingStoreItem(item_hash, embedding, s)
                    else:
                        logger.warning(f"è·³è¿‡å­˜å‚¨å¤±è´¥çš„åµŒå…¥: {s[:50]}...")

    def save_to_file(self) -> None:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        logger.info(f"æ­£åœ¨ä¿å­˜{self.namespace}åµŒå…¥åº“åˆ°æ–‡ä»¶{self.embedding_file_path}")
        data = [item.to_dict() for item in self.store.values()]
        data_frame = pd.DataFrame(data)

        if not os.path.exists(self.dir):
            os.makedirs(self.dir, exist_ok=True)
        if not os.path.exists(self.embedding_file_path):
            open(self.embedding_file_path, "w").close()

        data_frame.to_parquet(self.embedding_file_path, engine="pyarrow", index=False)
        logger.info(f"{self.namespace}åµŒå…¥åº“ä¿å­˜æˆåŠŸ")

        if self.faiss_index is not None and self.idx2hash is not None:
            logger.info(f"æ­£åœ¨ä¿å­˜{self.namespace}åµŒå…¥åº“çš„FaissIndexåˆ°æ–‡ä»¶{self.index_file_path}")
            faiss.write_index(self.faiss_index, self.index_file_path)
            logger.info(f"{self.namespace}åµŒå…¥åº“çš„FaissIndexä¿å­˜æˆåŠŸ")
            logger.info(f"æ­£åœ¨ä¿å­˜{self.namespace}åµŒå…¥åº“çš„idx2hashæ˜ å°„åˆ°æ–‡ä»¶{self.idx2hash_file_path}")
            with open(self.idx2hash_file_path, "w", encoding="utf-8") as f:
                f.write(orjson.dumps(self.idx2hash, option=orjson.OPT_INDENT_2).decode("utf-8"))
            logger.info(f"{self.namespace}åµŒå…¥åº“çš„idx2hashæ˜ å°„ä¿å­˜æˆåŠŸ")

    def load_from_file(self) -> None:
        """ä»æ–‡ä»¶ä¸­åŠ è½½"""
        if not os.path.exists(self.embedding_file_path):
            raise Exception(f"æ–‡ä»¶{self.embedding_file_path}ä¸å­˜åœ¨")
        logger.info("æ­£åœ¨åŠ è½½åµŒå…¥åº“...")
        logger.debug(f"æ­£åœ¨ä»æ–‡ä»¶{self.embedding_file_path}ä¸­åŠ è½½{self.namespace}åµŒå…¥åº“")
        data_frame = pd.read_parquet(self.embedding_file_path, engine="pyarrow")
        total = len(data_frame)
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            MofNCompleteColumn(),
            "â€¢",
            TimeElapsedColumn(),
            "<",
            TimeRemainingColumn(),
            transient=False,
        ) as progress:
            task = progress.add_task("åŠ è½½åµŒå…¥åº“", total=total)
            for _, row in data_frame.iterrows():
                self.store[row["hash"]] = EmbeddingStoreItem(row["hash"], row["embedding"], row["str"])
                progress.update(task, advance=1)
        logger.info(f"{self.namespace}åµŒå…¥åº“åŠ è½½æˆåŠŸ")

        try:
            if os.path.exists(self.index_file_path):
                logger.info(f"æ­£åœ¨åŠ è½½{self.namespace}åµŒå…¥åº“çš„FaissIndex...")
                logger.debug(f"æ­£åœ¨ä»æ–‡ä»¶{self.index_file_path}ä¸­åŠ è½½{self.namespace}åµŒå…¥åº“çš„FaissIndex")
                self.faiss_index = faiss.read_index(self.index_file_path)
                logger.info(f"{self.namespace}åµŒå…¥åº“çš„FaissIndexåŠ è½½æˆåŠŸ")
            else:
                raise Exception(f"æ–‡ä»¶{self.index_file_path}ä¸å­˜åœ¨")
            if os.path.exists(self.idx2hash_file_path):
                logger.info(f"æ­£åœ¨åŠ è½½{self.namespace}åµŒå…¥åº“çš„idx2hashæ˜ å°„...")
                logger.debug(f"æ­£åœ¨ä»æ–‡ä»¶{self.idx2hash_file_path}ä¸­åŠ è½½{self.namespace}åµŒå…¥åº“çš„idx2hashæ˜ å°„")
                with open(self.idx2hash_file_path) as f:
                    self.idx2hash = orjson.loads(f.read())
                logger.info(f"{self.namespace}åµŒå…¥åº“çš„idx2hashæ˜ å°„åŠ è½½æˆåŠŸ")
            else:
                raise Exception(f"æ–‡ä»¶{self.idx2hash_file_path}ä¸å­˜åœ¨")
        except Exception as e:
            logger.error(f"åŠ è½½{self.namespace}åµŒå…¥åº“çš„FaissIndexæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            logger.warning("æ­£åœ¨é‡å»ºFaissç´¢å¼•")
            self.build_faiss_index()
            logger.info(f"{self.namespace}åµŒå…¥åº“çš„FaissIndexé‡å»ºæˆåŠŸ")
            self.save_to_file()

    def build_faiss_index(self) -> None:
        """é‡æ–°æ„å»ºFaissç´¢å¼•ï¼Œä»¥ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºåº¦é‡"""
        assert global_config is not None
        # è·å–æ‰€æœ‰çš„embedding
        array = []
        self.idx2hash = {}
        for key in self.store:
            array.append(self.store[key].embedding)
            self.idx2hash[str(len(array) - 1)] = key

        if not array:
            logger.warning(f"åœ¨ {self.namespace} ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨äºæ„å»ºFaissç´¢å¼•çš„åµŒå…¥å‘é‡ã€‚")
            embedding_dim = resolve_embedding_dimension(global_config.lpmm_knowledge.embedding_dimension) or 1
            self.faiss_index = faiss.IndexFlatIP(embedding_dim)
            return

        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ‰€æœ‰ embedding çš„ç»´åº¦æ˜¯å¦ä¸€è‡´
        dimensions = [len(emb) for emb in array]
        unique_dims = set(dimensions)

        if len(unique_dims) > 1:
            logger.error(f"æ£€æµ‹åˆ°ä¸ä¸€è‡´çš„ embedding ç»´åº¦: {unique_dims}")
            logger.error(f"ç»´åº¦åˆ†å¸ƒ: {dict(zip(*np.unique(dimensions, return_counts=True)))}")

            # è·å–æœŸæœ›çš„ç»´åº¦ï¼ˆä½¿ç”¨æœ€å¸¸è§çš„ç»´åº¦ï¼‰
            from collections import Counter
            dim_counter = Counter(dimensions)
            expected_dim = dim_counter.most_common(1)[0][0]
            logger.warning(f"å°†ä½¿ç”¨æœ€å¸¸è§çš„ç»´åº¦: {expected_dim}")

            # è¿‡æ»¤æ‰ç»´åº¦ä¸åŒ¹é…çš„ embedding
            filtered_array = []
            filtered_idx2hash = {}
            skipped_count = 0

            for i, emb in enumerate(array):
                if len(emb) == expected_dim:
                    filtered_array.append(emb)
                    filtered_idx2hash[str(len(filtered_array) - 1)] = self.idx2hash[str(i)]
                else:
                    skipped_count += 1
                    hash_key = self.idx2hash[str(i)]
                    logger.warning(f"è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„ embedding: {hash_key}, ç»´åº¦={len(emb)}, æœŸæœ›={expected_dim}")

            logger.warning(f"å·²è¿‡æ»¤ {skipped_count} ä¸ªç»´åº¦ä¸åŒ¹é…çš„ embedding")
            array = filtered_array
            self.idx2hash = filtered_idx2hash

            if not array:
                logger.error("è¿‡æ»¤åæ²¡æœ‰å¯ç”¨çš„ embeddingï¼Œæ— æ³•æ„å»ºç´¢å¼•")
                embedding_dim = expected_dim
                self.faiss_index = faiss.IndexFlatIP(embedding_dim)
                return

        embeddings = np.array(array, dtype=np.float32)
        # L2å½’ä¸€åŒ–
        faiss.normalize_L2(embeddings)
        # æ„å»ºç´¢å¼•
        embedding_dim = resolve_embedding_dimension(global_config.lpmm_knowledge.embedding_dimension)
        if not embedding_dim:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®é™…æ£€æµ‹åˆ°çš„ç»´åº¦
            embedding_dim = embeddings.shape[1]
            logger.info(f"ä½¿ç”¨å®é™…æ£€æµ‹åˆ°çš„ embedding ç»´åº¦: {embedding_dim}")
        self.faiss_index = faiss.IndexFlatIP(embedding_dim)
        self.faiss_index.add(embeddings)
        logger.info(f"âœ… æˆåŠŸæ„å»º Faiss ç´¢å¼•: {len(embeddings)} ä¸ªå‘é‡, ç»´åº¦={embedding_dim}")

    def search_top_k(self, query: list[float], k: int) -> list[tuple[str, float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„kä¸ªé¡¹ï¼Œä»¥ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºåº¦é‡
        Args:
            query: æŸ¥è¯¢çš„embedding
            k: è¿”å›çš„æœ€ç›¸ä¼¼çš„kä¸ªé¡¹
        Returns:
            result: æœ€ç›¸ä¼¼çš„kä¸ªé¡¹çš„(hash, ä½™å¼¦ç›¸ä¼¼åº¦)åˆ—è¡¨
        """
        if self.faiss_index is None:
            logger.debug("FaissIndexå°šæœªæ„å»º,è¿”å›None")
            return []
        if self.idx2hash is None:
            logger.warning("idx2hashå°šæœªæ„å»º,è¿”å›None")
            return []

        # L2å½’ä¸€åŒ–
        faiss.normalize_L2(np.array([query], dtype=np.float32))
        # æœç´¢
        distances, indices = self.faiss_index.search(np.array([query]), k)
        # æ•´ç†ç»“æœ
        indices = list(indices.flatten())
        distances = list(distances.flatten())
        result = [
            (self.idx2hash[str(int(idx))], float(sim))
            for (idx, sim) in zip(indices, distances, strict=False)
            if idx in range(len(self.idx2hash))
        ]

        return result


class EmbeddingManager:
    def __init__(self, max_workers: int = DEFAULT_MAX_WORKERS, chunk_size: int = DEFAULT_CHUNK_SIZE):
        """
        åˆå§‹åŒ–EmbeddingManager

        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            chunk_size: æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„æ•°æ®å—å¤§å°
        """
        self.paragraphs_embedding_store = EmbeddingStore(
            "paragraph",  # type: ignore
            EMBEDDING_DATA_DIR_STR,
            max_workers=max_workers,
            chunk_size=chunk_size,
        )
        self.entities_embedding_store = EmbeddingStore(
            "entity",  # type: ignore
            EMBEDDING_DATA_DIR_STR,
            max_workers=max_workers,
            chunk_size=chunk_size,
        )
        self.relation_embedding_store = EmbeddingStore(
            "relation",  # type: ignore
            EMBEDDING_DATA_DIR_STR,
            max_workers=max_workers,
            chunk_size=chunk_size,
        )
        self.stored_pg_hashes = set()

    async def check_all_embedding_model_consistency(self):
        """å¯¹æ‰€æœ‰åµŒå…¥åº“åšæ¨¡å‹ä¸€è‡´æ€§æ ¡éªŒ"""
        return await self.paragraphs_embedding_store.check_embedding_model_consistency()

    async def _store_pg_into_embedding(self, raw_paragraphs: dict[str, str]):
        """å°†æ®µè½ç¼–ç å­˜å…¥Embeddingåº“"""
        await self.paragraphs_embedding_store.batch_insert_strs(list(raw_paragraphs.values()), times=1)

    async def _store_ent_into_embedding(self, triple_list_data: dict[str, list[list[str]]]):
        """å°†å®ä½“ç¼–ç å­˜å…¥Embeddingåº“"""
        entities = set()
        for triple_list in triple_list_data.values():
            for triple in triple_list:
                entities.add(triple[0])
                entities.add(triple[2])
        await self.entities_embedding_store.batch_insert_strs(list(entities), times=2)

    async def _store_rel_into_embedding(self, triple_list_data: dict[str, list[list[str]]]):
        """å°†å…³ç³»ç¼–ç å­˜å…¥Embeddingåº“"""
        graph_triples = []  # a list of unique relation triple (in tuple) from all chunks
        for triples in triple_list_data.values():
            graph_triples.extend([tuple(t) for t in triples])
        graph_triples = list(set(graph_triples))
        await self.relation_embedding_store.batch_insert_strs([str(triple) for triple in graph_triples], times=3)

    def load_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½"""
        self.paragraphs_embedding_store.load_from_file()
        self.entities_embedding_store.load_from_file()
        self.relation_embedding_store.load_from_file()
        # ä»æ®µè½åº“ä¸­è·å–å·²å­˜å‚¨çš„hash
        self.stored_pg_hashes = set(self.paragraphs_embedding_store.store.keys())

    async def store_new_data_set(
        self,
        raw_paragraphs: dict[str, str],
        triple_list_data: dict[str, list[list[str]]],
    ):
        if not await self.check_all_embedding_model_consistency():
            raise Exception("åµŒå…¥æ¨¡å‹ä¸æœ¬åœ°å­˜å‚¨ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è®¾ç½®æˆ–æ¸…ç©ºåµŒå…¥åº“åé‡è¯•ã€‚")
        """å­˜å‚¨æ–°çš„æ•°æ®é›†"""
        await self._store_pg_into_embedding(raw_paragraphs)
        await self._store_ent_into_embedding(triple_list_data)
        await self._store_rel_into_embedding(triple_list_data)
        self.stored_pg_hashes.update(raw_paragraphs.keys())

    def save_to_file(self):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        self.paragraphs_embedding_store.save_to_file()
        self.entities_embedding_store.save_to_file()
        self.relation_embedding_store.save_to_file()

    def rebuild_faiss_index(self):
        """é‡å»ºFaissç´¢å¼•ï¼ˆè¯·åœ¨æ·»åŠ æ–°æ•°æ®åè°ƒç”¨ï¼‰"""
        self.paragraphs_embedding_store.build_faiss_index()
        self.entities_embedding_store.build_faiss_index()
        self.relation_embedding_store.build_faiss_index()
