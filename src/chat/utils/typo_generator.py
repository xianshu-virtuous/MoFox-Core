"""
é”™åˆ«å­—ç”Ÿæˆå™¨ - åŸºäºæ‹¼éŸ³å’Œå­—é¢‘çš„ä¸­æ–‡é”™åˆ«å­—ç”Ÿæˆå·¥å…·

å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åˆ›å»ºæ‹¼éŸ³å­—å…¸ï¼ˆçº¦20992ä¸ªæ±‰å­—æ˜ å°„ï¼‰
"""

import math
import os
import random
import time
from collections import defaultdict
from pathlib import Path
from threading import Lock

import orjson
import rjieba
from pypinyin import Style, pinyin

from src.common.logger import get_logger

logger = get_logger("typo_gen")

# ğŸ”§ å…¨å±€å•ä¾‹å’Œç¼“å­˜
_typo_generator_singleton: "ChineseTypoGenerator | None" = None
_singleton_lock = Lock()
_shared_pinyin_dict: dict | None = None
_shared_char_frequency: dict | None = None


def get_typo_generator(
    error_rate: float = 0.3,
    min_freq: int = 5,
    tone_error_rate: float = 0.2,
    word_replace_rate: float = 0.3,
    max_freq_diff: int = 200,
) -> "ChineseTypoGenerator":
    """
    è·å–é”™åˆ«å­—ç”Ÿæˆå™¨å•ä¾‹ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
    
    å¦‚æœå‚æ•°ä¸ç¼“å­˜çš„å•ä¾‹ä¸åŒï¼Œä¼šæ›´æ–°å‚æ•°ä½†å¤ç”¨æ‹¼éŸ³å­—å…¸å’Œå­—é¢‘æ•°æ®ã€‚
    
    å‚æ•°:
        error_rate: å•å­—æ›¿æ¢æ¦‚ç‡
        min_freq: æœ€å°å­—é¢‘é˜ˆå€¼
        tone_error_rate: å£°è°ƒé”™è¯¯æ¦‚ç‡
        word_replace_rate: æ•´è¯æ›¿æ¢æ¦‚ç‡
        max_freq_diff: æœ€å¤§å…è®¸çš„é¢‘ç‡å·®å¼‚
        
    è¿”å›:
        ChineseTypoGenerator å®ä¾‹
    """
    global _typo_generator_singleton
    
    with _singleton_lock:
        if _typo_generator_singleton is None:
            _typo_generator_singleton = ChineseTypoGenerator(
                error_rate=error_rate,
                min_freq=min_freq,
                tone_error_rate=tone_error_rate,
                word_replace_rate=word_replace_rate,
                max_freq_diff=max_freq_diff,
            )
            logger.info("ChineseTypoGenerator å•ä¾‹å·²åˆ›å»º")
        else:
            # æ›´æ–°å‚æ•°ä½†å¤ç”¨å­—å…¸
            _typo_generator_singleton.set_params(
                error_rate=error_rate,
                min_freq=min_freq,
                tone_error_rate=tone_error_rate,
                word_replace_rate=word_replace_rate,
                max_freq_diff=max_freq_diff,
            )
    
    return _typo_generator_singleton


class ChineseTypoGenerator:
    def __init__(self, error_rate=0.3, min_freq=5, tone_error_rate=0.2, word_replace_rate=0.3, max_freq_diff=200):
        """
        åˆå§‹åŒ–é”™åˆ«å­—ç”Ÿæˆå™¨

        å‚æ•°:
            error_rate: å•å­—æ›¿æ¢æ¦‚ç‡
            min_freq: æœ€å°å­—é¢‘é˜ˆå€¼
            tone_error_rate: å£°è°ƒé”™è¯¯æ¦‚ç‡
            word_replace_rate: æ•´è¯æ›¿æ¢æ¦‚ç‡
            max_freq_diff: æœ€å¤§å…è®¸çš„é¢‘ç‡å·®å¼‚
        """
        global _shared_pinyin_dict, _shared_char_frequency
        
        self.error_rate = error_rate
        self.min_freq = min_freq
        self.tone_error_rate = tone_error_rate
        self.word_replace_rate = word_replace_rate
        self.max_freq_diff = max_freq_diff

        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šå¤ç”¨å…¨å±€ç¼“å­˜çš„æ‹¼éŸ³å­—å…¸å’Œå­—é¢‘æ•°æ®
        if _shared_pinyin_dict is None:
            _shared_pinyin_dict = self._create_pinyin_dict()
            logger.debug("æ‹¼éŸ³å­—å…¸å·²åˆ›å»ºå¹¶ç¼“å­˜")
        self.pinyin_dict = _shared_pinyin_dict
        
        if _shared_char_frequency is None:
            _shared_char_frequency = self._load_or_create_char_frequency()
            logger.debug("å­—é¢‘æ•°æ®å·²åŠ è½½å¹¶ç¼“å­˜")
        self.char_frequency = _shared_char_frequency

    def _load_or_create_char_frequency(self):
        """
        åŠ è½½æˆ–åˆ›å»ºæ±‰å­—é¢‘ç‡å­—å…¸
        """
        cache_file = Path("depends-data/char_frequency.json")

        # å¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
        if cache_file.exists():
            with open(cache_file, encoding="utf-8") as f:
                return orjson.loads(f.read())

        # ä½¿ç”¨å†…ç½®çš„è¯é¢‘æ–‡ä»¶
        char_freq = defaultdict(int)
        # ä»å½“å‰æ–‡ä»¶å‘ä¸Šè¿”å›ä¸‰çº§ç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç„¶åæ‹¼æ¥è·¯å¾„
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        dict_path = os.path.join(base_dir, "depends-data", "dict.txt")

        # è¯»å–rjiebaçš„è¯å…¸æ–‡ä»¶
        with open(dict_path, encoding="utf-8") as f:
            for line in f:
                word, freq = line.strip().split()[:2]
                # å¯¹è¯ä¸­çš„æ¯ä¸ªå­—è¿›è¡Œé¢‘ç‡ç´¯åŠ 
                for char in word:
                    if self._is_chinese_char(char):
                        char_freq[char] += int(freq)

        # å½’ä¸€åŒ–é¢‘ç‡å€¼
        max_freq = max(char_freq.values())
        normalized_freq = {char: freq / max_freq * 1000 for char, freq in char_freq.items()}

        # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
        with open(cache_file, "w", encoding="utf-8") as f:
            f.write(orjson.dumps(normalized_freq, option=orjson.OPT_INDENT_2).decode("utf-8"))

        return normalized_freq

    @staticmethod
    def _create_pinyin_dict():
        """
        åˆ›å»ºæ‹¼éŸ³åˆ°æ±‰å­—çš„æ˜ å°„å­—å…¸
        """
        # å¸¸ç”¨æ±‰å­—èŒƒå›´
        chars = [chr(i) for i in range(0x4E00, 0x9FFF)]
        pinyin_dict = defaultdict(list)

        # ä¸ºæ¯ä¸ªæ±‰å­—å»ºç«‹æ‹¼éŸ³æ˜ å°„
        for char in chars:
            try:
                py = pinyin(char, style=Style.TONE3)[0][0]
                pinyin_dict[py].append(char)
            except Exception:
                continue

        return pinyin_dict

    @staticmethod
    def _is_chinese_char(char):
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ±‰å­—
        """
        try:
            return "\u4e00" <= char <= "\u9fff"
        except Exception as e:
            logger.debug(str(e))
            return False

    def _get_pinyin(self, sentence):
        """
        å°†ä¸­æ–‡å¥å­æ‹†åˆ†æˆå•ä¸ªæ±‰å­—å¹¶è·å–å…¶æ‹¼éŸ³
        """
        # å°†å¥å­æ‹†åˆ†æˆå•ä¸ªå­—ç¬¦
        characters = list(sentence)

        # è·å–æ¯ä¸ªå­—ç¬¦çš„æ‹¼éŸ³
        result = []
        for char in characters:
            # è·³è¿‡ç©ºæ ¼å’Œéæ±‰å­—å­—ç¬¦
            if char.isspace() or not self._is_chinese_char(char):
                continue
            # è·å–æ‹¼éŸ³ï¼ˆæ•°å­—å£°è°ƒï¼‰
            py = pinyin(char, style=Style.TONE3)[0][0]
            result.append((char, py))

        return result

    @staticmethod
    def _get_similar_tone_pinyin(py):
        """
        è·å–ç›¸ä¼¼å£°è°ƒçš„æ‹¼éŸ³
        """
        # æ£€æŸ¥æ‹¼éŸ³æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
        if not py or len(py) < 1:
            return py

        # å¦‚æœæœ€åä¸€ä¸ªå­—ç¬¦ä¸æ˜¯æ•°å­—ï¼Œè¯´æ˜å¯èƒ½æ˜¯è½»å£°æˆ–å…¶ä»–ç‰¹æ®Šæƒ…å†µ
        if not py[-1].isdigit():
            # ä¸ºéæ•°å­—ç»“å°¾çš„æ‹¼éŸ³æ·»åŠ æ•°å­—å£°è°ƒ1
            return f"{py}1"

        base = py[:-1]  # å»æ‰å£°è°ƒ
        tone = int(py[-1])  # è·å–å£°è°ƒ

        # å¤„ç†è½»å£°ï¼ˆé€šå¸¸ç”¨5è¡¨ç¤ºï¼‰æˆ–æ— æ•ˆå£°è°ƒ
        if tone not in [1, 2, 3, 4]:
            return base + str(random.choice([1, 2, 3, 4]))

        # æ­£å¸¸å¤„ç†å£°è°ƒ
        possible_tones = [1, 2, 3, 4]
        possible_tones.remove(tone)  # ç§»é™¤åŸå£°è°ƒ
        new_tone = random.choice(possible_tones)  # éšæœºé€‰æ‹©ä¸€ä¸ªæ–°å£°è°ƒ
        return base + str(new_tone)

    def _calculate_replacement_probability(self, orig_freq, target_freq):
        """
        æ ¹æ®é¢‘ç‡å·®è®¡ç®—æ›¿æ¢æ¦‚ç‡
        """
        if target_freq > orig_freq:
            return 1.0  # å¦‚æœæ›¿æ¢å­—é¢‘ç‡æ›´é«˜ï¼Œä¿æŒåŸæœ‰æ¦‚ç‡

        freq_diff = orig_freq - target_freq
        if freq_diff > self.max_freq_diff:
            return 0.0  # é¢‘ç‡å·®å¤ªå¤§ï¼Œä¸æ›¿æ¢

        # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°è®¡ç®—æ¦‚ç‡
        # é¢‘ç‡å·®ä¸º0æ—¶æ¦‚ç‡ä¸º1ï¼Œé¢‘ç‡å·®ä¸ºmax_freq_diffæ—¶æ¦‚ç‡æ¥è¿‘0
        return math.exp(-3 * freq_diff / self.max_freq_diff)

    def _get_similar_frequency_chars(self, char, py, num_candidates=5):
        """
        è·å–ä¸ç»™å®šå­—é¢‘ç‡ç›¸è¿‘çš„åŒéŸ³å­—ï¼Œå¯èƒ½åŒ…å«å£°è°ƒé”™è¯¯
        """
        homophones = []

        # æœ‰ä¸€å®šæ¦‚ç‡ä½¿ç”¨é”™è¯¯å£°è°ƒ
        if random.random() < self.tone_error_rate:
            wrong_tone_py = self._get_similar_tone_pinyin(py)
            homophones.extend(self.pinyin_dict[wrong_tone_py])

        # æ·»åŠ æ­£ç¡®å£°è°ƒçš„åŒéŸ³å­—
        homophones.extend(self.pinyin_dict[py])

        if not homophones:
            return None

        # è·å–åŸå­—çš„é¢‘ç‡
        orig_freq = self.char_frequency.get(char, 0)

        # è®¡ç®—æ‰€æœ‰åŒéŸ³å­—ä¸åŸå­—çš„é¢‘ç‡å·®ï¼Œå¹¶è¿‡æ»¤æ‰ä½é¢‘å­—
        freq_diff = [
            (h, self.char_frequency.get(h, 0))
            for h in homophones
            if h != char and self.char_frequency.get(h, 0) >= self.min_freq
        ]

        if not freq_diff:
            return None

        # è®¡ç®—æ¯ä¸ªå€™é€‰å­—çš„æ›¿æ¢æ¦‚ç‡
        candidates_with_prob = []
        for h, freq in freq_diff:
            prob = self._calculate_replacement_probability(orig_freq, freq)
            if prob > 0:  # åªä¿ç•™æœ‰æ•ˆæ¦‚ç‡çš„å€™é€‰å­—
                candidates_with_prob.append((h, prob))

        if not candidates_with_prob:
            return None

        # æ ¹æ®æ¦‚ç‡æ’åº
        candidates_with_prob.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›æ¦‚ç‡æœ€é«˜çš„å‡ ä¸ªå­—
        return [char for char, _ in candidates_with_prob[:num_candidates]]

    @staticmethod
    def _get_word_pinyin(word):
        """
        è·å–è¯è¯­çš„æ‹¼éŸ³åˆ—è¡¨
        """
        return [py[0] for py in pinyin(word, style=Style.TONE3)]

    @staticmethod
    def _segment_sentence(sentence):
        """
        ä½¿ç”¨rjiebaåˆ†è¯ï¼Œè¿”å›è¯è¯­åˆ—è¡¨
        """
        return list(rjieba.cut(sentence))

    def _get_word_homophones(self, word):
        """
        è·å–æ•´ä¸ªè¯çš„åŒéŸ³è¯ï¼Œåªè¿”å›é«˜é¢‘çš„æœ‰æ„ä¹‰è¯è¯­
        """
        if len(word) == 1:
            return []

        # è·å–è¯çš„æ‹¼éŸ³
        word_pinyin = self._get_word_pinyin(word)

        # éå†æ‰€æœ‰å¯èƒ½çš„åŒéŸ³å­—ç»„åˆ
        candidates = []
        for py in word_pinyin:
            chars = self.pinyin_dict.get(py, [])
            if not chars:
                return []
            candidates.append(chars)

        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        import itertools

        all_combinations = itertools.product(*candidates)

        # è·å–rjiebaè¯å…¸å’Œè¯é¢‘ä¿¡æ¯
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        dict_path = os.path.join(base_dir, "depends-data", "dict.txt")
        valid_words = {}  # æ”¹ç”¨å­—å…¸å­˜å‚¨è¯è¯­åŠå…¶é¢‘ç‡
        with open(dict_path, encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    word_text = parts[0]
                    word_freq = float(parts[1])  # è·å–è¯é¢‘
                    valid_words[word_text] = word_freq

        # è·å–åŸè¯çš„è¯é¢‘ä½œä¸ºå‚è€ƒ
        original_word_freq = valid_words.get(word, 0)
        min_word_freq = original_word_freq * 0.1  # è®¾ç½®æœ€å°è¯é¢‘ä¸ºåŸè¯é¢‘çš„10%

        # è¿‡æ»¤å’Œè®¡ç®—é¢‘ç‡
        homophones = []
        for combo in all_combinations:
            new_word = "".join(combo)
            if new_word != word and new_word in valid_words:
                new_word_freq = valid_words[new_word]
                # åªä¿ç•™è¯é¢‘è¾¾åˆ°é˜ˆå€¼çš„è¯
                if new_word_freq >= min_word_freq:
                    # è®¡ç®—è¯çš„å¹³å‡å­—é¢‘ï¼ˆè€ƒè™‘å­—é¢‘å’Œè¯é¢‘ï¼‰
                    char_avg_freq = sum(self.char_frequency.get(c, 0) for c in new_word) / len(new_word)
                    # ç»¼åˆè¯„åˆ†ï¼šç»“åˆè¯é¢‘å’Œå­—é¢‘
                    combined_score = new_word_freq * 0.7 + char_avg_freq * 0.3
                    if combined_score >= self.min_freq:
                        homophones.append((new_word, combined_score))

        # æŒ‰ç»¼åˆåˆ†æ•°æ’åºå¹¶é™åˆ¶è¿”å›æ•°é‡
        sorted_homophones = sorted(homophones, key=lambda x: x[1], reverse=True)
        return [word for word, _ in sorted_homophones[:5]]  # é™åˆ¶è¿”å›å‰5ä¸ªç»“æœ

    def create_typo_sentence(self, sentence):
        """
        åˆ›å»ºåŒ…å«åŒéŸ³å­—é”™è¯¯çš„å¥å­ï¼Œæ”¯æŒè¯è¯­çº§åˆ«å’Œå­—çº§åˆ«çš„æ›¿æ¢

        å‚æ•°:
            sentence: è¾“å…¥çš„ä¸­æ–‡å¥å­

        è¿”å›:
            typo_sentence: åŒ…å«é”™åˆ«å­—çš„å¥å­
            correction_suggestion: éšæœºé€‰æ‹©çš„ä¸€ä¸ªçº æ­£å»ºè®®ï¼Œè¿”å›æ­£ç¡®çš„å­—/è¯
        """
        result = []
        typo_info = []
        word_typos = []  # è®°å½•è¯è¯­é”™è¯¯å¯¹(é”™è¯,æ­£ç¡®è¯)
        char_typos = []  # è®°å½•å•å­—é”™è¯¯å¯¹(é”™å­—,æ­£ç¡®å­—)
        current_pos = 0

        # åˆ†è¯
        words = self._segment_sentence(sentence)

        for word in words:
            # å¦‚æœæ˜¯æ ‡ç‚¹ç¬¦å·æˆ–ç©ºæ ¼ï¼Œç›´æ¥æ·»åŠ 
            if all(not self._is_chinese_char(c) for c in word):
                result.append(word)
                current_pos += len(word)
                continue

            # è·å–è¯è¯­çš„æ‹¼éŸ³
            word_pinyin = self._get_word_pinyin(word)

            # å°è¯•æ•´è¯æ›¿æ¢
            if len(word) > 1 and random.random() < self.word_replace_rate:
                word_homophones = self._get_word_homophones(word)
                if word_homophones:
                    typo_word = random.choice(word_homophones)
                    # è®¡ç®—è¯çš„å¹³å‡é¢‘ç‡
                    orig_freq = sum(self.char_frequency.get(c, 0) for c in word) / len(word)
                    typo_freq = sum(self.char_frequency.get(c, 0) for c in typo_word) / len(typo_word)

                    # æ·»åŠ åˆ°ç»“æœä¸­
                    result.append(typo_word)
                    typo_info.append(
                        (
                            word,
                            typo_word,
                            " ".join(word_pinyin),
                            " ".join(self._get_word_pinyin(typo_word)),
                            orig_freq,
                            typo_freq,
                        )
                    )
                    word_typos.append((typo_word, word))  # è®°å½•(é”™è¯,æ­£ç¡®è¯)å¯¹
                    current_pos += len(typo_word)
                    continue

            # å¦‚æœä¸è¿›è¡Œæ•´è¯æ›¿æ¢ï¼Œåˆ™è¿›è¡Œå•å­—æ›¿æ¢
            if len(word) == 1:
                char = word
                py = word_pinyin[0]
                if random.random() < self.error_rate:
                    similar_chars = self._get_similar_frequency_chars(char, py)
                    if similar_chars:
                        typo_char = random.choice(similar_chars)
                        typo_freq = self.char_frequency.get(typo_char, 0)
                        orig_freq = self.char_frequency.get(char, 0)
                        replace_prob = self._calculate_replacement_probability(orig_freq, typo_freq)
                        if random.random() < replace_prob:
                            result.append(typo_char)
                            typo_py = pinyin(typo_char, style=Style.TONE3)[0][0]
                            typo_info.append((char, typo_char, py, typo_py, orig_freq, typo_freq))
                            char_typos.append((typo_char, char))  # è®°å½•(é”™å­—,æ­£ç¡®å­—)å¯¹
                            current_pos += 1
                            continue
                result.append(char)
                current_pos += 1
            else:
                # å¤„ç†å¤šå­—è¯çš„å•å­—æ›¿æ¢
                word_result = []
                for _, (char, py) in enumerate(zip(word, word_pinyin, strict=False)):
                    # è¯ä¸­çš„å­—æ›¿æ¢æ¦‚ç‡é™ä½
                    word_error_rate = self.error_rate * (0.7 ** (len(word) - 1))

                    if random.random() < word_error_rate:
                        similar_chars = self._get_similar_frequency_chars(char, py)
                        if similar_chars:
                            typo_char = random.choice(similar_chars)
                            typo_freq = self.char_frequency.get(typo_char, 0)
                            orig_freq = self.char_frequency.get(char, 0)
                            replace_prob = self._calculate_replacement_probability(orig_freq, typo_freq)
                            if random.random() < replace_prob:
                                word_result.append(typo_char)
                                typo_py = pinyin(typo_char, style=Style.TONE3)[0][0]
                                typo_info.append((char, typo_char, py, typo_py, orig_freq, typo_freq))
                                char_typos.append((typo_char, char))  # è®°å½•(é”™å­—,æ­£ç¡®å­—)å¯¹
                                continue
                    word_result.append(char)
                result.append("".join(word_result))
                current_pos += len(word)

        # ä¼˜å…ˆä»è¯è¯­é”™è¯¯ä¸­é€‰æ‹©ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»å•å­—é”™è¯¯ä¸­é€‰æ‹©
        correction_suggestion = None
        # 50%æ¦‚ç‡è¿”å›çº æ­£å»ºè®®
        if random.random() < 0.5:
            if word_typos:
                wrong_word, correct_word = random.choice(word_typos)
                correction_suggestion = correct_word
            elif char_typos:
                wrong_char, correct_char = random.choice(char_typos)
                correction_suggestion = correct_char

        return "".join(result), correction_suggestion

    @staticmethod
    def format_typo_info(typo_info):
        """
        æ ¼å¼åŒ–é”™åˆ«å­—ä¿¡æ¯

        å‚æ•°:
            typo_info: é”™åˆ«å­—ä¿¡æ¯åˆ—è¡¨

        è¿”å›:
            æ ¼å¼åŒ–åçš„é”™åˆ«å­—ä¿¡æ¯å­—ç¬¦ä¸²
        """
        if not typo_info:
            return "æœªç”Ÿæˆé”™åˆ«å­—"

        result = []
        for orig, typo, orig_py, typo_py, orig_freq, typo_freq in typo_info:
            # åˆ¤æ–­æ˜¯å¦ä¸ºè¯è¯­æ›¿æ¢
            is_word = " " in orig_py
            if is_word:
                error_type = "æ•´è¯æ›¿æ¢"
            else:
                tone_error = orig_py[:-1] == typo_py[:-1] and orig_py[-1] != typo_py[-1]
                error_type = "å£°è°ƒé”™è¯¯" if tone_error else "åŒéŸ³å­—æ›¿æ¢"

            result.append(
                f"åŸæ–‡ï¼š{orig}({orig_py}) [é¢‘ç‡ï¼š{orig_freq:.2f}] -> "
                f"æ›¿æ¢ï¼š{typo}({typo_py}) [é¢‘ç‡ï¼š{typo_freq:.2f}] [{error_type}]"
            )

        return "\n".join(result)

    def set_params(self, **kwargs):
        """
        è®¾ç½®å‚æ•°ï¼ˆé™é»˜æ¨¡å¼ï¼Œä¾›å•ä¾‹å¤ç”¨æ—¶è°ƒç”¨ï¼‰

        å¯è®¾ç½®å‚æ•°:
            error_rate: å•å­—æ›¿æ¢æ¦‚ç‡
            min_freq: æœ€å°å­—é¢‘é˜ˆå€¼
            tone_error_rate: å£°è°ƒé”™è¯¯æ¦‚ç‡
            word_replace_rate: æ•´è¯æ›¿æ¢æ¦‚ç‡
            max_freq_diff: æœ€å¤§å…è®¸çš„é¢‘ç‡å·®å¼‚
        """
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)


def main():
    # åˆ›å»ºé”™åˆ«å­—ç”Ÿæˆå™¨å®ä¾‹
    typo_generator = ChineseTypoGenerator(error_rate=0.03, min_freq=7, tone_error_rate=0.02, word_replace_rate=0.3)

    # è·å–ç”¨æˆ·è¾“å…¥
    sentence = input("è¯·è¾“å…¥ä¸­æ–‡å¥å­ï¼š")

    # åˆ›å»ºåŒ…å«é”™åˆ«å­—çš„å¥å­
    start_time = time.time()
    typo_sentence, correction_suggestion = typo_generator.create_typo_sentence(sentence)

    # æ‰“å°ç»“æœ
    print("\nåŸå¥ï¼š", sentence)
    print("é”™å­—ç‰ˆï¼š", typo_sentence)

    # æ‰“å°çº æ­£å»ºè®®
    if correction_suggestion:
        print("\néšæœºçº æ­£å»ºè®®ï¼š")
        print(f"åº”è¯¥æ”¹ä¸ºï¼š{correction_suggestion}")

    # è®¡ç®—å¹¶æ‰“å°æ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    print(f"\næ€»è€—æ—¶ï¼š{total_time:.2f}ç§’")


if __name__ == "__main__":
    main()
