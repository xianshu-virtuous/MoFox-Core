import asyncio
import random
import re
import string
import time
from collections import Counter
from typing import Any

import numpy as np
import rjieba

# MessageRecv å·²è¢«ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ DatabaseMessages
from src.common.logger import get_logger
from src.common.message_repository import count_messages, find_messages
from src.config.config import global_config, model_config
from src.llm_models.utils_model import LLMRequest
from src.person_info.person_info import PersonInfoManager, get_person_info_manager
from src.common.data_models.database_data_model import DatabaseUserInfo
from .typo_generator import get_typo_generator

logger = get_logger("chat_utils")


def is_english_letter(char: str) -> bool:
    """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºè‹±æ–‡å­—æ¯ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰"""
    return "a" <= char.lower() <= "z"


def db_message_to_str(message_dict: dict) -> str:
    logger.debug(f"message_dict: {message_dict}")
    time_str = time.strftime("%m-%d %H:%M:%S", time.localtime(message_dict["time"]))
    try:
        name = f"[({message_dict['user_id']}){message_dict.get('user_nickname', '')}]{message_dict.get('user_cardname', '')}"
    except Exception:
        name = message_dict.get("user_nickname", "") or f"ç”¨æˆ·{message_dict['user_id']}"
    content = message_dict.get("processed_plain_text", "")
    result = f"[{time_str}] {name}: {content}\n"
    logger.debug(f"result: {result}")
    return result


def is_mentioned_bot_in_message(message) -> tuple[bool, float]:
    """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦æåˆ°äº†æœºå™¨äºº

    Args:
        message: DatabaseMessages æ¶ˆæ¯å¯¹è±¡

    Returns:
        tuple[bool, float]: (æ˜¯å¦æåŠ, æåŠç±»å‹)
        æåŠç±»å‹: 0=æœªæåŠ, 1=å¼±æåŠï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰, 2=å¼ºæåŠï¼ˆ@/å›å¤/ç§èŠï¼‰
    """
    assert global_config is not None
    nicknames = global_config.bot.alias_names
    mention_type = 0  # 0=æœªæåŠ, 1=å¼±æåŠ, 2=å¼ºæåŠ

    # æ£€æŸ¥ is_mentioned å±æ€§ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    mentioned_attr = getattr(message, "is_mentioned", None)
    if mentioned_attr is not None:
        try:
            # å¦‚æœå·²æœ‰ is_mentionedï¼Œç›´æ¥è¿”å›ï¼ˆå‡è®¾æ˜¯å¼ºæåŠï¼‰
            return bool(mentioned_attr), 2.0 if mentioned_attr else 0.0
        except (ValueError, TypeError):
            pass

    # æ£€æŸ¥ additional_configï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    additional_config = None

    # DatabaseMessages: additional_config æ˜¯ JSON å­—ç¬¦ä¸²
    if message.additional_config:
        try:
            import orjson
            additional_config = orjson.loads(message.additional_config)
        except Exception:
            pass

    if additional_config and additional_config.get("is_mentioned") is not None:
        try:
            mentioned_value = float(additional_config.get("is_mentioned"))  # type: ignore
            # å¦‚æœé…ç½®ä¸­æœ‰æåŠå€¼ï¼Œå‡è®¾æ˜¯å¼ºæåŠ
            return True, 2.0 if mentioned_value > 0 else 0.0
        except Exception as e:
            logger.warning(str(e))
            logger.warning(
                f"æ¶ˆæ¯ä¸­åŒ…å«ä¸åˆç†çš„è®¾ç½® is_mentioned: {additional_config.get('is_mentioned')}"
            )

    processed_text = message.processed_plain_text or ""

    # 1. åˆ¤æ–­æ˜¯å¦ä¸ºç§èŠï¼ˆå¼ºæåŠï¼‰
    group_info = getattr(message, "group_info", None)
    if not group_info or not getattr(group_info, "group_id", None):
        mention_type = 2
        logger.debug("æ£€æµ‹åˆ°ç§èŠæ¶ˆæ¯ - å¼ºæåŠ")

    # 2. åˆ¤æ–­æ˜¯å¦è¢«@ï¼ˆå¼ºæåŠï¼‰
    if re.search(rf"@<(.+?):{global_config.bot.qq_account}>", processed_text):
        mention_type = 2
        logger.debug("æ£€æµ‹åˆ°@æåŠ - å¼ºæåŠ")

    # 3. åˆ¤æ–­æ˜¯å¦è¢«å›å¤ï¼ˆå¼ºæåŠï¼‰
    if re.match(
        rf"\[å›å¤ (.+?)\({global_config.bot.qq_account!s}\)ï¼š(.+?)\]ï¼Œè¯´ï¼š", processed_text
    ) or re.match(
        rf"\[å›å¤<(.+?)(?=:{global_config.bot.qq_account!s}>)\:{global_config.bot.qq_account!s}>ï¼š(.+?)\]ï¼Œè¯´ï¼š",
        processed_text,
    ):
        mention_type = 2
        logger.debug("æ£€æµ‹åˆ°å›å¤æ¶ˆæ¯ - å¼ºæåŠ")

    # 4. åˆ¤æ–­æ–‡æœ¬ä¸­æ˜¯å¦æåŠbotåå­—æˆ–åˆ«åï¼ˆå¼±æåŠï¼‰
    if mention_type == 0:  # åªæœ‰åœ¨æ²¡æœ‰å¼ºæåŠæ—¶æ‰æ£€æŸ¥å¼±æåŠ
        # ç§»é™¤@å’Œå›å¤æ ‡è®°åå†æ£€æŸ¥
        message_content = re.sub(r"@(.+?)ï¼ˆ(\d+)ï¼‰", "", processed_text)
        message_content = re.sub(r"@<(.+?)(?=:(\d+))\:(\d+)>", "", message_content)
        message_content = re.sub(r"\[å›å¤ (.+?)\(((\d+)|æœªçŸ¥id)\)ï¼š(.+?)\]ï¼Œè¯´ï¼š", "", message_content)
        message_content = re.sub(r"\[å›å¤<(.+?)(?=:(\d+))\:(\d+)>ï¼š(.+?)\]ï¼Œè¯´ï¼š", "", message_content)

        # æ£€æŸ¥botä¸»åå­—
        if global_config.bot.nickname in message_content:
            mention_type = 1
            logger.debug(f"æ£€æµ‹åˆ°æ–‡æœ¬æåŠbotä¸»åå­— '{global_config.bot.nickname}' - å¼±æåŠ")
        # å¦‚æœä¸»åå­—æ²¡åŒ¹é…ï¼Œå†æ£€æŸ¥åˆ«å
        elif nicknames:
            for alias_name in nicknames:
                if alias_name in message_content:
                    mention_type = 1
                    logger.debug(f"æ£€æµ‹åˆ°æ–‡æœ¬æåŠbotåˆ«å '{alias_name}' - å¼±æåŠ")
                    break

    # è¿”å›ç»“æœ
    is_mentioned = mention_type > 0
    return is_mentioned, float(mention_type)

async def get_embedding(text, request_type="embedding") -> list[float] | None:
    """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
    assert model_config is not None
    # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„LLMRequestå®ä¾‹ä»¥é¿å…äº‹ä»¶å¾ªç¯å†²çª
    llm = LLMRequest(model_set=model_config.model_task_config.embedding, request_type=request_type)
    try:
        embedding, _ = await llm.get_embedding(text)
    except Exception as e:
        logger.error(f"è·å–embeddingå¤±è´¥: {e!s}")
        embedding = None
    return embedding  # type: ignore


async def get_recent_group_speaker(chat_stream_id: str, sender, limit: int = 12) -> list:
    # è·å–å½“å‰ç¾¤èŠè®°å½•å†…å‘è¨€çš„äºº
    assert global_config is not None
    filter_query = {"chat_id": chat_stream_id}
    sort_order = [("time", -1)]
    recent_messages = await find_messages(message_filter=filter_query, sort=sort_order, limit=limit)

    if not recent_messages:
        return []

    who_chat_in_group = []
    for msg_db_data in recent_messages:
        user_info = DatabaseUserInfo.from_dict(
            {
                "platform": msg_db_data["user_platform"],
                "user_id": msg_db_data["user_id"],
                "user_nickname": msg_db_data["user_nickname"],
                "user_cardname": msg_db_data.get("user_cardname", ""),
            }
        )
        if (
            (user_info.platform, user_info.user_id) != sender
            and user_info.user_id != str(global_config.bot.qq_account)
            and (user_info.platform, user_info.user_id, user_info.user_nickname) not in who_chat_in_group
            and len(who_chat_in_group) < 5
        ):  # æ’é™¤é‡å¤ï¼Œæ’é™¤æ¶ˆæ¯å‘é€è€…ï¼Œæ’é™¤botï¼Œé™åˆ¶åŠ è½½çš„å…³ç³»æ•°ç›®
            who_chat_in_group.append((user_info.platform, user_info.user_id, user_info.user_nickname))

    return who_chat_in_group


def split_into_sentences_w_remove_punctuation(text: str) -> list[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­ï¼Œå¹¶æ ¹æ®æ¦‚ç‡åˆå¹¶
    1. è¯†åˆ«åˆ†å‰²ç‚¹ï¼ˆ, ï¼Œ ã€‚ ; ç©ºæ ¼ï¼‰ï¼Œä½†å¦‚æœåˆ†å‰²ç‚¹å·¦å³éƒ½æ˜¯è‹±æ–‡å­—æ¯åˆ™ä¸åˆ†å‰²ã€‚
    2. å°†æ–‡æœ¬åˆ†å‰²æˆ (å†…å®¹, åˆ†éš”ç¬¦) çš„å…ƒç»„ã€‚
    3. æ ¹æ®åŸå§‹æ–‡æœ¬é•¿åº¦è®¡ç®—åˆå¹¶æ¦‚ç‡ï¼Œæ¦‚ç‡æ€§åœ°åˆå¹¶ç›¸é‚»æ®µè½ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°å‡å®šé¢œæ–‡å­—å·²åœ¨ä¸Šå±‚è¢«ä¿æŠ¤ã€‚
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬å­—ç¬¦ä¸² (å‡å®šé¢œæ–‡å­—å·²è¢«ä¿æŠ¤)
    Returns:
        List[str]: åˆ†å‰²å’Œåˆå¹¶åçš„å¥å­åˆ—è¡¨
    """
    # é¢„å¤„ç†ï¼šå¤„ç†å¤šä½™çš„æ¢è¡Œç¬¦
    # 1. å°†è¿ç»­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ªæ¢è¡Œç¬¦
    text = re.sub(r"\n\s*\n+", "\n", text)
    # 2. å¤„ç†æ¢è¡Œç¬¦å’Œå…¶ä»–åˆ†éš”ç¬¦çš„ç»„åˆ
    text = re.sub(r"\n\s*([ï¼Œ,ã€‚;\s])", r"\1", text)
    text = re.sub(r"([ï¼Œ,ã€‚;\s])\s*\n", r"\1", text)

    # å¤„ç†ä¸¤ä¸ªæ±‰å­—ä¸­é—´çš„æ¢è¡Œç¬¦
    text = re.sub(r"([\u4e00-\u9fff])\n([\u4e00-\u9fff])", r"\1ã€‚\2", text)

    len_text = len(text)
    if len_text < 3:
        return list(text) if random.random() < 0.01 else [text]

    # å®šä¹‰åˆ†éš”ç¬¦
    separators = {"ï¼Œ", ",", " ", "ã€‚", ";"}
    segments = []
    current_segment = ""

    # 1. åˆ†å‰²æˆ (å†…å®¹, åˆ†éš”ç¬¦) å…ƒç»„
    i = 0
    while i < len(text):
        char = text[i]
        if char in separators:
            # æ£€æŸ¥åˆ†å‰²æ¡ä»¶ï¼šå¦‚æœåˆ†éš”ç¬¦å·¦å³éƒ½æ˜¯è‹±æ–‡å­—æ¯ï¼Œåˆ™ä¸åˆ†å‰²
            can_split = True
            if 0 < i < len(text) - 1:
                prev_char = text[i - 1]
                next_char = text[i + 1]
                # if is_english_letter(prev_char) and is_english_letter(next_char) and char == ' ': # åŸè®¡åˆ’åªå¯¹ç©ºæ ¼åº”ç”¨æ­¤è§„åˆ™ï¼Œç°åº”ç”¨äºæ‰€æœ‰åˆ†éš”ç¬¦
                if is_english_letter(prev_char) and is_english_letter(next_char):
                    can_split = False

            if can_split:
                # åªæœ‰å½“å½“å‰æ®µä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
                if current_segment:
                    segments.append((current_segment, char))
                # å¦‚æœå½“å‰æ®µä¸ºç©ºï¼Œä½†åˆ†éš”ç¬¦æ˜¯ç©ºæ ¼ï¼Œåˆ™ä¹Ÿæ·»åŠ ä¸€ä¸ªç©ºæ®µï¼ˆä¿ç•™ç©ºæ ¼ï¼‰
                elif char == " ":
                    segments.append(("", char))
                current_segment = ""
            else:
                # ä¸åˆ†å‰²ï¼Œå°†åˆ†éš”ç¬¦åŠ å…¥å½“å‰æ®µ
                current_segment += char
        else:
            current_segment += char
        i += 1

    # æ·»åŠ æœ€åä¸€ä¸ªæ®µï¼ˆæ²¡æœ‰åç»­åˆ†éš”ç¬¦ï¼‰
    if current_segment:
        segments.append((current_segment, ""))

    # è¿‡æ»¤æ‰å®Œå…¨ç©ºçš„æ®µï¼ˆå†…å®¹å’Œåˆ†éš”ç¬¦éƒ½ä¸ºç©ºï¼‰
    segments = [(content, sep) for content, sep in segments if content or sep]

    # å¦‚æœåˆ†å‰²åä¸ºç©ºï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥å…¨æ˜¯åˆ†éš”ç¬¦ä¸”ä¸æ»¡è¶³ä¿ç•™æ¡ä»¶ï¼‰ï¼Œæ¢å¤é¢œæ–‡å­—å¹¶è¿”å›
    if not segments:
        return [text] if text else []  # å¦‚æœåŸå§‹æ–‡æœ¬éç©ºï¼Œåˆ™è¿”å›åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åªåŒ…å«æœªè¢«åˆ†å‰²çš„å­—ç¬¦æˆ–é¢œæ–‡å­—å ä½ç¬¦ï¼‰

    # 2. æ¦‚ç‡åˆå¹¶
    if len_text < 12:
        split_strength = 0.2
    elif len_text < 32:
        split_strength = 0.6
    else:
        split_strength = 0.7
    # åˆå¹¶æ¦‚ç‡ä¸åˆ†å‰²å¼ºåº¦ç›¸å
    merge_probability = 1.0 - split_strength

    merged_segments = []
    idx = 0
    while idx < len(segments):
        current_content, current_sep = segments[idx]

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸ä¸‹ä¸€æ®µåˆå¹¶
        # æ¡ä»¶ï¼šä¸æ˜¯æœ€åä¸€æ®µï¼Œä¸”éšæœºæ•°å°äºåˆå¹¶æ¦‚ç‡ï¼Œä¸”å½“å‰æ®µæœ‰å†…å®¹ï¼ˆé¿å…åˆå¹¶ç©ºæ®µï¼‰
        if idx + 1 < len(segments) and random.random() < merge_probability and current_content:
            next_content, next_sep = segments[idx + 1]
            # åˆå¹¶: (å†…å®¹1 + åˆ†éš”ç¬¦1 + å†…å®¹2, åˆ†éš”ç¬¦2)
            # åªæœ‰å½“ä¸‹ä¸€æ®µä¹Ÿæœ‰å†…å®¹æ—¶æ‰åˆå¹¶æ–‡æœ¬ï¼Œå¦åˆ™åªä¼ é€’åˆ†éš”ç¬¦
            if next_content:
                merged_content = current_content + current_sep + next_content
                merged_segments.append((merged_content, next_sep))
            else:  # ä¸‹ä¸€æ®µå†…å®¹ä¸ºç©ºï¼Œåªä¿ç•™å½“å‰å†…å®¹å’Œä¸‹ä¸€æ®µçš„åˆ†éš”ç¬¦
                merged_segments.append((current_content, next_sep))

            idx += 2  # è·³è¿‡ä¸‹ä¸€æ®µï¼Œå› ä¸ºå®ƒå·²è¢«åˆå¹¶
        else:
            # ä¸åˆå¹¶ï¼Œç›´æ¥æ·»åŠ å½“å‰æ®µ
            merged_segments.append((current_content, current_sep))
            idx += 1

    # æå–æœ€ç»ˆçš„å¥å­å†…å®¹
    final_sentences = [content for content, sep in merged_segments if content]  # åªä¿ç•™æœ‰å†…å®¹çš„æ®µ

    # æ¸…ç†å¯èƒ½å¼•å…¥çš„ç©ºå­—ç¬¦ä¸²å’Œä»…åŒ…å«ç©ºç™½çš„å­—ç¬¦ä¸²
    final_sentences = [
        s for s in final_sentences if s.strip()
    ]  # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ä»¥åŠä»…åŒ…å«ç©ºç™½ï¼ˆå¦‚æ¢è¡Œç¬¦ã€ç©ºæ ¼ï¼‰çš„å­—ç¬¦ä¸²

    logger.debug(f"åˆ†å‰²å¹¶åˆå¹¶åçš„å¥å­: {final_sentences}")
    return final_sentences


def random_remove_punctuation(text: str) -> str:
    """éšæœºå¤„ç†æ ‡ç‚¹ç¬¦å·ï¼Œæ¨¡æ‹Ÿäººç±»æ‰“å­—ä¹ æƒ¯

    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬

    Returns:
        str: å¤„ç†åçš„æ–‡æœ¬
    """
    result = ""
    text_len = len(text)

    for i, char in enumerate(text):
        if char == "ã€‚" and i == text_len - 1:  # ç»“å°¾çš„å¥å·
            if random.random() > 0.1:  # 90%æ¦‚ç‡åˆ é™¤ç»“å°¾å¥å·
                continue
        elif char == "ï¼Œ":
            rand = random.random()
            if rand < 0.05:  # 5%æ¦‚ç‡åˆ é™¤é€—å·
                continue
            elif rand < 0.25:  # 20%æ¦‚ç‡æŠŠé€—å·å˜æˆç©ºæ ¼
                result += " "
                continue
        result += char
    return result


def protect_special_blocks(text: str) -> tuple[str, dict[str, str]]:
    """è¯†åˆ«å¹¶ä¿æŠ¤æ•°å­¦å…¬å¼å’Œä»£ç å—ï¼Œè¿”å›å¤„ç†åçš„æ–‡æœ¬å’Œæ˜ å°„"""
    placeholder_map = {}

    # ç¬¬ä¸€å±‚é˜²æŠ¤ï¼šä¼˜å…ˆä¿æŠ¤æ ‡å‡†Markdownæ ¼å¼
    # ä½¿ç”¨ re.S æ¥è®© . åŒ¹é…æ¢è¡Œç¬¦
    markdown_patterns = {
        "code": r"```.*?```",
        "math": r"\$\$.*?\$\$",
    }

    placeholder_idx = 0
    for block_type, pattern in markdown_patterns.items():
        matches = re.findall(pattern, text, re.S)
        for match in matches:
            placeholder = f"__SPECIAL_{block_type.upper()}_{placeholder_idx}__"
            text = text.replace(match, placeholder, 1)
            placeholder_map[placeholder] = match
            placeholder_idx += 1

    # ç¬¬äºŒå±‚é˜²æŠ¤ï¼šä¿æŠ¤éæ ‡å‡†çš„ã€å¯èƒ½æ˜¯å…¬å¼æˆ–ä»£ç çš„ç‰‡æ®µ
    # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾è¿ç»­5ä¸ªä»¥ä¸Šçš„ã€ä¸»è¦ç”±éä¸­æ–‡å­—ç¬¦ç»„æˆçš„ç‰‡æ®µ
    general_pattern = r"(?:[a-zA-Z0-9\s.,;:(){}\[\]_+\-*/=<>^|&%?!'\"âˆšÂ²Â³â¿âˆ‘âˆ«â‰ â‰¥â‰¤]){5,}"

    # ä¸ºäº†é¿å…ä¸å·²ä¿æŠ¤çš„å ä½ç¬¦å†²çªï¼Œæˆ‘ä»¬åœ¨å‰©ä½™çš„æ–‡æœ¬ä¸Šè¿›è¡ŒæŸ¥æ‰¾
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¤„ç†ï¼Œæ›´ç¨³å¦¥çš„æ–¹å¼æ˜¯åˆ†æ®µæŸ¥æ‰¾ï¼Œä½†ç›®å‰è¿™æ ·è¶³ä»¥åº”å¯¹å¤šæ•°æƒ…å†µ
    try:
        matches = re.findall(general_pattern, text)
        for match in matches:
            # é¿å…å°†åŒ…å«å ä½ç¬¦çš„ç‰‡æ®µå†æ¬¡ä¿æŠ¤
            if "__SPECIAL_" in match:
                continue

            placeholder = f"__SPECIAL_GENERAL_{placeholder_idx}__"
            text = text.replace(match, placeholder, 1)
            placeholder_map[placeholder] = match
            placeholder_idx += 1
    except re.error as e:
        logger.error(f"ç‰¹æ®ŠåŒºåŸŸé˜²æŠ¤æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")

    return text, placeholder_map

def recover_special_blocks(sentences: list[str], placeholder_map: dict[str, str]) -> list[str]:
    """æ¢å¤è¢«ä¿æŠ¤çš„ç‰¹æ®Šå—"""
    recovered_sentences = []
    for sentence in sentences:
        for placeholder, original_block in placeholder_map.items():
            sentence = sentence.replace(placeholder, original_block)
        recovered_sentences.append(sentence)
    return recovered_sentences


def protect_quoted_content(text: str) -> tuple[str, dict[str, str]]:
    """è¯†åˆ«å¹¶ä¿æŠ¤å¥å­ä¸­è¢«å¼•å·åŒ…è£¹çš„å†…å®¹ï¼Œè¿”å›å¤„ç†åçš„æ–‡æœ¬å’Œæ˜ å°„"""
    placeholder_map = {}
    # åŒ¹é…ä¸­è‹±æ–‡å•åŒå¼•å·ï¼Œä½¿ç”¨éè´ªå©ªæ¨¡å¼
    quote_pattern = re.compile(r'(".*?")|(\'.*?\')|(â€œ.*?â€)|(â€˜.*?â€™)')

    matches = quote_pattern.finditer(text)

    # ä¸ºäº†é¿å…æ›¿æ¢æ—¶ç´¢å¼•é”™ä¹±ï¼Œæˆ‘ä»¬ä»åå¾€å‰æ›¿æ¢
    # finditer æ‰¾åˆ°çš„æ˜¯ match å¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸º list æ¥åè½¬
    match_list = list(matches)

    for idx, match in enumerate(reversed(match_list)):
        original_quoted_text = match.group(0)
        placeholder = f"__QUOTE_{len(match_list) - 1 - idx}__"

        # ç›´æ¥åœ¨åŸå§‹æ–‡æœ¬ä¸Šæ“ä½œï¼Œæ›¿æ¢ match å¯¹è±¡çš„ span
        start, end = match.span()
        text = text[:start] + placeholder + text[end:]

        placeholder_map[placeholder] = original_quoted_text

    return text, placeholder_map


def recover_quoted_content(sentences: list[str], placeholder_map: dict[str, str]) -> list[str]:
    """æ¢å¤è¢«ä¿æŠ¤çš„å¼•å·å†…å®¹"""
    recovered_sentences = []
    for sentence in sentences:
        for placeholder, original_block in placeholder_map.items():
            sentence = sentence.replace(placeholder, original_block)
        recovered_sentences.append(sentence)
    return recovered_sentences

def process_llm_response(text: str, enable_splitter: bool = True, enable_chinese_typo: bool = True) -> list[str]:
    assert global_config is not None
    if not global_config.response_post_process.enable_response_post_process:
        return [text]

    # --- ä¸‰å±‚é˜²æŠ¤ç³»ç»Ÿ ---
    # --- ä¸‰å±‚é˜²æŠ¤ç³»ç»Ÿ ---
    # ç¬¬ä¸€å±‚ï¼šä¿æŠ¤é¢œæ–‡å­—
    protected_text, kaomoji_mapping = protect_kaomoji(text) if global_config.response_splitter.enable_kaomoji_protection else (text, {})

    # ç¬¬äºŒå±‚ï¼šä¿æŠ¤å¼•å·å†…å®¹
    protected_text, quote_mapping = protect_quoted_content(protected_text)

    # ç¬¬ä¸‰å±‚ï¼šä¿æŠ¤æ•°å­¦å…¬å¼å’Œä»£ç å—
    protected_text, special_blocks_mapping = protect_special_blocks(protected_text)

    # æå–è¢« () æˆ– [] æˆ– ï¼ˆï¼‰åŒ…è£¹ä¸”åŒ…å«ä¸­æ–‡çš„å†…å®¹
    pattern = re.compile(r"[(\[ï¼ˆ](?=.*[ä¸€-é¿¿]).*?[)\]ï¼‰]")
    _extracted_contents = pattern.findall(protected_text)
    cleaned_text = pattern.sub("", protected_text)

    if cleaned_text.strip() == "":
        # å¦‚æœæ¸…ç†ååªå‰©ä¸‹ç‰¹æ®Šå—ï¼Œç›´æ¥æ¢å¤å¹¶è¿”å›
        if special_blocks_mapping:
             recovered = recover_special_blocks([protected_text], special_blocks_mapping)
             return recover_kaomoji(recovered, kaomoji_mapping)
        return ["å‘ƒå‘ƒ"]

    logger.debug(f"{text}å»é™¤æ‹¬å·å¤„ç†åçš„æ–‡æœ¬: {cleaned_text}")

    # å¯¹æ¸…ç†åçš„æ–‡æœ¬è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
    max_sentence_num = global_config.response_splitter.max_sentence_num

    # --- ç§»é™¤æ€»é•¿åº¦æ£€æŸ¥ ---
    # åŸæœ‰çš„æ€»é•¿åº¦æ£€æŸ¥ä¼šå¯¼è‡´é•¿å›å¤è¢«ç›´æ¥ä¸¢å¼ƒï¼Œç°å·²ç§»é™¤ï¼Œç”±åç»­çš„æ™ºèƒ½åˆå¹¶é€»è¾‘å¤„ç†ã€‚
    # max_length = global_config.response_splitter.max_length * 2
    # if get_western_ratio(cleaned_text) < 0.1 and len(cleaned_text) > max_length:
    #     logger.warning(f"å›å¤è¿‡é•¿ ({len(cleaned_text)} å­—ç¬¦)ï¼Œè¿”å›é»˜è®¤å›å¤")
    #     return ["æ‡’å¾—è¯´"]

    # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å•ä¾‹å·¥å‚å‡½æ•°ï¼Œé¿å…é‡å¤åˆ›å»ºæ‹¼éŸ³å­—å…¸
    typo_generator = get_typo_generator(
        error_rate=global_config.chinese_typo.error_rate,
        min_freq=global_config.chinese_typo.min_freq,
        tone_error_rate=global_config.chinese_typo.tone_error_rate,
        word_replace_rate=global_config.chinese_typo.word_replace_rate,
    )

    if global_config.response_splitter.enable and enable_splitter:
        logger.info(f"å›å¤åˆ†å‰²å™¨å·²å¯ç”¨ï¼Œæ¨¡å¼: {global_config.response_splitter.split_mode}ã€‚")

        if "[SPLIT]" in cleaned_text:
            logger.debug("æ£€æµ‹åˆ° [SPLIT] æ ‡è®°ï¼Œä½¿ç”¨ LLM è‡ªå®šä¹‰åˆ†å‰²ã€‚")
            split_sentences_raw = cleaned_text.split("[SPLIT]")
            split_sentences = [s.strip() for s in split_sentences_raw if s.strip()]
        else:
            logger.debug("ä½¿ç”¨åŸºäºæ ‡ç‚¹çš„ä¼ ç»Ÿæ¨¡å¼è¿›è¡Œåˆ†å‰²ã€‚")
            split_sentences = split_into_sentences_w_remove_punctuation(cleaned_text)
    else:
        logger.debug("å›å¤åˆ†å‰²å™¨å·²ç¦ç”¨ã€‚")
        split_sentences = [cleaned_text]

    sentences = []
    for sentence in split_sentences:
        # æ¸…é™¤å¼€å¤´å¯èƒ½å­˜åœ¨çš„ç©ºè¡Œ
        sentence = sentence.lstrip("\n").rstrip()
        if global_config.chinese_typo.enable and enable_chinese_typo:
            typoed_text, typo_corrections = typo_generator.create_typo_sentence(sentence)
            sentences.append(typoed_text)
            if typo_corrections:
                sentences.append(typo_corrections)
        else:
            sentences.append(sentence)

    # å¦‚æœåˆ†å‰²åçš„å¥å­æ•°é‡è¶…è¿‡ä¸Šé™ï¼Œåˆ™å¯åŠ¨æ™ºèƒ½åˆå¹¶é€»è¾‘
    if len(sentences) > max_sentence_num:
        logger.info(f"åˆ†å‰²åæ¶ˆæ¯æ•°é‡ ({len(sentences)}) è¶…è¿‡ä¸Šé™ ({max_sentence_num})ï¼Œå¯åŠ¨æ™ºèƒ½åˆå¹¶...")

        # è®¡ç®—éœ€è¦åˆå¹¶çš„æ¬¡æ•°
        num_to_merge = len(sentences) - max_sentence_num

        for _ in range(num_to_merge):
            # å¦‚æœå¥å­æ•°é‡å·²ç»è¾¾æ ‡ï¼Œæå‰é€€å‡º
            if len(sentences) <= max_sentence_num:
                break

            # å¯»æ‰¾æœ€çŸ­çš„ç›¸é‚»å¥å­å¯¹
            min_len = float("inf")
            merge_idx = -1
            for i in range(len(sentences) - 1):
                combined_len = len(sentences[i]) + len(sentences[i+1])
                if combined_len < min_len:
                    min_len = combined_len
                    merge_idx = i

            # å¦‚æœæ‰¾åˆ°äº†å¯ä»¥åˆå¹¶çš„å¯¹ï¼Œåˆ™æ‰§è¡Œåˆå¹¶
            if merge_idx != -1:
                # å°†åä¸€ä¸ªå¥å­åˆå¹¶åˆ°å‰ä¸€ä¸ªå¥å­
                # æˆ‘ä»¬åœ¨åˆå¹¶æ—¶ä¿ç•™åŸå§‹æ ‡ç‚¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œæˆ–è€…æ·»åŠ ä¸€ä¸ªé€—å·æ¥ç¡®ä¿å¯è¯»æ€§
                merged_sentence = sentences[merge_idx] + "ï¼Œ" + sentences[merge_idx + 1]
                sentences[merge_idx] = merged_sentence
                # åˆ é™¤åä¸€ä¸ªå¥å­
                del sentences[merge_idx + 1]

        logger.info(f"æ™ºèƒ½åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆæ¶ˆæ¯æ•°é‡: {len(sentences)}")

    # if extracted_contents:
    #     for content in extracted_contents:
    #         sentences.append(content)

    # --- æ¢å¤æ‰€æœ‰è¢«ä¿æŠ¤çš„å†…å®¹ ---
    sentences = recover_special_blocks(sentences, special_blocks_mapping)
    sentences = recover_quoted_content(sentences, quote_mapping)
    if global_config.response_splitter.enable_kaomoji_protection:
        sentences = recover_kaomoji(sentences, kaomoji_mapping)

    return sentences


def calculate_typing_time(
    input_string: str,
    thinking_start_time: float,
    chinese_time: float = 0.2,
    english_time: float = 0.1,
    is_emoji: bool = False,
) -> float:
    """
    è®¡ç®—è¾“å…¥å­—ç¬¦ä¸²æ‰€éœ€çš„æ—¶é—´ï¼Œä¸­æ–‡å’Œè‹±æ–‡å­—ç¬¦æœ‰ä¸åŒçš„è¾“å…¥æ—¶é—´
        input_string (str): è¾“å…¥çš„å­—ç¬¦ä¸²
        chinese_time (float): ä¸­æ–‡å­—ç¬¦çš„è¾“å…¥æ—¶é—´ï¼Œé»˜è®¤ä¸º0.2ç§’
        english_time (float): è‹±æ–‡å­—ç¬¦çš„è¾“å…¥æ—¶é—´ï¼Œé»˜è®¤ä¸º0.1ç§’
        is_emoji (bool): æ˜¯å¦ä¸ºemojiï¼Œé»˜è®¤ä¸ºFalse

    ç‰¹æ®Šæƒ…å†µï¼š
    - å¦‚æœåªæœ‰ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œå°†ä½¿ç”¨3å€çš„ä¸­æ–‡è¾“å…¥æ—¶é—´
    - åœ¨æ‰€æœ‰è¾“å…¥ç»“æŸåï¼Œé¢å¤–åŠ ä¸Šå›è½¦æ—¶é—´0.3ç§’
    - å¦‚æœis_emojiä¸ºTrueï¼Œå°†ä½¿ç”¨å›ºå®š1ç§’çš„è¾“å…¥æ—¶é—´
    """
    # # å°†0-1çš„å”¤é†’åº¦æ˜ å°„åˆ°-1åˆ°1
    # mood_arousal = mood_manager.current_mood.arousal
    # # æ˜ å°„åˆ°0.5åˆ°2å€çš„é€Ÿåº¦ç³»æ•°
    # typing_speed_multiplier = 1.5**mood_arousal  # å”¤é†’åº¦ä¸º1æ—¶é€Ÿåº¦ç¿»å€,ä¸º-1æ—¶é€Ÿåº¦å‡åŠ
    # chinese_time *= 1 / typing_speed_multiplier
    # english_time *= 1 / typing_speed_multiplier
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°
    chinese_chars = sum("\u4e00" <= char <= "\u9fff" for char in input_string)

    # å¦‚æœåªæœ‰ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œä½¿ç”¨3å€æ—¶é—´
    if chinese_chars == 1 and len(input_string.strip()) == 1:
        return chinese_time * 3 + 0.3  # åŠ ä¸Šå›è½¦æ—¶é—´

    # æ­£å¸¸è®¡ç®—æ‰€æœ‰å­—ç¬¦çš„è¾“å…¥æ—¶é—´
    total_time = 0.0
    for char in input_string:
        total_time += chinese_time if "\u4e00" <= char <= "\u9fff" else english_time
    if is_emoji:
        total_time = 1

    if time.time() - thinking_start_time > 10:
        total_time = 1

    # print(f"thinking_start_time:{thinking_start_time}")
    # print(f"nowtime:{time.time()}")
    # print(f"nowtime - thinking_start_time:{time.time() - thinking_start_time}")
    # print(f"{total_time}")

    return total_time  # åŠ ä¸Šå›è½¦æ—¶é—´


def cosine_similarity(v1, v2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    return 0 if norm1 == 0 or norm2 == 0 else dot_product / (norm1 * norm2)


def text_to_vector(text):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯é¢‘å‘é‡"""
    # åˆ†è¯
    words = rjieba.lcut(text) # type: ignore
    return Counter(words)


def find_similar_topics_simple(text: str, topics: list, top_k: int = 5) -> list:
    """ä½¿ç”¨ç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    # å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºè¯é¢‘å‘é‡
    text_vector = text_to_vector(text)

    # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„ç›¸ä¼¼åº¦
    similarities = []
    for topic in topics:
        topic_vector = text_to_vector(topic)
        # è·å–æ‰€æœ‰å”¯ä¸€è¯
        all_words = set(text_vector.keys()) | set(topic_vector.keys())
        # æ„å»ºå‘é‡
        v1 = [text_vector.get(word, 0) for word in all_words]
        v2 = [topic_vector.get(word, 0) for word in all_words]
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = cosine_similarity(v1, v2)
        similarities.append((topic, similarity))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åºå¹¶è¿”å›å‰kä¸ª
    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]


def truncate_message(message: str, max_length=20) -> str:
    """æˆªæ–­æ¶ˆæ¯ï¼Œä½¿å…¶ä¸è¶…è¿‡æŒ‡å®šé•¿åº¦"""
    if message is None:
        return ""
    return f"{message[:max_length]}..." if len(message) > max_length else message


def protect_kaomoji(sentence):
    """ "
    è¯†åˆ«å¹¶ä¿æŠ¤å¥å­ä¸­çš„é¢œæ–‡å­—ï¼ˆå«æ‹¬å·ä¸æ— æ‹¬å·ï¼‰ï¼Œå°†å…¶æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œ
    å¹¶è¿”å›æ›¿æ¢åçš„å¥å­å’Œå ä½ç¬¦åˆ°é¢œæ–‡å­—çš„æ˜ å°„è¡¨ã€‚
    Args:
        sentence (str): è¾“å…¥çš„åŸå§‹å¥å­
    Returns:
        tuple: (å¤„ç†åçš„å¥å­, {å ä½ç¬¦: é¢œæ–‡å­—})
    """
    kaomoji_pattern = re.compile(
        r"("
        r"[(\[ï¼ˆã€]"  # å·¦æ‹¬å·
        r"[^()\[\]ï¼ˆï¼‰ã€ã€‘]*?"  # éæ‹¬å·å­—ç¬¦ï¼ˆæƒ°æ€§åŒ¹é…ï¼‰
        r"[^ä¸€-é¾¥a-zA-Z0-9\s]"  # éä¸­æ–‡ã€éè‹±æ–‡ã€éæ•°å­—ã€éç©ºæ ¼å­—ç¬¦ï¼ˆå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªï¼‰
        r"[^()\[\]ï¼ˆï¼‰ã€ã€‘]*?"  # éæ‹¬å·å­—ç¬¦ï¼ˆæƒ°æ€§åŒ¹é…ï¼‰
        r"[)\]ï¼‰ã€‘"  # å³æ‹¬å·
        r"]"
        r")"
        r"|"
        r"([â–¼â–½ãƒ»á´¥Ï‰ï½¥ï¹^><â‰§â‰¦ï¿£ï½€Â´âˆ€ãƒ®Ğ”Ğ´ï¸¿ï¹€ã¸ï½¡ï¾Ÿâ•¥â•¯â•°ï¸¶ï¸¹â€¢â„]{2,15})"
    )

    kaomoji_matches = kaomoji_pattern.findall(sentence)
    placeholder_to_kaomoji = {}

    for idx, match in enumerate(kaomoji_matches):
        kaomoji = match[0] or match[1]
        placeholder = f"__KAOMOJI_{idx}__"
        sentence = sentence.replace(kaomoji, placeholder, 1)
        placeholder_to_kaomoji[placeholder] = kaomoji

    return sentence, placeholder_to_kaomoji


def recover_kaomoji(sentences, placeholder_to_kaomoji):
    """
    æ ¹æ®æ˜ å°„è¡¨æ¢å¤å¥å­ä¸­çš„é¢œæ–‡å­—ã€‚
    Args:
        sentences (list): å«æœ‰å ä½ç¬¦çš„å¥å­åˆ—è¡¨
        placeholder_to_kaomoji (dict): å ä½ç¬¦åˆ°é¢œæ–‡å­—çš„æ˜ å°„è¡¨
    Returns:
        list: æ¢å¤é¢œæ–‡å­—åçš„å¥å­åˆ—è¡¨
    """
    recovered_sentences = []
    for sentence in sentences:
        for placeholder, kaomoji in placeholder_to_kaomoji.items():
            sentence = sentence.replace(placeholder, kaomoji)
        recovered_sentences.append(sentence)
    return recovered_sentences


def get_western_ratio(paragraph):
    """è®¡ç®—æ®µè½ä¸­å­—æ¯æ•°å­—å­—ç¬¦çš„è¥¿æ–‡æ¯”ä¾‹
    åŸç†ï¼šæ£€æŸ¥æ®µè½ä¸­å­—æ¯æ•°å­—å­—ç¬¦çš„è¥¿æ–‡æ¯”ä¾‹
    é€šè¿‡is_english_letterå‡½æ•°åˆ¤æ–­æ¯ä¸ªå­—ç¬¦æ˜¯å¦ä¸ºè¥¿æ–‡
    åªæ£€æŸ¥å­—æ¯æ•°å­—å­—ç¬¦ï¼Œå¿½ç•¥æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ç­‰éå­—æ¯æ•°å­—å­—ç¬¦

    Args:
        paragraph: è¦æ£€æŸ¥çš„æ–‡æœ¬æ®µè½

    Returns:
        float: è¥¿æ–‡å­—ç¬¦æ¯”ä¾‹(0.0-1.0)ï¼Œå¦‚æœæ²¡æœ‰å­—æ¯æ•°å­—å­—ç¬¦åˆ™è¿”å›0.0
    """
    alnum_chars = [char for char in paragraph if char.isalnum()]
    if not alnum_chars:
        return 0.0

    western_count = sum(bool(is_english_letter(char)) for char in alnum_chars)
    return western_count / len(alnum_chars)


async def count_messages_between(start_time: float, end_time: float, stream_id: str) -> tuple[int, int]:
    """è®¡ç®—ä¸¤ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çš„æ¶ˆæ¯æ•°é‡å’Œæ–‡æœ¬æ€»é•¿åº¦

    Args:
        start_time (float): èµ·å§‹æ—¶é—´æˆ³ (ä¸åŒ…å«)
        end_time (float): ç»“æŸæ—¶é—´æˆ³ (åŒ…å«)
        stream_id (str): èŠå¤©æµID

    Returns:
        tuple[int, int]: (æ¶ˆæ¯æ•°é‡, æ–‡æœ¬æ€»é•¿åº¦)
    """
    count = 0
    total_length = 0

    # å‚æ•°æ ¡éªŒ (å¯é€‰ä½†æ¨è)
    if start_time >= end_time:
        # logger.debug(f"å¼€å§‹æ—¶é—´ {start_time} å¤§äºæˆ–ç­‰äºç»“æŸæ—¶é—´ {end_time}ï¼Œè¿”å› 0, 0")
        return 0, 0
    if not stream_id:
        logger.error("stream_id ä¸èƒ½ä¸ºç©º")
        return 0, 0

    # ä½¿ç”¨message_repositoryä¸­çš„count_messageså’Œfind_messageså‡½æ•°

    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    filter_query = {"chat_id": stream_id, "time": {"$gt": start_time, "$lte": end_time}}

    try:
        # å…ˆè·å–æ¶ˆæ¯æ•°é‡
        count = await count_messages(filter_query)

        # è·å–æ¶ˆæ¯å†…å®¹è®¡ç®—æ€»é•¿åº¦
        messages = await find_messages(message_filter=filter_query)
        total_length = sum(len(msg.get("processed_plain_text", "")) for msg in messages)

        return count, total_length

    except Exception as e:
        logger.error(f"è®¡ç®—æ¶ˆæ¯æ•°é‡æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return 0, 0


def translate_timestamp_to_human_readable(timestamp: float, mode: str = "normal") -> str:
    # sourcery skip: merge-comparisons, merge-duplicate-blocks, switch
    """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ—¶é—´æ ¼å¼

    Args:
        timestamp: æ—¶é—´æˆ³
        mode: è½¬æ¢æ¨¡å¼ï¼Œ"normal"ä¸ºæ ‡å‡†æ ¼å¼ï¼Œ"relative"ä¸ºç›¸å¯¹æ—¶é—´æ ¼å¼

    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if mode == "normal":
        return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(timestamp))
    elif mode == "normal_no_YMD":
        return time.strftime("%H:%M:%S", time.localtime(timestamp))
    elif mode == "relative":
        now = time.time()
        diff = now - timestamp

        if diff < 20:
            return "åˆšåˆš"
        elif diff < 60:
            return f"{int(diff)}ç§’å‰"
        elif diff < 3600:
            return f"{int(diff / 60)}åˆ†é’Ÿå‰"
        elif diff < 86400:
            return f"{int(diff / 3600)}å°æ—¶å‰"
        elif diff < 86400 * 2:
            return f"{int(diff / 86400)}å¤©å‰"
        else:
            return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(timestamp)) + ":"
    else:  # mode = "lite" or unknown
        # åªè¿”å›æ—¶åˆ†ç§’æ ¼å¼
        return time.strftime("%H:%M:%S", time.localtime(timestamp))


async def get_chat_type_and_target_info(chat_id: str) -> tuple[bool, dict | None]:
    """
    è·å–èŠå¤©ç±»å‹ï¼ˆæ˜¯å¦ç¾¤èŠï¼‰å’Œç§èŠå¯¹è±¡ä¿¡æ¯ã€‚

    Args:
        chat_id: èŠå¤©æµID

    Returns:
        Tuple[bool, Optional[Dict]]:
            - bool: æ˜¯å¦ä¸ºç¾¤èŠ (True æ˜¯ç¾¤èŠ, False æ˜¯ç§èŠæˆ–æœªçŸ¥)
            - Optional[Dict]: å¦‚æœæ˜¯ç§èŠï¼ŒåŒ…å«å¯¹æ–¹ä¿¡æ¯çš„å­—å…¸ï¼›å¦åˆ™ä¸º Noneã€‚
            å­—å…¸åŒ…å«: platform, user_id, user_nickname, person_id, person_name
    """
    is_group_chat = False  # Default to private/unknown
    chat_target_info = None

    try:
        from src.chat.message_receive.chat_stream import get_chat_manager
        if chat_stream := await get_chat_manager().get_stream(chat_id):
            if chat_stream.group_info:
                is_group_chat = True
                chat_target_info = None  # Explicitly None for group chat
            elif chat_stream.user_info:  # It's a private chat
                is_group_chat = False
                user_info = chat_stream.user_info
                platform: str = chat_stream.platform
                user_id: str = user_info.user_id  # type: ignore

                # Initialize target_info with basic info
                target_info = {
                    "platform": platform,
                    "user_id": user_id,
                    "user_nickname": user_info.user_nickname,
                    "person_id": None,
                    "person_name": None,
                }

                # Try to fetch person info
                try:
                    # Assume get_person_id is sync (as per original code), keep using to_thread
                    person_id = PersonInfoManager.get_person_id(platform, user_id)
                    person_name = None
                    if person_id:
                        person_info_manager = get_person_info_manager()
                        try:
                            # å¦‚æœæ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥ asyncio.run
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                # å¦‚æœäº‹ä»¶å¾ªç¯åœ¨è¿è¡Œï¼Œä»å…¶ä»–çº¿ç¨‹æäº¤å¹¶ç­‰å¾…ç»“æœ
                                try:
                                    fut = asyncio.run_coroutine_threadsafe(
                                        person_info_manager.get_value(person_id, "person_name"), loop
                                    )
                                    person_name = fut.result(timeout=2)
                                except Exception as e:
                                    # æ— æ³•åœ¨è¿è¡Œå¾ªç¯ä¸Šå®‰å…¨ç­‰å¾…ï¼Œé€€å›ä¸º None
                                    logger.debug(f"æ— æ³•é€šè¿‡è¿è¡Œçš„äº‹ä»¶å¾ªç¯è·å– person_name: {e}")
                                    person_name = None
                            else:
                                person_name = asyncio.run(person_info_manager.get_value(person_id, "person_name"))
                        except RuntimeError:
                            # get_event_loop åœ¨æŸäº›ä¸Šä¸‹æ–‡å¯èƒ½æŠ›å‡º RuntimeErrorï¼Œé€€å›åˆ° asyncio.run
                            try:
                                person_name = asyncio.run(person_info_manager.get_value(person_id, "person_name"))
                            except Exception as e:
                                logger.debug(f"è·å– person_name å¤±è´¥: {e}")
                                person_name = None

                    target_info["person_id"] = person_id
                    target_info["person_name"] = person_name
                except Exception as person_e:
                    logger.warning(
                        f"è·å– person_id æˆ– person_name æ—¶å‡ºé”™ for {platform}:{user_id} in utils: {person_e}"
                    )

                chat_target_info = target_info
        else:
            logger.warning(f"æ— æ³•è·å– chat_stream for {chat_id} in utils")
    except Exception as e:
        logger.error(f"è·å–èŠå¤©ç±»å‹å’Œç›®æ ‡ä¿¡æ¯æ—¶å‡ºé”™ for {chat_id}: {e}")
        # Keep defaults on error

    return is_group_chat, chat_target_info


def assign_message_ids(messages: list[Any]) -> list[dict[str, Any]]:
    """
    ä¸ºæ¶ˆæ¯åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¶ˆæ¯åˆ†é…å”¯ä¸€çš„ç®€çŸ­éšæœºID

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        åŒ…å« {'id': str, 'message': any} æ ¼å¼çš„å­—å…¸åˆ—è¡¨
    """
    result = []
    for i, message in enumerate(messages):
        # ä½¿ç”¨ç®€å•çš„ç´¢å¼•ä½œä¸ºID
        message_id = f"m{i + 1}"
        result.append({"id": message_id, "message": message})

    return result


def assign_message_ids_flexible(
    messages: list, prefix: str = "msg", id_length: int = 6, use_timestamp: bool = False
) -> list:
    """
    ä¸ºæ¶ˆæ¯åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¶ˆæ¯åˆ†é…å”¯ä¸€çš„ç®€çŸ­éšæœºIDï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        prefix: IDå‰ç¼€ï¼Œé»˜è®¤ä¸º"msg"
        id_length: IDçš„æ€»é•¿åº¦ï¼ˆä¸åŒ…æ‹¬å‰ç¼€ï¼‰ï¼Œé»˜è®¤ä¸º6
        use_timestamp: æ˜¯å¦åœ¨IDä¸­åŒ…å«æ—¶é—´æˆ³ï¼Œé»˜è®¤ä¸ºFalse

    Returns:
        åŒ…å« {'id': str, 'message': any} æ ¼å¼çš„å­—å…¸åˆ—è¡¨
    """
    result = []
    used_ids = set()

    for i, message in enumerate(messages):
        # ç”Ÿæˆå”¯ä¸€çš„ID
        while True:
            if use_timestamp:
                # ä½¿ç”¨æ—¶é—´æˆ³çš„åå‡ ä½ + éšæœºå­—ç¬¦
                timestamp_suffix = str(int(time.time() * 1000))[-3:]
                remaining_length = id_length - 3
                random_chars = "".join(random.choices(string.ascii_lowercase + string.digits, k=remaining_length))
                message_id = f"{prefix}{timestamp_suffix}{random_chars}"
            else:
                # ä½¿ç”¨ç´¢å¼• + éšæœºå­—ç¬¦
                index_str = str(i + 1)
                remaining_length = max(1, id_length - len(index_str))
                random_chars = "".join(random.choices(string.ascii_lowercase + string.digits, k=remaining_length))
                message_id = f"{prefix}{index_str}{random_chars}"

            if message_id not in used_ids:
                used_ids.add(message_id)
                break

        result.append({"id": message_id, "message": message})

    return result


# ä½¿ç”¨ç¤ºä¾‹:
# messages = ["Hello", "World", "Test message"]
#
# # åŸºç¡€ç‰ˆæœ¬
# result1 = assign_message_ids(messages)
# # ç»“æœ: [{'id': 'm1123', 'message': 'Hello'}, {'id': 'm2456', 'message': 'World'}, {'id': 'm3789', 'message': 'Test message'}]
#
# # å¢å¼ºç‰ˆæœ¬ - è‡ªå®šä¹‰å‰ç¼€å’Œé•¿åº¦
# result2 = assign_message_ids_flexible(messages, prefix="chat", id_length=8)
# # ç»“æœ: [{'id': 'chat1abc2', 'message': 'Hello'}, {'id': 'chat2def3', 'message': 'World'}, {'id': 'chat3ghi4', 'message': 'Test message'}]
#
# # å¢å¼ºç‰ˆæœ¬ - ä½¿ç”¨æ—¶é—´æˆ³
# result3 = assign_message_ids_flexible(messages, prefix="ts", use_timestamp=True)
# # ç»“æœ: [{'id': 'ts123a1b', 'message': 'Hello'}, {'id': 'ts123c2d', 'message': 'World'}, {'id': 'ts123e3f', 'message': 'Test message'}]


def filter_system_format_content(content: str | None) -> str:
    """
    è¿‡æ»¤ç³»ç»Ÿæ ¼å¼åŒ–å†…å®¹ï¼Œç§»é™¤å›å¤ã€@ã€å›¾ç‰‡ã€è¡¨æƒ…åŒ…ç­‰ç³»ç»Ÿç”Ÿæˆçš„æ ¼å¼æ–‡æœ¬

    æ­¤æ–¹æ³•è¿‡æ»¤ä»¥ä¸‹ç±»å‹çš„ç³»ç»Ÿæ ¼å¼åŒ–å†…å®¹ï¼š
    1. å›å¤æ ¼å¼ï¼š[å›å¤xxx]ï¼Œè¯´ï¼šxxx (åŒ…æ‹¬æ·±åº¦åµŒå¥—)
    2. è¡¨æƒ…åŒ…æ ¼å¼ï¼š[è¡¨æƒ…åŒ…ï¼šxxx]
    3. å›¾ç‰‡æ ¼å¼ï¼š[å›¾ç‰‡:xxx]
    4. @æ ¼å¼ï¼š@<xxx>
    5. é”™è¯¯æ ¼å¼ï¼š[è¡¨æƒ…åŒ…(...)]ã€[å›¾ç‰‡(...)]

    Args:
        content: åŸå§‹å†…å®¹

    Returns:
        è¿‡æ»¤åçš„çº¯æ–‡æœ¬å†…å®¹
    """
    if not content:
        return ""

    original_content = content
    cleaned_content = content.strip()

    # æ ¸å¿ƒé€»è¾‘ï¼šä¼˜å…ˆå¤„ç†æœ€å¤æ‚çš„[å›å¤...]æ ¼å¼ï¼Œç‰¹åˆ«æ˜¯åµŒå¥—æ ¼å¼ã€‚
    # è¿™ç§æ–¹æ³•æœ€ç¨³å¥ï¼šå¦‚æœä»¥[å›å¤å¼€å¤´ï¼Œå°±æ‰¾åˆ°æœ€åä¸€ä¸ª]ï¼Œç„¶ååˆ‡æ‰ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ã€‚
    if cleaned_content.startswith("[å›å¤"):
        last_bracket_index = cleaned_content.rfind("]")
        if last_bracket_index != -1:
            cleaned_content = cleaned_content[last_bracket_index + 1 :].strip()
            # ä¸“é—¨æ¸…ç† "ï¼Œè¯´ï¼š" æˆ– "è¯´ï¼š"
            cleaned_content = re.sub(r"^(ï¼Œ|,)è¯´ï¼š", "", cleaned_content).strip()

    # åœ¨å¤„ç†å®Œå›å¤æ ¼å¼åï¼Œå†æ¸…ç†å…¶ä»–ç®€å•çš„æ ¼å¼
    # æ–°å¢ï¼šç§»é™¤æ‰€æœ‰æ®‹ä½™çš„ [...] æ ¼å¼ï¼Œä¾‹å¦‚ [at=...] ç­‰
    cleaned_content = re.sub(r"\[.*?\]", "", cleaned_content)

    # ç§»é™¤@æ ¼å¼ï¼š@<xxx>
    cleaned_content = re.sub(r"@<[^>]*>", "", cleaned_content)

    # è®°å½•è¿‡æ»¤æ“ä½œ
    if cleaned_content != original_content.strip():
        logger.info(
            f"[ç³»ç»Ÿæ ¼å¼è¿‡æ»¤å™¨] æ£€æµ‹åˆ°å¹¶æ¸…ç†äº†ç³»ç»Ÿæ ¼å¼åŒ–æ–‡æœ¬ã€‚"
            f"åŸå§‹å†…å®¹: '{original_content}', "
            f"æ¸…ç†å: '{cleaned_content}'"
        )

    return cleaned_content
