#!/usr/bin/env python3
"""
è§†é¢‘åˆ†æå™¨æ¨¡å— - æ—§ç‰ˆæœ¬å…¼å®¹æ¨¡å—
æ”¯æŒå¤šç§åˆ†ææ¨¡å¼ï¼šæ‰¹å¤„ç†ã€é€å¸§ã€è‡ªåŠ¨é€‰æ‹©
åŒ…å«PythonåŸç”Ÿçš„æŠ½å¸§åŠŸèƒ½ï¼Œä½œä¸ºRustæ¨¡å—çš„é™çº§æ–¹æ¡ˆ
"""

import asyncio
import base64
import io
import os
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any

import cv2
import numpy as np
from PIL import Image

from src.common.logger import get_logger
from src.config.config import global_config, model_config
from src.llm_models.utils_model import LLMRequest

logger = get_logger("utils_video_legacy")


def _extract_frames_worker(
    video_path: str,
    max_frames: int,
    frame_quality: int,
    max_image_size: int,
    frame_extraction_mode: str,
    frame_interval_seconds: float | None,
) -> list[tuple[str, float]] | list[tuple[str, str]]:
    """çº¿ç¨‹æ± ä¸­æå–è§†é¢‘å¸§çš„å·¥ä½œå‡½æ•°"""
    frames: list[tuple[str, float]] = []
    try:
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        if frame_extraction_mode == "time_interval":
            # æ–°æ¨¡å¼ï¼šæŒ‰æ—¶é—´é—´éš”æŠ½å¸§
            time_interval = frame_interval_seconds or 2.0
            next_frame_time = 0.0
            extracted_count = 0  # åˆå§‹åŒ–æå–å¸§è®¡æ•°å™¨

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0

                if current_time >= next_frame_time:
                    # è½¬æ¢ä¸ºPILå›¾åƒå¹¶å‹ç¼©
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)

                    # è°ƒæ•´å›¾åƒå¤§å°
                    if max(pil_image.size) > max_image_size:
                        ratio = max_image_size / max(pil_image.size)
                        new_size = (int(pil_image.size[0] * ratio), int(pil_image.size[1] * ratio))
                        pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)

                    # è½¬æ¢ä¸ºbase64
                    buffer = io.BytesIO()
                    pil_image.save(buffer, format="JPEG", quality=frame_quality)
                    frame_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

                    frames.append((frame_base64, current_time))
                    extracted_count += 1

                    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨loggerï¼Œå› ä¸ºåœ¨çº¿ç¨‹æ± ä¸­
                    # logger.debug(f"æå–ç¬¬{extracted_count}å¸§ (æ—¶é—´: {current_time:.2f}s)")

                    next_frame_time += time_interval
        else:
            # ä½¿ç”¨numpyä¼˜åŒ–å¸§é—´éš”è®¡ç®—
            if duration > 0:
                frame_interval = max(1, int(duration / max_frames * fps))
            else:
                frame_interval = 30  # é»˜è®¤é—´éš”

            # ä½¿ç”¨numpyè®¡ç®—ç›®æ ‡å¸§ä½ç½®
            target_frames = np.arange(0, min(max_frames, total_frames // frame_interval + 1)) * frame_interval
            target_frames = target_frames[target_frames < total_frames].astype(int)

            for target_frame in target_frames:
                # è·³è½¬åˆ°ç›®æ ‡å¸§
                cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = cap.read()
                if not ret:
                    continue

                # ä½¿ç”¨numpyä¼˜åŒ–å›¾åƒå¤„ç†
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä½¿ç”¨numpyè¿›è¡Œå°ºå¯¸è®¡ç®—
                height, width = frame_rgb.shape[:2]
                max_dim = max(height, width)

                if max_dim > max_image_size:
                    # ä½¿ç”¨numpyè®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                    ratio = max_image_size / max_dim
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)

                    # ä½¿ç”¨opencvè¿›è¡Œé«˜æ•ˆç¼©æ”¾
                    frame_resized = cv2.resize(frame_rgb, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
                    pil_image = Image.fromarray(frame_resized)
                else:
                    pil_image = Image.fromarray(frame_rgb)

                # è½¬æ¢ä¸ºbase64
                buffer = io.BytesIO()
                pil_image.save(buffer, format="JPEG", quality=frame_quality)
                frame_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

                # è®¡ç®—æ—¶é—´æˆ³
                timestamp = target_frame / fps if fps > 0 else 0
                frames.append((frame_base64, timestamp))

        cap.release()
        return frames

    except Exception as e:
        # è¿”å›é”™è¯¯ä¿¡æ¯
        return [("ERROR", str(e))]


class LegacyVideoAnalyzer:
    """æ—§ç‰ˆæœ¬å…¼å®¹çš„è§†é¢‘åˆ†æå™¨ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–è§†é¢‘åˆ†æå™¨"""
        assert global_config is not None
        assert model_config is not None
        # ä½¿ç”¨ä¸“ç”¨çš„è§†é¢‘åˆ†æé…ç½®
        try:
            self.video_llm = LLMRequest(
                model_set=model_config.model_task_config.video_analysis, request_type="video_analysis"
            )
            logger.info("âœ… ä½¿ç”¨video_analysisæ¨¡å‹é…ç½®")
        except (AttributeError, KeyError) as e:
            # å¦‚æœvideo_analysisä¸å­˜åœ¨ï¼Œä½¿ç”¨vlmé…ç½®
            self.video_llm = LLMRequest(model_set=model_config.model_task_config.vlm, request_type="vlm")
            logger.warning(f"video_analysisé…ç½®ä¸å¯ç”¨({e})ï¼Œå›é€€ä½¿ç”¨vlmé…ç½®")

        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        config = global_config.video_analysis

        # ä½¿ç”¨ getattr ç»Ÿä¸€è·å–é…ç½®å‚æ•°ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.max_frames = getattr(config, "max_frames", 6)
        self.frame_quality = getattr(config, "frame_quality", 85)
        self.max_image_size = getattr(config, "max_image_size", 600)
        self.enable_frame_timing = getattr(config, "enable_frame_timing", True)

        # ä»personalityé…ç½®ä¸­è·å–äººæ ¼ä¿¡æ¯
        try:
            personality_config = global_config.personality
            self.personality_core = getattr(personality_config, "personality_core", "æ˜¯ä¸€ä¸ªç§¯æå‘ä¸Šçš„å¥³å¤§å­¦ç”Ÿ")
            self.personality_side = getattr(
                personality_config, "personality_side", "ç”¨ä¸€å¥è¯æˆ–å‡ å¥è¯æè¿°äººæ ¼çš„ä¾§é¢ç‰¹ç‚¹"
            )
        except AttributeError:
            # å¦‚æœæ²¡æœ‰personalityé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.personality_core = "æ˜¯ä¸€ä¸ªç§¯æå‘ä¸Šçš„å¥³å¤§å­¦ç”Ÿ"
            self.personality_side = "ç”¨ä¸€å¥è¯æˆ–å‡ å¥è¯æè¿°äººæ ¼çš„ä¾§é¢ç‰¹ç‚¹"

        self.batch_analysis_prompt = getattr(
            config,
            "batch_analysis_prompt",
            """è¯·ä»¥ç¬¬ä¸€äººç§°çš„è§†è§’æ¥è§‚çœ‹è¿™ä¸€ä¸ªè§†é¢‘ï¼Œä½ çœ‹åˆ°çš„è¿™äº›æ˜¯ä»è§†é¢‘ä¸­æŒ‰æ—¶é—´é¡ºåºæå–çš„å…³é”®å¸§ã€‚

ä½ çš„æ ¸å¿ƒäººè®¾æ˜¯ï¼š{personality_core}ã€‚
ä½ çš„äººæ ¼ç»†èŠ‚æ˜¯ï¼š{personality_side}ã€‚

è¯·æä¾›è¯¦ç»†çš„è§†é¢‘å†…å®¹æè¿°ï¼Œæ¶µç›–ä»¥ä¸‹æ–¹é¢ï¼š
1. è§†é¢‘çš„æ•´ä½“å†…å®¹å’Œä¸»é¢˜
2. ä¸»è¦äººç‰©ã€å¯¹è±¡å’Œåœºæ™¯æè¿°
3. åŠ¨ä½œã€æƒ…èŠ‚å’Œæ—¶é—´çº¿å‘å±•
4. è§†è§‰é£æ ¼å’Œè‰ºæœ¯ç‰¹ç‚¹
5. æ•´ä½“æ°›å›´å’Œæƒ…æ„Ÿè¡¨è¾¾
6. ä»»ä½•ç‰¹æ®Šçš„è§†è§‰æ•ˆæœæˆ–æ–‡å­—å†…å®¹

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æœè¦è¯¦ç»†å‡†ç¡®ã€‚""",
        )

        # æ–°å¢çš„çº¿ç¨‹æ± é…ç½®
        self.use_multiprocessing = getattr(config, "use_multiprocessing", True)
        self.max_workers = getattr(config, "max_workers", 2)
        self.frame_extraction_mode = getattr(config, "frame_extraction_mode", "fixed_number")
        self.frame_interval_seconds = getattr(config, "frame_interval_seconds", 2.0)

        # å°†é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å¼æ˜ å°„åˆ°å†…éƒ¨ä½¿ç”¨çš„æ¨¡å¼åç§°
        config_mode = getattr(config, "analysis_mode", "auto")
        if config_mode == "batch_frames":
            self.analysis_mode = "batch"
        elif config_mode == "frame_by_frame":
            self.analysis_mode = "sequential"
        elif config_mode == "auto":
            self.analysis_mode = "auto"
        else:
            logger.warning(f"æ— æ•ˆçš„åˆ†ææ¨¡å¼: {config_mode}ï¼Œä½¿ç”¨é»˜è®¤çš„autoæ¨¡å¼")
            self.analysis_mode = "auto"

        self.frame_analysis_delay = 0.3  # APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
        self.frame_interval = 1.0  # æŠ½å¸§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
        self.batch_size = 3  # æ‰¹å¤„ç†æ—¶æ¯æ‰¹å¤„ç†çš„å¸§æ•°
        self.timeout = 60.0  # åˆ†æè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        if config:
            logger.info("âœ… ä»é…ç½®æ–‡ä»¶è¯»å–è§†é¢‘åˆ†æå‚æ•°")
        else:
            logger.warning("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘video_analysisé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿç”¨æˆ·æä¾›çš„è§†é¢‘å…³é”®å¸§ï¼Œè¯¦ç»†æè¿°è§†é¢‘å†…å®¹ã€‚"

        logger.info(
            f"âœ… æ—§ç‰ˆæœ¬è§†é¢‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œåˆ†ææ¨¡å¼: {self.analysis_mode}, çº¿ç¨‹æ± : {self.use_multiprocessing}"
        )

    async def extract_frames(self, video_path: str) -> list[tuple[str, float]]:
        """æå–è§†é¢‘å¸§ - æ”¯æŒå¤šè¿›ç¨‹å’Œå•çº¿ç¨‹æ¨¡å¼"""
        # å…ˆè·å–è§†é¢‘ä¿¡æ¯
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        cap.release()

        logger.info(f"è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.2f}FPS, {duration:.2f}ç§’")

        # ä¼°ç®—æå–å¸§æ•°
        if duration > 0:
            frame_interval = max(1, int(duration / self.max_frames * fps))
            estimated_frames = min(self.max_frames, total_frames // frame_interval + 1)
        else:
            estimated_frames = self.max_frames
            frame_interval = 1

        logger.info(f"è®¡ç®—å¾—å‡ºå¸§é—´éš”: {frame_interval} (å°†æå–çº¦{estimated_frames}å¸§)")

        # æ ¹æ®é…ç½®é€‰æ‹©å¤„ç†æ–¹å¼
        if self.use_multiprocessing:
            return await self._extract_frames_multiprocess(video_path)
        else:
            return await self._extract_frames_fallback(video_path)

    async def _extract_frames_multiprocess(self, video_path: str) -> list[tuple[str, float]]:
        """çº¿ç¨‹æ± ç‰ˆæœ¬çš„å¸§æå–"""
        loop = asyncio.get_event_loop()

        try:
            logger.info("ğŸ”„ å¯åŠ¨çº¿ç¨‹æ± å¸§æå–...")
            # ä½¿ç”¨çº¿ç¨‹æ± ï¼Œé¿å…è¿›ç¨‹é—´çš„å¯¼å…¥é—®é¢˜
            with ThreadPoolExecutor(max_workers=1) as executor:
                frames = await loop.run_in_executor(
                    executor,
                    _extract_frames_worker,
                    video_path,
                    self.max_frames,
                    self.frame_quality,
                    self.max_image_size,
                    self.frame_extraction_mode,
                    self.frame_interval_seconds,
                )

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if frames and frames[0][0] == "ERROR":
                logger.error(f"çº¿ç¨‹æ± å¸§æå–å¤±è´¥: {frames[0][1]}")
                # é™çº§åˆ°å•çº¿ç¨‹æ¨¡å¼
                logger.info("ğŸ”„ é™çº§åˆ°å•çº¿ç¨‹æ¨¡å¼...")
                return await self._extract_frames_fallback(video_path)

            logger.info(f"âœ… æˆåŠŸæå–{len(frames)}å¸§ (çº¿ç¨‹æ± æ¨¡å¼)")
            return frames  # type: ignore

        except Exception as e:
            logger.error(f"çº¿ç¨‹æ± å¸§æå–å¤±è´¥: {e}")
            # é™çº§åˆ°åŸå§‹æ–¹æ³•
            logger.info("ğŸ”„ é™çº§åˆ°å•çº¿ç¨‹æ¨¡å¼...")
            return await self._extract_frames_fallback(video_path)

    async def _extract_frames_fallback(self, video_path: str) -> list[tuple[str, float]]:
        """å¸§æå–çš„é™çº§æ–¹æ³• - åŸå§‹å¼‚æ­¥ç‰ˆæœ¬"""
        frames = []
        extracted_count = 0
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        logger.info(f"è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.2f}FPS, {duration:.2f}ç§’")

        if self.frame_extraction_mode == "time_interval":
            # æ–°æ¨¡å¼ï¼šæŒ‰æ—¶é—´é—´éš”æŠ½å¸§
            time_interval = self.frame_interval_seconds
            next_frame_time = 0.0

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0

                if current_time >= next_frame_time:
                    # è½¬æ¢ä¸ºPILå›¾åƒå¹¶å‹ç¼©
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)

                    # è°ƒæ•´å›¾åƒå¤§å°
                    if max(pil_image.size) > self.max_image_size:
                        ratio = self.max_image_size / max(pil_image.size)
                        new_size = (int(pil_image.size[0] * ratio), int(pil_image.size[1] * ratio))
                        pil_image = pil_image.resize(new_size, Image.Resampling.LANCZOS)

                    # è½¬æ¢ä¸ºbase64
                    buffer = io.BytesIO()
                    pil_image.save(buffer, format="JPEG", quality=self.frame_quality)
                    frame_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

                    frames.append((frame_base64, current_time))
                    extracted_count += 1

                    logger.debug(f"æå–ç¬¬{extracted_count}å¸§ (æ—¶é—´: {current_time:.2f}s)")

                    next_frame_time += time_interval
        else:
            # ä½¿ç”¨numpyä¼˜åŒ–å¸§é—´éš”è®¡ç®—
            if duration > 0:
                frame_interval = max(1, int(duration / self.max_frames * fps))
            else:
                frame_interval = 30  # é»˜è®¤é—´éš”

            logger.info(
                f"è®¡ç®—å¾—å‡ºå¸§é—´éš”: {frame_interval} (å°†æå–çº¦{min(self.max_frames, total_frames // frame_interval + 1)}å¸§)"
            )

            # ä½¿ç”¨numpyè®¡ç®—ç›®æ ‡å¸§ä½ç½®
            target_frames = np.arange(0, min(self.max_frames, total_frames // frame_interval + 1)) * frame_interval
            target_frames = target_frames[target_frames < total_frames].astype(int)

            extracted_count = 0

            for target_frame in target_frames:
                # è·³è½¬åˆ°ç›®æ ‡å¸§
                cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
                ret, frame = cap.read()
                if not ret:
                    continue

                # ä½¿ç”¨numpyä¼˜åŒ–å›¾åƒå¤„ç†
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä½¿ç”¨numpyè¿›è¡Œå°ºå¯¸è®¡ç®—
                height, width = frame_rgb.shape[:2]
                max_dim = max(height, width)

                if max_dim > self.max_image_size:
                    # ä½¿ç”¨numpyè®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                    ratio = self.max_image_size / max_dim
                    new_width = int(width * ratio)
                    new_height = int(height * ratio)

                    # ä½¿ç”¨opencvè¿›è¡Œé«˜æ•ˆç¼©æ”¾
                    frame_resized = cv2.resize(frame_rgb, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
                    pil_image = Image.fromarray(frame_resized)
                else:
                    pil_image = Image.fromarray(frame_rgb)

                # è½¬æ¢ä¸ºbase64
                buffer = io.BytesIO()
                pil_image.save(buffer, format="JPEG", quality=self.frame_quality)
                frame_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")

                # è®¡ç®—æ—¶é—´æˆ³
                timestamp = target_frame / fps if fps > 0 else 0
                frames.append((frame_base64, timestamp))
                extracted_count += 1

                logger.debug(f"æå–ç¬¬{extracted_count}å¸§ (æ—¶é—´: {timestamp:.2f}s, å¸§å·: {target_frame})")

                # æ¯æå–ä¸€å¸§è®©æ­¥ä¸€æ¬¡
                await asyncio.sleep(0.001)

        cap.release()
        logger.info(f"âœ… æˆåŠŸæå–{len(frames)}å¸§")
        return frames

    async def analyze_frames_batch(self, frames: list[tuple[str, float]], user_question: str | None = None) -> str:
        """æ‰¹é‡åˆ†ææ‰€æœ‰å¸§"""
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ{len(frames)}å¸§")

        if not frames:
            return "âŒ æ²¡æœ‰å¯åˆ†æçš„å¸§"

        # æ„å»ºæç¤ºè¯å¹¶æ ¼å¼åŒ–äººæ ¼ä¿¡æ¯ï¼Œè¦ä¸ç„¶å ä½ç¬¦çš„é‚£ä¸ªä¼šçˆ†ç‚¸
        prompt = self.batch_analysis_prompt.format(
            personality_core=self.personality_core, personality_side=self.personality_side
        )

        if user_question:
            prompt += f"\n\nç”¨æˆ·é—®é¢˜: {user_question}"

        # æ·»åŠ å¸§ä¿¡æ¯åˆ°æç¤ºè¯
        frame_info = []
        for i, (_frame_base64, timestamp) in enumerate(frames):
            if self.enable_frame_timing:
                frame_info.append(f"ç¬¬{i + 1}å¸§ (æ—¶é—´: {timestamp:.2f}s)")
            else:
                frame_info.append(f"ç¬¬{i + 1}å¸§")

        prompt += f"\n\nè§†é¢‘åŒ…å«{len(frames)}å¸§å›¾åƒï¼š{', '.join(frame_info)}"
        prompt += "\n\nè¯·åŸºäºæ‰€æœ‰æä¾›çš„å¸§å›¾åƒè¿›è¡Œç»¼åˆåˆ†æï¼Œå…³æ³¨å¹¶æè¿°è§†é¢‘çš„å®Œæ•´å†…å®¹å’Œæ•…äº‹å‘å±•ã€‚"

        try:
            # å°è¯•ä½¿ç”¨å¤šå›¾ç‰‡åˆ†æ
            response = await self._analyze_multiple_frames(frames, prompt)
            logger.info("âœ… è§†é¢‘è¯†åˆ«å®Œæˆ")
            return response

        except Exception as e:
            logger.error(f"âŒ è§†é¢‘è¯†åˆ«å¤±è´¥: {e}")
            # é™çº§åˆ°å•å¸§åˆ†æ
            logger.warning("é™çº§åˆ°å•å¸§åˆ†ææ¨¡å¼")
            try:
                frame_base64, timestamp = frames[0]
                fallback_prompt = (
                    prompt
                    + f"\n\næ³¨æ„ï¼šç”±äºæŠ€æœ¯é™åˆ¶ï¼Œå½“å‰ä»…æ˜¾ç¤ºç¬¬1å¸§ (æ—¶é—´: {timestamp:.2f}s)ï¼Œè§†é¢‘å…±æœ‰{len(frames)}å¸§ã€‚è¯·åŸºäºè¿™ä¸€å¸§è¿›è¡Œåˆ†æã€‚"
                )

                response, _ = await self.video_llm.generate_response_for_image(
                    prompt=fallback_prompt, image_base64=frame_base64, image_format="jpeg"
                )
                logger.info("âœ… é™çº§çš„å•å¸§åˆ†æå®Œæˆ")
                return response
            except Exception as fallback_e:
                logger.error(f"âŒ é™çº§åˆ†æä¹Ÿå¤±è´¥: {fallback_e}")
                raise

    async def _analyze_multiple_frames(self, frames: list[tuple[str, float]], prompt: str) -> str:
        """ä½¿ç”¨å¤šå›¾ç‰‡åˆ†ææ–¹æ³•"""
        logger.info(f"å¼€å§‹æ„å»ºåŒ…å«{len(frames)}å¸§çš„åˆ†æè¯·æ±‚")

        # å¯¼å…¥MessageBuilderç”¨äºæ„å»ºå¤šå›¾ç‰‡æ¶ˆæ¯
        from src.llm_models.payload_content.message import MessageBuilder, RoleType
        from src.llm_models.utils_model import RequestType

        # æ„å»ºåŒ…å«å¤šå¼ å›¾ç‰‡çš„æ¶ˆæ¯
        message_builder = MessageBuilder().set_role(RoleType.User).add_text_content(prompt)

        # æ·»åŠ æ‰€æœ‰å¸§å›¾åƒ
        for _i, (frame_base64, _timestamp) in enumerate(frames):
            message_builder.add_image_content("jpeg", frame_base64)
            # logger.info(f"å·²æ·»åŠ ç¬¬{i+1}å¸§åˆ°åˆ†æè¯·æ±‚ (æ—¶é—´: {timestamp:.2f}s, å›¾ç‰‡å¤§å°: {len(frame_base64)} chars)")

        message = message_builder.build()
        # logger.info(f"âœ… å¤šå¸§æ¶ˆæ¯æ„å»ºå®Œæˆï¼ŒåŒ…å«{len(frames)}å¼ å›¾ç‰‡")

        # è·å–æ¨¡å‹ä¿¡æ¯å’Œå®¢æˆ·ç«¯
        model_info, api_provider, client = self.video_llm._select_model()  # type: ignore
        # logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_info.name} è¿›è¡Œå¤šå¸§åˆ†æ")

        # ç›´æ¥æ‰§è¡Œå¤šå›¾ç‰‡è¯·æ±‚
        api_response = await self.video_llm._execute_request(  # type: ignore
            api_provider=api_provider,
            client=client,
            request_type=RequestType.RESPONSE,
            model_info=model_info,
            message_list=[message],
            temperature=None,
            max_tokens=None,
        )

        logger.info(f"è§†é¢‘è¯†åˆ«å®Œæˆï¼Œå“åº”é•¿åº¦: {len(api_response.content or '')} ")
        return api_response.content or "âŒ æœªè·å¾—å“åº”å†…å®¹"

    async def analyze_frames_sequential(self, frames: list[tuple[str, float]], user_question: str | None = None) -> str:
        """é€å¸§åˆ†æå¹¶æ±‡æ€»"""
        logger.info(f"å¼€å§‹é€å¸§åˆ†æ{len(frames)}å¸§")

        frame_analyses = []

        for i, (frame_base64, timestamp) in enumerate(frames):
            try:
                prompt = f"è¯·åˆ†æè¿™ä¸ªè§†é¢‘çš„ç¬¬{i + 1}å¸§"
                if self.enable_frame_timing:
                    prompt += f" (æ—¶é—´: {timestamp:.2f}s)"
                prompt += "ã€‚æè¿°ä½ çœ‹åˆ°çš„å†…å®¹ï¼ŒåŒ…æ‹¬äººç‰©ã€åŠ¨ä½œã€åœºæ™¯ã€æ–‡å­—ç­‰ã€‚"

                if user_question:
                    prompt += f"\nç‰¹åˆ«å…³æ³¨: {user_question}"

                response, _ = await self.video_llm.generate_response_for_image(
                    prompt=prompt, image_base64=frame_base64, image_format="jpeg"
                )

                frame_analyses.append(f"ç¬¬{i + 1}å¸§ ({timestamp:.2f}s): {response}")
                logger.debug(f"âœ… ç¬¬{i + 1}å¸§åˆ†æå®Œæˆ")

                # APIè°ƒç”¨é—´éš”
                if i < len(frames) - 1:
                    await asyncio.sleep(self.frame_analysis_delay)

            except Exception as e:
                logger.error(f"âŒ ç¬¬{i + 1}å¸§åˆ†æå¤±è´¥: {e}")
                frame_analyses.append(f"ç¬¬{i + 1}å¸§: åˆ†æå¤±è´¥ - {e}")

        # ç”Ÿæˆæ±‡æ€»
        logger.info("å¼€å§‹ç”Ÿæˆæ±‡æ€»åˆ†æ")
        summary_prompt = f"""åŸºäºä»¥ä¸‹å„å¸§çš„åˆ†æç»“æœï¼Œè¯·æä¾›ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘å†…å®¹æ€»ç»“ï¼š

{chr(10).join(frame_analyses)}

è¯·ç»¼åˆæ‰€æœ‰å¸§çš„ä¿¡æ¯ï¼Œæè¿°è§†é¢‘çš„æ•´ä½“å†…å®¹ã€æ•…äº‹çº¿ã€ä¸»è¦å…ƒç´ å’Œç‰¹ç‚¹ã€‚"""

        if user_question:
            summary_prompt += f"\nç‰¹åˆ«å›ç­”ç”¨æˆ·çš„é—®é¢˜: {user_question}"

        try:
            # ä½¿ç”¨æœ€åä¸€å¸§è¿›è¡Œæ±‡æ€»åˆ†æ
            if frames:
                last_frame_base64, _ = frames[-1]
                summary, _ = await self.video_llm.generate_response_for_image(
                    prompt=summary_prompt, image_base64=last_frame_base64, image_format="jpeg"
                )
                logger.info("âœ… é€å¸§åˆ†æå’Œæ±‡æ€»å®Œæˆ")
                return summary
            else:
                return "âŒ æ²¡æœ‰å¯ç”¨äºæ±‡æ€»çš„å¸§"
        except Exception as e:
            logger.error(f"âŒ æ±‡æ€»åˆ†æå¤±è´¥: {e}")
            # å¦‚æœæ±‡æ€»å¤±è´¥ï¼Œè¿”å›å„å¸§åˆ†æç»“æœ
            return f"è§†é¢‘é€å¸§åˆ†æç»“æœï¼š\n\n{chr(10).join(frame_analyses)}"

    async def analyze_video(self, video_path: str, user_question: str | None = None) -> str:
        """åˆ†æè§†é¢‘çš„ä¸»è¦æ–¹æ³•"""
        try:
            logger.info(f"å¼€å§‹åˆ†æè§†é¢‘: {os.path.basename(video_path)}")

            # æå–å¸§
            frames = await self.extract_frames(video_path)
            if not frames:
                return "âŒ æ— æ³•ä»è§†é¢‘ä¸­æå–æœ‰æ•ˆå¸§"

            # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†ææ–¹æ³•
            if self.analysis_mode == "auto":
                # æ™ºèƒ½é€‰æ‹©ï¼šå°‘äºç­‰äº3å¸§ç”¨æ‰¹é‡ï¼Œå¦åˆ™ç”¨é€å¸§
                mode = "batch" if len(frames) <= 3 else "sequential"
                logger.info(f"è‡ªåŠ¨é€‰æ‹©åˆ†ææ¨¡å¼: {mode} (åŸºäº{len(frames)}å¸§)")
            else:
                mode = self.analysis_mode

            # æ‰§è¡Œåˆ†æ
            if mode == "batch":
                result = await self.analyze_frames_batch(frames, user_question)
            else:  # sequential
                result = await self.analyze_frames_sequential(frames, user_question)

            logger.info("âœ… è§†é¢‘åˆ†æå®Œæˆ")
            return result

        except Exception as e:
            error_msg = f"âŒ è§†é¢‘åˆ†æå¤±è´¥: {e!s}"
            logger.error(error_msg)
            return error_msg

    @staticmethod
    def is_supported_video(file_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘æ ¼å¼"""
        supported_formats = {".mp4", ".avi", ".mov", ".mkv", ".flv", ".wmv", ".m4v", ".3gp", ".webm"}
        return Path(file_path).suffix.lower() in supported_formats


# å…¨å±€å®ä¾‹
_legacy_video_analyzer = None


def get_legacy_video_analyzer() -> LegacyVideoAnalyzer:
    """è·å–æ—§ç‰ˆæœ¬è§†é¢‘åˆ†æå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _legacy_video_analyzer
    if _legacy_video_analyzer is None:
        _legacy_video_analyzer = LegacyVideoAnalyzer()
    return _legacy_video_analyzer
