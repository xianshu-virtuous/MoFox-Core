import asyncio
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Any

from src.common.database.compatibility import db_get, db_query
from src.common.database.api.query import QueryBuilder
from src.common.database.core.models import LLMUsage, Messages, OnlineTime
from src.common.logger import get_logger
from src.manager.async_task_manager import AsyncTask
from src.manager.local_store_manager import local_storage

logger = get_logger("maibot_statistic")

# ç»Ÿè®¡æŸ¥è¯¢çš„æ‰¹æ¬¡å¤§å°
STAT_BATCH_SIZE = 2000
# å†…å­˜ä¼˜åŒ–ï¼šå•æ¬¡ç»Ÿè®¡æœ€å¤§å¤„ç†è®°å½•æ•°ï¼ˆé˜²æ­¢æç«¯æƒ…å†µï¼‰
STAT_MAX_RECORDS = 100000

# å½»åº•å¼‚æ­¥åŒ–ï¼šåˆ é™¤åŸåŒæ­¥åŒ…è£…å™¨ _sync_db_getï¼Œæ‰€æœ‰æ•°æ®åº“è®¿é—®ç»Ÿä¸€ä½¿ç”¨ await db_getã€‚


from .report_generator import HTMLReportGenerator
from .statistic_keys import *


class OnlineTimeRecordTask(AsyncTask):
    """åœ¨çº¿æ—¶é—´è®°å½•ä»»åŠ¡"""

    def __init__(self):
        super().__init__(task_name="Online Time Record Task", run_interval=60)

        self.record_id: int | None = None
        """è®°å½•ID"""

    async def run(self):  # sourcery skip: use-named-expression
        try:
            current_time = datetime.now()
            extended_end_time = current_time + timedelta(minutes=1)

            if self.record_id:
                # å¦‚æœæœ‰è®°å½•ï¼Œåˆ™æ›´æ–°ç»“æŸæ—¶é—´
                updated_rows = await db_query(
                    model_class=OnlineTime,
                    query_type="update",
                    filters={"id": self.record_id},
                    data={"end_timestamp": extended_end_time},
                )
                if updated_rows == 0:
                    # Record might have been deleted or ID is stale, try to find/create
                    self.record_id = None

            if not self.record_id:
                # æŸ¥æ‰¾æœ€è¿‘ä¸€åˆ†é’Ÿå†…çš„è®°å½•
                recent_threshold = current_time - timedelta(minutes=1)
                recent_records = await db_get(
                    model_class=OnlineTime,
                    filters={"end_timestamp": {"$gte": recent_threshold}},
                    order_by="-end_timestamp",
                    limit=1,
                    single_result=True,
                )

                if recent_records:
                    # æ‰¾åˆ°è¿‘æœŸè®°å½•ï¼Œæ›´æ–°å®ƒ
                    record_to_use = recent_records[0] if isinstance(recent_records, list) else recent_records
                    self.record_id = record_to_use.get("id")
                    if self.record_id:
                        await db_query(
                            model_class=OnlineTime,
                            query_type="update",
                            filters={"id": self.record_id},
                            data={"end_timestamp": extended_end_time},
                        )
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_record = await db_query(
                        model_class=OnlineTime,
                        query_type="create",
                        data={
                            "timestamp": str(current_time),
                            "duration": 5,  # åˆå§‹æ—¶é•¿ä¸º5åˆ†é’Ÿ
                            "start_timestamp": current_time,
                            "end_timestamp": extended_end_time,
                        },
                    )
                    if new_record:
                        record_to_use = new_record[0] if isinstance(new_record, list) else new_record
                        self.record_id = record_to_use.get("id")
        except Exception as e:
            logger.error(f"åœ¨çº¿æ—¶é—´è®°å½•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")


def _format_online_time(online_seconds: int) -> str:
    """
    æ ¼å¼åŒ–åœ¨çº¿æ—¶é—´
    :param online_seconds: åœ¨çº¿æ—¶é—´ï¼ˆç§’ï¼‰
    :return: æ ¼å¼åŒ–åçš„åœ¨çº¿æ—¶é—´å­—ç¬¦ä¸²
    """
    total_online_time = timedelta(seconds=online_seconds)

    days = total_online_time.days
    hours = total_online_time.seconds // 3600
    minutes = (total_online_time.seconds // 60) % 60
    seconds = total_online_time.seconds % 60
    if days > 0:
        # å¦‚æœåœ¨çº¿æ—¶é—´è¶…è¿‡1å¤©ï¼Œåˆ™æ ¼å¼åŒ–ä¸º"Xå¤©Xå°æ—¶Xåˆ†é’Ÿ"
        return f"{total_online_time.days}å¤©{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"
    elif hours > 0:
        # å¦‚æœåœ¨çº¿æ—¶é—´è¶…è¿‡1å°æ—¶ï¼Œåˆ™æ ¼å¼åŒ–ä¸º"Xå°æ—¶Xåˆ†é’ŸXç§’"
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"
    else:
        # å…¶ä»–æƒ…å†µæ ¼å¼åŒ–ä¸º"Xåˆ†é’ŸXç§’"
        return f"{minutes}åˆ†é’Ÿ{seconds}ç§’"


class StatisticOutputTask(AsyncTask):
    """ç»Ÿè®¡è¾“å‡ºä»»åŠ¡"""

    SEP_LINE = "-" * 84

    def __init__(self, record_file_path: str = "mofox_bot_statistics.html"):
        # å»¶è¿Ÿ300ç§’å¯åŠ¨ï¼Œè¿è¡Œé—´éš”300ç§’
        super().__init__(task_name="Statistics Data Output Task", wait_before_start=0, run_interval=300)

        self.name_mapping: dict[str, tuple[str, float]] = {}
        """
            è”ç³»äºº/ç¾¤èŠåç§°æ˜ å°„ {èŠå¤©ID: (è”ç³»äºº/ç¾¤èŠåç§°, è®°å½•æ—¶é—´ï¼ˆtimestampï¼‰)}
            æ³¨ï¼šè®¾è®¡è®°å½•æ—¶é—´çš„ç›®çš„æ˜¯æ–¹ä¾¿æ›´æ–°åç§°ï¼Œä½¿è”ç³»äºº/ç¾¤èŠåç§°ä¿æŒæœ€æ–°
        """

        self.record_file_path: str = record_file_path
        """
        è®°å½•æ–‡ä»¶è·¯å¾„
        """

        now = datetime.now()
        deploy_time_ts = local_storage.get("deploy_time")
        if deploy_time_ts:
            # å¦‚æœå­˜åœ¨éƒ¨ç½²æ—¶é—´ï¼Œåˆ™ä½¿ç”¨è¯¥æ—¶é—´ä½œä¸ºå…¨é‡ç»Ÿè®¡çš„èµ·å§‹æ—¶é—´
            deploy_time = datetime.fromtimestamp(deploy_time_ts)  # type: ignore
        else:
            # å¦åˆ™ï¼Œä½¿ç”¨æœ€å¤§æ—¶é—´èŒƒå›´ï¼Œå¹¶è®°å½•éƒ¨ç½²æ—¶é—´ä¸ºå½“å‰æ—¶é—´
            deploy_time = datetime(2000, 1, 1)
            local_storage["deploy_time"] = now.timestamp()
        self.stat_period: list[tuple[str, timedelta, str]] = [
            ("all_time", now - deploy_time, "è‡ªéƒ¨ç½²ä»¥æ¥"),  # å¿…é¡»ä¿ç•™"all_time"
            ("last_7_days", timedelta(days=7), "æœ€è¿‘7å¤©"),
            ("last_24_hours", timedelta(days=1), "æœ€è¿‘24å°æ—¶"),
            ("last_3_hours", timedelta(hours=3), "æœ€è¿‘3å°æ—¶"),
            ("last_hour", timedelta(hours=1), "æœ€è¿‘1å°æ—¶"),
        ]
        """
        ç»Ÿè®¡æ—¶é—´æ®µ [(ç»Ÿè®¡åç§°, ç»Ÿè®¡æ—¶é—´æ®µ, ç»Ÿè®¡æè¿°), ...]
        """

    def _statistic_console_output(self, stats: dict[str, Any], now: datetime):
        """
        è¾“å‡ºç»Ÿè®¡æ•°æ®åˆ°æ§åˆ¶å°
        :param stats: ç»Ÿè®¡æ•°æ®
        :param now: åŸºå‡†å½“å‰æ—¶é—´
        """
        # è¾“å‡ºæœ€è¿‘ä¸€å°æ—¶çš„ç»Ÿè®¡æ•°æ®
        output = [
            self.SEP_LINE,
            f"  æœ€è¿‘1å°æ—¶çš„ç»Ÿè®¡æ•°æ®  (è‡ª{now.strftime('%Y-%m-%d %H:%M:%S')}å¼€å§‹ï¼Œè¯¦ç»†ä¿¡æ¯è§æ–‡ä»¶ï¼š{self.record_file_path})",
            self.SEP_LINE,
            self._format_total_stat(stats["last_hour"]),
            "",
            self._format_model_classified_stat(stats["last_hour"]),
            "",
            self._format_chat_stat(stats["last_hour"]),
            self.SEP_LINE,
            "",
        ]

        logger.info("\n" + "\n".join(output))

    @staticmethod
    async def _yield_control(iteration: int, interval: int = 200) -> None:
        """
        ï¿½Ú´ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ê±ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ì²½ï¿½Â¼ï¿½Ñ­ï¿½ï¿½ï¿½ï¿½ï¿½Ó¦

        Args:
            iteration: ï¿½ï¿½Ç°ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
            interval: Ã¿ï¿½ï¿½ï¿½ï¿½ï¿½Ù´ï¿½ï¿½Ğ»ï¿½Ò»ï¿½ï¿½
        """
        if iteration % interval == 0:
            await asyncio.sleep(0)

    async def run(self):
        try:
            now = datetime.now()
            logger.info("æ­£åœ¨æ”¶é›†ç»Ÿè®¡æ•°æ®(å¼‚æ­¥)...")
            stats = await self._collect_all_statistics(now)
            logger.info("ç»Ÿè®¡æ•°æ®æ”¶é›†å®Œæˆ")

            self._statistic_console_output(stats, now)
            # ä½¿ç”¨æ–°çš„ HTMLReportGenerator ç”ŸæˆæŠ¥å‘Š
            chart_data = await self._collect_chart_data(stats)
            deploy_time = datetime.fromtimestamp(float(local_storage.get("deploy_time", now.timestamp())))  # type: ignore
            report_generator = HTMLReportGenerator(
                name_mapping=self.name_mapping,
                stat_period=self.stat_period,
                deploy_time=deploy_time,
            )
            await report_generator.generate_report(stats, chart_data, now, self.record_file_path)
            logger.info("ç»Ÿè®¡æ•°æ®HTMLæŠ¥å‘Šè¾“å‡ºå®Œæˆ")

        except Exception as e:
            logger.exception(f"è¾“å‡ºç»Ÿè®¡æ•°æ®è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")

    async def run_async_background(self):
        """
        å¤‡é€‰æ–¹æ¡ˆï¼šå®Œå…¨å¼‚æ­¥åå°è¿è¡Œç»Ÿè®¡è¾“å‡º
        ä½¿ç”¨æ­¤æ–¹æ³•å¯ä»¥è®©ç»Ÿè®¡ä»»åŠ¡å®Œå…¨éé˜»å¡
        """

        async def _async_collect_and_output():
            try:
                now = datetime.now()
                logger.info("(åå°) æ­£åœ¨æ”¶é›†ç»Ÿè®¡æ•°æ®(å¼‚æ­¥)...")
                stats = await self._collect_all_statistics(now)
                self._statistic_console_output(stats, now)

                # ä½¿ç”¨æ–°çš„ HTMLReportGenerator ç”ŸæˆæŠ¥å‘Š
                chart_data = await self._collect_chart_data(stats)
                deploy_time = datetime.fromtimestamp(float(local_storage.get("deploy_time", now.timestamp())))  # type: ignore
                report_generator = HTMLReportGenerator(
                    name_mapping=self.name_mapping,
                    stat_period=self.stat_period,
                    deploy_time=deploy_time,
                )
                await report_generator.generate_report(stats, chart_data, now, self.record_file_path)

                logger.info("ç»Ÿè®¡æ•°æ®åå°è¾“å‡ºå®Œæˆ")
            except Exception as e:
                logger.exception(f"åå°ç»Ÿè®¡æ•°æ®è¾“å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼š{e}")

        # åˆ›å»ºåå°ä»»åŠ¡ï¼Œç«‹å³è¿”å›
        asyncio.create_task(_async_collect_and_output())  # noqa: RUF006
    # -- ä»¥ä¸‹ä¸ºç»Ÿè®¡æ•°æ®æ”¶é›†æ–¹æ³• --

    @staticmethod
    async def _collect_model_request_for_period(collect_period: list[tuple[str, datetime]]) -> dict[str, Any]:
        """
        æ”¶é›†æŒ‡å®šæ—¶é—´æ®µçš„LLMè¯·æ±‚ç»Ÿè®¡æ•°æ®

        :param collect_period: ç»Ÿè®¡æ—¶é—´æ®µ
        """
        if not collect_period:
            return {}

        # æ’åº-æŒ‰ç…§æ—¶é—´æ®µå¼€å§‹æ—¶é—´é™åºæ’åˆ—ï¼ˆæœ€æ™šçš„æ—¶é—´æ®µåœ¨å‰ï¼‰
        collect_period.sort(key=lambda x: x[1], reverse=True)

        stats = {
            period_key: {
                TOTAL_REQ_CNT: 0,
                REQ_CNT_BY_TYPE: defaultdict(int),
                REQ_CNT_BY_USER: defaultdict(int),
                REQ_CNT_BY_MODEL: defaultdict(int),
                REQ_CNT_BY_MODULE: defaultdict(int),
                REQ_CNT_BY_PROVIDER: defaultdict(int),  # New
                IN_TOK_BY_TYPE: defaultdict(int),
                IN_TOK_BY_USER: defaultdict(int),
                IN_TOK_BY_MODEL: defaultdict(int),
                IN_TOK_BY_MODULE: defaultdict(int),
                OUT_TOK_BY_TYPE: defaultdict(int),
                OUT_TOK_BY_USER: defaultdict(int),
                OUT_TOK_BY_MODEL: defaultdict(int),
                OUT_TOK_BY_MODULE: defaultdict(int),
                TOTAL_TOK_BY_TYPE: defaultdict(int),
                TOTAL_TOK_BY_USER: defaultdict(int),
                TOTAL_TOK_BY_MODEL: defaultdict(int),
                TOTAL_TOK_BY_MODULE: defaultdict(int),
                TOTAL_TOK_BY_PROVIDER: defaultdict(int),  # New
                TOTAL_COST: 0.0,
                COST_BY_TYPE: defaultdict(float),
                COST_BY_USER: defaultdict(float),
                COST_BY_MODEL: defaultdict(float),
                COST_BY_MODULE: defaultdict(float),
                COST_BY_PROVIDER: defaultdict(float),  # New
                TIME_COST_BY_TYPE: defaultdict(list),
                TIME_COST_BY_USER: defaultdict(list),
                TIME_COST_BY_MODEL: defaultdict(list),
                TIME_COST_BY_MODULE: defaultdict(list),
                TIME_COST_BY_PROVIDER: defaultdict(list),  # New
                AVG_TIME_COST_BY_TYPE: defaultdict(float),
                AVG_TIME_COST_BY_USER: defaultdict(float),
                AVG_TIME_COST_BY_MODEL: defaultdict(float),
                AVG_TIME_COST_BY_MODULE: defaultdict(float),
                STD_TIME_COST_BY_TYPE: defaultdict(float),
                STD_TIME_COST_BY_USER: defaultdict(float),
                STD_TIME_COST_BY_MODEL: defaultdict(float),
                STD_TIME_COST_BY_MODULE: defaultdict(float),
                AVG_TIME_COST_BY_PROVIDER: defaultdict(float),
                STD_TIME_COST_BY_PROVIDER: defaultdict(float),
                # New calculated fields
                TPS_BY_MODEL: defaultdict(float),
                COST_PER_KTOK_BY_MODEL: defaultdict(float),
                AVG_TOK_BY_MODEL: defaultdict(float),
                TPS_BY_PROVIDER: defaultdict(float),
                COST_PER_KTOK_BY_PROVIDER: defaultdict(float),
                # Chart data
                PIE_CHART_COST_BY_PROVIDER: {},
                PIE_CHART_REQ_BY_PROVIDER: {},
                PIE_CHART_COST_BY_MODULE: {},
                BAR_CHART_COST_BY_MODEL: {},
                BAR_CHART_REQ_BY_MODEL: {},
                BAR_CHART_TOKEN_COMPARISON: {},
                SCATTER_CHART_RESPONSE_TIME: {},
                RADAR_CHART_MODEL_EFFICIENCY: {},
                HEATMAP_CHAT_ACTIVITY: {},
                DOUGHNUT_CHART_PROVIDER_REQUESTS: {},
                LINE_CHART_COST_TREND: {},
                BAR_CHART_AVG_RESPONSE_TIME: {},
            }
            for period_key, _ in collect_period
        }

        # ä»¥æœ€æ—©çš„æ—¶é—´æˆ³ä¸ºèµ·å§‹æ—¶é—´è·å–è®°å½•
        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢ä»£æ›¿å…¨é‡åŠ è½½
        query_start_time = collect_period[-1][1]
        
        query_builder = (
            QueryBuilder(LLMUsage)
            .no_cache()
            .filter(timestamp__gte=query_start_time)
            .order_by("-timestamp")
        )
        
        total_processed = 0
        async for batch in query_builder.iter_batches(batch_size=STAT_BATCH_SIZE, as_dict=True):
            for record in batch:
                if total_processed >= STAT_MAX_RECORDS:
                    logger.warning(f"ç»Ÿè®¡å¤„ç†è®°å½•æ•°è¾¾åˆ°ä¸Šé™ {STAT_MAX_RECORDS}ï¼Œè·³è¿‡å‰©ä½™è®°å½•")
                    break
                    
                if not isinstance(record, dict):
                    continue

                record_timestamp = record.get("timestamp")
                if isinstance(record_timestamp, str):
                    record_timestamp = datetime.fromisoformat(record_timestamp)

                if not record_timestamp:
                    continue

                for period_idx, (_, period_start) in enumerate(collect_period):
                    if record_timestamp >= period_start:
                        for period_key, _ in collect_period[period_idx:]:
                            stats[period_key][TOTAL_REQ_CNT] += 1

                            request_type = record.get("request_type") or "unknown"
                            user_id = record.get("user_id") or "unknown"
                            model_name = record.get("model_name") or "unknown"
                            provider_name = record.get("model_api_provider") or "unknown"

                            # æå–æ¨¡å—åï¼šå¦‚æœè¯·æ±‚ç±»å‹åŒ…å«"."ï¼Œå–ç¬¬ä¸€ä¸ª"."ä¹‹å‰çš„éƒ¨åˆ†
                            module_name = request_type.split(".")[0] if "." in request_type else request_type

                            stats[period_key][REQ_CNT_BY_TYPE][request_type] += 1
                            stats[period_key][REQ_CNT_BY_USER][user_id] += 1
                            stats[period_key][REQ_CNT_BY_MODEL][model_name] += 1
                            stats[period_key][REQ_CNT_BY_MODULE][module_name] += 1
                            stats[period_key][REQ_CNT_BY_PROVIDER][provider_name] += 1

                            prompt_tokens = record.get("prompt_tokens") or 0
                            completion_tokens = record.get("completion_tokens") or 0
                            total_tokens = prompt_tokens + completion_tokens

                            stats[period_key][IN_TOK_BY_TYPE][request_type] += prompt_tokens
                            stats[period_key][IN_TOK_BY_USER][user_id] += prompt_tokens
                            stats[period_key][IN_TOK_BY_MODEL][model_name] += prompt_tokens
                            stats[period_key][IN_TOK_BY_MODULE][module_name] += prompt_tokens

                            stats[period_key][OUT_TOK_BY_TYPE][request_type] += completion_tokens
                            stats[period_key][OUT_TOK_BY_USER][user_id] += completion_tokens
                            stats[period_key][OUT_TOK_BY_MODEL][model_name] += completion_tokens
                            stats[period_key][OUT_TOK_BY_MODULE][module_name] += completion_tokens

                            stats[period_key][TOTAL_TOK_BY_TYPE][request_type] += total_tokens
                            stats[period_key][TOTAL_TOK_BY_USER][user_id] += total_tokens
                            stats[period_key][TOTAL_TOK_BY_MODEL][model_name] += total_tokens
                            stats[period_key][TOTAL_TOK_BY_MODULE][module_name] += total_tokens
                            stats[period_key][TOTAL_TOK_BY_PROVIDER][provider_name] += total_tokens

                            cost = record.get("cost") or 0.0
                            stats[period_key][TOTAL_COST] += cost
                            stats[period_key][COST_BY_TYPE][request_type] += cost
                            stats[period_key][COST_BY_USER][user_id] += cost
                            stats[period_key][COST_BY_MODEL][model_name] += cost
                            stats[period_key][COST_BY_MODULE][module_name] += cost
                            stats[period_key][COST_BY_PROVIDER][provider_name] += cost

                            # æ”¶é›†time_costæ•°æ®
                            time_cost = record.get("time_cost") or 0.0
                            if time_cost > 0:  # åªè®°å½•æœ‰æ•ˆçš„time_cost
                                stats[period_key][TIME_COST_BY_TYPE][request_type].append(time_cost)
                                stats[period_key][TIME_COST_BY_USER][user_id].append(time_cost)
                                stats[period_key][TIME_COST_BY_MODEL][model_name].append(time_cost)
                                stats[period_key][TIME_COST_BY_MODULE][module_name].append(time_cost)
                                stats[period_key][TIME_COST_BY_PROVIDER][provider_name].append(time_cost)
                        break

                total_processed += 1
                if total_processed % 500 == 0:
                    await StatisticOutputTask._yield_control(total_processed, interval=1)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™
            if total_processed >= STAT_MAX_RECORDS:
                break
            
            # æ¯æ‰¹å¤„ç†å®Œåè®©å‡ºæ§åˆ¶æƒ
            await asyncio.sleep(0)
        # -- è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡ --
        for period_key, period_stats in stats.items():
            # è®¡ç®—æ¨¡å‹ç›¸å…³æŒ‡æ ‡
            for model_idx, (model_name, req_count) in enumerate(period_stats[REQ_CNT_BY_MODEL].items(), 1):
                total_tok = period_stats[TOTAL_TOK_BY_MODEL][model_name] or 0
                total_cost = period_stats[COST_BY_MODEL][model_name] or 0
                time_costs = period_stats[TIME_COST_BY_MODEL][model_name] or []
                total_time_cost = sum(time_costs)

                # TPS
                if total_time_cost > 0:
                    period_stats[TPS_BY_MODEL][model_name] = round(total_tok / total_time_cost, 2)
                # Cost per 1K Tokens
                if total_tok > 0:
                    period_stats[COST_PER_KTOK_BY_MODEL][model_name] = round((total_cost / total_tok) * 1000, 4)
                # Avg Tokens per Request
                period_stats[AVG_TOK_BY_MODEL][model_name] = round(total_tok / req_count) if req_count > 0 else 0

                await StatisticOutputTask._yield_control(model_idx, interval=100)

            # è®¡ç®—ä¾›åº”å•†ç›¸å…³æŒ‡æ ‡
            for provider_idx, (provider_name, req_count) in enumerate(period_stats[REQ_CNT_BY_PROVIDER].items(), 1):
                total_tok = period_stats[TOTAL_TOK_BY_PROVIDER][provider_name]
                total_cost = period_stats[COST_BY_PROVIDER][provider_name]
                time_costs = period_stats[TIME_COST_BY_PROVIDER][provider_name]
                total_time_cost = sum(time_costs)

                # TPS
                if total_time_cost > 0:
                    period_stats[TPS_BY_PROVIDER][provider_name] = round(total_tok / total_time_cost, 2)
                # Cost per 1K Tokens
                if total_tok > 0:
                    period_stats[COST_PER_KTOK_BY_PROVIDER][provider_name] = round((total_cost / total_tok) * 1000, 4)

                await StatisticOutputTask._yield_control(provider_idx, interval=100)

            # è®¡ç®—å¹³å‡è€—æ—¶å’Œæ ‡å‡†å·®
            for category_key, items in [
                (REQ_CNT_BY_USER, "user"),
                (REQ_CNT_BY_MODEL, "model"),
                (REQ_CNT_BY_MODULE, "module"),
                (REQ_CNT_BY_PROVIDER, "provider"),
            ]:
                time_cost_key = f"time_costs_by_{items.lower()}"
                avg_key = f"avg_time_costs_by_{items.lower()}"
                std_key = f"std_time_costs_by_{items.lower()}"
                for idx, item_name in enumerate(period_stats[category_key], 1):
                    time_costs = period_stats[time_cost_key][item_name]
                    if time_costs:
                        avg_time = sum(time_costs) / len(time_costs)
                        period_stats[avg_key][item_name] = round(avg_time, 3)
                        if len(time_costs) > 1:
                            variance = sum((x - avg_time) ** 2 for x in time_costs) / len(time_costs)
                            period_stats[std_key][item_name] = round(variance**0.5, 3)
                        else:
                            period_stats[std_key][item_name] = 0.0
                    else:
                        period_stats[avg_key][item_name] = 0.0
                        period_stats[std_key][item_name] = 0.0

                    await StatisticOutputTask._yield_control(idx, interval=200)

            # å‡†å¤‡å›¾è¡¨æ•°æ®
            # æŒ‰ä¾›åº”å•†èŠ±è´¹é¥¼å›¾
            provider_costs = period_stats[COST_BY_PROVIDER]
            if provider_costs:
                sorted_providers = sorted(provider_costs.items(), key=lambda item: item[1], reverse=True)
                period_stats[PIE_CHART_COST_BY_PROVIDER] = {
                    "labels": [item[0] for item in sorted_providers],
                    "data": [round(item[1], 4) for item in sorted_providers],
                }

            # æŒ‰æ¨¡å—èŠ±è´¹é¥¼å›¾
            module_costs = period_stats[COST_BY_MODULE]
            if module_costs:
                sorted_modules = sorted(module_costs.items(), key=lambda item: item[1], reverse=True)
                period_stats[PIE_CHART_COST_BY_MODULE] = {
                    "labels": [item[0] for item in sorted_modules],
                    "data": [round(item[1], 4) for item in sorted_modules],
                }

            # æŒ‰æ¨¡å‹èŠ±è´¹æ¡å½¢å›¾
            model_costs = period_stats[COST_BY_MODEL]
            if model_costs:
                sorted_models = sorted(model_costs.items(), key=lambda item: item[1], reverse=True)
                period_stats[BAR_CHART_COST_BY_MODEL] = {
                    "labels": [item[0] for item in sorted_models],
                    "data": [round(item[1], 4) for item in sorted_models],
                }
            
            # 1. Tokenè¾“å…¥è¾“å‡ºå¯¹æ¯”æ¡å½¢å›¾
            model_names = list(period_stats[REQ_CNT_BY_MODEL].keys())
            if model_names:
                period_stats[BAR_CHART_TOKEN_COMPARISON] = {
                    "labels": model_names,
                    "input_tokens": [period_stats[IN_TOK_BY_MODEL].get(m, 0) for m in model_names],
                    "output_tokens": [period_stats[OUT_TOK_BY_MODEL].get(m, 0) for m in model_names],
                }
            
            # 2. å“åº”æ—¶é—´åˆ†å¸ƒæ•£ç‚¹å›¾æ•°æ®ï¼ˆé™åˆ¶æ•°æ®ç‚¹ä»¥æé«˜åŠ è½½é€Ÿåº¦ï¼‰
            scatter_data = []
            max_points_per_model = 50  # æ¯ä¸ªæ¨¡å‹æœ€å¤š50ä¸ªç‚¹
            for model_name, time_costs in period_stats[TIME_COST_BY_MODEL].items():
                # å¦‚æœæ•°æ®ç‚¹å¤ªå¤šï¼Œè¿›è¡Œé‡‡æ ·
                if len(time_costs) > max_points_per_model:
                    step = len(time_costs) // max_points_per_model
                    sampled_costs = time_costs[::step][:max_points_per_model]
                else:
                    sampled_costs = time_costs
                
                for idx, time_cost in enumerate(sampled_costs):
                    scatter_data.append({
                        "model": model_name,
                        "x": idx,
                        "y": round(time_cost, 3),
                        "tokens": period_stats[TOTAL_TOK_BY_MODEL].get(model_name, 0) // len(time_costs) if time_costs else 0
                    })
            period_stats[SCATTER_CHART_RESPONSE_TIME] = scatter_data
            
            # 3. æ¨¡å‹æ•ˆç‡é›·è¾¾å›¾
            if model_names:
                # å–å‰5ä¸ªæœ€å¸¸ç”¨çš„æ¨¡å‹
                top_models = sorted(period_stats[REQ_CNT_BY_MODEL].items(), key=lambda x: x[1], reverse=True)[:5]
                radar_data = []
                for model_name, _ in top_models:
                    # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡åˆ°0-100
                    req_count = period_stats[REQ_CNT_BY_MODEL].get(model_name, 0)
                    tps = period_stats[TPS_BY_MODEL].get(model_name, 0)
                    avg_time = period_stats[AVG_TIME_COST_BY_MODEL].get(model_name, 0)
                    cost_per_ktok = period_stats[COST_PER_KTOK_BY_MODEL].get(model_name, 0)
                    avg_tokens = period_stats[AVG_TOK_BY_MODEL].get(model_name, 0)
                    
                    # ç®€å•çš„å½’ä¸€åŒ–ï¼ˆåå‘å½’ä¸€åŒ–æ—¶é—´å’Œæˆæœ¬ï¼Œå€¼è¶Šå°è¶Šå¥½ï¼‰
                    max_req = max([period_stats[REQ_CNT_BY_MODEL].get(m[0], 1) for m in top_models])
                    max_tps = max([period_stats[TPS_BY_MODEL].get(m[0], 1) for m in top_models])
                    max_time = max([period_stats[AVG_TIME_COST_BY_MODEL].get(m[0], 0.1) for m in top_models])
                    max_cost = max([period_stats[COST_PER_KTOK_BY_MODEL].get(m[0], 0.001) for m in top_models])
                    max_tokens = max([period_stats[AVG_TOK_BY_MODEL].get(m[0], 1) for m in top_models])
                    
                    radar_data.append({
                        "model": model_name,
                        "metrics": [
                            round((req_count / max_req) * 100, 2) if max_req > 0 else 0,  # è¯·æ±‚é‡
                            round((tps / max_tps) * 100, 2) if max_tps > 0 else 0,  # TPS
                            round((1 - avg_time / max_time) * 100, 2) if max_time > 0 else 100,  # é€Ÿåº¦(åå‘)
                            round((1 - cost_per_ktok / max_cost) * 100, 2) if max_cost > 0 else 100,  # æˆæœ¬æ•ˆç›Š(åå‘)
                            round((avg_tokens / max_tokens) * 100, 2) if max_tokens > 0 else 0,  # Tokenå®¹é‡
                        ]
                    })
                period_stats[RADAR_CHART_MODEL_EFFICIENCY] = {
                    "labels": ["è¯·æ±‚é‡", "TPS", "å“åº”é€Ÿåº¦", "æˆæœ¬æ•ˆç›Š", "Tokenå®¹é‡"],
                    "datasets": radar_data
                }
            
            # 4. ä¾›åº”å•†è¯·æ±‚å æ¯”ç¯å½¢å›¾
            provider_requests = period_stats[REQ_CNT_BY_PROVIDER]
            if provider_requests:
                sorted_provider_reqs = sorted(provider_requests.items(), key=lambda item: item[1], reverse=True)
                period_stats[DOUGHNUT_CHART_PROVIDER_REQUESTS] = {
                    "labels": [item[0] for item in sorted_provider_reqs],
                    "data": [item[1] for item in sorted_provider_reqs],
                }
            
            # 5. å¹³å‡å“åº”æ—¶é—´æ¡å½¢å›¾
            if model_names:
                sorted_by_time = sorted(
                    [(m, period_stats[AVG_TIME_COST_BY_MODEL].get(m, 0)) for m in model_names],
                    key=lambda x: x[1],
                    reverse=True
                )
                period_stats[BAR_CHART_AVG_RESPONSE_TIME] = {
                    "labels": [item[0] for item in sorted_by_time],
                    "data": [round(item[1], 3) for item in sorted_by_time],
                }
        return stats

    @staticmethod
    async def _collect_online_time_for_period(
        collect_period: list[tuple[str, datetime]], now: datetime
    ) -> dict[str, Any]:
        """
        æ”¶é›†æŒ‡å®šæ—¶é—´æ®µçš„åœ¨çº¿æ—¶é—´ç»Ÿè®¡æ•°æ®

        :param collect_period: ç»Ÿè®¡æ—¶é—´æ®µ
        """
        if not collect_period:
            return {}

        collect_period.sort(key=lambda x: x[1], reverse=True)

        stats = {
            period_key: {
                ONLINE_TIME: 0.0,
            }
            for period_key, _ in collect_period
        }

        query_start_time = collect_period[-1][1]
        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢
        query_builder = (
            QueryBuilder(OnlineTime)
            .no_cache()
            .filter(end_timestamp__gte=query_start_time)
            .order_by("-end_timestamp")
        )

        async for batch in query_builder.iter_batches(batch_size=STAT_BATCH_SIZE, as_dict=True):
            for record in batch:
                if not isinstance(record, dict):
                    continue

                record_end_timestamp = record.get("end_timestamp")
                if isinstance(record_end_timestamp, str):
                    record_end_timestamp = datetime.fromisoformat(record_end_timestamp)

                record_start_timestamp = record.get("start_timestamp")
                if isinstance(record_start_timestamp, str):
                    record_start_timestamp = datetime.fromisoformat(record_start_timestamp)

                if not record_end_timestamp or not record_start_timestamp:
                    continue

                for boundary_idx, (_, period_boundary_start) in enumerate(collect_period):
                    if record_end_timestamp >= period_boundary_start:
                        # Calculate effective end time for this record in relation to 'now'
                        effective_end_time = min(record_end_timestamp, now)

                        for period_key, current_period_start_time in collect_period[boundary_idx:]:
                            # Determine the portion of the record that falls within this specific statistical period
                            overlap_start = max(record_start_timestamp, current_period_start_time)
                            overlap_end = effective_end_time  # Already capped by 'now' and record's own end

                            if overlap_end > overlap_start:
                                stats[period_key][ONLINE_TIME] += (overlap_end - overlap_start).total_seconds()
                        break
            
            # æ¯æ‰¹å¤„ç†å®Œåè®©å‡ºæ§åˆ¶æƒ
            await asyncio.sleep(0)

        return stats

    async def _collect_message_count_for_period(self, collect_period: list[tuple[str, datetime]]) -> dict[str, Any]:
        """
        æ”¶é›†æŒ‡å®šæ—¶é—´æ®µçš„æ¶ˆæ¯ç»Ÿè®¡æ•°æ®

        :param collect_period: ç»Ÿè®¡æ—¶é—´æ®µ
        """
        if not collect_period:
            return {}

        collect_period.sort(key=lambda x: x[1], reverse=True)

        stats = {
            period_key: {
                TOTAL_MSG_CNT: 0,
                MSG_CNT_BY_CHAT: defaultdict(int),
            }
            for period_key, _ in collect_period
        }

        query_start_timestamp = collect_period[-1][1].timestamp()  # Messages.time is a DoubleField (timestamp)
        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢
        query_builder = (
            QueryBuilder(Messages)
            .no_cache()
            .filter(time__gte=query_start_timestamp)
            .order_by("-time")
        )

        total_processed = 0
        async for batch in query_builder.iter_batches(batch_size=STAT_BATCH_SIZE, as_dict=True):
            for message in batch:
                if total_processed >= STAT_MAX_RECORDS:
                    logger.warning(f"æ¶ˆæ¯ç»Ÿè®¡å¤„ç†è®°å½•æ•°è¾¾åˆ°ä¸Šé™ {STAT_MAX_RECORDS}ï¼Œè·³è¿‡å‰©ä½™è®°å½•")
                    break
                    
                if not isinstance(message, dict):
                    continue
                message_time_ts = message.get("time")  # This is a float timestamp

                if not message_time_ts:
                    continue

                chat_id = None
                chat_name = None

                # Logic based on SQLAlchemy model structure, aiming to replicate original intent
                if message.get("chat_info_group_id"):
                    chat_id = f"g{message['chat_info_group_id']}"
                    chat_name = message.get("chat_info_group_name") or f"ç¾¤{message['chat_info_group_id']}"
                elif message.get("user_id"):  # Fallback to sender's info for chat_id if not a group_info based chat
                    # This uses the message SENDER's ID as per original logic's fallback
                    chat_id = f"u{message['user_id']}"  # SENDER's user_id
                    chat_name = message.get("user_nickname")  # SENDER's nickname
                else:
                    # If neither group_id nor sender_id is available for chat identification
                    continue

                if not chat_id:  # Should not happen if above logic is correct
                    continue

                # Update name_mapping
                if chat_name:
                    if chat_id in self.name_mapping:
                        if chat_name != self.name_mapping[chat_id][0] and message_time_ts > self.name_mapping[chat_id][1]:
                            self.name_mapping[chat_id] = (chat_name, message_time_ts)
                    else:
                        self.name_mapping[chat_id] = (chat_name, message_time_ts)
                for period_idx, (_, period_start_dt) in enumerate(collect_period):
                    if message_time_ts >= period_start_dt.timestamp():
                        for period_key, _ in collect_period[period_idx:]:
                            stats[period_key][TOTAL_MSG_CNT] += 1
                            stats[period_key][MSG_CNT_BY_CHAT][chat_id] += 1
                        break

                total_processed += 1
                if total_processed % 500 == 0:
                    await StatisticOutputTask._yield_control(total_processed, interval=1)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™
            if total_processed >= STAT_MAX_RECORDS:
                break
            
            # æ¯æ‰¹å¤„ç†å®Œåè®©å‡ºæ§åˆ¶æƒ
            await asyncio.sleep(0)

        return stats

    async def _collect_all_statistics(self, now: datetime) -> dict[str, dict[str, Any]]:
        """
        æ”¶é›†å„æ—¶é—´æ®µçš„ç»Ÿè®¡æ•°æ®
        :param now: åŸºå‡†å½“å‰æ—¶é—´
        """

        last_all_time_stat = None

        if "last_full_statistics" in local_storage:
            # å¦‚æœå­˜åœ¨ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ®ï¼Œåˆ™ä½¿ç”¨è¯¥æ•°æ®è¿›è¡Œå¢é‡ç»Ÿè®¡
            last_stat: dict[str, Any] = local_storage["last_full_statistics"]  # ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ® # type: ignore

            self.name_mapping = last_stat["name_mapping"]  # ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ®çš„åç§°æ˜ å°„
            last_all_time_stat = last_stat["stat_data"]  # ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡çš„ç»Ÿè®¡æ•°æ®
            last_stat_timestamp = datetime.fromtimestamp(last_stat["timestamp"])  # ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ®çš„æ—¶é—´æˆ³
            self.stat_period = [item for item in self.stat_period if item[0] != "all_time"]  # åˆ é™¤"æ‰€æœ‰æ—¶é—´"çš„ç»Ÿè®¡æ—¶æ®µ
            self.stat_period.append(("all_time", now - last_stat_timestamp, "è‡ªéƒ¨ç½²ä»¥æ¥çš„"))

        stat_start_timestamp = [(period[0], now - period[1]) for period in self.stat_period]

        stat = {item[0]: {} for item in self.stat_period}

        model_req_stat, online_time_stat, message_count_stat = await asyncio.gather(
            self._collect_model_request_for_period(stat_start_timestamp),
            self._collect_online_time_for_period(stat_start_timestamp, now),
            self._collect_message_count_for_period(stat_start_timestamp),
        )

        # ç»Ÿè®¡æ•°æ®åˆå¹¶
        # åˆå¹¶ä¸‰ç±»ç»Ÿè®¡æ•°æ®
        for period_key, _ in stat_start_timestamp:
            stat[period_key].update(model_req_stat.get(period_key, {}))
            stat[period_key].update(online_time_stat.get(period_key, {}))
            stat[period_key].update(message_count_stat.get(period_key, {}))
        if last_all_time_stat:
            # è‹¥å­˜åœ¨ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ®ï¼Œåˆ™å°†å…¶ä¸å½“å‰ç»Ÿè®¡æ•°æ®åˆå¹¶
            for key, val in last_all_time_stat.items():
                # If a key from old stats is not in the current period's stats, it means no new data was generated.
                # In this case, we carry over the old data.
                if key not in stat["all_time"]:
                    stat["all_time"][key] = val
                    continue

                # If the key exists in both, we merge.
                if isinstance(val, dict):
                    # It's a dictionary-like object (e.g., COST_BY_MODEL, TIME_COST_BY_TYPE)
                    current_dict = stat["all_time"][key]
                    for sub_key, sub_val in val.items():
                        if sub_key in current_dict:
                            current_val = current_dict[sub_key]
                            # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šå¤„ç†å‹ç¼©æ ¼å¼çš„ TIME_COST æ•°æ®
                            if isinstance(sub_val, dict) and "sum" in sub_val and "count" in sub_val:
                                # å‹ç¼©æ ¼å¼åˆå¹¶
                                if isinstance(current_val, dict) and "sum" in current_val:
                                    # ä¸¤è¾¹éƒ½æ˜¯å‹ç¼©æ ¼å¼
                                    current_dict[sub_key] = {
                                        "sum": current_val["sum"] + sub_val["sum"],
                                        "count": current_val["count"] + sub_val["count"],
                                        "sum_sq": current_val.get("sum_sq", 0) + sub_val.get("sum_sq", 0),
                                    }
                                elif isinstance(current_val, list):
                                    # å½“å‰æ˜¯åˆ—è¡¨ï¼Œå†å²æ˜¯å‹ç¼©æ ¼å¼ï¼šå…ˆå‹ç¼©å½“å‰å†åˆå¹¶
                                    curr_sum = sum(current_val) if current_val else 0
                                    curr_count = len(current_val)
                                    curr_sum_sq = sum(v * v for v in current_val) if current_val else 0
                                    current_dict[sub_key] = {
                                        "sum": curr_sum + sub_val["sum"],
                                        "count": curr_count + sub_val["count"],
                                        "sum_sq": curr_sum_sq + sub_val.get("sum_sq", 0),
                                    }
                                else:
                                    # æœªçŸ¥æƒ…å†µï¼Œä¿ç•™å†å²å€¼
                                    current_dict[sub_key] = sub_val
                            elif isinstance(sub_val, list):
                                # åˆ—è¡¨æ ¼å¼ï¼šextendï¼ˆå…¼å®¹æ—§æ•°æ®ï¼Œä½†æ–°ç‰ˆä¸ä¼šäº§ç”Ÿè¿™ç§æƒ…å†µï¼‰
                                if isinstance(current_val, list):
                                    current_dict[sub_key] = current_val + sub_val
                                else:
                                    current_dict[sub_key] = sub_val
                            else:
                                # æ•°å€¼ç±»å‹ï¼šç›´æ¥ç›¸åŠ 
                                current_dict[sub_key] += sub_val
                        else:
                            current_dict[sub_key] = sub_val
                else:
                    # It's a simple value (e.g., TOTAL_COST)
                    stat["all_time"][key] += val

        # æ›´æ–°ä¸Šæ¬¡å®Œæ•´ç»Ÿè®¡æ•°æ®çš„æ—¶é—´æˆ³
        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šåœ¨ä¿å­˜å‰å‹ç¼© TIME_COST åˆ—è¡¨ä¸ºèšåˆæ•°æ®ï¼Œé¿å…æ— é™å¢é•¿
        compressed_stat_data = self._compress_time_cost_lists(stat["all_time"])
        # å°†æ‰€æœ‰defaultdictè½¬æ¢ä¸ºæ™®é€šdictä»¥é¿å…ç±»å‹å†²çª
        clean_stat_data = self._convert_defaultdict_to_dict(compressed_stat_data)
        local_storage["last_full_statistics"] = {
            "name_mapping": self.name_mapping,
            "stat_data": clean_stat_data,
            "timestamp": now.timestamp(),
        }

        return stat

    def _compress_time_cost_lists(self, data: dict[str, Any]) -> dict[str, Any]:
        """ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šå°† TIME_COST_BY_* çš„ list å‹ç¼©ä¸ºèšåˆæ•°æ®
        
        åŸå§‹æ ¼å¼: {"model_a": [1.2, 2.3, 3.4, ...]}  (å¯èƒ½æ— é™å¢é•¿)
        å‹ç¼©æ ¼å¼: {"model_a": {"sum": 6.9, "count": 3, "sum_sq": 18.29}}
        
        è¿™æ ·åˆå¹¶æ—¶åªéœ€è¦ç´¯åŠ  sum/count/sum_sqï¼Œä¸ä¼šæ— é™å¢é•¿ã€‚
        avg = sum / count
        std = sqrt(sum_sq / count - (sum / count)^2)
        """
        # TIME_COST ç›¸å…³çš„ key å‰ç¼€
        time_cost_keys = [
            TIME_COST_BY_TYPE, TIME_COST_BY_USER, TIME_COST_BY_MODEL,
            TIME_COST_BY_MODULE, TIME_COST_BY_PROVIDER
        ]
        
        result = dict(data)  # æµ…æ‹·è´
        
        for key in time_cost_keys:
            if key not in result:
                continue
            
            original = result[key]
            if not isinstance(original, dict):
                continue
            
            compressed = {}
            for sub_key, values in original.items():
                if isinstance(values, list):
                    # åŸå§‹åˆ—è¡¨æ ¼å¼ï¼Œéœ€è¦å‹ç¼©
                    if values:
                        total = sum(values)
                        count = len(values)
                        sum_sq = sum(v * v for v in values)
                        compressed[sub_key] = {"sum": total, "count": count, "sum_sq": sum_sq}
                    else:
                        compressed[sub_key] = {"sum": 0.0, "count": 0, "sum_sq": 0.0}
                elif isinstance(values, dict) and "sum" in values and "count" in values:
                    # å·²ç»æ˜¯å‹ç¼©æ ¼å¼ï¼Œç›´æ¥ä¿ç•™
                    compressed[sub_key] = values
                else:
                    # æœªçŸ¥æ ¼å¼ï¼Œä¿ç•™åŸå€¼
                    compressed[sub_key] = values
            
            result[key] = compressed
        
        return result

    def _convert_defaultdict_to_dict(self, data):
        # sourcery skip: dict-comprehension, extract-duplicate-method, inline-immediately-returned-variable, merge-duplicate-blocks
        """é€’å½’è½¬æ¢defaultdictä¸ºæ™®é€šdict"""
        if isinstance(data, defaultdict):
            # è½¬æ¢defaultdictä¸ºæ™®é€šdict
            result = {}
            for key, value in data.items():
                result[key] = self._convert_defaultdict_to_dict(value)
            return result
        elif isinstance(data, dict):
            # é€’å½’å¤„ç†æ™®é€šdict
            result = {}
            for key, value in data.items():
                result[key] = self._convert_defaultdict_to_dict(value)
            return result
        else:
            # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
            return data

    # -- ä»¥ä¸‹ä¸ºç»Ÿè®¡æ•°æ®æ ¼å¼åŒ–æ–¹æ³• --

    @staticmethod
    def _format_total_stat(stats: dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ€»ç»Ÿè®¡æ•°æ®
        """

        output = [
            f"æ€»åœ¨çº¿æ—¶é—´: {_format_online_time(stats.get(ONLINE_TIME, 0))}",
            f"æ€»æ¶ˆæ¯æ•°: {stats.get(TOTAL_MSG_CNT, 0)}",
            f"æ€»è¯·æ±‚æ•°: {stats.get(TOTAL_REQ_CNT, 0)}",
            f"æ€»èŠ±è´¹: {stats.get(TOTAL_COST, 0.0):.4f}Â¥",
            "",
        ]

        return "\n".join(output)

    @staticmethod
    def _format_model_classified_stat(stats: dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æŒ‰æ¨¡å‹åˆ†ç±»çš„ç»Ÿè®¡æ•°æ®
        """
        if stats.get(TOTAL_REQ_CNT, 0) <= 0:
            return ""
        data_fmt = "{:<32}  {:>10}  {:>12}  {:>12}  {:>12}  {:>9.4f}Â¥  {:>10}  {:>10}"

        output = [
            " æ¨¡å‹åç§°                          è°ƒç”¨æ¬¡æ•°    è¾“å…¥Token     è¾“å‡ºToken     Tokenæ€»é‡     ç´¯è®¡èŠ±è´¹    å¹³å‡è€—æ—¶(ç§’)  æ ‡å‡†å·®(ç§’)",
        ]
        for model_name, count in sorted(stats[REQ_CNT_BY_MODEL].items()):
            name = f"{model_name[:29]}..." if len(model_name) > 32 else model_name
            in_tokens = stats[IN_TOK_BY_MODEL][model_name]
            out_tokens = stats[OUT_TOK_BY_MODEL][model_name]
            tokens = stats[TOTAL_TOK_BY_MODEL][model_name]
            cost = stats[COST_BY_MODEL][model_name]
            avg_time_cost = stats[AVG_TIME_COST_BY_MODEL][model_name]
            std_time_cost = stats[STD_TIME_COST_BY_MODEL][model_name]
            output.append(
                data_fmt.format(name, count, in_tokens, out_tokens, tokens, cost, avg_time_cost, std_time_cost)
            )

        output.append("")
        return "\n".join(output)

    def _format_chat_stat(self, stats: dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–èŠå¤©ç»Ÿè®¡æ•°æ®
        """
        if stats.get(TOTAL_MSG_CNT, 0) <= 0:
            return ""
        output = ["èŠå¤©æ¶ˆæ¯ç»Ÿè®¡:", " è”ç³»äºº/ç¾¤ç»„åç§°                  æ¶ˆæ¯æ•°é‡"]
        output.extend(
            f"{self.name_mapping.get(chat_id, (chat_id, 0))[0][:32]:<32}  {count:>10}"
            for chat_id, count in sorted(stats.get(MSG_CNT_BY_CHAT, {}).items())
        )
        output.append("")
        return "\n".join(output)

    async def _collect_chart_data(self, stat: dict[str, Any]) -> dict:
        """ç”Ÿæˆå›¾è¡¨æ•°æ® (å¼‚æ­¥)"""
        now = datetime.now()
        chart_data: dict[str, Any] = {}

        time_ranges = [
            ("6h", 6, 10),
            ("12h", 12, 15),
            ("24h", 24, 15),
            ("48h", 48, 30),
        ]

        # ä¾æ¬¡å¤„ç†ï¼ˆæ•°æ®é‡ä¸å¤§ï¼Œé¿å…å¤æ‚åº¦ï¼›å¦‚éœ€å¯æ”¹ gatherï¼‰
        for range_key, hours, interval_minutes in time_ranges:
            chart_data[range_key] = await self._collect_interval_data(now, hours, interval_minutes)
        return chart_data

    async def _collect_interval_data(self, now: datetime, hours: int, interval_minutes: int) -> dict:
        start_time = now - timedelta(hours=hours)
        time_points: list[datetime] = []
        current_time = start_time
        while current_time <= now:
            time_points.append(current_time)
            current_time += timedelta(minutes=interval_minutes)

        total_cost_data = [0.0] * len(time_points)
        cost_by_model: dict[str, list[float]] = {}
        cost_by_module: dict[str, list[float]] = {}
        message_by_chat: dict[str, list[int]] = {}
        time_labels = [t.strftime("%H:%M") for t in time_points]
        interval_seconds = interval_minutes * 60

        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢ LLMUsage
        llm_query_builder = (
            QueryBuilder(LLMUsage)
            .no_cache()
            .filter(timestamp__gte=start_time)
            .order_by("-timestamp")
        )
        
        async for batch in llm_query_builder.iter_batches(batch_size=STAT_BATCH_SIZE, as_dict=True):
            for record in batch:
                if not isinstance(record, dict) or not record.get("timestamp"):
                    continue
                record_time = record["timestamp"]
                if isinstance(record_time, str):
                    try:
                        record_time = datetime.fromisoformat(record_time)
                    except Exception:
                        continue
                time_diff = (record_time - start_time).total_seconds()
                idx = int(time_diff // interval_seconds)
                if 0 <= idx < len(time_points):
                    cost = record.get("cost") or 0.0
                    total_cost_data[idx] += cost
                    model_name = record.get("model_name") or "unknown"
                    if model_name not in cost_by_model:
                        cost_by_model[model_name] = [0.0] * len(time_points)
                    cost_by_model[model_name][idx] += cost
                    request_type = record.get("request_type") or "unknown"
                    module_name = request_type.split(".")[0] if "." in request_type else request_type
                    if module_name not in cost_by_module:
                        cost_by_module[module_name] = [0.0] * len(time_points)
                    cost_by_module[module_name][idx] += cost
            
            await asyncio.sleep(0)

        # ğŸ”§ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢ Messages
        msg_query_builder = (
            QueryBuilder(Messages)
            .no_cache()
            .filter(time__gte=start_time.timestamp())
            .order_by("-time")
        )
        
        async for batch in msg_query_builder.iter_batches(batch_size=STAT_BATCH_SIZE, as_dict=True):
            for msg in batch:
                if not isinstance(msg, dict) or not msg.get("time"):
                    continue
                msg_ts = msg["time"]
                time_diff = msg_ts - start_time.timestamp()
                idx = int(time_diff // interval_seconds)
                if 0 <= idx < len(time_points):
                    chat_id = None
                    if msg.get("chat_info_group_id"):
                        chat_id = f"g{msg['chat_info_group_id']}"
                    elif msg.get("user_id"):
                        chat_id = f"u{msg['user_id']}"

                    if chat_id:
                        chat_name = self.name_mapping.get(chat_id, (chat_id, 0))[0]
                        if chat_name not in message_by_chat:
                            message_by_chat[chat_name] = [0] * len(time_points)
                        message_by_chat[chat_name][idx] += 1
            
            await asyncio.sleep(0)

        return {
            "time_labels": time_labels,
            "total_cost_data": total_cost_data,
            "cost_by_model": cost_by_model,
            "cost_by_module": cost_by_module,
            "message_by_chat": message_by_chat,
        }
