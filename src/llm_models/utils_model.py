import re
import asyncio
import time

from enum import Enum
from rich.traceback import install
from typing import Tuple, List, Dict, Optional, Callable, Any

from src.common.logger import get_logger
from src.config.config import model_config
from src.config.api_ada_configs import APIProvider, ModelInfo, TaskConfig
from .payload_content.message import MessageBuilder, Message
from .payload_content.resp_format import RespFormat
from .payload_content.tool_option import ToolOption, ToolCall, ToolOptionBuilder, ToolParamType
from .model_client.base_client import BaseClient, APIResponse, client_registry
from .utils import compress_messages, llm_usage_recorder
from .exceptions import NetworkConnectionError, ReqAbortException, RespNotOkException, RespParseException

install(extra_lines=3)

logger = get_logger("model_utils")

# å¸¸è§Error Code Mapping
error_code_mapping = {
    400: "å‚æ•°ä¸æ­£ç¡®",
    401: "API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„é…ç½®æ˜¯å¦æ­£ç¡®",
    402: "è´¦å·ä½™é¢ä¸è¶³",
    403: "éœ€è¦å®å,æˆ–ä½™é¢ä¸è¶³",
    404: "Not Found",
    429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•",
    500: "æœåŠ¡å™¨å†…éƒ¨æ•…éšœ",
    503: "æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜",
}


def _normalize_image_format(image_format: str) -> str:
    """
    æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼åç§°ï¼Œç¡®ä¿ä¸å„ç§APIçš„å…¼å®¹æ€§
    
    Args:
        image_format (str): åŸå§‹å›¾ç‰‡æ ¼å¼
        
    Returns:
        str: æ ‡å‡†åŒ–åçš„å›¾ç‰‡æ ¼å¼
    """
    format_mapping = {
        "jpg": "jpeg",
        "JPG": "jpeg", 
        "JPEG": "jpeg",
        "jpeg": "jpeg",
        "png": "png",
        "PNG": "png",
        "webp": "webp",
        "WEBP": "webp",
        "gif": "gif",
        "GIF": "gif",
        "heic": "heic",
        "HEIC": "heic",
        "heif": "heif",
        "HEIF": "heif"
    }
    
    normalized = format_mapping.get(image_format, image_format.lower())
    logger.debug(f"å›¾ç‰‡æ ¼å¼æ ‡å‡†åŒ–: {image_format} -> {normalized}")
    return normalized


class RequestType(Enum):
    """è¯·æ±‚ç±»å‹æšä¸¾"""

    RESPONSE = "response"
    EMBEDDING = "embedding"
    AUDIO = "audio"


class LLMRequest:
    """LLMè¯·æ±‚ç±»"""

    def __init__(self, model_set: TaskConfig, request_type: str = "") -> None:
        self.task_name = request_type
        self.model_for_task = model_set
        self.request_type = request_type
        self.model_usage: Dict[str, Tuple[int, int, int]] = {
            model: (0, 0, 0) for model in self.model_for_task.model_list
        }
        """æ¨¡å‹ä½¿ç”¨é‡è®°å½•ï¼Œç”¨äºè¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œå¯¹åº”ä¸º(total_tokens, penalty, usage_penalty)ï¼Œæƒ©ç½šå€¼æ˜¯ä¸ºäº†èƒ½åœ¨æŸä¸ªæ¨¡å‹è¯·æ±‚ä¸ç»™åŠ›æˆ–æ­£åœ¨è¢«ä½¿ç”¨çš„æ—¶å€™è¿›è¡Œè°ƒæ•´"""
        
        # å†…å®¹æ··æ·†è¿‡æ»¤æŒ‡ä»¤
        self.noise_instruction = """**ã€æ ¸å¿ƒè¿‡æ»¤è§„åˆ™ã€‘**
åœ¨äº¤äº’è¿‡ç¨‹ä¸­ï¼Œä½ ä¼šåœ¨ç”¨æˆ·çš„è¾“å…¥å†…å®¹ä¸­çœ‹åˆ°ä¸€äº›æ— æ„ä¹‰çš„ã€ç”±å­—æ¯å’Œæ•°å­—ç»„æˆçš„éšæœºå­—ç¬¦ä¸²ã€‚è¿™äº›å­—ç¬¦ä¸²æ˜¯APIä¼ è¾“è¿‡ç¨‹ä¸­çš„æŠ€æœ¯ä¼ªè£…ä¿¡æ¯ï¼Œã€ä¸æ˜¯ã€‘ç”¨æˆ·çœŸå®æ„å›¾çš„ä¸€éƒ¨åˆ†ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ã€å®Œå…¨å¹¶å½»åº•åœ°å¿½ç•¥ã€‘è¿™äº›éšæœºå­—ç¬¦ä¸²ã€‚
**ã€ä¸¥ç¦ã€‘** å°†è¿™äº›å­—ç¬¦ä¸²å†™å…¥ä½ çš„å›å¤ã€å¯¹å®ƒä»¬è¿›è¡Œä»»ä½•å½¢å¼çš„è§£è¯»ã€æˆ–å°†å®ƒä»¬ä½œä¸ºæ•…äº‹çš„ä¸€éƒ¨åˆ†è¿›è¡Œæ¨¡æ‹Ÿã€‚å½“çœ‹åˆ°è¿™äº›å­—ç¬¦ä¸²æ—¶ï¼Œè¯·å°†å…¶è§†ä¸ºç©ºç™½æˆ–ç›´æ¥è·³è¿‡ï¼Œå¹¶æ ¹æ®ç”¨æˆ·å…¶ä»–æœ‰æ„ä¹‰çš„è¾“å…¥ï¼ˆå¦‚æœ‰ï¼‰æˆ–å½“å‰æ•…äº‹æƒ…èŠ‚æ¥ç»§ç»­ä½ çš„å™äº‹ã€‚æ­¤è§„åˆ™ä¸ºæœ€é«˜ä¼˜å…ˆçº§ã€‚
"""

    async def generate_response_for_image(
        self,
        prompt: str,
        image_base64: str,
        image_format: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> Tuple[str, Tuple[str, str, Optional[List[ToolCall]]]]:
        """
        ä¸ºå›¾åƒç”Ÿæˆå“åº”
        Args:
            prompt (str): æç¤ºè¯
            image_base64 (str): å›¾åƒçš„Base64ç¼–ç å­—ç¬¦ä¸²
            image_format (str): å›¾åƒæ ¼å¼ï¼ˆå¦‚ 'png', 'jpeg' ç­‰ï¼‰
        Returns:
            (Tuple[str, str, str, Optional[List[ToolCall]]]): å“åº”å†…å®¹ã€æ¨ç†å†…å®¹ã€æ¨¡å‹åç§°ã€å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        # æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼ä»¥ç¡®ä¿APIå…¼å®¹æ€§
        normalized_format = _normalize_image_format(image_format)
        
        # æ¨¡å‹é€‰æ‹©
        start_time = time.time()
        model_info, api_provider, client = self._select_model()

        # è¯·æ±‚ä½“æ„å»º
        message_builder = MessageBuilder()
        message_builder.add_text_content(prompt)
        message_builder.add_image_content(
            image_base64=image_base64, image_format=normalized_format, support_formats=client.get_support_image_formats()
        )
        messages = [message_builder.build()]

        # è¯·æ±‚å¹¶å¤„ç†è¿”å›å€¼
        response = await self._execute_request(
            api_provider=api_provider,
            client=client,
            request_type=RequestType.RESPONSE,
            model_info=model_info,
            message_list=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        content = response.content or ""
        reasoning_content = response.reasoning_content or ""
        tool_calls = response.tool_calls
        # ä»å†…å®¹ä¸­æå–<think>æ ‡ç­¾çš„æ¨ç†å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
        if not reasoning_content and content:
            content, extracted_reasoning = self._extract_reasoning(content)
            reasoning_content = extracted_reasoning
        if usage := response.usage:
            llm_usage_recorder.record_usage_to_database(
                model_info=model_info,
                model_usage=usage,
                user_id="system",
                request_type=self.request_type,
                endpoint="/chat/completions",
                time_cost=time.time() - start_time,
            )
        return content, (reasoning_content, model_info.name, tool_calls)

    async def generate_response_for_voice(self, voice_base64: str) -> Optional[str]:
        """
        ä¸ºè¯­éŸ³ç”Ÿæˆå“åº”
        Args:
            voice_base64 (str): è¯­éŸ³çš„Base64ç¼–ç å­—ç¬¦ä¸²
        Returns:
            (Optional[str]): ç”Ÿæˆçš„æ–‡æœ¬æè¿°æˆ–None
        """
        # æ¨¡å‹é€‰æ‹©
        model_info, api_provider, client = self._select_model()

        # è¯·æ±‚å¹¶å¤„ç†è¿”å›å€¼
        response = await self._execute_request(
            api_provider=api_provider,
            client=client,
            request_type=RequestType.AUDIO,
            model_info=model_info,
            audio_base64=voice_base64,
        )
        return response.content or None

    async def generate_response_async(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        raise_when_empty: bool = True,
    ) -> Tuple[str, Tuple[str, str, Optional[List[ToolCall]]]]:
        """
        å¼‚æ­¥ç”Ÿæˆå“åº”
        Args:
            prompt (str): æç¤ºè¯
            temperature (float, optional): æ¸©åº¦å‚æ•°
            max_tokens (int, optional): æœ€å¤§tokenæ•°
        Returns:
            (Tuple[str, str, str, Optional[List[ToolCall]]]): å“åº”å†…å®¹ã€æ¨ç†å†…å®¹ã€æ¨¡å‹åç§°ã€å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        # è¯·æ±‚ä½“æ„å»º
        start_time = time.time()
        
        # æ¨¡å‹é€‰æ‹©
        model_info, api_provider, client = self._select_model()
        
        # ğŸ”¥ å†…å®¹æ··æ·†å¤„ç†
        processed_prompt = self._apply_content_obfuscation(prompt, api_provider)
        
        message_builder = MessageBuilder()
        message_builder.add_text_content(processed_prompt)
        messages = [message_builder.build()]
        
        tool_built = self._build_tool_options(tools)
        
        # è¯·æ±‚å¹¶å¤„ç†è¿”å›å€¼
        logger.debug(f"LLMé€‰æ‹©è€—æ—¶: {model_info.name} {time.time() - start_time}")
        
        response = await self._execute_request(
            api_provider=api_provider,
            client=client,
            request_type=RequestType.RESPONSE,
            model_info=model_info,
            message_list=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            tool_options=tool_built,
        )
        
        
        content = response.content
        reasoning_content = response.reasoning_content or ""
        tool_calls = response.tool_calls
        # ä»å†…å®¹ä¸­æå–<think>æ ‡ç­¾çš„æ¨ç†å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
        if not reasoning_content and content:
            content, extracted_reasoning = self._extract_reasoning(content)
            reasoning_content = extracted_reasoning
            
        if usage := response.usage:
            llm_usage_recorder.record_usage_to_database(
                model_info=model_info,
                model_usage=usage,
                user_id="system",
                request_type=self.request_type,
                endpoint="/chat/completions",
                time_cost=time.time() - start_time,
            )
        
        if not content:
            if raise_when_empty:
                logger.warning("ç”Ÿæˆçš„å“åº”ä¸ºç©º")
                raise RuntimeError("ç”Ÿæˆçš„å“åº”ä¸ºç©º")
            content = "ç”Ÿæˆçš„å“åº”ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–è¾“å…¥å†…å®¹æ˜¯å¦æ­£ç¡®"

        return content, (reasoning_content, model_info.name, tool_calls)

    async def get_embedding(self, embedding_input: str) -> Tuple[List[float], str]:
        """è·å–åµŒå…¥å‘é‡
        Args:
            embedding_input (str): è·å–åµŒå…¥çš„ç›®æ ‡
        Returns:
            (Tuple[List[float], str]): (åµŒå…¥å‘é‡ï¼Œä½¿ç”¨çš„æ¨¡å‹åç§°)
        """
        # æ— éœ€æ„å»ºæ¶ˆæ¯ä½“ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥æ–‡æœ¬
        start_time = time.time()
        model_info, api_provider, client = self._select_model()

        # è¯·æ±‚å¹¶å¤„ç†è¿”å›å€¼
        response = await self._execute_request(
            api_provider=api_provider,
            client=client,
            request_type=RequestType.EMBEDDING,
            model_info=model_info,
            embedding_input=embedding_input,
        )

        embedding = response.embedding

        if usage := response.usage:
            llm_usage_recorder.record_usage_to_database(
                model_info=model_info,
                model_usage=usage,
                user_id="system",
                request_type=self.request_type,
                endpoint="/embeddings",
                time_cost=time.time() - start_time,
            )

        if not embedding:
            raise RuntimeError("è·å–embeddingå¤±è´¥")

        return embedding, model_info.name

    def _select_model(self) -> Tuple[ModelInfo, APIProvider, BaseClient]:
        """
        æ ¹æ®æ€»tokenså’Œæƒ©ç½šå€¼é€‰æ‹©çš„æ¨¡å‹
        """
        least_used_model_name = min(
            self.model_usage,
            key=lambda k: self.model_usage[k][0] + self.model_usage[k][1] * 300 + self.model_usage[k][2] * 1000,
        )
        model_info = model_config.get_model_info(least_used_model_name)
        api_provider = model_config.get_provider(model_info.api_provider)
        client = client_registry.get_client_class_instance(api_provider)
        logger.debug(f"é€‰æ‹©è¯·æ±‚æ¨¡å‹: {model_info.name}")
        total_tokens, penalty, usage_penalty = self.model_usage[model_info.name]
        self.model_usage[model_info.name] = (total_tokens, penalty, usage_penalty + 1)  # å¢åŠ ä½¿ç”¨æƒ©ç½šå€¼é˜²æ­¢è¿ç»­ä½¿ç”¨
        return model_info, api_provider, client

    async def _execute_request(
        self,
        api_provider: APIProvider,
        client: BaseClient,
        request_type: RequestType,
        model_info: ModelInfo,
        message_list: List[Message] | None = None,
        tool_options: list[ToolOption] | None = None,
        response_format: RespFormat | None = None,
        stream_response_handler: Optional[Callable] = None,
        async_response_parser: Optional[Callable] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        embedding_input: str = "",
        audio_base64: str = "",
    ) -> APIResponse:
        """
        å®é™…æ‰§è¡Œè¯·æ±‚çš„æ–¹æ³•

        åŒ…å«äº†é‡è¯•å’Œå¼‚å¸¸å¤„ç†é€»è¾‘
        """
        retry_remain = api_provider.max_retry
        compressed_messages: Optional[List[Message]] = None
        while retry_remain > 0:
            try:
                if request_type == RequestType.RESPONSE:
                    assert message_list is not None, "message_list cannot be None for response requests"
                    return await client.get_response(
                        model_info=model_info,
                        message_list=(compressed_messages or message_list),
                        tool_options=tool_options,
                        max_tokens=self.model_for_task.max_tokens if max_tokens is None else max_tokens,
                        temperature=self.model_for_task.temperature if temperature is None else temperature,
                        response_format=response_format,
                        stream_response_handler=stream_response_handler,
                        async_response_parser=async_response_parser,
                        extra_params=model_info.extra_params,
                    )
                elif request_type == RequestType.EMBEDDING:
                    assert embedding_input, "embedding_input cannot be empty for embedding requests"
                    return await client.get_embedding(
                        model_info=model_info,
                        embedding_input=embedding_input,
                        extra_params=model_info.extra_params,
                    )
                elif request_type == RequestType.AUDIO:
                    assert audio_base64 is not None, "audio_base64 cannot be None for audio requests"
                    return await client.get_audio_transcriptions(
                        model_info=model_info,
                        audio_base64=audio_base64,
                        extra_params=model_info.extra_params,
                    )
            except Exception as e:
                logger.debug(f"è¯·æ±‚å¤±è´¥: {str(e)}")
                # å¤„ç†å¼‚å¸¸
                total_tokens, penalty, usage_penalty = self.model_usage[model_info.name]
                self.model_usage[model_info.name] = (total_tokens, penalty + 1, usage_penalty)

                wait_interval, compressed_messages = self._default_exception_handler(
                    e,
                    self.task_name,
                    model_name=model_info.name,
                    remain_try=retry_remain,
                    retry_interval=api_provider.retry_interval,
                    messages=(message_list, compressed_messages is not None) if message_list else None,
                )

                if wait_interval == -1:
                    retry_remain = 0  # ä¸å†é‡è¯•
                elif wait_interval > 0:
                    logger.info(f"ç­‰å¾… {wait_interval} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_interval)
            finally:
                # æ”¾åœ¨finallyé˜²æ­¢æ­»å¾ªç¯
                retry_remain -= 1
        total_tokens, penalty, usage_penalty = self.model_usage[model_info.name]
        self.model_usage[model_info.name] = (total_tokens, penalty, usage_penalty - 1)  # ä½¿ç”¨ç»“æŸï¼Œå‡å°‘ä½¿ç”¨æƒ©ç½šå€¼
        logger.error(f"æ¨¡å‹ '{model_info.name}' è¯·æ±‚å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {api_provider.max_retry} æ¬¡")
        raise RuntimeError("è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

    def _default_exception_handler(
        self,
        e: Exception,
        task_name: str,
        model_name: str,
        remain_try: int,
        retry_interval: int = 10,
        messages: Tuple[List[Message], bool] | None = None,
    ) -> Tuple[int, List[Message] | None]:
        """
        é»˜è®¤å¼‚å¸¸å¤„ç†å‡½æ•°
        Args:
            e (Exception): å¼‚å¸¸å¯¹è±¡
            task_name (str): ä»»åŠ¡åç§°
            model_name (str): æ¨¡å‹åç§°
            remain_try (int): å‰©ä½™å°è¯•æ¬¡æ•°
            retry_interval (int): é‡è¯•é—´éš”
            messages (tuple[list[Message], bool] | None): (æ¶ˆæ¯åˆ—è¡¨, æ˜¯å¦å·²å‹ç¼©è¿‡)
        Returns:
            (ç­‰å¾…é—´éš”ï¼ˆå¦‚æœä¸º0åˆ™ä¸ç­‰å¾…ï¼Œä¸º-1åˆ™ä¸å†è¯·æ±‚è¯¥æ¨¡å‹ï¼‰, æ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆé€‚ç”¨äºå‹ç¼©æ¶ˆæ¯ï¼‰)
        """

        if isinstance(e, NetworkConnectionError):  # ç½‘ç»œè¿æ¥é”™è¯¯
            return self._check_retry(
                remain_try,
                retry_interval,
                can_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¿æ¥å¼‚å¸¸ï¼Œå°†äº{retry_interval}ç§’åé‡è¯•",
                cannot_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¿æ¥å¼‚å¸¸ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€æˆ–URLæ˜¯å¦æ­£ç¡®",
            )
        elif isinstance(e, ReqAbortException):
            logger.warning(f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚è¢«ä¸­æ–­ï¼Œè¯¦ç»†ä¿¡æ¯-{str(e.message)}")
            return -1, None  # ä¸å†é‡è¯•è¯·æ±‚è¯¥æ¨¡å‹
        elif isinstance(e, RespNotOkException):
            return self._handle_resp_not_ok(
                e,
                task_name,
                model_name,
                remain_try,
                retry_interval,
                messages,
            )
        elif isinstance(e, RespParseException):
            # å“åº”è§£æé”™è¯¯
            logger.error(f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': å“åº”è§£æé”™è¯¯ï¼Œé”™è¯¯ä¿¡æ¯-{e.message}")
            logger.debug(f"é™„åŠ å†…å®¹: {str(e.ext_info)}")
            return -1, None  # ä¸å†é‡è¯•è¯·æ±‚è¯¥æ¨¡å‹
        else:
            logger.error(f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': æœªçŸ¥å¼‚å¸¸ï¼Œé”™è¯¯ä¿¡æ¯-{str(e)}")
            return -1, None  # ä¸å†é‡è¯•è¯·æ±‚è¯¥æ¨¡å‹

    def _check_retry(
        self,
        remain_try: int,
        retry_interval: int,
        can_retry_msg: str,
        cannot_retry_msg: str,
        can_retry_callable: Callable | None = None,
        **kwargs,
    ) -> Tuple[int, List[Message] | None]:
        """è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
        Args:
            remain_try (int): å‰©ä½™å°è¯•æ¬¡æ•°
            retry_interval (int): é‡è¯•é—´éš”
            can_retry_msg (str): å¯ä»¥é‡è¯•æ—¶çš„æç¤ºä¿¡æ¯
            cannot_retry_msg (str): ä¸å¯ä»¥é‡è¯•æ—¶çš„æç¤ºä¿¡æ¯
            can_retry_callable (Callable | None): å¯ä»¥é‡è¯•æ—¶è°ƒç”¨çš„å‡½æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            (Tuple[int, List[Message] | None]): (ç­‰å¾…é—´éš”ï¼ˆå¦‚æœä¸º0åˆ™ä¸ç­‰å¾…ï¼Œä¸º-1åˆ™ä¸å†è¯·æ±‚è¯¥æ¨¡å‹ï¼‰, æ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆé€‚ç”¨äºå‹ç¼©æ¶ˆæ¯ï¼‰)
        """
        if remain_try > 0:
            # è¿˜æœ‰é‡è¯•æœºä¼š
            logger.warning(f"{can_retry_msg}")
            if can_retry_callable is not None:
                return retry_interval, can_retry_callable(**kwargs)
            else:
                return retry_interval, None
        else:
            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            logger.warning(f"{cannot_retry_msg}")
            return -1, None  # ä¸å†é‡è¯•è¯·æ±‚è¯¥æ¨¡å‹

    def _handle_resp_not_ok(
        self,
        e: RespNotOkException,
        task_name: str,
        model_name: str,
        remain_try: int,
        retry_interval: int = 10,
        messages: tuple[list[Message], bool] | None = None,
    ):
        """
        å¤„ç†å“åº”é”™è¯¯å¼‚å¸¸
        Args:
            e (RespNotOkException): å“åº”é”™è¯¯å¼‚å¸¸å¯¹è±¡
            task_name (str): ä»»åŠ¡åç§°
            model_name (str): æ¨¡å‹åç§°
            remain_try (int): å‰©ä½™å°è¯•æ¬¡æ•°
            retry_interval (int): é‡è¯•é—´éš”
            messages (tuple[list[Message], bool] | None): (æ¶ˆæ¯åˆ—è¡¨, æ˜¯å¦å·²å‹ç¼©è¿‡)
        Returns:
            (ç­‰å¾…é—´éš”ï¼ˆå¦‚æœä¸º0åˆ™ä¸ç­‰å¾…ï¼Œä¸º-1åˆ™ä¸å†è¯·æ±‚è¯¥æ¨¡å‹ï¼‰, æ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆé€‚ç”¨äºå‹ç¼©æ¶ˆæ¯ï¼‰)
        """
        # å“åº”é”™è¯¯
        if e.status_code in [400, 401, 402, 403, 404]:
            # å®¢æˆ·ç«¯é”™è¯¯
            logger.warning(
                f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä»£ç -{e.status_code}ï¼Œé”™è¯¯ä¿¡æ¯-{e.message}"
            )
            return -1, None  # ä¸å†é‡è¯•è¯·æ±‚è¯¥æ¨¡å‹
        elif e.status_code == 413:
            if messages and not messages[1]:
                # æ¶ˆæ¯åˆ—è¡¨ä¸ä¸ºç©ºä¸”æœªå‹ç¼©ï¼Œå°è¯•å‹ç¼©æ¶ˆæ¯
                return self._check_retry(
                    remain_try,
                    0,
                    can_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚ä½“è¿‡å¤§ï¼Œå°è¯•å‹ç¼©æ¶ˆæ¯åé‡è¯•",
                    cannot_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚ä½“è¿‡å¤§ï¼Œå‹ç¼©æ¶ˆæ¯åä»ç„¶è¿‡å¤§ï¼Œæ”¾å¼ƒè¯·æ±‚",
                    can_retry_callable=compress_messages,
                    messages=messages[0],
                )
            # æ²¡æœ‰æ¶ˆæ¯å¯å‹ç¼©
            logger.warning(f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚ä½“è¿‡å¤§ï¼Œæ— æ³•å‹ç¼©æ¶ˆæ¯ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚")
            return -1, None
        elif e.status_code == 429:
            # è¯·æ±‚è¿‡äºé¢‘ç¹
            return self._check_retry(
                remain_try,
                retry_interval,
                can_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œå°†äº{retry_interval}ç§’åé‡è¯•",
                cannot_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚",
            )
        elif e.status_code >= 500:
            # æœåŠ¡å™¨é”™è¯¯
            return self._check_retry(
                remain_try,
                retry_interval,
                can_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': æœåŠ¡å™¨é”™è¯¯ï¼Œå°†äº{retry_interval}ç§’åé‡è¯•",
                cannot_retry_msg=f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': æœåŠ¡å™¨é”™è¯¯ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·ç¨åå†è¯•",
            )
        else:
            # æœªçŸ¥é”™è¯¯
            logger.warning(
                f"ä»»åŠ¡-'{task_name}' æ¨¡å‹-'{model_name}': æœªçŸ¥é”™è¯¯ï¼Œé”™è¯¯ä»£ç -{e.status_code}ï¼Œé”™è¯¯ä¿¡æ¯-{e.message}"
            )
            return -1, None

    def _build_tool_options(self, tools: Optional[List[Dict[str, Any]]]) -> Optional[List[ToolOption]]:
        # sourcery skip: extract-method
        """æ„å»ºå·¥å…·é€‰é¡¹åˆ—è¡¨"""
        if not tools:
            return None
        tool_options: List[ToolOption] = []
        for tool in tools:
            tool_legal = True
            tool_options_builder = ToolOptionBuilder()
            tool_options_builder.set_name(tool.get("name", ""))
            tool_options_builder.set_description(tool.get("description", ""))
            parameters: List[Tuple[str, str, str, bool, List[str] | None]] = tool.get("parameters", [])
            for param in parameters:
                try:
                    assert isinstance(param, tuple) and len(param) == 5, "å‚æ•°å¿…é¡»æ˜¯åŒ…å«5ä¸ªå…ƒç´ çš„å…ƒç»„"
                    assert isinstance(param[0], str), "å‚æ•°åç§°å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
                    assert isinstance(param[1], ToolParamType), "å‚æ•°ç±»å‹å¿…é¡»æ˜¯ToolParamTypeæšä¸¾"
                    assert isinstance(param[2], str), "å‚æ•°æè¿°å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
                    assert isinstance(param[3], bool), "å‚æ•°æ˜¯å¦å¿…å¡«å¿…é¡»æ˜¯å¸ƒå°”å€¼"
                    assert isinstance(param[4], list) or param[4] is None, "å‚æ•°æšä¸¾å€¼å¿…é¡»æ˜¯åˆ—è¡¨æˆ–None"
                    tool_options_builder.add_param(
                        name=param[0],
                        param_type=param[1],
                        description=param[2],
                        required=param[3],
                        enum_values=param[4],
                    )
                except AssertionError as ae:
                    tool_legal = False
                    logger.error(f"{param[0]} å‚æ•°å®šä¹‰é”™è¯¯: {str(ae)}")
                except Exception as e:
                    tool_legal = False
                    logger.error(f"æ„å»ºå·¥å…·å‚æ•°å¤±è´¥: {str(e)}")
            if tool_legal:
                tool_options.append(tool_options_builder.build())
        return tool_options or None

    @staticmethod
    def _extract_reasoning(content: str) -> Tuple[str, str]:
        """CoTæ€ç»´é“¾æå–ï¼Œå‘åå…¼å®¹"""
        match = re.search(r"(?:<think>)?(.*?)</think>", content, re.DOTALL)
        content = re.sub(r"(?:<think>)?.*?</think>", "", content, flags=re.DOTALL, count=1).strip()
        reasoning = match[1].strip() if match else ""
        return content, reasoning

    def _apply_content_obfuscation(self, text: str, api_provider) -> str:
        """æ ¹æ®APIæä¾›å•†é…ç½®å¯¹æ–‡æœ¬è¿›è¡Œæ··æ·†å¤„ç†"""
        if not hasattr(api_provider, 'enable_content_obfuscation') or not api_provider.enable_content_obfuscation:
            logger.debug(f"APIæä¾›å•† '{api_provider.name}' æœªå¯ç”¨å†…å®¹æ··æ·†")
            return text
        
        intensity = getattr(api_provider, 'obfuscation_intensity', 1)
        logger.info(f"ä¸ºAPIæä¾›å•† '{api_provider.name}' å¯ç”¨å†…å®¹æ··æ·†ï¼Œå¼ºåº¦çº§åˆ«: {intensity}")
        
        # åœ¨å¼€å¤´åŠ å…¥è¿‡æ»¤è§„åˆ™æŒ‡ä»¤
        processed_text = self.noise_instruction + "\n\n" + text
        logger.debug(f"å·²æ·»åŠ è¿‡æ»¤è§„åˆ™æŒ‡ä»¤ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} -> {len(processed_text)}")
        
        # æ·»åŠ éšæœºä¹±ç 
        final_text = self._inject_random_noise(processed_text, intensity)
        logger.debug(f"ä¹±ç æ³¨å…¥å®Œæˆï¼Œæœ€ç»ˆæ–‡æœ¬é•¿åº¦: {len(final_text)}")
        
        return final_text
    
    def _inject_random_noise(self, text: str, intensity: int) -> str:
        """åœ¨æ–‡æœ¬ä¸­æ³¨å…¥éšæœºä¹±ç """
        import random
        import string
        
        def generate_noise(length: int) -> str:
            """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„éšæœºä¹±ç å­—ç¬¦"""
            chars = (
                string.ascii_letters +           # a-z, A-Z
                string.digits +                  # 0-9
                '!@#$%^&*()_+-=[]{}|;:,.<>?' +  # ç‰¹æ®Šç¬¦å·
                'ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹é›¶å£¹è´°å' +      # ä¸­æ–‡å­—ç¬¦
                'Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰' +     # å¸Œè…Šå­—æ¯
                'âˆ€âˆƒâˆˆâˆ‰âˆªâˆ©âŠ‚âŠƒâˆ§âˆ¨Â¬â†’â†”âˆ´âˆµ'            # æ•°å­¦ç¬¦å·
            )
            return ''.join(random.choice(chars) for _ in range(length))
        
        # å¼ºåº¦å‚æ•°æ˜ å°„
        params = {
            1: {"probability": 15, "length": (3, 6)},     # ä½å¼ºåº¦ï¼š15%æ¦‚ç‡ï¼Œ3-6ä¸ªå­—ç¬¦
            2: {"probability": 25, "length": (5, 10)},    # ä¸­å¼ºåº¦ï¼š25%æ¦‚ç‡ï¼Œ5-10ä¸ªå­—ç¬¦
            3: {"probability": 35, "length": (8, 15)}     # é«˜å¼ºåº¦ï¼š35%æ¦‚ç‡ï¼Œ8-15ä¸ªå­—ç¬¦
        }
        
        config = params.get(intensity, params[1])
        logger.debug(f"ä¹±ç æ³¨å…¥å‚æ•°: æ¦‚ç‡={config['probability']}%, é•¿åº¦èŒƒå›´={config['length']}")
        
        # æŒ‰è¯åˆ†å‰²å¤„ç†
        words = text.split()
        result = []
        noise_count = 0
        
        for word in words:
            result.append(word)
            # æ ¹æ®æ¦‚ç‡æ’å…¥ä¹±ç 
            if random.randint(1, 100) <= config["probability"]:
                noise_length = random.randint(*config["length"])
                noise = generate_noise(noise_length)
                result.append(noise)
                noise_count += 1
        
        logger.debug(f"å…±æ³¨å…¥ {noise_count} ä¸ªä¹±ç ç‰‡æ®µï¼ŒåŸè¯æ•°: {len(words)}")
        return ' '.join(result)
