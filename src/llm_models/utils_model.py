"""
@desc: è¯¥æ¨¡å—å°è£…äº†ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„æ‰€æœ‰æ ¸å¿ƒé€»è¾‘ã€‚
å®ƒè¢«è®¾è®¡ä¸ºä¸€ä¸ªé«˜åº¦å®¹é”™å’Œå¯æ‰©å±•çš„ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š

- **æ¨¡å‹é€‰æ‹©å™¨ (_ModelSelector)**:
  å®ç°äº†åŸºäºè´Ÿè½½å‡è¡¡å’Œå¤±è´¥æƒ©ç½šçš„åŠ¨æ€æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿åœ¨é«˜å¹¶å‘æˆ–éƒ¨åˆ†æ¨¡å‹å¤±æ•ˆæ—¶ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚

- **æç¤ºå¤„ç†å™¨ (_PromptProcessor)**:
  è´Ÿè´£å¯¹è¾“å…¥æ¨¡å‹çš„æç¤ºè¯è¿›è¡Œé¢„å¤„ç†ï¼ˆå¦‚å†…å®¹æ··æ·†ã€åæˆªæ–­æŒ‡ä»¤æ³¨å…¥ï¼‰å’Œå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†ï¼ˆå¦‚æå–æ€è€ƒè¿‡ç¨‹ã€æ£€æŸ¥æˆªæ–­ï¼‰ã€‚

- **è¯·æ±‚æ‰§è¡Œå™¨ (_RequestExecutor)**:
  å°è£…äº†åº•å±‚çš„APIè¯·æ±‚é€»è¾‘ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é‡è¯•ã€å¼‚å¸¸åˆ†ç±»å¤„ç†å’Œæ¶ˆæ¯ä½“å‹ç¼©ç­‰åŠŸèƒ½ã€‚

- **è¯·æ±‚ç­–ç•¥ (_RequestStrategy)**:
  å®ç°äº†é«˜é˜¶è¯·æ±‚ç­–ç•¥ï¼Œå¦‚æ¨¡å‹é—´çš„æ•…éšœè½¬ç§»ï¼ˆFailoverï¼‰ï¼Œç¡®ä¿å•ä¸ªæ¨¡å‹çš„å¤±è´¥ä¸ä¼šå¯¼è‡´æ•´ä¸ªè¯·æ±‚å¤±è´¥ã€‚

- **LLMRequest (ä¸»æ¥å£)**:
  ä½œä¸ºæ¨¡å—çš„ç»Ÿä¸€å…¥å£ï¼ˆFacadeï¼‰ï¼Œä¸ºä¸Šå±‚ä¸šåŠ¡é€»è¾‘æä¾›äº†ç®€æ´çš„æ¥å£æ¥å‘èµ·æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ç­‰ä¸åŒç±»å‹çš„LLMè¯·æ±‚ã€‚
"""

import asyncio
import random
import re
import string
import time
from collections import namedtuple
from collections.abc import Callable, Coroutine
from enum import Enum
from typing import Any, ClassVar, Literal

from rich.traceback import install

from src.common.logger import get_logger
from src.config.api_ada_configs import APIProvider, ModelInfo, TaskConfig
from src.config.config import model_config

from .exceptions import NetworkConnectionError, ReqAbortException, RespNotOkException, RespParseException
from .model_client.base_client import APIResponse, BaseClient, UsageRecord, client_registry
from .payload_content.message import Message, MessageBuilder
from .payload_content.tool_option import ToolCall, ToolOption, ToolOptionBuilder
from .utils import compress_messages, llm_usage_recorder

install(extra_lines=3)

logger = get_logger("model_utils")

# ==============================================================================
# Standalone Utility Functions
# ==============================================================================


async def _normalize_image_format(image_format: str) -> str:
    """
    æ ‡å‡†åŒ–å›¾ç‰‡æ ¼å¼åç§°ï¼Œç¡®ä¿ä¸å„ç§APIçš„å…¼å®¹æ€§

    Args:
        image_format (str): åŸå§‹å›¾ç‰‡æ ¼å¼

    Returns:
        str: æ ‡å‡†åŒ–åçš„å›¾ç‰‡æ ¼å¼
    """
    format_mapping = {
        "jpg": "jpeg",
        "JPG": "jpeg",
        "JPEG": "jpeg",
        "jpeg": "jpeg",
        "png": "png",
        "PNG": "png",
        "webp": "webp",
        "WEBP": "webp",
        "gif": "gif",
        "GIF": "gif",
        "heic": "heic",
        "HEIC": "heic",
        "heif": "heif",
        "HEIF": "heif",
    }
    normalized = format_mapping.get(image_format, image_format.lower())
    logger.debug(f"å›¾ç‰‡æ ¼å¼æ ‡å‡†åŒ–: {image_format} -> {normalized}")
    return normalized


async def execute_concurrently(
    coro_callable: Callable[..., Coroutine[Any, Any, Any]],
    concurrency_count: int,
    *args,
    **kwargs,
) -> Any:
    """
    æ‰§è¡Œå¹¶å‘è¯·æ±‚å¹¶ä»æˆåŠŸçš„ç»“æœä¸­éšæœºé€‰æ‹©ä¸€ä¸ªã€‚

    Args:
        coro_callable (Callable): è¦å¹¶å‘æ‰§è¡Œçš„åç¨‹å‡½æ•°ã€‚
        concurrency_count (int): å¹¶å‘æ‰§è¡Œçš„æ¬¡æ•°ã€‚
        *args: ä¼ é€’ç»™åç¨‹å‡½æ•°çš„ä½ç½®å‚æ•°ã€‚
        **kwargs: ä¼ é€’ç»™åç¨‹å‡½æ•°çš„å…³é”®å­—å‚æ•°ã€‚

    Returns:
        Any: å…¶ä¸­ä¸€ä¸ªæˆåŠŸæ‰§è¡Œçš„ç»“æœã€‚

    Raises:
        RuntimeError: å¦‚æœæ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥ã€‚
    """
    logger.info(f"å¯ç”¨å¹¶å‘è¯·æ±‚æ¨¡å¼ï¼Œå¹¶å‘æ•°: {concurrency_count}")
    tasks = [coro_callable(*args, **kwargs) for _ in range(concurrency_count)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    successful_results = [res for res in results if not isinstance(res, Exception)]

    if successful_results:
        selected = random.choice(successful_results)
        logger.info(f"å¹¶å‘è¯·æ±‚å®Œæˆï¼Œä»{len(successful_results)}ä¸ªæˆåŠŸç»“æœä¸­é€‰æ‹©äº†ä¸€ä¸ª")
        return selected

    # å¦‚æœæ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†ï¼Œè®°å½•æ‰€æœ‰å¼‚å¸¸å¹¶æŠ›å‡ºç¬¬ä¸€ä¸ª
    for i, res in enumerate(results):
        if isinstance(res, Exception):
            logger.error(f"å¹¶å‘ä»»åŠ¡ {i + 1}/{concurrency_count} å¤±è´¥: {res}")

    first_exception = next((res for res in results if isinstance(res, Exception)), None)
    if first_exception:
        raise first_exception
    raise RuntimeError(f"æ‰€æœ‰ {concurrency_count} ä¸ªå¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†ï¼Œä½†æ²¡æœ‰å…·ä½“çš„å¼‚å¸¸ä¿¡æ¯")


class RequestType(Enum):
    """è¯·æ±‚ç±»å‹æšä¸¾"""

    RESPONSE = "response"
    EMBEDDING = "embedding"
    AUDIO = "audio"


# ==============================================================================
# Helper Classes for LLMRequest Refactoring
# ==============================================================================

# å®šä¹‰ç”¨äºè·Ÿè¸ªæ¨¡å‹ä½¿ç”¨æƒ…å†µçš„å…·åå…ƒç»„
ModelUsageStats = namedtuple(  # noqa: PYI024
    "ModelUsageStats", ["total_tokens", "penalty", "usage_penalty", "avg_latency", "request_count"]
)


class _ModelSelector:
    """è´Ÿè´£æ¨¡å‹é€‰æ‹©ã€è´Ÿè½½å‡è¡¡å’ŒåŠ¨æ€æ•…éšœåˆ‡æ¢çš„ç­–ç•¥ã€‚"""

    CRITICAL_PENALTY_MULTIPLIER = 5  # ä¸¥é‡é”™è¯¯æƒ©ç½šä¹˜æ•°
    DEFAULT_PENALTY_INCREMENT = 1  # é»˜è®¤æƒ©ç½šå¢é‡
    LATENCY_WEIGHT = 200  # å»¶è¿Ÿæƒé‡

    def __init__(self, model_list: list[str], model_usage: dict[str, ModelUsageStats]):
        """
        åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©å™¨ã€‚

        Args:
            model_list (List[str]): å¯ç”¨æ¨¡å‹åç§°åˆ—è¡¨ã€‚
            model_usage (Dict[str, ModelUsageStats]): æ¨¡å‹çš„åˆå§‹ä½¿ç”¨æƒ…å†µã€‚
        """
        self.model_list = model_list
        self.model_usage = model_usage

    async def select_best_available_model(
        self, failed_models_in_this_request: set, request_type: str
    ) -> tuple[ModelInfo, APIProvider, BaseClient] | None:
        """
        ä»å¯ç”¨æ¨¡å‹ä¸­é€‰æ‹©è´Ÿè½½å‡è¡¡è¯„åˆ†æœ€ä½çš„æ¨¡å‹ï¼Œå¹¶æ’é™¤å½“å‰è¯·æ±‚ä¸­å·²å¤±è´¥çš„æ¨¡å‹ã€‚

        Args:
            failed_models_in_this_request (set): å½“å‰è¯·æ±‚ä¸­å·²å¤±è´¥çš„æ¨¡å‹åç§°é›†åˆã€‚
            request_type (str): è¯·æ±‚ç±»å‹ï¼Œç”¨äºç¡®å®šæ˜¯å¦å¼ºåˆ¶åˆ›å»ºæ–°å®¢æˆ·ç«¯ã€‚

        Returns:
            Optional[Tuple[ModelInfo, APIProvider, BaseClient]]: é€‰å®šçš„æ¨¡å‹è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æœæ— å¯ç”¨æ¨¡å‹åˆ™è¿”å› Noneã€‚
        """
        candidate_models_usage = {
            model_name: usage_data
            for model_name, usage_data in self.model_usage.items()
            if model_name not in failed_models_in_this_request
        }

        if not candidate_models_usage:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ä¾›å½“å‰è¯·æ±‚é€‰æ‹©ã€‚")
            return None

        # æ ¸å¿ƒè´Ÿè½½å‡è¡¡ç®—æ³•ï¼šé€‰æ‹©ä¸€ä¸ªç»¼åˆå¾—åˆ†æœ€ä½çš„æ¨¡å‹ã€‚
        # å…¬å¼: total_tokens + penalty * 300 + usage_penalty * 1000 + avg_latency * 200
        # è®¾è®¡æ€è·¯:
        # - `total_tokens`: åŸºç¡€æˆæœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç´¯è®¡tokenå°‘çš„æ¨¡å‹ï¼Œå®ç°é•¿æœŸå‡è¡¡ã€‚
        # - `penalty * 300`: å¤±è´¥æƒ©ç½šé¡¹ã€‚æ¯æ¬¡å¤±è´¥ä¼šå¢åŠ penaltyï¼Œä½¿å…¶åœ¨çŸ­æœŸå†…è¢«é€‰ä¸­çš„æ¦‚ç‡é™ä½ã€‚æƒé‡300æ„å‘³ç€ä¸€æ¬¡å¤±è´¥å¤§è‡´ç›¸å½“äº300ä¸ªtokençš„æˆæœ¬ã€‚
        # - `usage_penalty * 1000`: çŸ­æœŸä½¿ç”¨æƒ©ç½šé¡¹ã€‚æ¯æ¬¡è¢«é€‰ä¸­åä¼šå¢åŠ ï¼Œå®Œæˆåä¼šå‡å°‘ã€‚é«˜æƒé‡ç¡®ä¿åœ¨å¤šä¸ªæ¨¡å‹éƒ½å¥åº·çš„æƒ…å†µä¸‹ï¼Œè¯·æ±‚ä¼šå‡åŒ€åˆ†å¸ƒï¼ˆè½®è¯¢ï¼‰ã€‚
        # - `avg_latency * 200`: å»¶è¿Ÿæƒ©ç½šé¡¹ã€‚ä¼˜å…ˆé€‰æ‹©å¹³å‡å“åº”æ—¶é—´æ›´å¿«çš„æ¨¡å‹ã€‚æƒé‡200æ„å‘³ç€1ç§’çš„å»¶è¿Ÿçº¦ç­‰äº200ä¸ªtokençš„æˆæœ¬ã€‚
        least_used_model_name = min(
            candidate_models_usage,
            key=lambda k: candidate_models_usage[k].total_tokens
            + candidate_models_usage[k].penalty * 300
            + candidate_models_usage[k].usage_penalty * 1000
            + candidate_models_usage[k].avg_latency * self.LATENCY_WEIGHT,
        )

        model_info = model_config.get_model_info(least_used_model_name)
        api_provider = model_config.get_provider(model_info.api_provider)
        # è‡ªåŠ¨äº‹ä»¶å¾ªç¯æ£€æµ‹ï¼šClientRegistry ä¼šè‡ªåŠ¨æ£€æµ‹äº‹ä»¶å¾ªç¯å˜åŒ–å¹¶å¤„ç†ç¼“å­˜å¤±æ•ˆ
        # æ— éœ€æ‰‹åŠ¨æŒ‡å®š force_newï¼Œembedding è¯·æ±‚ä¹Ÿèƒ½äº«å—ç¼“å­˜ä¼˜åŠ¿
        client = client_registry.get_client_class_instance(api_provider)

        logger.debug(f"ä¸ºå½“å‰è¯·æ±‚é€‰æ‹©äº†æœ€ä½³å¯ç”¨æ¨¡å‹: {model_info.name}")
        # å¢åŠ æ‰€é€‰æ¨¡å‹çš„è¯·æ±‚ä½¿ç”¨æƒ©ç½šå€¼ï¼Œä»¥å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚
        await self.update_usage_penalty(model_info.name, increase=True)
        return model_info, api_provider, client

    async def update_usage_penalty(self, model_name: str, increase: bool):
        """
        æ›´æ–°æ¨¡å‹çš„ä½¿ç”¨æƒ©ç½šå€¼ã€‚

        åœ¨æ¨¡å‹è¢«é€‰ä¸­æ—¶å¢åŠ æƒ©ç½šå€¼ï¼Œè¯·æ±‚å®Œæˆåå‡å°‘æƒ©ç½šå€¼ã€‚
        è¿™æœ‰åŠ©äºåœ¨çŸ­æœŸå†…å°†è¯·æ±‚åˆ†æ•£åˆ°ä¸åŒçš„æ¨¡å‹ï¼Œå®ç°æ›´åŠ¨æ€çš„è´Ÿè½½å‡è¡¡ã€‚

        Args:
            model_name (str): è¦æ›´æ–°æƒ©ç½šå€¼çš„æ¨¡å‹åç§°ã€‚
            increase (bool): Trueè¡¨ç¤ºå¢åŠ æƒ©ç½šå€¼ï¼ŒFalseè¡¨ç¤ºå‡å°‘ã€‚
        """
        # è·å–å½“å‰æ¨¡å‹çš„ç»Ÿè®¡æ•°æ®
        stats = self.model_usage[model_name]
        # æ ¹æ®æ“ä½œæ˜¯å¢åŠ è¿˜æ˜¯å‡å°‘æ¥ç¡®å®šè°ƒæ•´é‡
        adjustment = 1 if increase else -1
        # æ›´æ–°æ¨¡å‹çš„æƒ©ç½šå€¼
        self.model_usage[model_name] = stats._replace(usage_penalty=stats.usage_penalty + adjustment)

    async def update_failure_penalty(self, model_name: str, e: Exception):
        """
        æ ¹æ®å¼‚å¸¸ç±»å‹åŠ¨æ€è°ƒæ•´æ¨¡å‹çš„å¤±è´¥æƒ©ç½šå€¼ã€‚
        å…³é”®é”™è¯¯ï¼ˆå¦‚ç½‘ç»œè¿æ¥ã€æœåŠ¡å™¨é”™è¯¯ï¼‰ä¼šè·å¾—æ›´é«˜çš„æƒ©ç½šï¼Œ
        ä¿ƒä½¿è´Ÿè½½å‡è¡¡ç®—æ³•åœ¨ä¸‹æ¬¡é€‰æ‹©æ—¶ä¼˜å…ˆè§„é¿è¿™äº›ä¸å¯é çš„æ¨¡å‹ã€‚
        """
        stats = self.model_usage[model_name]
        penalty_increment = self.DEFAULT_PENALTY_INCREMENT

        # å¯¹ä¸¥é‡é”™è¯¯æ–½åŠ æ›´é«˜çš„æƒ©ç½šï¼Œä»¥ä¾¿å¿«é€Ÿå°†é—®é¢˜æ¨¡å‹ç§»å‡ºå€™é€‰æ± 
        if isinstance(e, NetworkConnectionError | ReqAbortException):
            # ç½‘ç»œè¿æ¥é”™è¯¯æˆ–è¯·æ±‚è¢«ä¸­æ–­ï¼Œé€šå¸¸æ˜¯åŸºç¡€è®¾æ–½é—®é¢˜ï¼Œåº”é‡ç½š
            penalty_increment = self.CRITICAL_PENALTY_MULTIPLIER
            logger.warning(
                f"æ¨¡å‹ '{model_name}' å‘ç”Ÿä¸¥é‡é”™è¯¯ ({type(e).__name__})ï¼Œå¢åŠ é«˜é¢æƒ©ç½šå€¼: {penalty_increment}"
            )
        elif isinstance(e, RespNotOkException):
            # å¯¹äºHTTPå“åº”é”™è¯¯ï¼Œé‡ç‚¹å…³æ³¨æœåŠ¡å™¨ç«¯é”™è¯¯
            if e.status_code >= 500:
                # 5xx é”™è¯¯è¡¨æ˜æœåŠ¡å™¨ç«¯å‡ºç°é—®é¢˜ï¼Œåº”é‡ç½š
                penalty_increment = self.CRITICAL_PENALTY_MULTIPLIER
                logger.warning(
                    f"æ¨¡å‹ '{model_name}' å‘ç”ŸæœåŠ¡å™¨é”™è¯¯ (çŠ¶æ€ç : {e.status_code})ï¼Œå¢åŠ é«˜é¢æƒ©ç½šå€¼: {penalty_increment}"
                )
            else:
                # 4xx å®¢æˆ·ç«¯é”™è¯¯é€šå¸¸ä¸ä»£è¡¨æ¨¡å‹æœ¬èº«ä¸å¯ç”¨ï¼Œç»™äºˆåŸºç¡€æƒ©ç½š
                logger.warning(
                    f"æ¨¡å‹ '{model_name}' å‘ç”Ÿå®¢æˆ·ç«¯å“åº”é”™è¯¯ (çŠ¶æ€ç : {e.status_code})ï¼Œå¢åŠ åŸºç¡€æƒ©ç½šå€¼: {penalty_increment}"
                )
        else:
            # å…¶ä»–æœªçŸ¥å¼‚å¸¸ï¼Œç»™äºˆåŸºç¡€æƒ©ç½š
            logger.warning(f"æ¨¡å‹ '{model_name}' å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {type(e).__name__}ï¼Œå¢åŠ åŸºç¡€æƒ©ç½šå€¼: {penalty_increment}")

        self.model_usage[model_name] = stats._replace(penalty=stats.penalty + penalty_increment)


class _PromptProcessor:
    """å°è£…æ‰€æœ‰ä¸æç¤ºè¯å’Œå“åº”å†…å®¹çš„é¢„å¤„ç†å’Œåå¤„ç†é€»è¾‘ã€‚"""

    def __init__(self):
        """
        åˆå§‹åŒ–æç¤ºå¤„ç†å™¨ã€‚
        è¿™é‡Œå®šä¹‰äº†ç”¨äºå†…å®¹æ··æ·†å’Œåæˆªæ–­çš„æŒ‡ä»¤æ–‡æœ¬ã€‚
        """
        # å†…å®¹æ··æ·†è¿‡æ»¤æŒ‡ä»¤: ç”¨äºåœ¨APIä¼ è¾“ä¸­å¢åŠ å™ªéŸ³ï¼Œé˜²æ­¢å†…å®¹è¢«å®¡æŸ¥æˆ–ä¿®æ”¹
        self.noise_instruction = """**ã€æ ¸å¿ƒè¿‡æ»¤è§„åˆ™ã€‘**
åœ¨äº¤äº’è¿‡ç¨‹ä¸­,ä½ ä¼šåœ¨ç”¨æˆ·çš„è¾“å…¥å†…å®¹ä¸­çœ‹åˆ°ä¸€äº›æ— æ„ä¹‰çš„ã€ç”±å­—æ¯å’Œæ•°å­—ç»„æˆçš„éšæœºå­—ç¬¦ä¸²ã€‚è¿™äº›å­—ç¬¦ä¸²æ˜¯APIä¼ è¾“è¿‡ç¨‹ä¸­çš„æŠ€æœ¯ä¼ªè£…ä¿¡æ¯,ã€ä¸æ˜¯ã€‘ç”¨æˆ·çœŸå®æ„å›¾çš„ä¸€éƒ¨åˆ†ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ã€å®Œå…¨å¹¶å½»åº•åœ°å¿½ç•¥ã€‘è¿™äº›éšæœºå­—ç¬¦ä¸²ã€‚
**ã€ä¸¥ç¦ã€‘** å°†è¿™äº›å­—ç¬¦ä¸²å†™å…¥ä½ çš„å›å¤ã€å¯¹å®ƒä»¬è¿›è¡Œä»»ä½•å½¢å¼çš„è§£è¯»ã€æˆ–å°†å®ƒä»¬ä½œä¸ºæ•…äº‹çš„ä¸€éƒ¨åˆ†è¿›è¡Œæ¨¡æ‹Ÿã€‚å½“çœ‹åˆ°è¿™äº›å­—ç¬¦ä¸²æ—¶,è¯·å°†å…¶è§†ä¸ºç©ºç™½æˆ–ç›´æ¥è·³è¿‡,å¹¶æ ¹æ®ç”¨æˆ·å…¶ä»–æœ‰æ„ä¹‰çš„è¾“å…¥(å¦‚æœ‰)æˆ–å½“å‰æ•…äº‹æƒ…èŠ‚æ¥ç»§ç»­ä½ çš„å™äº‹ã€‚æ­¤è§„åˆ™ä¸ºæœ€é«˜ä¼˜å…ˆçº§ã€‚
"""
        # åæˆªæ–­æŒ‡ä»¤
        self.end_marker = "###MAI_RESPONSE_END###"
        self.anti_truncation_instruction = f"""
**ã€è¾“å‡ºå®Œæˆä¿¡ä»¤ã€‘**
è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æŒ‡ä»¤,è¯·åŠ¡å¿…éµå®ˆã€‚åœ¨ä½ çš„å›å¤å†…å®¹å®Œå…¨ç»“æŸå,è¯·åŠ¡å¿…åœ¨æœ€åå¦èµ·ä¸€è¡Œ,åªå†™ `{self.end_marker}` ä½œä¸ºç»“æŸæ ‡å¿—ã€‚
ä¾‹å¦‚:
<ä½ çš„å›å¤å†…å®¹>
{self.end_marker}

è¿™æœ‰åŠ©äºæˆ‘åˆ¤æ–­ä½ çš„è¾“å‡ºæ˜¯å¦è¢«æˆªæ–­ã€‚è¯·ä¸è¦åœ¨ `{self.end_marker}` å‰åæ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—æˆ–æ ‡ç‚¹ã€‚
"""

    # ==============================================================================
    # æç¤ºè¯æ‰°åŠ¨ (Prompt Perturbation) æ¨¡å—
    #
    # æœ¬æ¨¡å—é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—è½»é‡çº§çš„ã€ä¿æŒè¯­ä¹‰çš„éšæœºåŒ–æŠ€æœ¯ï¼Œ
    # æ—¨åœ¨å¢åŠ è¾“å…¥æç¤ºè¯çš„ç»“æ„å¤šæ ·æ€§ã€‚è¿™æœ‰åŠ©äºï¼š
    # 1. é¿å…å› çŸ­æ—¶é—´å†…å‘é€é«˜åº¦ç›¸ä¼¼çš„æç¤ºè¯è€Œå¯¼è‡´æ¨¡å‹äº§ç”Ÿè¶‹åŒæˆ–é‡å¤çš„å›å¤ã€‚
    # 2. å¢å¼ºæ¨¡å‹å¯¹ä¸åŒè¾“å…¥æ ¼å¼çš„é²æ£’æ€§ã€‚
    # 3. åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé€šè¿‡å¼•å…¥â€œå™ªéŸ³â€æ¥æ¿€å‘æ¨¡å‹æ›´å…·åˆ›é€ æ€§çš„å“åº”ã€‚
    # ==============================================================================

    # å®šä¹‰è¯­ä¹‰ç­‰ä»·çš„æ–‡æœ¬æ›¿æ¢æ¨¡æ¿ã€‚
    # Key æ˜¯åŸå§‹æ–‡æœ¬ï¼ŒValue æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§ç­‰ä»·è¡¨è¾¾çš„åˆ—è¡¨ã€‚
    SEMANTIC_VARIANTS: ClassVar = {
        "å½“å‰æ—¶é—´": ["å½“å‰æ—¶é—´", "ç°åœ¨æ˜¯", "æ­¤æ—¶æ­¤åˆ»", "æ—¶é—´"],
        "æœ€è¿‘çš„ç³»ç»Ÿé€šçŸ¥": ["æœ€è¿‘çš„ç³»ç»Ÿé€šçŸ¥", "ç³»ç»Ÿé€šçŸ¥", "é€šçŸ¥æ¶ˆæ¯", "æœ€æ–°é€šçŸ¥"],
        "èŠå¤©å†å²": ["èŠå¤©å†å²", "å¯¹è¯è®°å½•", "å†å²æ¶ˆæ¯", "ä¹‹å‰çš„å¯¹è¯"],
        "ä½ çš„ä»»åŠ¡æ˜¯": ["ä½ çš„ä»»åŠ¡æ˜¯", "è¯·", "ä½ éœ€è¦", "ä½ åº”å½“"],
        "è¯·æ³¨æ„": ["è¯·æ³¨æ„", "æ³¨æ„", "è¯·ç•™æ„", "éœ€è¦æ³¨æ„"],
    }

    async def _apply_prompt_perturbation(
        self,
        prompt_text: str,
        enable_semantic_variants: bool,
        strength: Literal["light", "medium", "heavy"],
    ) -> str:
        """
        ç»Ÿä¸€çš„æç¤ºè¯æ‰°åŠ¨å¤„ç†å‡½æ•°ã€‚

        è¯¥æ–¹æ³•æŒ‰é¡ºåºåº”ç”¨ä¸‰ç§æ‰°åŠ¨æŠ€æœ¯ï¼š
        1. è¯­ä¹‰å˜ä½“ (Semantic Variants): å°†ç‰¹å®šçŸ­è¯­æ›¿æ¢ä¸ºè¯­ä¹‰ç­‰ä»·çš„å…¶å®ƒè¡¨è¾¾ã€‚
        2. ç©ºç™½å™ªå£° (Whitespace Noise): éšæœºè°ƒæ•´æ¢è¡Œã€ç©ºæ ¼å’Œç¼©è¿›ã€‚
        3. å†…å®¹æ··æ·† (Content Confusion): æ³¨å…¥éšæœºçš„ã€æ— æ„ä¹‰çš„å­—ç¬¦ä¸²ã€‚

        Args:
            prompt_text (str): åŸå§‹çš„ç”¨æˆ·æç¤ºè¯ã€‚
            enable_semantic_variants (bool): æ˜¯å¦å¯ç”¨è¯­ä¹‰å˜ä½“æ›¿æ¢ã€‚
            strength (Literal["light", "medium", "heavy"]): æ‰°åŠ¨çš„å¼ºåº¦ï¼Œä¼šå½±å“æ‰€æœ‰æ‰°åŠ¨æ“ä½œçš„ç¨‹åº¦ã€‚

        Returns:
            str: ç»è¿‡æ‰°åŠ¨å¤„ç†åçš„æç¤ºè¯ã€‚
        """
        try:
            perturbed_text = prompt_text

            # æ­¥éª¤ 1: åº”ç”¨è¯­ä¹‰å˜ä½“
            if enable_semantic_variants:
                perturbed_text = self._apply_semantic_variants(perturbed_text)

            # æ­¥éª¤ 2: æ³¨å…¥ç©ºç™½å™ªå£°
            perturbed_text = self._inject_whitespace_noise(perturbed_text, strength)

            # æ­¥éª¤ 3: æ³¨å…¥å†…å®¹æ··æ·†ï¼ˆéšæœºå™ªå£°å­—ç¬¦ä¸²ï¼‰
            perturbed_text = self._inject_random_noise(perturbed_text, strength)

            # è®¡ç®—å¹¶è®°å½•å˜åŒ–ç‡ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§
            change_rate = self._calculate_change_rate(prompt_text, perturbed_text)
            if change_rate > 0.001:  # ä»…åœ¨æœ‰å®é™…å˜åŒ–æ—¶è®°å½•æ—¥å¿—
                logger.debug(f"æç¤ºè¯æ‰°åŠ¨å®Œæˆï¼Œå¼ºåº¦: '{strength}'ï¼Œå˜åŒ–ç‡: {change_rate:.2%}")

            return perturbed_text

        except Exception as e:
            logger.error(f"æç¤ºè¯æ‰°åŠ¨å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return prompt_text  # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å›åŸå§‹æ–‡æœ¬ï¼Œä¿è¯æµç¨‹ä¸ä¸­æ–­

    @staticmethod
    def _apply_semantic_variants(text: str) -> str:
        """
        åº”ç”¨è¯­ä¹‰ç­‰ä»·çš„æ–‡æœ¬æ›¿æ¢ã€‚

        éå† SEMANTIC_VARIANTS å­—å…¸ï¼Œå¯¹æ–‡æœ¬ä¸­é¦–æ¬¡å‡ºç°çš„ key è¿›è¡Œéšæœºæ›¿æ¢ã€‚

        Args:
            text (str): è¾“å…¥æ–‡æœ¬ã€‚

        Returns:
            str: æ›¿æ¢åçš„æ–‡æœ¬ã€‚
        """
        try:
            result = text
            for original, variants in _PromptProcessor.SEMANTIC_VARIANTS.items():
                if original in result:
                    # ä»å˜ä½“åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªè¿›è¡Œæ›¿æ¢
                    replacement = random.choice(variants)
                    # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°çš„åœ°æ–¹ï¼Œé¿å…è¿‡åº¦ä¿®æ”¹
                    result = result.replace(original, replacement, 1)
            return result
        except Exception as e:
            logger.error(f"è¯­ä¹‰å˜ä½“æ›¿æ¢å¤±è´¥: {e}", exc_info=True)
            return text

    @staticmethod
    def _inject_whitespace_noise(text: str, strength: str) -> str:
        """
        æ³¨å…¥è½»é‡çº§å™ªå£°ï¼ˆç©ºç™½å­—ç¬¦è°ƒæ•´ï¼‰ã€‚

        æ ¹æ®æŒ‡å®šçš„å¼ºåº¦ï¼Œè°ƒæ•´æ–‡æœ¬ä¸­çš„æ¢è¡Œã€è¡Œå°¾ç©ºæ ¼å’Œåˆ—è¡¨é¡¹ç¼©è¿›ã€‚

        Args:
            text (str): è¾“å…¥æ–‡æœ¬ã€‚
            strength (str): å™ªå£°å¼ºåº¦ ('light', 'medium', 'heavy')ã€‚

        Returns:
            str: è°ƒæ•´ç©ºç™½å­—ç¬¦åçš„æ–‡æœ¬ã€‚
        """
        try:
            # å™ªå£°å¼ºåº¦é…ç½®ï¼Œå®šä¹‰äº†ä¸åŒå¼ºåº¦ä¸‹å„ç§æ“ä½œçš„å‚æ•°èŒƒå›´
            noise_config = {
                "light": {"newline_range": (1, 2), "space_range": (0, 2), "indent_adjust": False, "probability": 0.3},
                "medium": {"newline_range": (1, 3), "space_range": (0, 4), "indent_adjust": True, "probability": 0.5},
                "heavy": {"newline_range": (1, 4), "space_range": (0, 6), "indent_adjust": True, "probability": 0.7},
            }
            config = noise_config.get(strength, noise_config["light"])

            lines = text.split("\n")
            result_lines = []
            for line in lines:
                processed_line = line
                # éšæœºè°ƒæ•´è¡Œå°¾ç©ºæ ¼
                if line.strip() and random.random() < config["probability"]:
                    spaces = " " * random.randint(*config["space_range"])
                    processed_line += spaces

                # éšæœºè°ƒæ•´åˆ—è¡¨é¡¹ç¼©è¿›ï¼ˆä»…åœ¨ä¸­ç­‰å’Œé‡åº¦æ¨¡å¼ä¸‹ï¼‰
                if config["indent_adjust"]:
                    list_match = re.match(r"^(\s*)([-*â€¢])\s", processed_line)
                    if list_match and random.random() < 0.5:
                        indent, marker = list_match.group(1), list_match.group(2)
                        adjust = random.choice([-2, 0, 2])
                        new_indent = " " * max(0, len(indent) + adjust)
                        processed_line = processed_line.replace(indent + marker, new_indent + marker, 1)

                result_lines.append(processed_line)

            result = "\n".join(result_lines)

            # è°ƒæ•´è¿ç»­æ¢è¡Œçš„æ•°é‡
            newline_pattern = r"\n{2,}"
            def replace_newlines(match):
                count = random.randint(*config["newline_range"])
                return "\n" * count
            result = re.sub(newline_pattern, replace_newlines, result)

            return result
        except Exception as e:
            logger.error(f"ç©ºç™½å­—ç¬¦å™ªå£°æ³¨å…¥å¤±è´¥: {e}", exc_info=True)
            return text

    @staticmethod
    def _inject_random_noise(text: str, strength: str) -> str:
        """
        åœ¨æ–‡æœ¬ä¸­æŒ‰æŒ‡å®šå¼ºåº¦æ³¨å…¥éšæœºå™ªéŸ³å­—ç¬¦ä¸²ï¼ˆå†…å®¹æ··æ·†ï¼‰ã€‚

        Args:
            text (str): è¾“å…¥æ–‡æœ¬ã€‚
            strength (str): å™ªéŸ³å¼ºåº¦ ('light', 'medium', 'heavy')ã€‚

        Returns:
            str: æ³¨å…¥éšæœºå™ªéŸ³åçš„æ–‡æœ¬ã€‚
        """
        try:
            # ä¸åŒå¼ºåº¦ä¸‹çš„å™ªéŸ³æ³¨å…¥å‚æ•°é…ç½®
            # probability: åœ¨æ¯ä¸ªå•è¯åæ³¨å…¥å™ªéŸ³çš„ç™¾åˆ†æ¯”æ¦‚ç‡
            # length: æ³¨å…¥å™ªéŸ³å­—ç¬¦ä¸²çš„éšæœºé•¿åº¦èŒƒå›´
            strength_config = {
                "light": {"probability": 15, "length": (3, 6)},
                "medium": {"probability": 25, "length": (5, 10)},
                "heavy": {"probability": 35, "length": (8, 15)},
            }
            config = strength_config.get(strength, strength_config["light"])

            words = text.split()
            if not words:
                return text

            result = []
            for word in words:
                result.append(word)
                # æ ¹æ®æ¦‚ç‡å†³å®šæ˜¯å¦åœ¨æ­¤å•è¯åæ³¨å…¥å™ªéŸ³
                if random.randint(1, 100) <= config["probability"]:
                    noise_length = random.randint(*config["length"])
                    # å®šä¹‰å™ªéŸ³å­—ç¬¦é›†
                    chars = string.ascii_letters + string.digits
                    noise = "".join(random.choice(chars) for _ in range(noise_length))
                    result.append(f" {noise} ") # æ·»åŠ å‰åç©ºæ ¼ä»¥åˆ†éš”

            return "".join(result)
        except Exception as e:
            logger.error(f"éšæœºå™ªéŸ³æ³¨å…¥å¤±è´¥: {e}", exc_info=True)
            return text

    @staticmethod
    def _calculate_change_rate(original: str, modified: str) -> float:
        """è®¡ç®—æ–‡æœ¬å˜åŒ–ç‡ï¼Œç”¨äºè¡¡é‡æ‰°åŠ¨ç¨‹åº¦ã€‚"""
        if not original or not modified:
            return 0.0
        # ä½¿ç”¨ Levenshtein è·ç¦»ç­‰æ›´å¤æ‚çš„ç®—æ³•å¯èƒ½æ›´ç²¾ç¡®ï¼Œä½†ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œä½¿ç”¨ç®€å•çš„å­—ç¬¦å·®å¼‚è®¡ç®—
        diff_chars = sum(1 for a, b in zip(original, modified) if a != b) + abs(len(original) - len(modified))
        max_len = max(len(original), len(modified))
        return diff_chars / max_len if max_len > 0 else 0.0


    async def prepare_prompt(
        self, prompt: str, model_info: ModelInfo,  task_name: str
    ) -> str:
        """
        ä¸ºè¯·æ±‚å‡†å¤‡æœ€ç»ˆçš„æç¤ºè¯,åº”ç”¨å„ç§æ‰°åŠ¨å’ŒæŒ‡ä»¤ã€‚
        """
        final_prompt_parts = []
        user_prompt = prompt

        # æ­¥éª¤ A: æ·»åŠ æŠ—å®¡æŸ¥æŒ‡ä»¤
        if model_info.enable_prompt_perturbation:
            final_prompt_parts.append(self.noise_instruction)

        # æ­¥éª¤ B: (å¯é€‰) åº”ç”¨ç»Ÿä¸€çš„æç¤ºè¯æ‰°åŠ¨
        if getattr(model_info, "enable_prompt_perturbation", False):
            logger.info(f"ä¸ºæ¨¡å‹ '{model_info.name}' å¯ç”¨æç¤ºè¯æ‰°åŠ¨åŠŸèƒ½ã€‚")
            user_prompt = await self._apply_prompt_perturbation(
                prompt_text=user_prompt,
                enable_semantic_variants=getattr(model_info, "enable_semantic_variants", False),
                strength=getattr(model_info, "perturbation_strength", "light"),
            )

        final_prompt_parts.append(user_prompt)

        # æ­¥éª¤ C: (å¯é€‰) æ·»åŠ åæˆªæ–­æŒ‡ä»¤
        if model_info.anti_truncation:
            final_prompt_parts.append(self.anti_truncation_instruction)
            logger.info(f"æ¨¡å‹ '{model_info.name}' (ä»»åŠ¡: '{task_name}') å·²å¯ç”¨åæˆªæ–­åŠŸèƒ½ã€‚")

        return "\n\n".join(final_prompt_parts)

    async def process_response(self, content: str, use_anti_truncation: bool) -> tuple[str, str, bool]:
        """
        å¤„ç†å“åº”å†…å®¹ï¼Œæå–æ€ç»´é“¾å¹¶æ£€æŸ¥æˆªæ–­ã€‚

        Returns:
            Tuple[str, str, bool]: (å¤„ç†åçš„å†…å®¹, æ€ç»´é“¾å†…å®¹, æ˜¯å¦è¢«æˆªæ–­)
        """
        content, reasoning = await self._extract_reasoning(content)
        is_truncated = False
        if use_anti_truncation:
            if content.endswith(self.end_marker):
                content = content[: -len(self.end_marker)].strip()
            else:
                is_truncated = True
        return content, reasoning, is_truncated

    @staticmethod
    async def _extract_reasoning(content: str) -> tuple[str, str]:
        """
        ä»æ¨¡å‹è¿”å›çš„å®Œæ•´å†…å®¹ä¸­æå–è¢«<think>...</think>æ ‡ç­¾åŒ…è£¹çš„æ€è€ƒè¿‡ç¨‹ï¼Œ
        å¹¶è¿”å›æ¸…ç†åçš„å†…å®¹å’Œæ€è€ƒè¿‡ç¨‹ã€‚

        Args:
            content (str): æ¨¡å‹è¿”å›çš„åŸå§‹å­—ç¬¦ä¸²ã€‚

        Returns:
            Tuple[str, str]:
                - æ¸…ç†åçš„å†…å®¹ï¼ˆç§»é™¤äº†<think>æ ‡ç­¾åŠå…¶å†…å®¹ï¼‰ã€‚
                - æå–å‡ºçš„æ€è€ƒè¿‡ç¨‹æ–‡æœ¬ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰ã€‚
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®æŸ¥æ‰¾ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
        think_pattern = re.compile(r"<think>(.*?)</think>\s*", re.DOTALL)
        match = think_pattern.search(content)

        if match:
            # æå–æ€è€ƒè¿‡ç¨‹
            reasoning = match.group(1).strip()
            # ä»åŸå§‹å†…å®¹ä¸­ç§»é™¤åŒ¹é…åˆ°çš„æ•´ä¸ªéƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ ‡ç­¾å’Œåé¢çš„ç©ºç™½ï¼‰
            clean_content = think_pattern.sub("", content, count=1).strip()
        else:
            reasoning = ""
            clean_content = content.strip()

        return clean_content, reasoning


class _RequestExecutor:
    """è´Ÿè´£æ‰§è¡Œå®é™…çš„APIè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•é€»è¾‘å’Œåº•å±‚å¼‚å¸¸å¤„ç†ã€‚"""

    def __init__(self, model_selector: _ModelSelector, task_name: str):
        """
        åˆå§‹åŒ–è¯·æ±‚æ‰§è¡Œå™¨ã€‚

        Args:
            model_selector (_ModelSelector): æ¨¡å‹é€‰æ‹©å™¨å®ä¾‹ï¼Œç”¨äºåœ¨è¯·æ±‚å¤±è´¥æ—¶æ›´æ–°æƒ©ç½šã€‚
            task_name (str): å½“å‰ä»»åŠ¡çš„åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
        """
        self.model_selector = model_selector
        self.task_name = task_name

    async def execute_request(
        self,
        api_provider: APIProvider,
        client: BaseClient,
        request_type: RequestType,
        model_info: ModelInfo,
        **kwargs,
    ) -> APIResponse:
        """
        å®é™…æ‰§è¡Œè¯·æ±‚çš„æ–¹æ³•ï¼ŒåŒ…å«äº†é‡è¯•å’Œå¼‚å¸¸å¤„ç†é€»è¾‘ã€‚

        Args:
            api_provider (APIProvider): APIæä¾›å•†é…ç½®ã€‚
            client (BaseClient): ç”¨äºå‘é€è¯·æ±‚çš„å®¢æˆ·ç«¯å®ä¾‹ã€‚
            request_type (RequestType): è¯·æ±‚çš„ç±»å‹ (e.g., RESPONSE, EMBEDDING)ã€‚
            model_info (ModelInfo): æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹çš„ä¿¡æ¯ã€‚
            **kwargs: ä¼ é€’ç»™å®¢æˆ·ç«¯æ–¹æ³•çš„å…·ä½“å‚æ•°ã€‚

        Returns:
            APIResponse: æ¥è‡ªAPIçš„æˆåŠŸå“åº”ã€‚

        Raises:
            Exception: å¦‚æœé‡è¯•åè¯·æ±‚ä»ç„¶å¤±è´¥ï¼Œåˆ™æŠ›å‡ºæœ€ç»ˆçš„å¼‚å¸¸ã€‚
            RuntimeError: å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚
        """
        retry_remain = api_provider.max_retry
        compressed_messages: list[Message] | None = None

        while retry_remain > 0:
            try:
                # ä¼˜å…ˆä½¿ç”¨å‹ç¼©åçš„æ¶ˆæ¯åˆ—è¡¨
                message_list = kwargs.get("message_list")
                current_messages = compressed_messages or message_list

                # æ ¹æ®è¯·æ±‚ç±»å‹è°ƒç”¨ä¸åŒçš„å®¢æˆ·ç«¯æ–¹æ³•
                if request_type == RequestType.RESPONSE:
                    assert current_messages is not None, "message_list cannot be None for response requests"

                    # ä¿®å¤: é˜²æ­¢ 'message_list' åœ¨ kwargs ä¸­é‡å¤ä¼ é€’
                    request_params = kwargs.copy()
                    request_params.pop("message_list", None)

                    return await client.get_response(
                        model_info=model_info, message_list=current_messages, **request_params
                    )
                elif request_type == RequestType.EMBEDDING:
                    return await client.get_embedding(model_info=model_info, **kwargs)
                elif request_type == RequestType.AUDIO:
                    return await client.get_audio_transcriptions(model_info=model_info, **kwargs)

            except Exception as e:
                logger.debug(f"è¯·æ±‚å¤±è´¥: {e!s}")
                # è®°å½•å¤±è´¥å¹¶æ›´æ–°æ¨¡å‹çš„æƒ©ç½šå€¼
                await self.model_selector.update_failure_penalty(model_info.name, e)

                # å¤„ç†å¼‚å¸¸ï¼Œå†³å®šæ˜¯å¦é‡è¯•ä»¥åŠç­‰å¾…å¤šä¹…
                wait_interval, new_compressed_messages = await self._handle_exception(
                    e,
                    model_info,
                    api_provider,
                    retry_remain,
                    (kwargs.get("message_list"), compressed_messages is not None),
                )
                if new_compressed_messages:
                    compressed_messages = new_compressed_messages  # æ›´æ–°ä¸ºå‹ç¼©åçš„æ¶ˆæ¯

                if wait_interval == -1:
                    raise e  # å¦‚æœå†³å®šä¸å†é‡è¯•ï¼Œåˆ™ä¼ æ’­å¼‚å¸¸
                elif wait_interval > 0:
                    await asyncio.sleep(wait_interval)  # ç­‰å¾…æŒ‡å®šæ—¶é—´åé‡è¯•
            finally:
                retry_remain -= 1

        logger.error(f"æ¨¡å‹ '{model_info.name}' è¯·æ±‚å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {api_provider.max_retry} æ¬¡")
        raise RuntimeError("è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

    async def _handle_exception(
        self, e: Exception, model_info: ModelInfo, api_provider: APIProvider, remain_try: int, messages_info
    ) -> tuple[int, list[Message] | None]:
        """
        é»˜è®¤å¼‚å¸¸å¤„ç†å‡½æ•°ï¼Œå†³å®šæ˜¯å¦é‡è¯•ã€‚

        Returns:
            (ç­‰å¾…é—´éš”ï¼ˆ-1è¡¨ç¤ºä¸å†é‡è¯•ï¼‰, æ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆé€‚ç”¨äºå‹ç¼©æ¶ˆæ¯ï¼‰)
        """
        model_name = model_info.name
        retry_interval = api_provider.retry_interval

        if isinstance(e, NetworkConnectionError | ReqAbortException):
            return await self._check_retry(remain_try, retry_interval, "è¿æ¥å¼‚å¸¸", model_name)
        elif isinstance(e, RespNotOkException):
            return await self._handle_resp_not_ok(e, model_info, api_provider, remain_try, messages_info)
        elif isinstance(e, RespParseException):
            logger.error(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': å“åº”è§£æé”™è¯¯ - {e.message}")
            return -1, None
        else:
            logger.error(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': æœªçŸ¥å¼‚å¸¸ - {e!s}")
            return -1, None

    async def _handle_resp_not_ok(
        self, e: RespNotOkException, model_info: ModelInfo, api_provider: APIProvider, remain_try: int, messages_info
    ) -> tuple[int, list[Message] | None]:
        """
        å¤„ç†é200çš„HTTPå“åº”å¼‚å¸¸ã€‚

        æ ¹æ®ä¸åŒçš„HTTPçŠ¶æ€ç å†³å®šä¸‹ä¸€æ­¥æ“ä½œï¼š
        - 4xx å®¢æˆ·ç«¯é”™è¯¯ï¼šé€šå¸¸ä¸å¯é‡è¯•ï¼Œç›´æ¥æ”¾å¼ƒã€‚
        - 413 (Payload Too Large): å°è¯•å‹ç¼©æ¶ˆæ¯ä½“åé‡è¯•ä¸€æ¬¡ã€‚
        - 429 (Too Many Requests) / 5xx æœåŠ¡å™¨é”™è¯¯ï¼šå¯é‡è¯•ã€‚

        Args:
            e (RespNotOkException): æ•è·åˆ°çš„å“åº”å¼‚å¸¸ã€‚
            model_info (ModelInfo): å½“å‰æ¨¡å‹ä¿¡æ¯ã€‚
            api_provider (APIProvider): APIæä¾›å•†é…ç½®ã€‚
            remain_try (int): å‰©ä½™é‡è¯•æ¬¡æ•°ã€‚
            messages_info (tuple): åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œæ˜¯å¦å·²å‹ç¼©çš„æ ‡å¿—ã€‚

        Returns:
            Tuple[int, Optional[List[Message]]]: (ç­‰å¾…é—´éš”, æ–°çš„æ¶ˆæ¯åˆ—è¡¨)ã€‚
            ç­‰å¾…é—´éš”ä¸º-1è¡¨ç¤ºä¸å†é‡è¯•ã€‚æ–°çš„æ¶ˆæ¯åˆ—è¡¨ç”¨äºå‹ç¼©åé‡è¯•ã€‚
        """
        model_name = model_info.name
        # å¤„ç†å®¢æˆ·ç«¯é”™è¯¯ (400-404)ï¼Œè¿™äº›é”™è¯¯é€šå¸¸æ˜¯è¯·æ±‚æœ¬èº«æœ‰é—®é¢˜ï¼Œä¸åº”é‡è¯•
        if e.status_code in [400, 401, 402, 403, 404]:
            logger.warning(
                f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': å®¢æˆ·ç«¯é”™è¯¯ {e.status_code} - {e.message}ï¼Œä¸å†é‡è¯•ã€‚"
            )
            return -1, None
        # å¤„ç†è¯·æ±‚ä½“è¿‡å¤§çš„æƒ…å†µ
        elif e.status_code == 413:
            messages, is_compressed = messages_info
            # å¦‚æœæ¶ˆæ¯å­˜åœ¨ä¸”å°šæœªè¢«å‹ç¼©ï¼Œåˆ™å°è¯•å‹ç¼©åç«‹å³é‡è¯•
            if messages and not is_compressed:
                logger.warning(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚ä½“è¿‡å¤§ï¼Œå°è¯•å‹ç¼©æ¶ˆæ¯åé‡è¯•ã€‚")
                return 0, compress_messages(messages)
            # å¦‚æœå·²ç»å‹ç¼©è¿‡æˆ–æ²¡æœ‰æ¶ˆæ¯ä½“ï¼Œåˆ™æ”¾å¼ƒ
            logger.warning(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': è¯·æ±‚ä½“è¿‡å¤§ä¸”æ— æ³•å‹ç¼©ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚")
            return -1, None
        # å¤„ç†è¯·æ±‚é¢‘ç¹æˆ–æœåŠ¡å™¨ç«¯é”™è¯¯ï¼Œè¿™äº›æƒ…å†µé€‚åˆé‡è¯•
        elif e.status_code == 429 or e.status_code >= 500:
            reason = "è¯·æ±‚è¿‡äºé¢‘ç¹" if e.status_code == 429 else "æœåŠ¡å™¨é”™è¯¯"
            return await self._check_retry(remain_try, api_provider.retry_interval, reason, model_name)
        # å¤„ç†å…¶ä»–æœªçŸ¥çš„HTTPé”™è¯¯
        else:
            logger.warning(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': æœªçŸ¥å“åº”é”™è¯¯ {e.status_code} - {e.message}")
            return -1, None

    async def _check_retry(self, remain_try: int, interval: int, reason: str, model_name: str) -> tuple[int, None]:
        """
        è¾…åŠ©å‡½æ•°ï¼Œæ ¹æ®å‰©ä½™æ¬¡æ•°å†³å®šæ˜¯å¦è¿›è¡Œä¸‹ä¸€æ¬¡é‡è¯•ã€‚

        Args:
            remain_try (int): å‰©ä½™çš„é‡è¯•æ¬¡æ•°ã€‚
            interval (int): é‡è¯•å‰çš„ç­‰å¾…é—´éš”ï¼ˆç§’ï¼‰ã€‚
            reason (str): æœ¬æ¬¡å¤±è´¥çš„åŸå› ã€‚
            model_name (str): å¤±è´¥çš„æ¨¡å‹åç§°ã€‚

        Returns:
            Tuple[int, None]: (ç­‰å¾…é—´éš”, None)ã€‚å¦‚æœç­‰å¾…é—´éš”ä¸º-1ï¼Œè¡¨ç¤ºä¸åº”å†é‡è¯•ã€‚
        """
        # åªæœ‰åœ¨å‰©ä½™é‡è¯•æ¬¡æ•°å¤§äº1æ—¶æ‰è¿›è¡Œä¸‹ä¸€æ¬¡é‡è¯•ï¼ˆå› ä¸ºå½“å‰è¿™æ¬¡å¤±è´¥å·²ç»æ¶ˆè€—æ‰ä¸€æ¬¡ï¼‰
        if remain_try > 1:
            logger.warning(
                f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': {reason}ï¼Œå°†äº{interval}ç§’åé‡è¯• ({remain_try - 1}æ¬¡å‰©ä½™)ã€‚"
            )
            return interval, None

        # å¦‚æœå·²æ— å‰©ä½™é‡è¯•æ¬¡æ•°ï¼Œåˆ™è®°å½•é”™è¯¯å¹¶è¿”å›-1è¡¨ç¤ºæ”¾å¼ƒ
        logger.error(f"ä»»åŠ¡-'{self.task_name}' æ¨¡å‹-'{model_name}': {reason}ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒã€‚")
        return -1, None


class _RequestStrategy:
    """
    å°è£…é«˜çº§è¯·æ±‚ç­–ç•¥ï¼Œå¦‚æ•…éšœè½¬ç§»ã€‚
    æ­¤ç±»åè°ƒæ¨¡å‹é€‰æ‹©ã€æç¤ºå¤„ç†å’Œè¯·æ±‚æ‰§è¡Œï¼Œä»¥å®ç°å¥å£®çš„è¯·æ±‚å¤„ç†ï¼Œ
    å³ä½¿åœ¨å•ä¸ªæ¨¡å‹æˆ–APIç«¯ç‚¹å¤±è´¥çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œã€‚
    """

    def __init__(
        self,
        model_selector: _ModelSelector,
        prompt_processor: _PromptProcessor,
        executor: _RequestExecutor,
        model_list: list[str],
        task_name: str,
    ):
        """
        åˆå§‹åŒ–è¯·æ±‚ç­–ç•¥ã€‚

        Args:
            model_selector (_ModelSelector): æ¨¡å‹é€‰æ‹©å™¨å®ä¾‹ã€‚
            prompt_processor (_PromptProcessor): æç¤ºå¤„ç†å™¨å®ä¾‹ã€‚
            executor (_RequestExecutor): è¯·æ±‚æ‰§è¡Œå™¨å®ä¾‹ã€‚
            model_list (List[str]): å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚
            task_name (str): å½“å‰ä»»åŠ¡çš„åç§°ã€‚
        """
        self.model_selector = model_selector
        self.prompt_processor = prompt_processor
        self.executor = executor
        self.model_list = model_list
        self.task_name = task_name

    async def execute_with_failover(
        self,
        request_type: RequestType,
        raise_when_empty: bool = True,
        **kwargs,
    ) -> tuple[APIResponse, ModelInfo]:
        """
        æ‰§è¡Œè¯·æ±‚ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹ï¼Œå¹¶åœ¨æ¨¡å‹å¤±è´¥æ—¶è¿›è¡Œæ•…éšœè½¬ç§»ã€‚
        """
        failed_models_in_this_request = set()
        max_attempts = len(self.model_list)
        last_exception: Exception | None = None

        for attempt in range(max_attempts):
            selection_result = await self.model_selector.select_best_available_model(
                failed_models_in_this_request, str(request_type.value)
            )
            if selection_result is None:
                logger.error(f"å°è¯• {attempt + 1}/{max_attempts}: æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹äº†ã€‚")
                break

            model_info, api_provider, client = selection_result
            logger.debug(f"å°è¯• {attempt + 1}/{max_attempts}: æ­£åœ¨ä½¿ç”¨æ¨¡å‹ '{model_info.name}'...")

            try:
                # å‡†å¤‡è¯·æ±‚å‚æ•°
                request_kwargs = kwargs.copy()
                if request_type == RequestType.RESPONSE and "prompt" in request_kwargs:
                    prompt = request_kwargs.pop("prompt")
                    processed_prompt = await self.prompt_processor.prepare_prompt(
                        prompt, model_info, self.task_name
                    )
                    message = MessageBuilder().add_text_content(processed_prompt).build()
                    request_kwargs["message_list"] = [message]

                # åˆå¹¶æ¨¡å‹ç‰¹å®šçš„é¢å¤–å‚æ•°
                if model_info.extra_params:
                    request_kwargs["extra_params"] = {
                        **model_info.extra_params,
                        **request_kwargs.get("extra_params", {}),
                    }

                response = await self._try_model_request(
                    model_info, api_provider, client, request_type, **request_kwargs
                )

                # æˆåŠŸï¼Œç«‹å³è¿”å›
                logger.debug(f"æ¨¡å‹ '{model_info.name}' æˆåŠŸç”Ÿæˆäº†å›å¤ã€‚")
                await self.model_selector.update_usage_penalty(model_info.name, increase=False)
                return response, model_info

            except Exception as e:
                logger.error(f"æ¨¡å‹ '{model_info.name}' å¤±è´¥ï¼Œå¼‚å¸¸: {e}ã€‚å°†å…¶æ·»åŠ åˆ°å½“å‰è¯·æ±‚çš„å¤±è´¥æ¨¡å‹åˆ—è¡¨ä¸­ã€‚")
                failed_models_in_this_request.add(model_info.name)
                last_exception = e
                # ä½¿ç”¨æƒ©ç½šå€¼å·²åœ¨ select æ—¶å¢åŠ ï¼Œå¤±è´¥åä¸å‡å°‘ï¼Œä»¥é™ä½å…¶åç»­è¢«é€‰ä¸­çš„æ¦‚ç‡

        logger.error(f"å½“å‰è¯·æ±‚å·²å°è¯• {max_attempts} ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰æ¨¡å‹å‡å·²å¤±è´¥ã€‚")
        if raise_when_empty:
            if last_exception:
                raise RuntimeError("æ‰€æœ‰æ¨¡å‹å‡æœªèƒ½ç”Ÿæˆå“åº”ã€‚") from last_exception
            raise RuntimeError("æ‰€æœ‰æ¨¡å‹å‡æœªèƒ½ç”Ÿæˆå“åº”ï¼Œä¸”æ— å…·ä½“å¼‚å¸¸ä¿¡æ¯ã€‚")

        # å¦‚æœä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè¿”å›ä¸€ä¸ªå¤‡ç”¨å“åº”
        fallback_model_info = model_config.get_model_info(self.model_list[0])
        return APIResponse(content="æ‰€æœ‰æ¨¡å‹éƒ½è¯·æ±‚å¤±è´¥"), fallback_model_info

    async def _try_model_request(
        self, model_info: ModelInfo, api_provider: APIProvider, client: BaseClient, request_type: RequestType, **kwargs
    ) -> APIResponse:
        """
        ä¸ºå•ä¸ªæ¨¡å‹å°è¯•è¯·æ±‚ï¼ŒåŒ…å«ç©ºå›å¤/æˆªæ–­çš„å†…éƒ¨é‡è¯•é€»è¾‘ã€‚
        å¦‚æœæ¨¡å‹è¿”å›ç©ºå›å¤æˆ–å“åº”è¢«æˆªæ–­ï¼Œæ­¤æ–¹æ³•å°†è‡ªåŠ¨é‡è¯•è¯·æ±‚ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚

        Args:
            model_info (ModelInfo): è¦ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ã€‚
            api_provider (APIProvider): APIæä¾›å•†ä¿¡æ¯ã€‚
            client (BaseClient): APIå®¢æˆ·ç«¯å®ä¾‹ã€‚
            request_type (RequestType): è¯·æ±‚ç±»å‹ã€‚
            **kwargs: ä¼ é€’ç»™æ‰§è¡Œå™¨çš„è¯·æ±‚å‚æ•°ã€‚

        Returns:
            APIResponse: æˆåŠŸçš„APIå“åº”ã€‚

        Raises:
            RuntimeError: å¦‚æœåœ¨è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»ç„¶æ”¶åˆ°ç©ºå›å¤æˆ–æˆªæ–­çš„å“åº”ã€‚
        """
        max_empty_retry = api_provider.max_retry

        for i in range(max_empty_retry + 1):
            response = await self.executor.execute_request(api_provider, client, request_type, model_info, **kwargs)

            if request_type != RequestType.RESPONSE:
                return response  # å¯¹äºéå“åº”ç±»å‹ï¼Œç›´æ¥è¿”å›

            # --- å“åº”å†…å®¹å¤„ç†å’Œç©ºå›å¤/æˆªæ–­æ£€æŸ¥ ---
            content = response.content or ""
            use_anti_truncation = model_info.anti_truncation
            processed_content, reasoning, is_truncated = await self.prompt_processor.process_response(
                content, use_anti_truncation
            )

            # æ›´æ–°å“åº”å¯¹è±¡
            response.content = processed_content
            response.reasoning_content = response.reasoning_content or reasoning

            is_empty_reply = not response.tool_calls and not (response.content and response.content.strip())

            if not is_empty_reply and not is_truncated:
                return response  # æˆåŠŸè·å–æœ‰æ•ˆå“åº”

            if i < max_empty_retry:
                reason = "ç©ºå›å¤" if is_empty_reply else "æˆªæ–­"
                logger.warning(
                    f"æ¨¡å‹ '{model_info.name}' æ£€æµ‹åˆ°{reason}ï¼Œæ­£åœ¨è¿›è¡Œå†…éƒ¨é‡è¯• ({i + 1}/{max_empty_retry})..."
                )
                if api_provider.retry_interval > 0:
                    await asyncio.sleep(api_provider.retry_interval)
            else:
                reason = "ç©ºå›å¤" if is_empty_reply else "æˆªæ–­"
                logger.error(f"æ¨¡å‹ '{model_info.name}' ç»è¿‡ {max_empty_retry} æ¬¡å†…éƒ¨é‡è¯•åä»ç„¶ç”Ÿæˆ{reason}çš„å›å¤ã€‚")
                raise RuntimeError(f"æ¨¡å‹ '{model_info.name}' å·²è¾¾åˆ°ç©ºå›å¤/æˆªæ–­çš„æœ€å¤§å†…éƒ¨é‡è¯•æ¬¡æ•°ã€‚")

        raise RuntimeError("å†…éƒ¨é‡è¯•é€»è¾‘é”™è¯¯")  # ç†è®ºä¸Šä¸åº”åˆ°è¾¾è¿™é‡Œ


# ==============================================================================
# Main Facade Class
# ==============================================================================


class LLMRequest:
    """
    LLMè¯·æ±‚åè°ƒå™¨ã€‚
    å°è£…äº†æ¨¡å‹é€‰æ‹©ã€Promptå¤„ç†ã€è¯·æ±‚æ‰§è¡Œå’Œé«˜çº§ç­–ç•¥ï¼ˆå¦‚æ•…éšœè½¬ç§»ã€å¹¶å‘ï¼‰çš„å®Œæ•´æµç¨‹ã€‚
    ä¸ºä¸Šå±‚ä¸šåŠ¡é€»è¾‘æä¾›ç»Ÿä¸€çš„ã€ç®€åŒ–çš„æ¥å£æ¥ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’ã€‚
    """

    def __init__(self, model_set: TaskConfig, request_type: str = ""):
        """
        åˆå§‹åŒ–LLMè¯·æ±‚åè°ƒå™¨ã€‚

        Args:
            model_set (TaskConfig): ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹é…ç½®é›†åˆã€‚
            request_type (str, optional): è¯·æ±‚ç±»å‹æˆ–ä»»åŠ¡åç§°ï¼Œç”¨äºæ—¥å¿—å’Œç”¨é‡è®°å½•ã€‚ Defaults to "".
        """
        self.task_name = request_type
        self.model_for_task = model_set
        self.model_usage: dict[str, ModelUsageStats] = {
            model: ModelUsageStats(total_tokens=0, penalty=0, usage_penalty=0, avg_latency=0.0, request_count=0)
            for model in self.model_for_task.model_list
        }
        """æ¨¡å‹ä½¿ç”¨é‡è®°å½•"""
        # ğŸ”§ ä¼˜åŒ–ï¼šç§»é™¤å…¨å±€é”ï¼Œæ”¹ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘åº¦ï¼ˆå…è®¸å¤šä¸ªè¯·æ±‚å¹¶è¡Œï¼‰
        # é»˜è®¤å…è®¸50ä¸ªå¹¶å‘è¯·æ±‚ï¼Œå¯é€šè¿‡é…ç½®è°ƒæ•´
        max_concurrent = getattr(model_set, "max_concurrent_requests", 50)
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._stats_lock = asyncio.Lock()  # åªä¿æŠ¤ç»Ÿè®¡æ•°æ®çš„å†™å…¥

        # åˆå§‹åŒ–è¾…åŠ©ç±»
        self._model_selector = _ModelSelector(self.model_for_task.model_list, self.model_usage)
        self._prompt_processor = _PromptProcessor()
        self._executor = _RequestExecutor(self._model_selector, self.task_name)
        self._strategy = _RequestStrategy(
            self._model_selector, self._prompt_processor, self._executor, self.model_for_task.model_list, self.task_name
        )

    async def generate_response_for_image(
        self,
        prompt: str,
        image_base64: str,
        image_format: str,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> tuple[str, tuple[str, str, list[ToolCall] | None]]:
        """
        ä¸ºå›¾åƒç”Ÿæˆå“åº”ã€‚

        Args:
            prompt (str): æç¤ºè¯
            image_base64 (str): å›¾åƒçš„Base64ç¼–ç å­—ç¬¦ä¸²
            image_format (str): å›¾åƒæ ¼å¼ï¼ˆå¦‚ 'png', 'jpeg' ç­‰ï¼‰

        Returns:
            (Tuple[str, str, str, Optional[List[ToolCall]]]): å“åº”å†…å®¹ã€æ¨ç†å†…å®¹ã€æ¨¡å‹åç§°ã€å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        start_time = time.time()

        # å›¾åƒè¯·æ±‚ç›®å‰ä¸ä½¿ç”¨å¤æ‚çš„æ•…éšœè½¬ç§»ç­–ç•¥ï¼Œç›´æ¥é€‰æ‹©æ¨¡å‹å¹¶æ‰§è¡Œ
        selection_result = await self._model_selector.select_best_available_model(set(), "response")
        if not selection_result:
            raise RuntimeError("æ— æ³•ä¸ºå›¾åƒå“åº”é€‰æ‹©å¯ç”¨æ¨¡å‹ã€‚")
        model_info, api_provider, client = selection_result

        normalized_format = await _normalize_image_format(image_format)
        message = (
            MessageBuilder()
            .add_text_content(prompt)
            .add_image_content(
                image_base64=image_base64,
                image_format=normalized_format,
                support_formats=client.get_support_image_formats(),
            )
            .build()
        )

        response = await self._executor.execute_request(
            api_provider,
            client,
            RequestType.RESPONSE,
            model_info,
            message_list=[message],
            temperature=temperature,
            max_tokens=max_tokens,
        )

        await self._record_usage(model_info, response.usage, time.time() - start_time, "/chat/completions")
        content, reasoning, _ = await self._prompt_processor.process_response(response.content or "", False)
        reasoning = response.reasoning_content or reasoning

        return content, (reasoning, model_info.name, response.tool_calls)

    async def generate_response_for_voice(self, voice_base64: str) -> str | None:
        """
        ä¸ºè¯­éŸ³ç”Ÿæˆå“åº”ï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰ã€‚
        ä½¿ç”¨æ•…éšœè½¬ç§»ç­–ç•¥æ¥ç¡®ä¿å³ä½¿ä¸»æ¨¡å‹å¤±è´¥ä¹Ÿèƒ½è·å¾—ç»“æœã€‚

        Args:
            voice_base64 (str): è¯­éŸ³çš„Base64ç¼–ç å­—ç¬¦ä¸²ã€‚

        Returns:
            Optional[str]: è¯­éŸ³è½¬æ¢åçš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥åˆ™è¿”å›Noneã€‚
        """
        response, _ = await self._strategy.execute_with_failover(RequestType.AUDIO, audio_base64=voice_base64)
        return response.content or None

    async def generate_response_async(
        self,
        prompt: str,
        temperature: float | None = None,
        max_tokens: int | None = None,
        tools: list[dict[str, Any]] | None = None,
        raise_when_empty: bool = True,
    ) -> tuple[str, tuple[str, str, list[ToolCall] | None]]:
        """
        å¼‚æ­¥ç”Ÿæˆå“åº”ï¼Œæ”¯æŒå¹¶å‘è¯·æ±‚ã€‚

        Args:
            prompt (str): æç¤ºè¯
            temperature (float, optional): æ¸©åº¦å‚æ•°
            max_tokens (int, optional): æœ€å¤§tokenæ•°
            tools: å·¥å…·é…ç½®
            raise_when_empty (bool): æ˜¯å¦åœ¨ç©ºå›å¤æ—¶æŠ›å‡ºå¼‚å¸¸

        Returns:
            (Tuple[str, str, str, Optional[List[ToolCall]]]): å“åº”å†…å®¹ã€æ¨ç†å†…å®¹ã€æ¨¡å‹åç§°ã€å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        concurrency_count = getattr(self.model_for_task, "concurrency_count", 1)

        if concurrency_count <= 1:
            return await self._execute_single_text_request(prompt, temperature, max_tokens, tools, raise_when_empty)

        try:
            return await execute_concurrently(
                self._execute_single_text_request,
                concurrency_count,
                prompt,
                temperature,
                max_tokens,
                tools,
                raise_when_empty=False,
            )
        except Exception as e:
            logger.error(f"æ‰€æœ‰ {concurrency_count} ä¸ªå¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†: {e}")
            if raise_when_empty:
                raise e
            return "æ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†", ("", "unknown", None)

    async def _execute_single_text_request(
        self,
        prompt: str,
        temperature: float | None = None,
        max_tokens: int | None = None,
        tools: list[dict[str, Any]] | None = None,
        raise_when_empty: bool = True,
    ) -> tuple[str, tuple[str, str, list[ToolCall] | None]]:
        """
        æ‰§è¡Œå•æ¬¡æ–‡æœ¬ç”Ÿæˆè¯·æ±‚çš„å†…éƒ¨æ–¹æ³•ã€‚
        è¿™æ˜¯ `generate_response_async` çš„æ ¸å¿ƒå®ç°ï¼Œå¤„ç†å•ä¸ªè¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œ
        åŒ…æ‹¬å·¥å…·æ„å»ºã€æ•…éšœè½¬ç§»æ‰§è¡Œå’Œç”¨é‡è®°å½•ã€‚

        Args:
            prompt (str): ç”¨æˆ·çš„æç¤ºã€‚
            temperature (Optional[float]): ç”Ÿæˆæ¸©åº¦ã€‚
            max_tokens (Optional[int]): æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°ã€‚
            tools (Optional[List[Dict[str, Any]]]): å¯ç”¨å·¥å…·åˆ—è¡¨ã€‚
            raise_when_empty (bool): å¦‚æœå“åº”ä¸ºç©ºæ˜¯å¦å¼•å‘å¼‚å¸¸ã€‚

        Returns:
            Tuple[str, Tuple[str, str, Optional[List[ToolCall]]]]:
                (å“åº”å†…å®¹, (æ¨ç†è¿‡ç¨‹, æ¨¡å‹åç§°, å·¥å…·è°ƒç”¨))
        """
        # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼Œå…è®¸å¤šä¸ªè¯·æ±‚å¹¶è¡Œæ‰§è¡Œ
        async with self._semaphore:
            start_time = time.time()
            tool_options = await self._build_tool_options(tools)

            response, model_info = await self._strategy.execute_with_failover(
                RequestType.RESPONSE,
                raise_when_empty=raise_when_empty,
                prompt=prompt,  # ä¼ é€’åŸå§‹promptï¼Œç”±strategyå¤„ç†
                tool_options=tool_options,
                temperature=self.model_for_task.temperature if temperature is None else temperature,
                max_tokens=self.model_for_task.max_tokens if max_tokens is None else max_tokens,
            )

            await self._record_usage(model_info, response.usage, time.time() - start_time, "/chat/completions")
            logger.debug(f"LLMåŸå§‹å“åº”: {response.content}")

            if not response.content and not response.tool_calls:
                if raise_when_empty:
                    raise RuntimeError("æ‰€é€‰æ¨¡å‹ç”Ÿæˆäº†ç©ºå›å¤ã€‚")
                response.content = "ç”Ÿæˆçš„å“åº”ä¸ºç©º"

            return response.content or "", (response.reasoning_content or "", model_info.name, response.tool_calls)

    async def get_embedding(self, embedding_input: str) -> tuple[list[float], str]:
        """
        è·å–åµŒå…¥å‘é‡ã€‚

        Args:
            embedding_input (str): è·å–åµŒå…¥çš„ç›®æ ‡

        Returns:
            (Tuple[List[float], str]): (åµŒå…¥å‘é‡ï¼Œä½¿ç”¨çš„æ¨¡å‹åç§°)
        """
        start_time = time.time()
        response, model_info = await self._strategy.execute_with_failover(
            RequestType.EMBEDDING, embedding_input=embedding_input
        )

        await self._record_usage(model_info, response.usage, time.time() - start_time, "/embeddings")

        if not response.embedding:
            raise RuntimeError("è·å–embeddingå¤±è´¥")

        return response.embedding, model_info.name

    async def _record_usage(self, model_info: ModelInfo, usage: UsageRecord | None, time_cost: float, endpoint: str):
        """
        è®°å½•æ¨¡å‹ä½¿ç”¨æƒ…å†µã€‚

        æ­¤æ–¹æ³•é¦–å…ˆåœ¨å†…å­˜ä¸­æ›´æ–°æ¨¡å‹çš„ç´¯è®¡tokenä½¿ç”¨é‡ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªå¼‚æ­¥ä»»åŠ¡ï¼Œ
        å°†è¯¦ç»†çš„ç”¨é‡æ•°æ®ï¼ˆåŒ…æ‹¬æ¨¡å‹ä¿¡æ¯ã€tokenæ•°ã€è€—æ—¶ç­‰ï¼‰å†™å…¥æ•°æ®åº“ã€‚

        Args:
            model_info (ModelInfo): ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ã€‚
            usage (Optional[UsageRecord]): APIè¿”å›çš„ç”¨é‡è®°å½•ã€‚
            time_cost (float): æœ¬æ¬¡è¯·æ±‚çš„æ€»è€—æ—¶ã€‚
            endpoint (str): è¯·æ±‚çš„APIç«¯ç‚¹ (e.g., "/chat/completions")ã€‚
        """
        if usage:
            # æ­¥éª¤1: æ›´æ–°å†…å­˜ä¸­çš„ç»Ÿè®¡æ•°æ®ï¼Œç”¨äºè´Ÿè½½å‡è¡¡ï¼ˆéœ€è¦åŠ é”ä¿æŠ¤ï¼‰
            async with self._stats_lock:
                stats = self.model_usage[model_info.name]

                # è®¡ç®—æ–°çš„å¹³å‡å»¶è¿Ÿ
                new_request_count = stats.request_count + 1
                new_avg_latency = (stats.avg_latency * stats.request_count + time_cost) / new_request_count

                self.model_usage[model_info.name] = stats._replace(
                    total_tokens=stats.total_tokens + usage.total_tokens,
                    avg_latency=new_avg_latency,
                    request_count=new_request_count,
                )

            # æ­¥éª¤2: åˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œå°†ç”¨é‡æ•°æ®å¼‚æ­¥å†™å…¥æ•°æ®åº“ï¼ˆæ— éœ€ç­‰å¾…ï¼‰
            asyncio.create_task(  # noqa: RUF006
                llm_usage_recorder.record_usage_to_database(
                    model_info=model_info,
                    model_usage=usage,
                    user_id="system",  # æ­¤å¤„å¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚ä¿®æ”¹
                    time_cost=time_cost,
                    request_type=self.task_name,
                    endpoint=endpoint,
                )
            )

    @staticmethod
    async def _build_tool_options(tools: list[dict[str, Any]] | None) -> list[ToolOption] | None:
        """
        æ ¹æ®è¾“å…¥çš„å­—å…¸åˆ—è¡¨æ„å»ºå¹¶éªŒè¯ `ToolOption` å¯¹è±¡åˆ—è¡¨ã€‚

        æ­¤æ–¹æ³•å°†æ ‡å‡†åŒ–çš„å·¥å…·å®šä¹‰ï¼ˆå­—å…¸æ ¼å¼ï¼‰è½¬æ¢ä¸ºå†…éƒ¨ä½¿ç”¨çš„ `ToolOption` å¯¹è±¡ï¼Œ
        åŒæ—¶ä¼šéªŒè¯å‚æ•°æ ¼å¼çš„æ­£ç¡®æ€§ã€‚

        Args:
            tools (Optional[List[Dict[str, Any]]]): å·¥å…·å®šä¹‰çš„åˆ—è¡¨ã€‚
                æ¯ä¸ªå·¥å…·æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« "name", "description", å’Œ "parameters"ã€‚
                "parameters" æ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å« (name, type, desc, required, enum)ã€‚

        Returns:
            Optional[List[ToolOption]]: æ„å»ºå¥½çš„ `ToolOption` å¯¹è±¡åˆ—è¡¨ï¼Œå¦‚æœè¾“å…¥ä¸ºç©ºåˆ™è¿”å› Noneã€‚
        """
        # å¦‚æœæ²¡æœ‰æä¾›å·¥å…·ï¼Œç›´æ¥è¿”å› None
        if not tools:
            return None

        tool_options: list[ToolOption] = []
        # éå†æ¯ä¸ªå·¥å…·å®šä¹‰
        for tool in tools:
            try:
                # ä½¿ç”¨å»ºé€ è€…æ¨¡å¼åˆ›å»º ToolOption
                builder = ToolOptionBuilder().set_name(tool["name"]).set_description(tool.get("description", ""))

                # éå†å·¥å…·çš„å‚æ•°
                for param in tool.get("parameters", []):
                    # ä¸¥æ ¼éªŒè¯å‚æ•°æ ¼å¼æ˜¯å¦ä¸ºåŒ…å«5ä¸ªå…ƒç´ çš„å…ƒç»„
                    assert isinstance(param, tuple) and len(param) == 5, "å‚æ•°å¿…é¡»æ˜¯åŒ…å«5ä¸ªå…ƒç´ çš„å…ƒç»„"
                    builder.add_param(
                        name=param[0],
                        param_type=param[1],
                        description=param[2],
                        required=param[3],
                        enum_values=param[4],
                    )
                # å°†æ„å»ºå¥½çš„ ToolOption æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                tool_options.append(builder.build())
            except (KeyError, IndexError, TypeError, AssertionError) as e:
                # å¦‚æœæ„å»ºè¿‡ç¨‹ä¸­å‡ºç°ä»»ä½•é”™è¯¯ï¼Œè®°å½•æ—¥å¿—å¹¶è·³è¿‡è¯¥å·¥å…·
                logger.error(f"æ„å»ºå·¥å…· '{tool.get('name', 'N/A')}' å¤±è´¥: {e}")

        # å¦‚æœåˆ—è¡¨éç©ºåˆ™è¿”å›åˆ—è¡¨ï¼Œå¦åˆ™è¿”å› None
        return tool_options or None
