import asyncio
import base64
import io
import re
from collections.abc import Callable, Coroutine, Iterable
from typing import Any, ClassVar

import orjson
from json_repair import repair_json
from openai import (
    NOT_GIVEN,
    APIConnectionError,
    APIStatusError,
    AsyncOpenAI,
    AsyncStream,
)
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionChunk,
    ChatCompletionMessageParam,
    ChatCompletionToolParam,
)
from openai.types.chat.chat_completion_chunk import ChoiceDelta

from src.common.logger import get_logger
from src.config.api_ada_configs import APIProvider, ModelInfo

from ..exceptions import (
    NetworkConnectionError,
    ReqAbortException,
    RespNotOkException,
    RespParseException,
)
from ..payload_content.message import Message, RoleType
from ..payload_content.resp_format import RespFormat
from ..payload_content.tool_option import ToolCall, ToolOption, ToolParam
from .base_client import APIResponse, BaseClient, UsageRecord, client_registry

logger = get_logger("OpenAIå®¢æˆ·ç«¯")


def _convert_messages(messages: list[Message]) -> list[ChatCompletionMessageParam]:
    """
    è½¬æ¢æ¶ˆæ¯æ ¼å¼ - å°†æ¶ˆæ¯è½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
    :param messages: æ¶ˆæ¯åˆ—è¡¨
    :return: è½¬æ¢åçš„æ¶ˆæ¯åˆ—è¡¨
    """

    def _convert_message_item(message: Message) -> ChatCompletionMessageParam:
        """
        è½¬æ¢å•ä¸ªæ¶ˆæ¯æ ¼å¼
        :param message: æ¶ˆæ¯å¯¹è±¡
        :return: è½¬æ¢åçš„æ¶ˆæ¯å­—å…¸
        """

        # æ·»åŠ Content
        content: str | list[dict[str, Any]]
        if isinstance(message.content, str):
            content = message.content
        elif isinstance(message.content, list):
            content = []
            for item in message.content:
                if isinstance(item, tuple):
                    content.append(
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/{item[0].lower()};base64,{item[1]}"},
                        }
                    )
                elif isinstance(item, str):
                    content.append({"type": "text", "text": item})
        else:
            raise RuntimeError("æ— æ³•è§¦åŠçš„ä»£ç ï¼šè¯·ä½¿ç”¨MessageBuilderç±»æ„å»ºæ¶ˆæ¯å¯¹è±¡")

        ret = {
            "role": message.role.value,
            "content": content,
        }

        # æ·»åŠ å·¥å…·è°ƒç”¨ID
        if message.role == RoleType.Tool:
            if not message.tool_call_id:
                raise ValueError("æ— æ³•è§¦åŠçš„ä»£ç ï¼šè¯·ä½¿ç”¨MessageBuilderç±»æ„å»ºæ¶ˆæ¯å¯¹è±¡")
            ret["tool_call_id"] = message.tool_call_id

        return ret  # type: ignore

    return [_convert_message_item(message) for message in messages]


def _convert_tool_options(tool_options: list[ToolOption]) -> list[dict[str, Any]]:
    """
    è½¬æ¢å·¥å…·é€‰é¡¹æ ¼å¼ - å°†å·¥å…·é€‰é¡¹è½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
    :param tool_options: å·¥å…·é€‰é¡¹åˆ—è¡¨
    :return: è½¬æ¢åçš„å·¥å…·é€‰é¡¹åˆ—è¡¨
    """

    def _convert_tool_param(tool_option_param: ToolParam) -> dict[str, Any]:
        """
        è½¬æ¢å•ä¸ªå·¥å…·å‚æ•°æ ¼å¼
        :param tool_option_param: å·¥å…·å‚æ•°å¯¹è±¡
        :return: è½¬æ¢åçš„å·¥å…·å‚æ•°å­—å…¸
        """
        return_dict: dict[str, Any] = {
            "type": tool_option_param.param_type.value,
            "description": tool_option_param.description,
        }
        if tool_option_param.enum_values:
            return_dict["enum"] = tool_option_param.enum_values
        return return_dict

    def _convert_tool_option_item(tool_option: ToolOption) -> dict[str, Any]:
        """
        è½¬æ¢å•ä¸ªå·¥å…·é¡¹æ ¼å¼
        :param tool_option: å·¥å…·é€‰é¡¹å¯¹è±¡
        :return: è½¬æ¢åçš„å·¥å…·é€‰é¡¹å­—å…¸
        """
        ret: dict[str, Any] = {
            "name": tool_option.name,
            "description": tool_option.description,
        }
        if tool_option.params:
            ret["parameters"] = {
                "type": "object",
                "properties": {param.name: _convert_tool_param(param) for param in tool_option.params},
                "required": [param.name for param in tool_option.params if param.required],
            }
        return ret

    return [
        {
            "type": "function",
            "function": _convert_tool_option_item(tool_option),
        }
        for tool_option in tool_options
    ]


def _process_delta(
    delta: ChoiceDelta,
    has_rc_attr_flag: bool,
    in_rc_flag: bool,
    rc_delta_buffer: io.StringIO,
    fc_delta_buffer: io.StringIO,
    tool_calls_buffer: list[tuple[str, str, io.StringIO]],
) -> bool:
    # æ¥æ”¶content
    if has_rc_attr_flag:
        # æœ‰ç‹¬ç«‹çš„æ¨ç†å†…å®¹å—ï¼Œåˆ™æ— éœ€è€ƒè™‘contentå†…å®¹çš„åˆ¤è¯»
        if hasattr(delta, "reasoning_content") and delta.reasoning_content:  # type: ignore
            # å¦‚æœæœ‰æ¨ç†å†…å®¹ï¼Œåˆ™å°†å…¶å†™å…¥æ¨ç†å†…å®¹ç¼“å†²åŒº
            assert isinstance(delta.reasoning_content, str)  # type: ignore
            rc_delta_buffer.write(delta.reasoning_content)  # type: ignore
        elif delta.content:
            # å¦‚æœæœ‰æ­£å¼å†…å®¹ï¼Œåˆ™å°†å…¶å†™å…¥æ­£å¼å†…å®¹ç¼“å†²åŒº
            fc_delta_buffer.write(delta.content)
    elif hasattr(delta, "content") and delta.content is not None:
        # æ²¡æœ‰ç‹¬ç«‹çš„æ¨ç†å†…å®¹å—ï¼Œä½†æœ‰æ­£å¼å†…å®¹
        if in_rc_flag:
            # å½“å‰åœ¨æ¨ç†å†…å®¹å—ä¸­
            if delta.content == "</think>":
                # å¦‚æœå½“å‰å†…å®¹æ˜¯</think>ï¼Œåˆ™å°†å…¶è§†ä¸ºæ¨ç†å†…å®¹çš„ç»“æŸæ ‡è®°ï¼Œé€€å‡ºæ¨ç†å†…å®¹å—
                in_rc_flag = False
            else:
                # å…¶ä»–æƒ…å†µè§†ä¸ºæ¨ç†å†…å®¹ï¼ŒåŠ å…¥æ¨ç†å†…å®¹ç¼“å†²åŒº
                rc_delta_buffer.write(delta.content)
        elif delta.content == "<think>" and not fc_delta_buffer.getvalue():
            # å¦‚æœå½“å‰å†…å®¹æ˜¯<think>ï¼Œä¸”æ­£å¼å†…å®¹ç¼“å†²åŒºä¸ºç©ºï¼Œè¯´æ˜<think>ä¸ºè¾“å‡ºçš„é¦–ä¸ªtoken
            # åˆ™å°†å…¶è§†ä¸ºæ¨ç†å†…å®¹çš„å¼€å§‹æ ‡è®°ï¼Œè¿›å…¥æ¨ç†å†…å®¹å—
            in_rc_flag = True
        else:
            # å…¶ä»–æƒ…å†µè§†ä¸ºæ­£å¼å†…å®¹ï¼ŒåŠ å…¥æ­£å¼å†…å®¹ç¼“å†²åŒº
            fc_delta_buffer.write(delta.content)
    # æ¥æ”¶tool_calls
    if hasattr(delta, "tool_calls") and delta.tool_calls:
        tool_call_delta = delta.tool_calls[0]

        if tool_call_delta.index >= len(tool_calls_buffer):
            # è°ƒç”¨ç´¢å¼•å·å¤§äºç­‰äºç¼“å†²åŒºé•¿åº¦ï¼Œè¯´æ˜æ˜¯æ–°çš„å·¥å…·è°ƒç”¨
            if tool_call_delta.id and tool_call_delta.function and tool_call_delta.function.name:
                tool_calls_buffer.append(
                    (
                        tool_call_delta.id,
                        tool_call_delta.function.name,
                        io.StringIO(),
                    )
                )
            else:
                logger.warning("å·¥å…·è°ƒç”¨ç´¢å¼•å·å¤§äºç­‰äºç¼“å†²åŒºé•¿åº¦ï¼Œä½†ç¼ºå°‘IDæˆ–å‡½æ•°ä¿¡æ¯ã€‚")

        if tool_call_delta.function and tool_call_delta.function.arguments:
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨å‚æ•°ï¼Œåˆ™æ·»åŠ åˆ°å¯¹åº”çš„å·¥å…·è°ƒç”¨çš„å‚æ•°ä¸²ç¼“å†²åŒºä¸­
            tool_calls_buffer[tool_call_delta.index][2].write(tool_call_delta.function.arguments)

    return in_rc_flag


def _build_stream_api_resp(
    _fc_delta_buffer: io.StringIO,
    _rc_delta_buffer: io.StringIO,
    _tool_calls_buffer: list[tuple[str, str, io.StringIO]],
) -> APIResponse:
    resp = APIResponse()

    if _rc_delta_buffer.tell() > 0:
        # å¦‚æœæ¨ç†å†…å®¹ç¼“å†²åŒºä¸ä¸ºç©ºï¼Œåˆ™å°†å…¶å†™å…¥APIResponseå¯¹è±¡
        resp.reasoning_content = _rc_delta_buffer.getvalue()
    _rc_delta_buffer.close()
    if _fc_delta_buffer.tell() > 0:
        # å¦‚æœæ­£å¼å†…å®¹ç¼“å†²åŒºä¸ä¸ºç©ºï¼Œåˆ™å°†å…¶å†™å…¥APIResponseå¯¹è±¡
        resp.content = _fc_delta_buffer.getvalue()
    _fc_delta_buffer.close()
    if _tool_calls_buffer:
        # å¦‚æœå·¥å…·è°ƒç”¨ç¼“å†²åŒºä¸ä¸ºç©ºï¼Œåˆ™å°†å…¶è§£æä¸ºToolCallå¯¹è±¡åˆ—è¡¨
        resp.tool_calls = []
        for call_id, function_name, arguments_buffer in _tool_calls_buffer:
            if arguments_buffer.tell() > 0:
                # å¦‚æœå‚æ•°ä¸²ç¼“å†²åŒºä¸ä¸ºç©ºï¼Œåˆ™è§£æä¸ºJSONå¯¹è±¡
                raw_arg_data = arguments_buffer.getvalue()
                arguments_buffer.close()
                try:
                    arguments = orjson.loads(repair_json(raw_arg_data))
                    if not isinstance(arguments, dict):
                        raise RespParseException(
                            None,
                            f"å“åº”è§£æå¤±è´¥ï¼Œå·¥å…·è°ƒç”¨å‚æ•°æ— æ³•è§£æä¸ºå­—å…¸ç±»å‹ã€‚å·¥å…·è°ƒç”¨å‚æ•°åŸå§‹å“åº”ï¼š\n{raw_arg_data}",
                        )
                except orjson.JSONDecodeError as e:
                    raise RespParseException(
                        None,
                        f"å“åº”è§£æå¤±è´¥ï¼Œæ— æ³•è§£æå·¥å…·è°ƒç”¨å‚æ•°ã€‚å·¥å…·è°ƒç”¨å‚æ•°åŸå§‹å“åº”ï¼š{raw_arg_data}",
                    ) from e
            else:
                arguments_buffer.close()
                arguments = None

            resp.tool_calls.append(ToolCall(call_id, function_name, arguments))

    return resp


async def _default_stream_response_handler(
    resp_stream: AsyncStream[ChatCompletionChunk],
    interrupt_flag: asyncio.Event | None,
) -> tuple[APIResponse, tuple[int, int, int] | None]:
    """
    æµå¼å“åº”å¤„ç†å‡½æ•° - å¤„ç†OpenAI APIçš„æµå¼å“åº”
    :param resp_stream: æµå¼å“åº”å¯¹è±¡
    :return: APIResponseå¯¹è±¡
    """

    _has_rc_attr_flag = False  # æ ‡è®°æ˜¯å¦æœ‰ç‹¬ç«‹çš„æ¨ç†å†…å®¹å—
    _in_rc_flag = False  # æ ‡è®°æ˜¯å¦åœ¨æ¨ç†å†…å®¹å—ä¸­
    _rc_delta_buffer = io.StringIO()  # æ¨ç†å†…å®¹ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨æ¥æ”¶åˆ°çš„æ¨ç†å†…å®¹
    _fc_delta_buffer = io.StringIO()  # æ­£å¼å†…å®¹ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨æ¥æ”¶åˆ°çš„æ­£å¼å†…å®¹
    _tool_calls_buffer: list[tuple[str, str, io.StringIO]] = []  # å·¥å…·è°ƒç”¨ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨æ¥æ”¶åˆ°çš„å·¥å…·è°ƒç”¨
    _usage_record = None  # ä½¿ç”¨æƒ…å†µè®°å½•

    def _insure_buffer_closed():
        # ç¡®ä¿ç¼“å†²åŒºè¢«å…³é—­
        if _rc_delta_buffer and not _rc_delta_buffer.closed:
            _rc_delta_buffer.close()
        if _fc_delta_buffer and not _fc_delta_buffer.closed:
            _fc_delta_buffer.close()
        for _, _, buffer in _tool_calls_buffer:
            if buffer and not buffer.closed:
                buffer.close()

    async for event in resp_stream:
        if interrupt_flag and interrupt_flag.is_set():
            # å¦‚æœä¸­æ–­é‡è¢«è®¾ç½®ï¼Œåˆ™æŠ›å‡ºReqAbortException
            _insure_buffer_closed()
            raise ReqAbortException("è¯·æ±‚è¢«å¤–éƒ¨ä¿¡å·ä¸­æ–­")

        delta = event.choices[0].delta  # è·å–å½“å‰å—çš„deltaå†…å®¹

        if hasattr(delta, "reasoning_content") and delta.reasoning_content:  # type: ignore
            # æ ‡è®°ï¼šæœ‰ç‹¬ç«‹çš„æ¨ç†å†…å®¹å—
            _has_rc_attr_flag = True

        _in_rc_flag = _process_delta(
            delta,
            _has_rc_attr_flag,
            _in_rc_flag,
            _rc_delta_buffer,
            _fc_delta_buffer,
            _tool_calls_buffer,
        )

        if event.usage:
            # å¦‚æœæœ‰ä½¿ç”¨æƒ…å†µï¼Œåˆ™å°†å…¶å­˜å‚¨åœ¨APIResponseå¯¹è±¡ä¸­
            _usage_record = (
                getattr(event.usage, "prompt_tokens", 0) or 0,
                getattr(event.usage, "completion_tokens", 0) or 0,
                getattr(event.usage, "total_tokens", 0) or 0,
            )

    try:
        return _build_stream_api_resp(
            _fc_delta_buffer,
            _rc_delta_buffer,
            _tool_calls_buffer,
        ), _usage_record
    except Exception:
        # ç¡®ä¿ç¼“å†²åŒºè¢«å…³é—­
        _insure_buffer_closed()
        raise


pattern = re.compile(
    r"<think>(?P<think>.*?)</think>(?P<content>.*)|<think>(?P<think_unclosed>.*)|(?P<content_only>.+)",
    re.DOTALL,
)
"""ç”¨äºè§£ææ¨ç†å†…å®¹çš„æ­£åˆ™è¡¨è¾¾å¼"""


def _default_normal_response_parser(
    resp: ChatCompletion,
) -> tuple[APIResponse, tuple[int, int, int] | None]:
    """
    è§£æå¯¹è¯è¡¥å…¨å“åº” - å°†OpenAI APIå“åº”è§£æä¸ºAPIResponseå¯¹è±¡
    :param resp: å“åº”å¯¹è±¡
    :return: APIResponseå¯¹è±¡
    """
    api_response = APIResponse()

    if not hasattr(resp, "choices") or len(resp.choices) == 0:
        raise RespParseException(resp, "å“åº”è§£æå¤±è´¥ï¼Œç¼ºå¤±choiceså­—æ®µ")
    message_part = resp.choices[0].message

    if hasattr(message_part, "reasoning_content") and message_part.reasoning_content:  # type: ignore
        # æœ‰æœ‰æ•ˆçš„æ¨ç†å­—æ®µ
        api_response.content = message_part.content
        api_response.reasoning_content = message_part.reasoning_content  # type: ignore
    elif message_part.content:
        # æå–æ¨ç†å’Œå†…å®¹
        match = pattern.match(message_part.content)
        if not match:
            raise RespParseException(resp, "å“åº”è§£æå¤±è´¥ï¼Œæ— æ³•æ•è·æ¨ç†å†…å®¹å’Œè¾“å‡ºå†…å®¹")
        if match.group("think") is not None:
            result = match.group("think").strip(), match.group("content").strip()
        elif match.group("think_unclosed") is not None:
            result = match.group("think_unclosed").strip(), None
        else:
            result = None, match.group("content_only").strip()
        api_response.reasoning_content, api_response.content = result

    # æå–å·¥å…·è°ƒç”¨
    if message_part.tool_calls:
        api_response.tool_calls = []
        for call in message_part.tool_calls:
            try:
                arguments = orjson.loads(repair_json(call.function.arguments)) # type: ignore
                if not isinstance(arguments, dict):
                    raise RespParseException(resp, "å“åº”è§£æå¤±è´¥ï¼Œå·¥å…·è°ƒç”¨å‚æ•°æ— æ³•è§£æä¸ºå­—å…¸ç±»å‹")
                api_response.tool_calls.append(ToolCall(call.id, call.function.name, arguments)) # type: ignore
            except orjson.JSONDecodeError as e:
                raise RespParseException(resp, "å“åº”è§£æå¤±è´¥ï¼Œæ— æ³•è§£æå·¥å…·è°ƒç”¨å‚æ•°") from e

    # æå–Usageä¿¡æ¯
    if resp.usage:
        _usage_record = (
            getattr(resp.usage, "prompt_tokens", 0) or 0,
            getattr(resp.usage, "completion_tokens", 0) or 0,
            getattr(resp.usage, "total_tokens", 0) or 0,
        )
    else:
        _usage_record = None

    # å°†åŸå§‹å“åº”å­˜å‚¨åœ¨åŸå§‹æ•°æ®ä¸­
    api_response.raw_data = resp

    return api_response, _usage_record


@client_registry.register_client_class("openai")
class OpenaiClient(BaseClient):
    # ç±»çº§åˆ«çš„å…¨å±€ç¼“å­˜ï¼šæ‰€æœ‰ OpenaiClient å®ä¾‹å…±äº«
    _global_client_cache: ClassVar[dict[tuple[int, int | None], AsyncOpenAI]] = {}
    """å…¨å±€ AsyncOpenAI å®¢æˆ·ç«¯ç¼“å­˜ï¼š(config_hash, loop_id) -> AsyncOpenAI å®ä¾‹"""

    def __init__(self, api_provider: APIProvider):
        super().__init__(api_provider)
        self._config_hash = self._calculate_config_hash()
        """å½“å‰ provider çš„é…ç½®å“ˆå¸Œå€¼"""

    def _calculate_config_hash(self) -> int:
        """è®¡ç®—å½“å‰é…ç½®çš„å“ˆå¸Œå€¼"""
        config_tuple = (
            self.api_provider.base_url,
            self.api_provider.get_api_key(),
            self.api_provider.timeout,
        )
        return hash(config_tuple)

    @staticmethod
    def _get_current_loop_id() -> int | None:
        """è·å–å½“å‰äº‹ä»¶å¾ªç¯çš„ID"""
        try:
            loop = asyncio.get_running_loop()
            return id(loop)
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
            return None

    def _create_client(self) -> AsyncOpenAI:
        """
        è·å–æˆ–åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå…¨å±€ç¼“å­˜ï¼Œæ”¯æŒäº‹ä»¶å¾ªç¯æ£€æµ‹ï¼‰

        å¤šä¸ª OpenaiClient å®ä¾‹å¦‚æœé…ç½®ç›¸åŒï¼ˆbase_url + api_key + timeoutï¼‰ä¸”åœ¨åŒä¸€äº‹ä»¶å¾ªç¯ä¸­ï¼Œ
        å°†å…±äº«åŒä¸€ä¸ª AsyncOpenAI å®¢æˆ·ç«¯å®ä¾‹ï¼Œæœ€å¤§åŒ–è¿æ¥æ± å¤ç”¨ã€‚
        å½“äº‹ä»¶å¾ªç¯å˜åŒ–æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹ã€‚
        """
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ID
        current_loop_id = self._get_current_loop_id()
        cache_key = (self._config_hash, current_loop_id)

        # æ¸…ç†å…¶ä»–äº‹ä»¶å¾ªç¯çš„è¿‡æœŸç¼“å­˜
        keys_to_remove = [
            key for key in self._global_client_cache.keys()
            if key[0] == self._config_hash and key[1] != current_loop_id
        ]
        for key in keys_to_remove:
            logger.debug(f"æ¸…ç†è¿‡æœŸçš„ AsyncOpenAI å®¢æˆ·ç«¯ç¼“å­˜ (loop_id={key[1]})")
            del self._global_client_cache[key]

        # æ£€æŸ¥å½“å‰äº‹ä»¶å¾ªç¯çš„ç¼“å­˜
        if cache_key in self._global_client_cache:
            return self._global_client_cache[cache_key]

        # åˆ›å»ºæ–°çš„ AsyncOpenAI å®ä¾‹
        logger.debug(
            f"åˆ›å»ºæ–°çš„ AsyncOpenAI å®¢æˆ·ç«¯å®ä¾‹ (base_url={self.api_provider.base_url}, config_hash={self._config_hash}, loop_id={current_loop_id})"
        )

        # ğŸ”§ ä¼˜åŒ–ï¼šå¢åŠ è¿æ¥æ± é™åˆ¶ï¼Œæ”¯æŒé«˜å¹¶å‘embeddingè¯·æ±‚
        # é»˜è®¤httpxé™åˆ¶ä¸º100ï¼Œå¯¹äºé«˜é¢‘embeddingåœºæ™¯ä¸å¤Ÿç”¨
        import httpx

        limits = httpx.Limits(
            max_keepalive_connections=200,  # ä¿æŒæ´»è·ƒè¿æ¥æ•°ï¼ˆåŸ100ï¼‰
            max_connections=300,  # æœ€å¤§æ€»è¿æ¥æ•°ï¼ˆåŸ100ï¼‰
            keepalive_expiry=30.0,  # è¿æ¥ä¿æ´»æ—¶é—´
        )

        client = AsyncOpenAI(
            base_url=self.api_provider.base_url,
            api_key=self.api_provider.get_api_key(),
            max_retries=0,
            timeout=self.api_provider.timeout,
            http_client=httpx.AsyncClient(limits=limits),  # ğŸ”§ è‡ªå®šä¹‰è¿æ¥æ± é…ç½®
        )

        # å­˜å…¥å…¨å±€ç¼“å­˜ï¼ˆå¸¦äº‹ä»¶å¾ªç¯IDï¼‰
        self._global_client_cache[cache_key] = client

        return client

    @classmethod
    def get_cache_stats(cls) -> dict:
        """è·å–å…¨å±€ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cached_openai_clients": len(cls._global_client_cache),
            "cache_keys": [
                {"config_hash": k[0], "loop_id": k[1]}
                for k in cls._global_client_cache.keys()
            ],
        }

    async def get_response(
        self,
        model_info: ModelInfo,
        message_list: list[Message],
        tool_options: list[ToolOption] | None = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        response_format: RespFormat | None = None,
        stream_response_handler: Callable[
            [AsyncStream[ChatCompletionChunk], asyncio.Event | None],
            Coroutine[Any, Any, tuple[APIResponse, tuple[int, int, int] | None]],
        ]
        | None = None,
        async_response_parser: Callable[[ChatCompletion], tuple[APIResponse, tuple[int, int, int] | None]]
        | None = None,
        interrupt_flag: asyncio.Event | None = None,
        extra_params: dict[str, Any] | None = None,
    ) -> APIResponse:
        """
        è·å–å¯¹è¯å“åº”
        Args:
            model_info: æ¨¡å‹ä¿¡æ¯
            message_list: å¯¹è¯ä½“
            tool_options: å·¥å…·é€‰é¡¹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºNoneï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º1024ï¼‰
            temperature: æ¸©åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º0.7ï¼‰
            response_format: å“åº”æ ¼å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º NotGiven ï¼‰
            stream_response_handler: æµå¼å“åº”å¤„ç†å‡½æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºdefault_stream_response_handlerï¼‰
            async_response_parser: å“åº”è§£æå‡½æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºdefault_response_parserï¼‰
            interrupt_flag: ä¸­æ–­ä¿¡å·é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºNoneï¼‰
        Returns:
            (å“åº”æ–‡æœ¬, æ¨ç†æ–‡æœ¬, å·¥å…·è°ƒç”¨, å…¶ä»–æ•°æ®)
        """
        if stream_response_handler is None:
            stream_response_handler = _default_stream_response_handler

        if async_response_parser is None:
            async_response_parser = _default_normal_response_parser

        # å°†messagesæ„é€ ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
        messages: Iterable[ChatCompletionMessageParam] = _convert_messages(message_list)
        # å°†tool_optionsè½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
        tools: Iterable[ChatCompletionToolParam] = _convert_tool_options(tool_options) if tool_options else NOT_GIVEN  # type: ignore

        client = self._create_client()
        try:
            if model_info.force_stream_mode:
                req_task = asyncio.create_task(
                    client.chat.completions.create(
                        model=model_info.model_identifier,
                        messages=messages,
                        tools=tools,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=True,
                        response_format=NOT_GIVEN,
                        extra_body=extra_params,
                    )
                )
                while not req_task.done():
                    if interrupt_flag and interrupt_flag.is_set():
                        # å¦‚æœä¸­æ–­é‡å­˜åœ¨ä¸”è¢«è®¾ç½®ï¼Œåˆ™å–æ¶ˆä»»åŠ¡å¹¶æŠ›å‡ºå¼‚å¸¸
                        req_task.cancel()
                        raise ReqAbortException("è¯·æ±‚è¢«å¤–éƒ¨ä¿¡å·ä¸­æ–­")
                    await asyncio.sleep(0.1)  # ç­‰å¾…0.1ç§’åå†æ¬¡æ£€æŸ¥ä»»åŠ¡&ä¸­æ–­ä¿¡å·é‡çŠ¶æ€

                resp, usage_record = await stream_response_handler(req_task.result(), interrupt_flag)
            else:
                # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
                # start_time = time.time()
                req_task = asyncio.create_task(
                    client.chat.completions.create(
                        model=model_info.model_identifier,
                        messages=messages,
                        tools=tools,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=False,
                        response_format=NOT_GIVEN,
                        extra_body=extra_params,
                    )
                )
                while not req_task.done():
                    if interrupt_flag and interrupt_flag.is_set():
                        # å¦‚æœä¸­æ–­é‡å­˜åœ¨ä¸”è¢«è®¾ç½®ï¼Œåˆ™å–æ¶ˆä»»åŠ¡å¹¶æŠ›å‡ºå¼‚å¸¸
                        req_task.cancel()
                        raise ReqAbortException("è¯·æ±‚è¢«å¤–éƒ¨ä¿¡å·ä¸­æ–­")
                    await asyncio.sleep(0.1)  # ç­‰å¾…0.5ç§’åå†æ¬¡æ£€æŸ¥ä»»åŠ¡&ä¸­æ–­ä¿¡å·é‡çŠ¶æ€

                # logger.info(f"OpenAIè¯·æ±‚æ—¶é—´: {model_info.model_identifier}  {time.time() - start_time} \n{messages}")

                resp, usage_record = async_response_parser(req_task.result())
        except APIConnectionError as e:
            # é‡å°è£…APIConnectionErrorä¸ºNetworkConnectionError
            raise NetworkConnectionError() from e
        except APIStatusError as e:
            # é‡å°è£…APIErrorä¸ºRespNotOkException
            raise RespNotOkException(e.status_code, e.message) from e

        if usage_record:
            resp.usage = UsageRecord(
                model_name=model_info.name,
                provider_name=model_info.api_provider,
                prompt_tokens=usage_record[0],
                completion_tokens=usage_record[1],
                total_tokens=usage_record[2],
            )

        return resp

    async def get_embedding(
        self,
        model_info: ModelInfo,
        embedding_input: str | list[str],
        extra_params: dict[str, Any] | None = None,
    ) -> APIResponse:
        """
        è·å–æ–‡æœ¬åµŒå…¥
        :param model_info: æ¨¡å‹ä¿¡æ¯
        :param embedding_input: åµŒå…¥è¾“å…¥æ–‡æœ¬
        :return: åµŒå…¥å“åº”
        """
        client = self._create_client()
        is_batch_request = isinstance(embedding_input, list)
        try:
            raw_response = await client.embeddings.create(
                model=model_info.model_identifier,
                input=embedding_input,
                extra_body=extra_params,
            )
        except APIConnectionError as e:
            # æ·»åŠ è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
            logger.error(f"OpenAI APIè¿æ¥é”™è¯¯ï¼ˆåµŒå…¥æ¨¡å‹ï¼‰: {e!s}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            if hasattr(e, "__cause__") and e.__cause__:
                logger.error(f"åº•å±‚é”™è¯¯: {e.__cause__!s}")
            raise NetworkConnectionError() from e
        except APIStatusError as e:
            # é‡å°è£…APIErrorä¸ºRespNotOkException
            raise RespNotOkException(e.status_code) from e
        except Exception as e:
            # æ·»åŠ é€šç”¨å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
            logger.error(f"è·å–åµŒå…¥æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e!s}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            raise

        response = APIResponse()

        # è§£æåµŒå…¥å“åº”
        if len(raw_response.data) > 0:
            embeddings = [item.embedding for item in raw_response.data]
            response.embedding = embeddings if is_batch_request else embeddings[0]
        else:
            raise RespParseException(
                raw_response,
                "å“åº”è§£æå¤±è´¥ï¼Œç¼ºå¤±åµŒå…¥æ•°æ®ã€‚",
            )

        # è§£æä½¿ç”¨æƒ…å†µ
        if hasattr(raw_response, "usage"):
            response.usage = UsageRecord(
                model_name=model_info.name,
                provider_name=model_info.api_provider,
                prompt_tokens=raw_response.usage.prompt_tokens or 0,
                completion_tokens=getattr(raw_response.usage, "completion_tokens", 0) or 0,
                total_tokens=raw_response.usage.total_tokens or 0,
            )

        return response

    async def get_audio_transcriptions(
        self,
        model_info: ModelInfo,
        audio_base64: str,
        extra_params: dict[str, Any] | None = None,
    ) -> APIResponse:
        """
        è·å–éŸ³é¢‘è½¬å½•
        :param model_info: æ¨¡å‹ä¿¡æ¯
        :param audio_base64: base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
        :extra_params: é™„åŠ çš„è¯·æ±‚å‚æ•°
        :return: éŸ³é¢‘è½¬å½•å“åº”
        """
        client = self._create_client()
        try:
            raw_response = await client.audio.transcriptions.create(
                model=model_info.model_identifier,
                file=("audio.wav", io.BytesIO(base64.b64decode(audio_base64))),
                extra_body=extra_params,
            )
        except APIConnectionError as e:
            raise NetworkConnectionError() from e
        except APIStatusError as e:
            # é‡å°è£…APIErrorä¸ºRespNotOkException
            raise RespNotOkException(e.status_code) from e
        response = APIResponse()
        # è§£æè½¬å½•å“åº”
        if hasattr(raw_response, "text"):
            response.content = raw_response.text
        else:
            raise RespParseException(
                raw_response,
                "å“åº”è§£æå¤±è´¥ï¼Œç¼ºå¤±è½¬å½•æ–‡æœ¬ã€‚",
            )
        return response

    def get_support_image_formats(self) -> list[str]:
        """
        è·å–æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        :return: æ”¯æŒçš„å›¾ç‰‡æ ¼å¼åˆ—è¡¨
        """
        return ["jpg", "jpeg", "png", "webp", "gif"]
